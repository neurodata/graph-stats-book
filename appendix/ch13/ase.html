

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Estimating Parameters for the RDPG &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'appendix/ch13/ase';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../coverpage.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/terminology.html">Terminology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch1/ch1.html">1. The Network Machine Learning Landscape</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">1.1. What is network machine learning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/why-study-networks.html">1.2. Why do we study networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">1.3. Types of Network Machine Learning Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">1.4. Examples of applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">1.5. Challenges of Network Machine Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch2/ch2.html">2. End-to-end Biology Network Machine Learning Project</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/big-picture.html">2.1. Look at the big picture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/get-the-data.html">2.2. Get the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">2.3. Prepare the Data for Network Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/select-and-train.html">2.4. Select and Train a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/fine-tune.html">2.5. Fine-Tune your Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">2.6. Discover and Visualize the Data to Gain Insights</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Representations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch4/ch4.html">3. Properties of Networks as a Statistical Object</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/matrix-representations.html">3.1. Matrix Representations Of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/properties-of-networks.html">3.2. Properties of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/network-representations.html">3.3. Representations of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/regularization.html">3.4. Regularization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch5/ch5.html">4. Why Use Statistical Models?</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">4.1. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">4.2. Stochastic Block Models (SBM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">4.3. Random Dot Product Graphs (RDPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SIEM.html">4.5. Structured Independent Edge Model (SIEM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/multi-network-models.html">4.6. Multiple Network Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/models-with-covariates.html">4.7. Network Models with Network Covariates</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch6/ch6.html">5. Learning Network Representations</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">5.1. Estimating Parameters in Network Models via MLE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/why-embed-networks.html">5.2. Why embed networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/spectral-embedding.html">5.3. Spectral embedding methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">5.4. Multiple-Network Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">5.5. Joint Representation Learning</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch7/ch7.html">6. Applications When You Have One Network</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/community-detection.html">6.1. Community Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/testing-differences.html">6.2. Testing for Differences between Groups of Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/model-selection.html">6.3. Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/vertex-nomination.html">6.4. Single-Network Vertex Nomination</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/out-of-sample.html">6.5. Out-of-sample Embedding</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch8/ch8.html">7. Applications for Two Networks</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html">7.1. Latent Two-Sample Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/significant-communities.html">7.2. Two-sample hypothesis testing in SBMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html">7.3. Graph Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/multiple-vertex-nomination.html">7.4. Vertex Nomination For Two Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch9/ch9.html">8. Applications for Many Networks</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/anomaly-detection.html">8.1. Anomaly Detection For Timeseries of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-edges.html">8.2. Testing for Significant Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-vertices.html">8.3. Testing for Significant Vertices</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Next Steps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../next/ch10/ch10.html">9. Where do we go from here?</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/random-walk-diffusion-methods.html">9.1. Random walk and diffusion-based methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/gnn.html">9.2. Graph Neural Networks</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch11/ch11.html">10. Representations (Extended)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch11/alt-reps.html">10.1. Alternative Network Representations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch12/ch12.html">11. Network Model Theory</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch12/background.html">11.2. Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/foundation.html">11.3. Foundation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/ers.html">11.4. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/sbms.html">11.5. Stochastic Block Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/rdpgs.html">11.6. RDPGs and more general network models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ch13.html">12. Learning Representations Theory</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="mle-theory.html">12.1. Maximum Likelihood Estimate Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="spectral-theory.html">12.2. Spectral Method Theory</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch14/ch14.html">13. Applications (Extended)</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch14/hypothesis.html">13.1. Hypothesis Testing with coin flips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch14/unsupervised.html">13.2. Unsupervised learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch14/bayes.html">13.3. Bayes Plugin Classifier</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">Graspologic Documentation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/appendix/ch13/ase.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fappendix/ch13/ase.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/appendix/ch13/ase.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Estimating Parameters for the RDPG</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-nodes-from-the-same-community-similar">How are nodes from the same community similar?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-low-rank-property-and-the-probability-matrix">The low-rank property and the probability matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-best-representation-of-the-probability-matrix">The best representation of the probability matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-square-root-matrix">The Square Root Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-latent-positions-are-distinct-for-each-community">The latent positions are distinct for each community</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-you-do">But wait: we don’t have the probability matrix! What do you do?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-use-ase-and-when-should-you-use-lse">When should you use ASE and when should you use LSE?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="estimating-parameters-for-the-rdpg">
<span id="app-ch13-ase"></span><h1>Estimating Parameters for the RDPG<a class="headerlink" href="#estimating-parameters-for-the-rdpg" title="Permalink to this heading">#</a></h1>
<p>In this section, we’ll cover a special case of the Adajacency Spectral Embedding, like we learned back in <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-numref">Section 5.3</span></a>. We’ll double back on some concepts, such as the singular value decomposition and matrix rank, so that we can explain the importance of these concepts as they relate directly to the probability matrix of an RDPG. We’ll learn about this in the context of an example which is an SBM, but the logic applies to RDPGs, too.</p>
<p>For this example, we’ll work with the school example we’ve seen previously. The nodes are 100 students in total, from one of two schools. Here, the first 50 students are from the first school, and the second 50 students are from the second school. The probability of two students who both go to the first school being friends is <span class="math notranslate nohighlight">\(0.5\)</span>, and the probability of two students who both go to school two being friends will also be <span class="math notranslate nohighlight">\(0.5\)</span>. If two students go to different schools, their probability of being friends will be <span class="math notranslate nohighlight">\(0.2\)</span>. The statistical model has parameters which look like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">graspologic</span> <span class="k">as</span> <span class="nn">gp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">ns1</span> <span class="o">=</span> <span class="mi">50</span><span class="p">;</span> <span class="n">ns2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="n">ns1</span><span class="p">,</span> <span class="n">ns2</span><span class="p">]</span>

<span class="c1"># zvec is a column vector of 50 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 100 students are from</span>
<span class="n">zvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;S1&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;S2&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns2</span><span class="p">)])</span>

<span class="c1"># the block matrix</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>

<span class="c1"># the probability matrix</span>
<span class="n">zvec_ohe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns2</span><span class="p">)])</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">zvec_ohe</span> <span class="o">@</span> <span class="n">B</span> <span class="o">@</span> <span class="n">zvec_ohe</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span><span class="p">,</span> <span class="n">heatmap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix $B$&quot;</span><span class="p">,</span> 
                <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;S1&quot;</span><span class="p">,</span> <span class="s2">&quot;S2&quot;</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;S1&quot;</span><span class="p">,</span> <span class="s2">&quot;S2&quot;</span><span class="p">],</span>
                <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">])</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix $P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/6c4e77d5e0171882c3bd5cff64469b55d07d38c1fc1d4b1ce3d511a444f9d3cf.png" src="../../_images/6c4e77d5e0171882c3bd5cff64469b55d07d38c1fc1d4b1ce3d511a444f9d3cf.png" />
</div>
</div>
<p>The procedure for generating the probability matrix from the block probability matrix that we used above is a linear algebra “cheat”, but in reality, all that’s happening is that we are comparing whether two nodes are in the first or the second school, and then taking the appropriate entry from the block matrix accordingly. The operation is exactly the same as we had in the section on <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html#ch5-rdpg"><span class="std std-numref">Section 4.3</span></a>. For two nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, the probability they are connected is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p_{ji} = p_{ij}, p_{ij} = \begin{cases}
        b_{11} &amp; z_i = 1, z_j = 1 \\
        b_{12} &amp; z_i = 1, z_j = 2 \\
        b_{22} &amp; z_i = 2, z_j = 2
    \end{cases}
\end{align*}\]</div>
<p>Next, we will use this probability matrix and the corresponding community assignment vector to generate a sample of the stochastic block model you saw above. This is our “real network”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">simulations</span><span class="o">.</span><span class="n">sbm</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see the adjacency matrix and the probability matrix below. Notice that there are two distrinct blocks in the adjacency matrix, which are shared with the probability matrix: in its upper-left, you can see the edges between the first 50 nodes (the individuals in the first school), and in the bottom right, you can see the edges between the second 50 nodes (the individuals in the second school).</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix $P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Sample of SBM Random Network, $A$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/7911cc5ad857ffbbeeaa4f373850d77f97b252a55f34d5d7a423fa672925c0bd.png" src="../../_images/7911cc5ad857ffbbeeaa4f373850d77f97b252a55f34d5d7a423fa672925c0bd.png" />
</div>
</div>
<p>As you can see, both the probability matrix <span class="math notranslate nohighlight">\(P\)</span> and the sample of the random network <span class="math notranslate nohighlight">\(A\)</span> both have a notable “structure”, in that it’s clear that there are more nodes when both individuals are in the first school (the top left and bottom right “squares” of the adjacency matrix and probability matrix have more entries and are darker, respectively) and fewer nodes when both individuals are in different schools (the top right and bottom left “squares” of the adjacency matrix have fewer entries and are lighter, respectively).</p>
<p>Next, we have the key conceptual leap we need to take: remember that if a network is an SBM, there is also an underlying RDPG that <em>also</em> describes that network. When we learned about this in <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html#ch5-rdpg"><span class="std std-numref">Section 4.3</span></a>, we learned that this was because the RDPG is a more broad statistical model that was more general than the SBM. So, to learn about the SBM which underlies <span class="math notranslate nohighlight">\(A\)</span>, we could also learn about an RDPG which underlies <span class="math notranslate nohighlight">\(A\)</span>, too. This will prove critical in later applications sections, such as <a class="reference internal" href="../../applications/ch7/community-detection.html#ch7-comm-detect"><span class="std std-numref">Section 6.1</span></a>, when we try to decipher which nodes are in which communities of the network, without having them handed to you in this nice organized way.</p>
<section id="how-are-nodes-from-the-same-community-similar">
<h2>How are nodes from the same community similar?<a class="headerlink" href="#how-are-nodes-from-the-same-community-similar" title="Permalink to this heading">#</a></h2>
<p>As it turns out, there’s a really important property that is shared by nodes in an SBM random network. Remember that the probability matrix <span class="math notranslate nohighlight">\(P\)</span> gives the probabilities <span class="math notranslate nohighlight">\(p_{ji}\)</span> of each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of being connected. We’re going to introduce a new piece of notation here, called the <em>vector of probabilities</em> for a single node. The <strong>vector of probabilities</strong> for a node <span class="math notranslate nohighlight">\(i\)</span> is the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec p_i &amp;= \begin{bmatrix}
        p_{i1} \\
        \vdots \\
        p_{in}
    \end{bmatrix}
\end{align*}\]</div>
<p>In words, it is basically just the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of the probability matrix <span class="math notranslate nohighlight">\(P\)</span>. Now, what happens when you look at the probability vectors for nodes which are in the same, versus different communities? Here, what you will do is take the probability vectors for students <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span>, who both attend school one, and compare them to the probability vectors for students <span class="math notranslate nohighlight">\(51\)</span> and <span class="math notranslate nohighlight">\(52\)</span>, who both attend school two:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># grab the probability vectors for the students you outlined above</span>
<span class="n">Psubset</span> <span class="o">=</span> <span class="n">P</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">],:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psubset</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Student 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 51&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 52&quot;</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Probability that student $i$ is friends with student $j$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Student $i$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Student $j$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d494b680ca1ca0c6bcee44d309e385f061ff76cfb966ced8e8007e7d3e55d094.png" src="../../_images/d494b680ca1ca0c6bcee44d309e385f061ff76cfb966ced8e8007e7d3e55d094.png" />
</div>
</div>
<p>The probability vectors for students <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\(51\)</span>, and <span class="math notranslate nohighlight">\(52\)</span> are shown as the rows of the above heatmap. What do you notice about the probability vectors for students <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span> in comparison to students <span class="math notranslate nohighlight">\(51\)</span> and <span class="math notranslate nohighlight">\(52\)</span>? As it turns out, they are exactly the same, and the probability vectors are identical! In general, what this means is that, for an SBM, <em>all</em> of the nodes in a single community have the <em>exact same</em> probability vector! If you are interested in some more details as to why this is the case for an SBM, the reason is that the probability vector for a given node is <em>fully specified</em> by just knowing the block matrix and the community assignment vector for the nodes in the network. What this means is that there is nothing special about one node versus another node in the same community, in the probability sense.</p>
</section>
<section id="the-low-rank-property-and-the-probability-matrix">
<h2>The low-rank property and the probability matrix<a class="headerlink" href="#the-low-rank-property-and-the-probability-matrix" title="Permalink to this heading">#</a></h2>
<p>Now, what does this mean for <em>us</em>? What this means is that the probability matrix has a special property, called the <em>low-rank</em> property. We already studied matrix rank, which if you recall, described how many unique row or column vectors you would need to define <em>all</em> of the other row and column vectors of a matrix.</p>
<p>If an SBM is has <span class="math notranslate nohighlight">\(K\)</span> communities, what does this mean about its probability matrix? Well, its probability matrix is <em>also</em> exactly rank-<span class="math notranslate nohighlight">\(K\)</span>! From what you learned about the Laplacian Spectral Embedding, you might reasonably expect that a very similar procedure might produce a similarly interesting result for us, so let’s get started.</p>
<p>We’ll take the svd of the probability matrix of <em>the random network itself</em> this time (instead of the Laplacian of a sample of the random network). As we remember, the <code class="docutils literal notranslate"><span class="pre">svd</span></code> from <code class="xref std std-numref docutils literal notranslate"><span class="pre">app:lse:svd</span></code> created three matrices, <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span>, for us, with the property that <span class="math notranslate nohighlight">\(U\)</span> had <span class="math notranslate nohighlight">\(n\)</span> columns called the left singular vectors, <span class="math notranslate nohighlight">\(\Sigma\)</span> was a diagonal matrix whose entries were the singular values in non-increasing order (each singular value can be at most the previous), and <span class="math notranslate nohighlight">\(V\)</span> also had <span class="math notranslate nohighlight">\(n\)</span> columns called the right singular vectors.</p>
<p>Remember that when you take an svd, you start with looking at the scree plot, which was a plot of the singular values ordered by their index:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># take the singular value decomposition</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
<span class="c1"># U is the matrix whose columns are the left singular vectors</span>
<span class="c1"># s is the vector whose entries are the singular values</span>
<span class="c1"># Vt is the matrix whose rows are the right singular vectors</span>
<span class="c1"># and whose tranpose has columns which are the right singular vectors</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="k">def</span> <span class="nf">plot_scree</span><span class="p">(</span><span class="n">svs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">sv_dat</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">:</span> <span class="n">svs</span><span class="p">,</span> <span class="s2">&quot;Dimension&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)})</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">sv_dat</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Dimension&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dimension&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">plot_scree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $P$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/0509b58fb573494beb6fc1c43b06a84fa6731101d9ed8e7b4fe9c50c1409f15e.png" src="../../_images/0509b58fb573494beb6fc1c43b06a84fa6731101d9ed8e7b4fe9c50c1409f15e.png" />
</div>
</div>
<p>What you see in the scree plot is that it just so happens that for the probability matrix for a <span class="math notranslate nohighlight">\(K\)</span>-community SBM, the probability matrix had <span class="math notranslate nohighlight">\(K\)</span> non-zero singular values! This fact will be important to you later on, so we will highlight this again later.</p>
<p>Next, you see that <span class="math notranslate nohighlight">\(P = U\Sigma V^\top\)</span>, and that the expression you learned previously, is still true: that <span class="math notranslate nohighlight">\(P = \sum_{i = 1}^n \vec u_i \vec v_i^\top\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># matrix product form of expression for P</span>
<span class="n">Psvd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">@</span> <span class="n">Vt</span>

<span class="c1"># sum expression for P</span>
<span class="c1"># remember that U[:,i] is the ith column of U (the ith left singular vector)</span>
<span class="c1"># and Vt[i,:] is the ith column of V (the ith right singular vector)</span>
<span class="c1"># and the ith row of Vt (transpose)</span>
<span class="n">Psum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">U</span><span class="p">[:,[</span><span class="n">i</span><span class="p">]])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Vt</span><span class="p">[[</span><span class="n">i</span><span class="p">],:])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span> <span class="o">+</span> <span class="n">ns2</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
 
<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Psvd</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U</span><span class="se">\\</span><span class="s2">Sigma V^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Psum</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sum_{i = 1}^n \sigma_i u_i v_i^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/9dee398d2867e0ca8088c704abeb3f1373675be9cdba432b1f3474f722983f11.png" src="../../_images/9dee398d2867e0ca8088c704abeb3f1373675be9cdba432b1f3474f722983f11.png" />
</div>
</div>
<section id="the-best-representation-of-the-probability-matrix">
<h3>The best representation of the probability matrix<a class="headerlink" href="#the-best-representation-of-the-probability-matrix" title="Permalink to this heading">#</a></h3>
<p>As it turns out, the relationships and intuition you learned about the Laplacian Spectral Embedding work here, too. Since the probability matrix <span class="math notranslate nohighlight">\(P\)</span> is square, symmetric, and has all positive real entries, it is <em>also</em> going to have <span class="math notranslate nohighlight">\(K\)</span> positive, real singular values, and the rest will all be zero. Further, the first <span class="math notranslate nohighlight">\(K\)</span> left and right singular vectors will all be the same! We’ll borrow the same notation you used with the section on the Laplacian Spectral Embedding, remembering that this means that you have two matrices, <span class="math notranslate nohighlight">\(U_K\)</span> and <span class="math notranslate nohighlight">\(\Sigma_K\)</span>, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    U_K &amp;= \begin{bmatrix}
        \uparrow &amp; &amp; \uparrow \\
        \vec u_1 &amp; ... &amp; \vec u_K \\
        \downarrow &amp; &amp; \downarrow
    \end{bmatrix},\;\;\;\Sigma_K &amp;= \begin{bmatrix}
        \sigma_1 &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \sigma_2 &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sigma_K
    \end{bmatrix}
\end{align*}\]</div>
<p>and you learned that <span class="math notranslate nohighlight">\(P = U_K\Sigma_K U_K^\top\)</span>, or stated another way:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P &amp;= \sum_{i = 1}^K \sigma_i \vec u_i \vec u_i^\top
\end{align*}\]</div>
<p>We can see this using the probability matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">UK</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">];</span> <span class="n">SK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">])</span>

<span class="c1"># Psymm is UK * SigmaK * UK^transpose</span>
<span class="n">Psymm</span> <span class="o">=</span> <span class="n">UK</span> <span class="o">@</span> <span class="n">SK</span> <span class="o">@</span> <span class="n">UK</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Psymm</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Psymm</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U_K </span><span class="se">\\</span><span class="s2">Sigma_K U_K^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Psymm</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - U_K </span><span class="se">\\</span><span class="s2">Sigma_K U_K^</span><span class="se">\\</span><span class="s2">top$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/6cc260ff20174d52dcb4c361cfa7a2774694b3916e16b20fccb3c9773c0fc85e.png" src="../../_images/6cc260ff20174d52dcb4c361cfa7a2774694b3916e16b20fccb3c9773c0fc85e.png" />
</div>
</div>
</section>
<section id="the-square-root-matrix">
<h3>The Square Root Matrix<a class="headerlink" href="#the-square-root-matrix" title="Permalink to this heading">#</a></h3>
<p>It’s been quite a ride; don’t fall off just yet! Now you are really in the home stretch! We’ve saved the easiest part for last. Remember that <span class="math notranslate nohighlight">\(\Sigma_K\)</span> is just a diagonal matrix, whose entries are the singular values <span class="math notranslate nohighlight">\(\sigma_i\)</span> for the first <span class="math notranslate nohighlight">\(K\)</span> singular values. As it turns out, since <span class="math notranslate nohighlight">\(P\)</span> is positive, these singular values are going to be positive too, which means that you can break each singular value into its square root, as <span class="math notranslate nohighlight">\(\sigma_i = \sqrt{\sigma_i}\sqrt{\sigma_i}\)</span>.</p>
<p>Like for the Laplacian Spectral Embedding, this meant you could factor the first <span class="math notranslate nohighlight">\(K\)</span> singular value matrix <span class="math notranslate nohighlight">\(\Sigma_K\)</span> into the product of its square root matrix and its transpose, as <span class="math notranslate nohighlight">\(\Sigma_K = \sqrt{\Sigma_K}\sqrt{\Sigma_K}\)</span>.</p>
<p>This means that your probability matrix is just <span class="math notranslate nohighlight">\(P = U_K \sqrt{\Sigma_K}\sqrt{\Sigma_K}^\top U_K^\top\)</span>, which is very similar to what you got for the Laplacian spectral embedding. The difference here is that <span class="math notranslate nohighlight">\(P\)</span> <em>itself</em> is equal to this quantity, not just a “reduced rank” representation of <span class="math notranslate nohighlight">\(P\)</span> like you had for the Laplacian spectral embedding.</p>
<p>If you let <span class="math notranslate nohighlight">\(X = U_K \sqrt{\Sigma_K}\)</span>, then <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. This means that <span class="math notranslate nohighlight">\(X\)</span> is a latent position matrix for <span class="math notranslate nohighlight">\(P\)</span>! If you remember back to the <span class="xref myst">Section on RDPGs</span>, this means that you have found the latent position parameter for the corresponding RDPG for your SBM random network!</p>
<p>Let’s see this using numpy again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SKsqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">SK</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">UK</span> <span class="o">@</span> <span class="n">SKsqrt</span>
<span class="n">Prdpg</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Prdpg</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Prdpg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$XX^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Prdpg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - XX^</span><span class="se">\\</span><span class="s2">top$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/5f045c963cffb491d83e3b44dcc5b9e6463095ec9e725ac4c83675db3bb56dea.png" src="../../_images/5f045c963cffb491d83e3b44dcc5b9e6463095ec9e725ac4c83675db3bb56dea.png" />
</div>
</div>
<p>This matrix <span class="math notranslate nohighlight">\(X\)</span>, the latent position matrix, for an SBM will look like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_lpm</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticklabs</span><span class="o">=</span><span class="p">[],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="n">cbar</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xticklabs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">yticklabs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt{</span><span class="se">\\</span><span class="s2">sigma_1}</span><span class="se">\\</span><span class="s2">vec u_1$&quot;</span><span class="p">,</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt{</span><span class="se">\\</span><span class="s2">sigma_2}</span><span class="se">\\</span><span class="s2">vec u_2$&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Heatmap of Latent Position Matrix $X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/8e6c7cd0e7c28d006be5ef09da2ad282c14ed73eb64f82159db08bc08f6922b7.png" src="../../_images/8e6c7cd0e7c28d006be5ef09da2ad282c14ed73eb64f82159db08bc08f6922b7.png" />
</div>
</div>
<p>and is the “low-rank structure” which describes the probability matrix.</p>
</section>
<section id="the-latent-positions-are-distinct-for-each-community">
<h3>The latent positions are distinct for each community<a class="headerlink" href="#the-latent-positions-are-distinct-for-each-community" title="Permalink to this heading">#</a></h3>
<p>Now, you will notice a very interesting property about the latent position matrix for an SBM. Remember that for an RDPG (and an SBM is also an RDPG), the latent position vectors for the node <span class="math notranslate nohighlight">\(i\)</span> are the <em>rows</em> <span class="math notranslate nohighlight">\(\vec x_i\)</span> of the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. This means that the latent position vector for node <span class="math notranslate nohighlight">\(1\)</span> is the vector <span class="math notranslate nohighlight">\((\sqrt{\sigma_1}u_{11}, \sqrt{\sigma_2}u_{21})\)</span>. What do you notice about the latent positions vector for node <span class="math notranslate nohighlight">\(2\)</span>? It’s <em>exactly</em> the same!</p>
<p>More generally, the latent position vector for <em>all</em> nodes which are in the same community will be identical. This is why if you look across the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> above, there are only two unique latent position vectors: there are one unique vector (which is black in the first dimension and red in the second dimension) for the nodes in the first community, and a second unique vector (which is black in the first dimension and beige in the second dimension) for the nodes of the second community.</p>
<div class="admonition-putting-it-all-together admonition">
<p class="admonition-title">Putting it all together</p>
<p>What have you learned so far? What we’ve learned so far is that, if you have a probability matrix that is symmetric and rank-<span class="math notranslate nohighlight">\(K\)</span>:</p>
<ol class="arabic simple">
<li><p>We can decompose this probability matrix using the singular value decomposition.</p></li>
<li><p>We can ignore singular values/vectors other than the first <span class="math notranslate nohighlight">\(K\)</span> of them.</p></li>
<li><p>For the first <span class="math notranslate nohighlight">\(K\)</span> singular vectors, the left and right vectors are identical.</p></li>
<li><p>We can decompose the singular value matrix into the product of the square root matrix with its transpose.</p></li>
<li><p>We can express the matrix <span class="math notranslate nohighlight">\(P\)</span> using the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, which is the product of the first <span class="math notranslate nohighlight">\(K\)</span> singular vectors with the first <span class="math notranslate nohighlight">\(K\)</span> singular values.
This means that you have found a latent position matrix <span class="math notranslate nohighlight">\(X\)</span> for the probability matrix using the singular values and singular vectors of <span class="math notranslate nohighlight">\(P\)</span>, by effectively just discarding the ones that don’t matter (and have singular values of <span class="math notranslate nohighlight">\(0\)</span>). We have succeeded in our goal of finding a much lower rank structure, the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, to describe the probability matrix <span class="math notranslate nohighlight">\(P\)</span>.</p></li>
</ol>
</div>
<p>If you remember from the section on RDPGs, this probability matrix has the property that each entry <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i \vec x_j^\top\)</span>.</p>
</section>
</section>
<section id="but-wait-we-don-t-have-the-probability-matrix-what-do-you-do">
<h2>But wait: we don’t have the probability matrix! What do you do?<a class="headerlink" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-you-do" title="Permalink to this heading">#</a></h2>
<p>All of the logic we developed above was with respect to the probability matrix, <span class="math notranslate nohighlight">\(P\)</span>, for a SBM. More generally, this logic extends to the probability matrix <span class="math notranslate nohighlight">\(P\)</span> for any RDPG, which is because an RDPG with <span class="math notranslate nohighlight">\(d\)</span> latent dimensions will <em>always</em> have a probability matrix that is <em>exactly</em> rank <span class="math notranslate nohighlight">\(d\)</span>. If you took the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> which had <span class="math notranslate nohighlight">\(d\)</span> latent dimensions, and then used the svd to find the <span class="math notranslate nohighlight">\(U_d\)</span> and <span class="math notranslate nohighlight">\(\Sigma_d\)</span> where <span class="math notranslate nohighlight">\(P = U_d \Sigma_d U_d^\top\)</span>, you could find another latent position matrix <span class="math notranslate nohighlight">\(Y = U_d\sqrt{\Sigma_d}\)</span> where <span class="math notranslate nohighlight">\(P = YY^\top\)</span>.</p>
<p>But, you have a slight issue: when you perform machine learning, you don’t know the probability matrix! The probability matrix is a <em>parameter</em> of the statistical model itself, it is <em>not</em> a function of the sample of data you get. All you have is the adjacency matrix itself, <span class="math notranslate nohighlight">\(A\)</span>, which is your data! We don’t actually know what the underlying probability matrix is! How the heck can you find this low rank structure you want to be able to estimate?</p>
<p>As it turns out, if you kept obtaining more and more networks <span class="math notranslate nohighlight">\(A\)</span> from the underlying RDPG random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>, you would <em>expect</em> that the network <span class="math notranslate nohighlight">\(A\)</span> you saw would be the probability matrix <span class="math notranslate nohighlight">\(P\)</span>. We’ll explain what you mean by expect here by turning back to your coin flip example. As you remember, you perform a coin flip at each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of an RDPG, where the coin lands on heads with probability <span class="math notranslate nohighlight">\(\vec x_i\vec x_j^\top\)</span>, and lands on tails with probability <span class="math notranslate nohighlight">\(1 - \vec x_i\vec x_j^\top\)</span>. This means you can <em>expect</em> the coin to land on heads with probability <span class="math notranslate nohighlight">\(\vec x_i\vec x_j^\top\)</span>. In the same sense, you can expect the <span class="math notranslate nohighlight">\((i,j)\)</span> entry of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> to exist with probability <span class="math notranslate nohighlight">\(\vec x_i \vec x_j^\top\)</span>. In this sense, the expected value of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> <em>is</em> the probability matrix <span class="math notranslate nohighlight">\(P = XX^\top\)</span>.</p>
<p>So, since the expected value of the adjacency matrix <em>is</em> the probability matrix, what if you were to just embed the adjacency matrix instead? Let’s see how this might work. Again, you’ll use the singular value decomposition on <span class="math notranslate nohighlight">\(A\)</span>, and take a look at the scree plot for <span class="math notranslate nohighlight">\(A\)</span>, and compare it to the scree plot of <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UA</span><span class="p">,</span> <span class="n">sA</span><span class="p">,</span> <span class="n">VAt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plot_scree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $P$&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_scree</span><span class="p">(</span><span class="n">sA</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $A$&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/16afea7d945b0bdf2ac1fd7871273ef6ead9c9eaaeb59a63022e2334dd252370.png" src="../../_images/16afea7d945b0bdf2ac1fd7871273ef6ead9c9eaaeb59a63022e2334dd252370.png" />
</div>
</div>
<p>Now that’s really funky! The singular values of both <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(A\)</span> tend to fall off around <em>roughly</em> the same spot, right around dimension <span class="math notranslate nohighlight">\(2\)</span>! The singular value of <span class="math notranslate nohighlight">\(P\)</span> go directly to <span class="math notranslate nohighlight">\(0\)</span>, but the singular values for <span class="math notranslate nohighlight">\(A\)</span> tend to “round off” in the direction of <span class="math notranslate nohighlight">\(0\)</span>, but it isn’t <em>too</em> far off!</p>
<p>As it turns out, this is no coincidence: the singular values for a network which can be described by an RDPG will tend to “elbow” off right around the number of true latent dimensions for the probability matrix of the underlying random network. If the RDPG has <span class="math notranslate nohighlight">\(d\)</span> latent dimensions, this will occur right around <span class="math notranslate nohighlight">\(d\)</span>. For this reason, it is usually a good idea when you think a network might be well described by an RDPG to let the elbow selection algorithm do the work for us, and then take a good look at the scree plot to make sure the number of latent dimensions chosen seems reasonable to us. If you don’t know how many latent dimensions to retain, you’ll call the number of embedding dimensions <span class="math notranslate nohighlight">\(\hat d\)</span>, which just means, “estimate of the number of latent dimensions”.</p>
<p>What does it look like when you use the spectral embedding on <span class="math notranslate nohighlight">\(A\)</span>? You’ll compare the embedding of the adjacency matrix to the latent positions of the probability matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UAK</span> <span class="o">=</span> <span class="n">UA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">];</span> <span class="n">USK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sA</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">]))</span>
<span class="n">Aembedded</span> <span class="o">=</span> <span class="n">UAK</span> <span class="o">@</span> <span class="n">USK</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Heatmap of Latent Position Matrix $X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">Aembedded</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedded Adjacency Matrix&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f4a2f7a178bc28985b999503e024f526f3052a9c4cb62a7912cc6b2143912989.png" src="../../_images/f4a2f7a178bc28985b999503e024f526f3052a9c4cb62a7912cc6b2143912989.png" />
</div>
</div>
<p>Wow! When you take the adjacency matrix and embed it into <span class="math notranslate nohighlight">\(2\)</span> dimensions, it doesn’t look <em>identical</em> to the latent position matrix, but it shares some major patterns with it! In particular, it looks like the second latent dimension for the embedded adjacency matrix tends to capture that the second latent dimension of <span class="math notranslate nohighlight">\(X\)</span> has higher values for the first <span class="math notranslate nohighlight">\(50\)</span> nodes, and lower values for the second <span class="math notranslate nohighlight">\(50\)</span> nodes. For a variety of reasons, you will call this “embedding of <span class="math notranslate nohighlight">\(A\)</span>” an <em>estimate</em> of the latent position matrix for the underlying RDPG, which you will denote by <span class="math notranslate nohighlight">\(\hat X\)</span>.</p>
<p>As you learned in the last section, this entire procedure is automated for you by <code class="docutils literal notranslate"><span class="pre">graspologic</span></code> with the <code class="docutils literal notranslate"><span class="pre">AdjacencySpectralEmbed()</span></code> class, or alternatively, the <code class="docutils literal notranslate"><span class="pre">RDPGEstimator()</span></code>. The <code class="docutils literal notranslate"><span class="pre">RDPGEstimator()</span></code> class just makes clear that you are estimating parameters for an RDPG:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rdpgest</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">RDPGEstimator</span><span class="p">()</span>
<span class="n">rdpgest</span> <span class="o">=</span> <span class="n">rdpgest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Xhat</span> <span class="o">=</span> <span class="n">rdpgest</span><span class="o">.</span><span class="n">latent_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedded Adjacency Matrix, $\hat X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/5f584cdf88ae25c67cb5dbb4bf0aa9fdfd6addb26a0bc10820b920720fee95c0.png" src="../../_images/5f584cdf88ae25c67cb5dbb4bf0aa9fdfd6addb26a0bc10820b920720fee95c0.png" />
</div>
</div>
<p>As it turns out, this example here is a good instance of another property of the latent position matrix. Remember that the latent position matrix is the matrix <span class="math notranslate nohighlight">\(X\)</span> where <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. As it turns out, sometimes the columns of this matrix can get flipped around a little bit, through something called a rotation. You can see this by noticing that it looks a lot like the entries of the estimates of the latent position matrix are positive in one case are negative for the other, and it basically looks like the colorbar just got flipped around on us. The rotation doesn’t really matter (yet!), and <span class="math notranslate nohighlight">\(P\)</span> still is equal to <span class="math notranslate nohighlight">\(XX^\top\)</span>, regardless of how that <span class="math notranslate nohighlight">\(X\)</span> is rotated. You’ll learn more about rotation matrices in the upcoming section on <span class="xref myst">Multiple Network Representation Learning</span> and in the section on <span class="xref myst">Two Sample Hypothesis Testing</span>, but for now, all you need to know is that this “flippage” of what’s big and small in the latent position matrix is not too important (again, yet!).</p>
</section>
<section id="when-should-you-use-ase-and-when-should-you-use-lse">
<h2>When should you use ASE and when should you use LSE?<a class="headerlink" href="#when-should-you-use-ase-and-when-should-you-use-lse" title="Permalink to this heading">#</a></h2>
<p>In the last two sections, you learned about <code class="docutils literal notranslate"><span class="pre">ASE</span></code> and<code class="docutils literal notranslate"><span class="pre">LSE</span></code> , and discussed how each can be used to reasonably embed network data. So, what’s the difference? When should you use one approach versus the other?</p>
<p>Well, it turns out that<code class="docutils literal notranslate"><span class="pre">LSE</span></code>  and <code class="docutils literal notranslate"><span class="pre">ASE</span></code> capture different notions of “structure” when embedding a network. Carey Priebe and collaborators at Johns Hopkins University investigated this recently, in 2018, and discovered that<code class="docutils literal notranslate"><span class="pre">LSE</span></code>  tends to capture “affinity” structure, whereas <code class="docutils literal notranslate"><span class="pre">ASE</span></code> tends to capture “core-periphery” structure (their paper is called “On a two-truths phenomenon in spectral graph clustering” - it’s an interesting read for the curious). What do these words mean?</p>
<p>In a sentence, affinity structure is a property under which there exist communities of nodes in the network, and those nodes tend to be more heavily connected within-community than between-community (nodes in the same community have more <em>affinity</em> for one another, also known as <em>homophily</em>). Think of a friend network in two schools, where people within the same school are much more likely to be friends than people in different schools. This is a type of structure we’ve seen a lot in this book in your Stochastic Block Model examples.</p>
<p>On the other hand, a core-periphery structure is a property under which there exists a subset of nodes in the network which have much higher degrees than other nodes in the network (the <em>core</em> of the network, since most of the edges are found in the <em>core</em> nodes), and there are other subsets of nodes which tend to have much lower degrees than other nodes in the network (the <em>periphery</em> of the network, since these nodes tend to take a secondary position edge wise to the <em>core</em> nodes). Think of a core of popular, well-liked, and charismatic kids at a high school, with a periphery of loners or people who prefer not to socialize as much.</p>
<p>First, you going to generate an affinity structure in an SBM with <span class="math notranslate nohighlight">\(2\)</span> communities, and plot both the probability matrix and a sample of an example of this type of network. The network will have <span class="math notranslate nohighlight">\(100\)</span> nodes, and <span class="math notranslate nohighlight">\(50\)</span> nodes in each community:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n1</span> <span class="o">=</span> <span class="mi">50</span><span class="p">;</span> <span class="n">n2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">Baff</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>

<span class="n">zvecaff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;C1&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;C2&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n2</span><span class="p">)])</span>

<span class="c1"># the probability matrix</span>
<span class="n">zvecaff_ohe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n2</span><span class="p">)])</span>
<span class="n">Paff</span> <span class="o">=</span> <span class="n">zvecaff_ohe</span> <span class="o">@</span> <span class="n">Baff</span> <span class="o">@</span> <span class="n">zvecaff_ohe</span><span class="o">.</span><span class="n">T</span>

<span class="n">Aaff</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">simulations</span><span class="o">.</span><span class="n">sbm</span><span class="p">([</span><span class="n">n1</span><span class="p">,</span> <span class="n">n2</span><span class="p">],</span> <span class="n">Baff</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Paff</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvecaff</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Affinity probability matrix $P^{(a)}$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Aaff</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvecaff</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Affinity sample $A^{(a)}$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/2e0ad8e6b1c4c5da0dcd69880f31b8894e1a51581be22d8fc654e1c0f1a24489.png" src="../../_images/2e0ad8e6b1c4c5da0dcd69880f31b8894e1a51581be22d8fc654e1c0f1a24489.png" />
</div>
</div>
<p>As you can see, the nodes in communities one and two tend to be more highly connected within community (homophilic affinity) than between.</p>
<p>Next, you generate another network with core-periphery structure in an SBM with 3 communities, where the first <span class="math notranslate nohighlight">\(15\)</span> nodes are the first peripheral group, the second <span class="math notranslate nohighlight">\(70\)</span> nodes are the “core”, and the last <span class="math notranslate nohighlight">\(15\)</span> nodes are the second periphery group:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np1</span> <span class="o">=</span> <span class="mi">15</span><span class="p">;</span> <span class="n">nc</span> <span class="o">=</span> <span class="mi">70</span><span class="p">;</span> <span class="n">np2</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">Bcp</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
      <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
      <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]]</span>


<span class="n">zveccp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;Per.&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np1</span><span class="p">)]</span> <span class="o">+</span>
                  <span class="p">[</span><span class="s2">&quot;Core&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nc</span><span class="p">)]</span> <span class="o">+</span>
                  <span class="p">[</span><span class="s2">&quot;Per.&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np2</span><span class="p">)])</span>

<span class="c1"># the probability matrix</span>
<span class="n">zveccp_ohe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np1</span><span class="p">)]</span> <span class="o">+</span> 
                       <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nc</span><span class="p">)]</span> <span class="o">+</span>
                       <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np2</span><span class="p">)])</span>
<span class="n">Pcp</span> <span class="o">=</span> <span class="n">zveccp_ohe</span> <span class="o">@</span> <span class="n">Bcp</span> <span class="o">@</span> <span class="n">zveccp_ohe</span><span class="o">.</span><span class="n">T</span>

<span class="n">Acp</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">simulations</span><span class="o">.</span><span class="n">sbm</span><span class="p">([</span><span class="n">np1</span><span class="p">,</span> <span class="n">nc</span><span class="p">,</span> <span class="n">np2</span><span class="p">],</span> <span class="n">Bcp</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Pcp</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zveccp</span><span class="p">,</span> <span class="n">sort_nodes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Core-periphery probability matrix $P^{(cp)}$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Acp</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zveccp</span><span class="p">,</span> <span class="n">sort_nodes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Core-periphery sample $A^{(cp)}$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/e056f44e505172eaecfeee405686a3caf4b9bb6a382229eecde9cedc29a8c55a.png" src="../../_images/e056f44e505172eaecfeee405686a3caf4b9bb6a382229eecde9cedc29a8c55a.png" />
</div>
</div>
<p>What if the network has <em>both</em> core-periphery <em>and</em> affinity structure? Let’s consider a slight augmentation of the above networks, where within the probability of a given node <span class="math notranslate nohighlight">\(i\)</span> being connected to another node <span class="math notranslate nohighlight">\(j\)</span> is an <em>average</em> of its core-periphery and affinity probabilities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Pcp_and_aff</span> <span class="o">=</span> <span class="p">(</span><span class="n">Paff</span> <span class="o">+</span> <span class="n">Pcp</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
<span class="n">Acp_and_aff</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">simulations</span><span class="o">.</span><span class="n">sample_edges</span><span class="p">(</span><span class="n">Pcp_and_aff</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Pcp_and_aff</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zveccp</span><span class="p">,</span> <span class="n">outer_hier_labels</span><span class="o">=</span><span class="n">zvecaff</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Acp_and_aff</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zveccp</span><span class="p">,</span> <span class="n">outer_hier_labels</span><span class="o">=</span><span class="n">zvecaff</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/4652dc921becd0d01043747a867c8b828aefa8e71362f4c3e181cc74830b4d40.png" src="../../_images/4652dc921becd0d01043747a867c8b828aefa8e71362f4c3e181cc74830b4d40.png" />
</div>
</div>
<p>When you embed the networks using <code class="docutils literal notranslate"><span class="pre">ASE</span></code>, you tend to obtain estimated latent positions where nodes in the same community tend to have similar estimates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>
<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Xhat_ase</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Acp_and_aff</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">pairplot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat_ase</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">zvecaff</span><span class="p">,</span> <span class="n">legend_name</span><span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="p">,</span> 
             <span class="n">title</span><span class="o">=</span><span class="s2">&quot;ASE captures affinity structure&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/885c99259fadf9b1ab12960cdf455fb7e8842b8935a764774501fc41b0ae5c3d.png" src="../../_images/885c99259fadf9b1ab12960cdf455fb7e8842b8935a764774501fc41b0ae5c3d.png" />
</div>
</div>
<p>And when you embed the networks using <code class="docutils literal notranslate"><span class="pre">LSE</span></code>, you tend to obtain estimated latent positions where nodes which are in the core are distinct from the nodes in the periphery:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">LaplacianSpectralEmbed</span> <span class="k">as</span> <span class="n">LSE</span>
<span class="n">lse</span> <span class="o">=</span> <span class="n">LSE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Xhat_lse</span> <span class="o">=</span> <span class="n">lse</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Acp_and_aff</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">pairplot</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat_lse</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">zveccp</span><span class="p">,</span> <span class="n">legend_name</span><span class="o">=</span><span class="s2">&quot;Node type&quot;</span><span class="p">,</span> 
             <span class="n">title</span><span class="o">=</span><span class="s2">&quot;LSE captures core-periphery structure&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/6d0e11ce8c11744d3af984f1fa61681dc0966ff6b1b093ae94649fa682933c1b.png" src="../../_images/6d0e11ce8c11744d3af984f1fa61681dc0966ff6b1b093ae94649fa682933c1b.png" />
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ASE</span></code> and<code class="docutils literal notranslate"><span class="pre">LSE</span></code>  each captured a different, but still <em>true</em>, truth about the underlying network.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./appendix/ch13"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-nodes-from-the-same-community-similar">How are nodes from the same community similar?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-low-rank-property-and-the-probability-matrix">The low-rank property and the probability matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-best-representation-of-the-probability-matrix">The best representation of the probability matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-square-root-matrix">The Square Root Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-latent-positions-are-distinct-for-each-community">The latent positions are distinct for each community</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-you-do">But wait: we don’t have the probability matrix! What do you do?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-should-you-use-ase-and-when-should-you-use-lse">When should you use ASE and when should you use LSE?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>