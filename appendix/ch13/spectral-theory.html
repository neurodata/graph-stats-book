

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>12.2. Spectral Method Theory &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'appendix/ch13/spectral-theory';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13. Applications (Extended)" href="../ch14/ch14.html" />
    <link rel="prev" title="12.1. Maximum Likelihood Estimate Theory" href="mle-theory.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../coverpage.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/terminology.html">Terminology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch1/ch1.html">1. The Network Machine Learning Landscape</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">1.1. What is network machine learning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/why-study-networks.html">1.2. Why do we study networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">1.3. Types of Network Machine Learning Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">1.4. Examples of applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">1.5. Challenges of Network Machine Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch2/ch2.html">2. End-to-end Biology Network Machine Learning Project</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/big-picture.html">2.1. Look at the big picture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/get-the-data.html">2.2. Get the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">2.3. Prepare the Data for Network Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/select-and-train.html">2.4. Select and Train a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/fine-tune.html">2.5. Fine-Tune your Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">2.6. Discover and Visualize the Data to Gain Insights</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Representations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch4/ch4.html">3. Properties of Networks as a Statistical Object</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/matrix-representations.html">3.1. Matrix Representations Of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/properties-of-networks.html">3.2. Properties of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/network-representations.html">3.3. Representations of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/regularization.html">3.4. Regularization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch5/ch5.html">4. Why Use Statistical Models?</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">4.1. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">4.2. Stochastic Block Models (SBM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">4.3. Random Dot Product Graphs (RDPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SIEM.html">4.5. Structured Independent Edge Model (SIEM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/multi-network-models.html">4.6. Multiple Network Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/models-with-covariates.html">4.7. Network Models with Network Covariates</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch6/ch6.html">5. Learning Network Representations</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">5.1. Estimating Parameters in Network Models via MLE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/why-embed-networks.html">5.2. Why embed networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/spectral-embedding.html">5.3. Spectral embedding methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">5.4. Multiple-Network Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">5.5. Joint Representation Learning</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch7/ch7.html">6. Applications When You Have One Network</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/community-detection.html">6.1. Community Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/testing-differences.html">6.2. Testing for Differences between Groups of Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/model-selection.html">6.3. Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/vertex-nomination.html">6.4. Single-Network Vertex Nomination</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/out-of-sample.html">6.5. Out-of-sample Embedding</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch8/ch8.html">7. Applications for Two Networks</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html">7.1. Latent Two-Sample Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/significant-communities.html">7.2. Two-sample hypothesis testing in SBMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html">7.3. Graph Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/multiple-vertex-nomination.html">7.4. Vertex Nomination For Two Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch9/ch9.html">8. Applications for Many Networks</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/anomaly-detection.html">8.1. Anomaly Detection For Timeseries of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-edges.html">8.2. Testing for Significant Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-vertices.html">8.3. Testing for Significant Vertices</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Next Steps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../next/ch10/ch10.html">9. Where do we go from here?</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/random-walk-diffusion-methods.html">9.1. Random walk and diffusion-based methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/gnn.html">9.2. Graph Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/sparsity.html">9.3. Network Sparsity</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch11/ch11.html">10. Representations (Extended)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch11/alt-reps.html">10.1. Alternative Network Representations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch12/ch12.html">11. Network Model Theory</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch12/background.html">11.2. Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/foundation.html">11.3. Foundation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/ers.html">11.4. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/sbms.html">11.5. Stochastic Block Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/rdpgs.html">11.6. RDPGs and more general network models</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ch13.html">12. Learning Representations Theory</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="mle-theory.html">12.1. Maximum Likelihood Estimate Theory</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">12.2. Spectral Method Theory</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch14/ch14.html">13. Applications (Extended)</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch14/hypothesis.html">13.1. Hypothesis Testing with coin flips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch14/unsupervised.html">13.2. Unsupervised learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch14/bayes.html">13.3. Bayes Plugin Classifier</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">Graspologic Documentation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/appendix/ch13/spectral-theory.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fappendix/ch13/spectral-theory.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/appendix/ch13/spectral-theory.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Spectral Method Theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#disclaimer-about-classical-statistical-asymptotic-theory">12.2.1. Disclaimer about classical statistical asymptotic theory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjacency-spectral-embedding">12.2.2. Adjacency spectral embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-error-of-the-adjacency-spectral-embedding">12.2.2.1. Statistical error of the adjacency spectral embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency">12.2.2.1.1. Consistency</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#asymptotic-normality">12.2.2.1.2. Asymptotic normality</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding">12.2.2.2. Example: Erdős-Rényi graphs and the adjacency spectral embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-two-graph-hypothesis-testing">12.2.2.3. Application: two-graph hypothesis testing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theory-for-multiple-network-models">12.2.3. Theory for multiple network models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-matching-for-correlated-networks">12.2.3.1. Graph Matching for Correlated Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-spectral-embeddings">12.2.3.2. Joint spectral embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#omnibus-embedding-omni">12.2.3.3. Omnibus Embedding (omni)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-adjacency-spectral-embedding-mase">12.2.3.4. Multiple adjacency spectral embedding (MASE)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">12.2.4. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="spectral-method-theory">
<span id="app-ch13-spectral"></span><h1><span class="section-number">12.2. </span>Spectral Method Theory<a class="headerlink" href="#spectral-method-theory" title="Permalink to this heading">#</a></h1>
<p>We’ve tried to present this material so far in a manner which is as easy to understand as possible with a distant understanding of probability/statistics and a working knowledge of machine learning. Unfortunately, there is really no way around it now, so this section is about to get hairy mathematically. To understand this section properly, you should have an extremely firm understanding of linear algebra, and more than likely, should have a working knowledge of matrix analysis and multivariate probability theory. While we’ve already seen some concentration inequalities in the last section (Chebyshev’s inequality) you should have a working knowledge of how this term can be extended to random vectors and matrices before proceeding. Before taking on this section, we would recommend checking out the excellent primer <span id="id1">[<a class="reference internal" href="#id31" title="Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press, Cambridge, England, UK, September 2018. ISBN 978-1-10823159-6. doi:10.1017/9781108231596.">6</a>]</span>, which should get you a good foundation to understand many of the results we will take a look at here. We aren’t going to prove many of these results; if you want more details, please check out <span id="id2">[<a class="reference internal" href="../../representations/ch6/spectral-embedding.html#id58" title="Avanti Athreya, Donniell E. Fishkind, Minh Tang, Carey E. Priebe, Youngser Park, Joshua T. Vogelstein, Keith Levin, Vince Lyzinski, and Yichen Qin. Statistical inference on random dot product graphs: a survey. J. Mach. Learn. Res., 18(1):8393–8484, January 2017. doi:10.5555/3122009.3242083.">3</a>]</span>.</p>
<p>Buckle up!</p>
<section id="disclaimer-about-classical-statistical-asymptotic-theory">
<h2><span class="section-number">12.2.1. </span>Disclaimer about classical statistical asymptotic theory<a class="headerlink" href="#disclaimer-about-classical-statistical-asymptotic-theory" title="Permalink to this heading">#</a></h2>
<p>While in classic statistics there is a large literature that derives the large sample properties of an estimator, these concepts are more challenging in network analysis for multiple reasons. To start with, the very basic concept of sample size is not particularly clear. We often associate sample size with the number of observations, which are usually assumed to be independent from each other (for example, think of the number of poll participants to estimate polling preferences). In a network, having independent observations is no longer possible in the same way since all we observe are edges, and they are related to some type of interaction between two vertices. We therefore often assume that the sampled units are the vertices. However, everytime a new node is added, a new set of interactions with all the existing vertices is added to the model, which often results in the need of including more parameters, leading to the second important challenge in studying networks. A body of new literature has addressed some of these challenges for the models and estimators introduced in the previous sections, and we review some of these results here.</p>
</section>
<section id="adjacency-spectral-embedding">
<h2><span class="section-number">12.2.2. </span>Adjacency spectral embedding<a class="headerlink" href="#adjacency-spectral-embedding" title="Permalink to this heading">#</a></h2>
<p>In the following sections, we summarize some of the main results in the literature about spectral embeddings. A more in-deep review of this results is presented in <span id="id3">[<a class="reference internal" href="../../representations/ch6/spectral-embedding.html#id58" title="Avanti Athreya, Donniell E. Fishkind, Minh Tang, Carey E. Priebe, Youngser Park, Joshua T. Vogelstein, Keith Levin, Vince Lyzinski, and Yichen Qin. Statistical inference on random dot product graphs: a survey. J. Mach. Learn. Res., 18(1):8393–8484, January 2017. doi:10.5555/3122009.3242083.">3</a>]</span> and <span id="id4">[<a class="reference internal" href="#id32" title="Daniel L. Sussman, Minh Tang, Donniell E. Fishkind, and Carey E. Priebe. A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs. J. Am. Stat. Assoc., 107(499):1119–1128, September 2012. doi:10.1080/01621459.2012.699795.">7</a>]</span>. In this section, we review some theoretical properties for the adjacency spectral embedding (ASE), introduced in <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-numref">Section 5.3</span></a>. We focus on contextualizing this method using the random dot product graph (RDPG) model from <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html#ch5-rdpg"><span class="std std-numref">Section 4.3</span></a>. If you haven’t, we’d recommend you check out the appendix section as well from <a class="reference internal" href="../ch12/rdpgs.html#app-ch12-rdpg"><span class="std std-numref">Section 11.6</span></a>.</p>
<p>The results we present in this section aim to understand the general question of how effective the spectral embedding methods are in estimating the latent positions of a random network generated from the RDPG model. Ideally, we would like these embeddings to be as close as possible to the true latent positions. The results we present on this section show that these estimates are indeed close if the network is sufficiently large. Moreover, the limiting distribution of these estimates can be characterized explicitly in a similar fashion as the classic central limit theorems in statistics.</p>
<p>In the rest of the section, we consider a random adjacency matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with <span class="math notranslate nohighlight">\(n\)</span> vertices sampled from the RDPG model with latent positions <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times d}\)</span>. We write this matrix as <span class="math notranslate nohighlight">\(\mathbf{X}= [\mathbf{x}_1, \ldots, \mathbf{x}_n]^\top\)</span>, so that the row <span class="math notranslate nohighlight">\(i\)</span> contains a vector <span class="math notranslate nohighlight">\(\mathbf{x}_i\in\mathbb{R}^d\)</span> which represents the latent position of node <span class="math notranslate nohighlight">\(i\)</span>, and as such, the upper-tiangular entries of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> are independent with probability</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}(\mathbf{A}_{ij}=1)= \vec{\mathbf x}_i^\top\vec{\mathbf x}_j, \quad\quad\text{for all i&lt;j}.
\end{align*}\]</div>
<p>The rows of <span class="math notranslate nohighlight">\(\mathbf X\)</span>, are assumed to be independent and identically distributed with <span class="math notranslate nohighlight">\(\vec{\mathbf x}_1, \ldots, \vec{\mathbf x}_n\overset{\text{i.i.d.}}{\sim} F\)</span>, where <span class="math notranslate nohighlight">\(F\)</span> is a distribution with support in  <span class="math notranslate nohighlight">\(\mathbb R^d\)</span>.  We use <span class="math notranslate nohighlight">\(\widehat{\mathbf X}=ASE(\mathbf A)\in\mathbb R^{n\times d}\)</span> to denote the <span class="math notranslate nohighlight">\(d\)</span>-dimensional adjacency spectral embedding of <span class="math notranslate nohighlight">\(\mathbf A\)</span>.</p>
<section id="statistical-error-of-the-adjacency-spectral-embedding">
<h3><span class="section-number">12.2.2.1. </span>Statistical error of the adjacency spectral embedding<a class="headerlink" href="#statistical-error-of-the-adjacency-spectral-embedding" title="Permalink to this heading">#</a></h3>
<section id="consistency">
<h4><span class="section-number">12.2.2.1.1. </span>Consistency<a class="headerlink" href="#consistency" title="Permalink to this heading">#</a></h4>
<p>As we have observed before (for instance, see <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html#ch5-rdpg"><span class="std std-numref">Section 4.3</span></a>), the latent position matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> of the network <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> encodes important node properties, such as community assignments, or more generally, a latent geometry that characterizes the probabilites of the edges. As such, it is fundamental to understand how close the true and the estimated latent positions (obtained from the ASE) are from each other. The answer depends on several factors that we review here.</p>
<p>When we want to compare the estimates and the true latent positions, we face a unavoidable problem: the parameters of a random dot product graph, namely, the latent positions, are <em>not identifiable</em>. In other words, given a matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, it is possible to find another matrix <span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{n \times d}\)</span> that produces exactly the same edge probability matrix, that is, <span class="math notranslate nohighlight">\(\mathbf{P} = \mathbf{X} \mathbf{X}^\top = \mathbf{Y}\mathbf{Y}^\top\)</span>. For instance, changing the signs of the columns of <span class="math notranslate nohighlight">\(\mathbf X\)</span> does not change the inner products of their rows, and thus, the matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> remains unaffected to these type of transformations. In general,  as long as the columns of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are linearly independent, all the possible matrices <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> that satisfy the previous relation are equal up to an orthogonal transformation. In other words, there exists an orthogonal matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> of size <span class="math notranslate nohighlight">\(d\times d\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{Y} =\mathbf{X}\mathbf{W}\)</span>. Here we need to make a technical assumption  to guarantee that this condition (that the columns of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are linearly independent) holds with high probability. In particular, we will  assume that the second moment matrix <span class="math notranslate nohighlight">\(\mathbf{\Delta} = \mathbb{E}[\vec{\mathbf x}_1\vec{\mathbf x}_1^\top]\in\mathbb R^{d\times d}\)</span> has non-zero eigenvalues. This condition simplifies the type of non-identifiabilities we may face, since the probability of having correlated columns in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> would be very small if <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large.</p>
<p>The first result we review concerns the error of the adjacency spectral embedding (ASE) method; that is, the difference between <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and the estimated latent position matrix <span class="math notranslate nohighlight">\(\widehat{\mathbf X}\)</span>. This estimator has been shown to consistently estimate the latent positions. In other words, as the sample size (number of nodes) increases, the estimated latent positions approach the true latent positions. The typical distance between these two matrices can be quantified explicitly in terms of the sample size <span class="math notranslate nohighlight">\(n\)</span>, dimension of the latent positions <span class="math notranslate nohighlight">\(d\)</span> and a constant <span class="math notranslate nohighlight">\(C\)</span> that only depends on the particular distribution of the latent positions <span class="math notranslate nohighlight">\(F\)</span>. In particular, with probability tending to one as <span class="math notranslate nohighlight">\(n\)</span> goes to infinity, it holds that the largest distance between the true and estimated latent positions satisfies:</p>
<div class="amsmath math notranslate nohighlight" id="equation-22c7c4e7-c821-41bd-8e14-a7ab82d42be4">
<span class="eqno">(12.1)<a class="headerlink" href="#equation-22c7c4e7-c821-41bd-8e14-a7ab82d42be4" title="Permalink to this equation">#</a></span>\[\begin{equation}
\max_{i\in[n]}\|\widehat{\vec{\mathbf x}}_i - \mathbf W\vec{\mathbf x}_i\| \leq \frac{Cd^{1/2}\log^2 n}{\sqrt{n}}.
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf W\in\mathbb R^{d\times d}\)</span>  is an orthogonal matrix (which depends on the specific values of <span class="math notranslate nohighlight">\(\widehat{\mathbf X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>) that accounts for the non-identifiability of the latent positions we mentioned before. In other words, this equation shows that the true and estimated latent positions (after a proper orthogonal rotation) are uniformly close to each other, and they get closer as <span class="math notranslate nohighlight">\(n\)</span> grows. We refer to them as <em>uniformly close</em> because the choice of a constant <span class="math notranslate nohighlight">\(C\)</span> holds <em>for all</em> <span class="math notranslate nohighlight">\(n\)</span> nodes. Note that the error rate also depends on the dimension of the model <span class="math notranslate nohighlight">\(d\)</span>, since as <span class="math notranslate nohighlight">\(d\)</span> gets larger, the dimension of the parameter space increases and the estimation error also increases.</p>
</section>
<section id="asymptotic-normality">
<h4><span class="section-number">12.2.2.1.2. </span>Asymptotic normality<a class="headerlink" href="#asymptotic-normality" title="Permalink to this heading">#</a></h4>
<p>A further result on the asymptotic properties of the ASE concerns to the distribution of the estimation error; that is, the difference between the estimated latent position <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf x}}_i\)</span> (for a given node <span class="math notranslate nohighlight">\(i\)</span>) and the true parameter <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i\)</span>. Because the estimator is consistent, this difference shrinks with <span class="math notranslate nohighlight">\(n\)</span> (after the proper orthogonal rotation), but knowing this fact is not enough if one desires to quantify their difference more carefully. This is important, as some statistical tasks, such as hypothesis testing or confidence interval estimation, require to know distributional properties of the estimation error.</p>
<p>Distributional results on the rows of the adjacency spectral embedding show that the error in estimating the true latent positions converge to a type of multivariate normal normal distribution as the size of the network grows. In particular, it has been shown that the
difference between <span class="math notranslate nohighlight">\(\widehat{\mathbf{X}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, after a proper orthogonal transformation <span class="math notranslate nohighlight">\(\mathbf{W}_n\)</span>, converge to a mixture of multivariate normal distributions. Formally, we can write the multivariate cumulative distribution function for any given vector <span class="math notranslate nohighlight">\(\vec{\mathbf{z}}\in\mathbb R^{d}\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-5d5da539-ee57-4c66-93bf-4086b9c5ae2a">
<span class="eqno">(12.2)<a class="headerlink" href="#equation-5d5da539-ee57-4c66-93bf-4086b9c5ae2a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathbb{P}\left(\sqrt{n}\left(\widehat{\vec{\mathbf x}}_i - \mathbf W_n \vec{\mathbf x}_i\right)\leq \vec{\mathbf{z}} \right) \approx \int_{\mathbb{R}^d}\Phi(\vec{\mathbf{z}}, \mathbf{\Sigma}(\vec{\mathbf{x}}))\  \text dF(\vec{\mathbf{x}}),
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(\cdot, \mathbf{\Sigma}(\vec{\mathbf{x}}))\)</span> is the cumulative distribution function of a <span class="math notranslate nohighlight">\(d\)</span>-dimensional multivariate normal distribution with mean zero and a covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\vec{\mathbf{x}})\in\mathbb R^{d\times d}\)</span>. It is worth noting that the integral on the right hand side operates over all possible values of <span class="math notranslate nohighlight">\(\vec{\mathbf{x}}\in\mathbb{R}^d\)</span>, and it is integrated with respect to the distribution of the latent positions, which are sample independently from <span class="math notranslate nohighlight">\(F\)</span>. The covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\vec{\mathbf x})\)</span> depends on a specific value of a latent position <span class="math notranslate nohighlight">\(\vec{\mathbf x}\in\mathbb{R}^d\)</span>, and is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-643601fd-bdc9-4dcd-b10b-afc20048f77f">
<span class="eqno">(12.3)<a class="headerlink" href="#equation-643601fd-bdc9-4dcd-b10b-afc20048f77f" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathbf{\Sigma}(\vec{\mathbf x}) = \mathbf{\Delta}^{-1}\mathbb{E}[\vec{\mathbf x}_1\vec{\mathbf x}_1^\top(\vec{\mathbf x}^\top\vec{\mathbf x}_1 - (\vec{\mathbf x}^\top\vec{\mathbf x}_1)^2) ]\mathbf{\Delta}^{-1}.
\end{equation}\]</div>
<p>For certain classes of distributions, it is possible to simplify this expression further to get a more specific form of the asymptotic distribution of <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf x}}_i\)</span>. We present an example later on this section.</p>
<p>The two results presented on this section constitute the basic properties of the adjacency spectral embedding under the RDPG model, but existing results are not limited to this particular method and model. Extensions of these results to a broader class of network models have been already developed (included the so-called <em>generalized random dot product graphs</em>). Analogous results also exist for other type of embedding methodologies. For instance, the Laplacian spectral embedding (LSE, from <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-numref">Section 5.3</span></a>) possess analogous properties to the ASE (namely, consistent estimates with known asymptotic distribution), although the particular form of this theorem is a bit more complicated. The study of different network models and embedding methods is still a research area under development, and new results keep coming every day.</p>
</section>
</section>
<section id="example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding">
<h3><span class="section-number">12.2.2.2. </span>Example: Erdős-Rényi graphs and the adjacency spectral embedding<a class="headerlink" href="#example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding" title="Permalink to this heading">#</a></h3>
<p>The ER model (from <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html#ch5-er"><span class="std std-numref">Section 4.1</span></a> and <a class="reference internal" href="../ch12/ers.html#app-ch12-ers"><span class="std std-numref">Section 11.4</span></a>) is the simplest example of a Random Dot Product Graph, in which all edge probabilities are the same. We will use this model to derive concrete expressions for the equations in the theory we have presented. Suppose that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network with a <span class="math notranslate nohighlight">\(ER_n(p)\)</span> distribution, for some <span class="math notranslate nohighlight">\(p\in(0,1)\)</span>. We can write the distribution of this network using a unidimensional latent position matrix <span class="math notranslate nohighlight">\(\vec{\mathbf x} = (\mathbf x_1, \ldots, \mathbf x_n)^\top\)</span>, with all entries equal to <span class="math notranslate nohighlight">\(\sqrt{p}\)</span>, since this will give</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Pr(\mathbf{A}_{ij}) = \mathbf{x}_i\mathbf{x}_j = p.
\end{align*}\]</div>
<p>Thus, the ER graph <span class="math notranslate nohighlight">\(ER_n(p)\)</span> can be thought as a RDPG with distribution of the latent positions <span class="math notranslate nohighlight">\(F=\delta_{\sqrt{p}}\)</span>, where <span class="math notranslate nohighlight">\(\delta_{\sqrt{p}}\)</span> is the Dirac delta distribution that only assigns positive probability to the mass point <span class="math notranslate nohighlight">\(\sqrt{p}\)</span>. It is worth noting that from this representation we can already notice a non-identifiability in the model, as both <span class="math notranslate nohighlight">\(\vec{\mathbf x}\)</span> and <span class="math notranslate nohighlight">\(-\vec{\mathbf x}\)</span> will result in the same edge probabilities. For practical purposes, it does not matter which of these two representations we recover.</p>
<p>The adjacency spectral embedding of the ER graph <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the unidimensional vector <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf x}} = \widehat{\lambda}\widehat{\vec{\mathbf v}}\)</span>, where  <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf v}}\in\mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(\widehat{\lambda}\in\mathbb{R}\)</span> are the leading eigenvector and eigenvalue of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. The adjacency spectral embedding of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is simply given by <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf x}} = ASE(\mathbf{A}) = \sqrt{\widehat{\lambda}}\widehat{\vec{\mathbf v}}\)</span>. According to the theory presented before, we have that the error in estimating <span class="math notranslate nohighlight">\(\vec{\mathbf{x}}\)</span> from <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf{x}}}\)</span> should decrease as the size of the network increases. In particular, we have that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\max_{i\in[n]}|\widehat{\mathbf x}_i - \sqrt{p}w_n| \leq \frac{C\log^2 n}{\sqrt{n}}.
\end{align*}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(w_n\)</span> plays the role of  a one-dimensional orthogonal matrix that accounts for the non-identifiability of the model; in other words, <span class="math notranslate nohighlight">\(w_n\)</span> is either equal to <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(-1\)</span>, which comes from the sign ambiguity of the singular vectors. Let’s see how this works out in practice. We’ll begin by simulating <span class="math notranslate nohighlight">\(50\)</span> networks as-specified above, for networks with increasing numbers of nodes from around <span class="math notranslate nohighlight">\(30\)</span> to around <span class="math notranslate nohighlight">\(3000\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># set the probability ahead of time</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># run the simulations for each number of nodes</span>
<span class="n">As</span> <span class="o">=</span> <span class="p">[[</span><span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ns</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<p>Next, we spectrally embed each network into a single dimension, and then check to make sure the signs of the estimated latent positions and the true latent positions align due to the orthogonality issue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>
<span class="c1"># instantiate an ASE instance with 1 embedding dimension</span>
<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># check for sign alignment, and flip the signs if Xhat and X disagree</span>
<span class="k">def</span> <span class="nf">orthogonal_align</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">Xhat</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">Xhat</span> <span class="o">=</span> <span class="o">-</span><span class="n">Xhat</span>
    <span class="k">return</span> <span class="n">Xhat</span>

<span class="c1"># compute the estimated latent positions and realign in one step</span>
<span class="n">Xhats_aligned</span> <span class="o">=</span> <span class="p">[[</span><span class="n">orthogonal_align</span><span class="p">(</span><span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">))</span> <span class="k">for</span> <span class="n">A</span> <span class="ow">in</span> <span class="n">An</span><span class="p">]</span> <span class="k">for</span> <span class="n">An</span> <span class="ow">in</span> <span class="n">As</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we compute the maximum difference between the estimated latent positions (after alignment) and <span class="math notranslate nohighlight">\(\sqrt{p}\)</span> for each network. We divide by <span class="math notranslate nohighlight">\(\log^2(n)\)</span> and multiply by <span class="math notranslate nohighlight">\(\sqrt{n}\)</span>, and then plot the result as the average (solid line) and <span class="math notranslate nohighlight">\(95\%\)</span> probability interval (shaded ribbon) for a given value of <span class="math notranslate nohighlight">\(n\)</span>. If there exists a constant value <span class="math notranslate nohighlight">\(C\)</span> where as <span class="math notranslate nohighlight">\(n\)</span> grows the values are consistently under this constant with increasing probability, we have demonstrated the desired result empirically:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">network_numbers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)])</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ns</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">node_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)])</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ns</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">node_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)])</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ns</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">Xhats_aligned_flat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">Xhat</span> <span class="k">for</span> <span class="n">Xhat</span> <span class="ow">in</span> <span class="n">Xhats_n</span><span class="p">])</span> <span class="k">for</span> <span class="n">Xhats_n</span> <span class="ow">in</span> <span class="n">Xhats_aligned</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Xhat&quot;</span><span class="p">:</span> <span class="n">Xhats_aligned_flat</span><span class="p">,</span> <span class="s2">&quot;i&quot;</span> <span class="p">:</span> <span class="n">node_indices</span><span class="p">,</span> <span class="s2">&quot;n&quot;</span> <span class="p">:</span> <span class="n">node_counts</span><span class="p">,</span>
                   <span class="s2">&quot;j&quot;</span> <span class="p">:</span> <span class="n">network_numbers</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p</span><span class="p">)})</span>

<span class="n">df</span><span class="p">[</span><span class="s2">&quot;abs_diff&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Xhat&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;X&quot;</span><span class="p">])</span>
<span class="n">max_pernet</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="s2">&quot;j&quot;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span>
    <span class="s2">&quot;abs_diff&quot;</span><span class="p">:</span> <span class="s2">&quot;max&quot;</span>
<span class="p">})</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>

<span class="n">max_pernet</span><span class="p">[</span><span class="s2">&quot;norm_factor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">max_pernet</span><span class="p">[</span><span class="s2">&quot;n&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">max_pernet</span><span class="p">[</span><span class="s2">&quot;n&quot;</span><span class="p">])</span>
<span class="n">max_pernet</span><span class="p">[</span><span class="s2">&quot;norm_diff&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_pernet</span><span class="p">[</span><span class="s2">&quot;abs_diff&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">max_pernet</span><span class="p">[</span><span class="s2">&quot;norm_factor&quot;</span><span class="p">]</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">max_pernet</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;norm_diff&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of Nodes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$ </span><span class="se">\\</span><span class="s2">frac{</span><span class="se">\\</span><span class="s2">sqrt</span><span class="si">{n}</span><span class="s2">\,\, max_i |\hat x_i - \sqrt</span><span class="si">{p}</span><span class="s2"> w_n|}{\log^2(n)}$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/9934c5f7e9a6d871fa529b56c595e81a649d41ea590ae5ea6013f6c6722dcf47.png" src="../../_images/9934c5f7e9a6d871fa529b56c595e81a649d41ea590ae5ea6013f6c6722dcf47.png" />
</div>
</div>
<p>As we can see, this value drops off pretty rapidly as we increase the number of nodes. Therefore, we could choose basically <em>any</em> constant attained by this curve (such as, for instance, <span class="math notranslate nohighlight">\(0.10\)</span>), and the above plot demonstrates that empirically, as the number of nodes approaches infinity, the probability that the <span class="math notranslate nohighlight">\(y\)</span>-axis values are less than our choice of a constant tends towards <span class="math notranslate nohighlight">\(1\)</span>, as desired. Pretty neat, right?</p>
<p>Similarly, we can also obtain an expression for the asymptotic distribution of the difference between the true and estimated positions. First, notice that since <span class="math notranslate nohighlight">\(F\)</span> has only one point mass at <span class="math notranslate nohighlight">\(\sqrt{p}\)</span>, we can evaluate integrals and expections with respect to this distribution, resulting in</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\int_{\mathbb{R}^d}\Phi(\mathbf{z}, \mathbf{\Sigma}(\mathbf{x}))\,\text dF(\mathbf{x}) = \Phi(\mathbf{z}, \mathbf{\Sigma}(\sqrt{p}).
\end{align*}\]</div>
<p>The exact form of the covariance term <span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\mathbf{x})\)</span> can be obtained from the second moment matrix <span class="math notranslate nohighlight">\(\mathbf{\Delta} = \mathbb{E}[\mathbf{x}_1^2] = p\)</span>, and it is given by:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{\Sigma}(\mathbf{x}) = \frac{1}{p^2}\mathbb{E}[p(\mathbf{x}\sqrt{p} - (\mathbf{x}\sqrt{p})^2)].
\end{align*}\]</div>
<p>Combining these results, we get that the limiting distribution of the difference between <span class="math notranslate nohighlight">\(\widehat{\mathbf{x}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> satisfies</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sqrt{n}\left(\widehat{\mathbf{x}}_i - \sqrt{p}\right)\ \rightarrow\  N(0, 1-p)\quad\quad\text{for each node }i=1, \ldots, n.
\end{align*}\]</div>
<p>The asymptotic normality result is illustrated in the simulation setting below. In particular, observe that, as the size of the network <span class="math notranslate nohighlight">\(n\)</span> increases, the distribution of the entries of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> resembles the normal distribution. We’ll repeat this for one network from each number of nodes to see this result, by showing a histogram of the limiting factors <span class="math notranslate nohighlight">\(\sqrt{n}(\hat x_i - \sqrt{p})\)</span> in our realized networks for all <span class="math notranslate nohighlight">\(n\)</span> nodes. We will plot it against the pdf for an appropriate normal distribution with mean <span class="math notranslate nohighlight">\(0\)</span> and standard deviation <span class="math notranslate nohighlight">\(1 - p\)</span> in red:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_reduced</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;j&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># check what happens for the first network from each set</span>
<span class="n">df_reduced</span> <span class="o">=</span> <span class="n">df_reduced</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># isolate the factor sqrt(n)*(xhat_i - sqrt(p)) that we want the limiting distribution of</span>
<span class="n">df_reduced</span><span class="p">[</span><span class="s2">&quot;limiting_factor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_reduced</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">Xhat</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">X</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># np.sqrt(df_reduced[&quot;n&quot;]) * (df_reduced[&quot;Xhat&quot;] - df_reduced[&quot;X&quot;])</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">df_reduced</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;n&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">,</span> <span class="s2">&quot;limiting_factor&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">)</span>
<span class="n">truth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;x&quot;</span> <span class="p">:</span>  <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)})</span>
<span class="n">truth</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">truth</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">))</span>

<span class="n">axes</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">axes</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">truth</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_axis_labels</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt</span><span class="si">{n}</span><span class="s2">(</span><span class="se">\\</span><span class="s2">hat x_i - \sqrt</span><span class="si">{p}</span><span class="s2">)$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/seaborn/axisgrid.py:703: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.
  plot_args = [v for k, v in plot_data.iteritems()]
/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/seaborn/axisgrid.py:703: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.
  plot_args = [v for k, v in plot_data.iteritems()]
/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/seaborn/axisgrid.py:703: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.
  plot_args = [v for k, v in plot_data.iteritems()]
/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/seaborn/axisgrid.py:703: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.
  plot_args = [v for k, v in plot_data.iteritems()]
/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/seaborn/axisgrid.py:703: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.
  plot_args = [v for k, v in plot_data.iteritems()]
</pre></div>
</div>
<img alt="../../_images/80b42611f66295c74644ec74ebd73496665cc4ec1757d76be4b3ed1c9ef7aac0.png" src="../../_images/80b42611f66295c74644ec74ebd73496665cc4ec1757d76be4b3ed1c9ef7aac0.png" />
</div>
</div>
<p>As <span class="math notranslate nohighlight">\(n\)</span> grows, the limiting distribution of <span class="math notranslate nohighlight">\(\sqrt n(\hat x_i - \sqrt{p})\)</span> clearly approaches the desired normal distribution.</p>
</section>
<section id="application-two-graph-hypothesis-testing">
<h3><span class="section-number">12.2.2.3. </span>Application: two-graph hypothesis testing<a class="headerlink" href="#application-two-graph-hypothesis-testing" title="Permalink to this heading">#</a></h3>
<p>The results previously discussed demonstrate that the true and estimated latent positions are close to each other, and in fact, their distance gets smaller as <span class="math notranslate nohighlight">\(n\)</span> increases. As such, the ASE provides an accurate estimator of the latent positions. This result justifies the use of <span class="math notranslate nohighlight">\(\mathbf {\hat X}\)</span> in place of <span class="math notranslate nohighlight">\(\mathbf X\)</span> for subsequent inference tasks, such as community detection, vertex nomination, or classification (see Sections <a class="reference internal" href="../../applications/ch7/community-detection.html#ch7-comm-detect"><span class="std std-numref">Section 6.1</span></a>, <a class="reference internal" href="../../applications/ch7/vertex-nomination.html#ch7-vn"><span class="std std-numref">Section 6.4</span></a>, or <a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html#ch8-gm"><span class="std std-numref">Section 7.3</span></a> for a more thorough discussion on these topics). The theoretical results for the ASE have multiple implications. One of those  is that the estimated latent positions carry almost the same information as the true latent positions, and we can even quantify how different they are. This is particularly useful for performing statistical inference tasks about node properties. Here we consider one of these tasks: two-graph hypothesis testing <span id="id5">[<a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html#id69" title="Minh Tang, Avanti Athreya, Daniel L. Sussman, Vince Lyzinski, Youngser Park, and Carey E. Priebe. A Semiparametric Two-Sample Hypothesis Testing Problem for Random Graphs. J. Comput. Graph. Stat., 26(2):344–354, April 2017. doi:10.1080/10618600.2016.1193505.">2</a>]</span>.</p>
<p>Comparing the distribution of two populations is a frequent problem in statistics and across multiple domains. In classical statistics, a typical strategy to perform this task is to compare the mean of two populations by using an appropriate test statistic. Theoretical results on the distribution of this statistic (either exact or asymptotic) are then used to derive a measure of uncertainty for this problem (such as p-values or confidence intervals). Similarly, when comparing two observed graphs, we may wonder whether they were generated by the same mechanism. The results discussed before have been used to develop valid statistical tests for two-network hypothesis testing questions.</p>
<p>A network hypothesis test for the equivalence between the latent positions of the vertices of a pair of networks with aligned vertices can be constructed by using the estimates of the latent positions. Formally, let <span class="math notranslate nohighlight">\(X, Y\)</span> be the latent position matrices, and define
<span class="math notranslate nohighlight">\(\mathbf A\sim RDPG(X)\)</span>, <span class="math notranslate nohighlight">\(\mathbf B\sim RDPG(Y)\)</span> as independent random adjacency matrices. We can test whether the two networks have the same distribution by comparing their latent positions via a hypothesis test of the form</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{H}_0:X =_{W} Y\quad\quad\quad \text{ vs.}\quad\quad\quad \mathcal{H}_a:X\neq_{W} Y,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf X =_{W}Y\)</span> denotes that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are equivalent up to an orthogonal transformation <span class="math notranslate nohighlight">\(W\in\mathcal{O}_d\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{O}_d\)</span> is the set of <span class="math notranslate nohighlight">\(d\times d\)</span> orthogonal matrices. Since we do not have access to the true latent positions, we can use the estimates <span class="math notranslate nohighlight">\(\widehat{X}\)</span> and <span class="math notranslate nohighlight">\(\widehat{Y}\)</span> to construct a test statsistic. This test statistic is defined as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf t : = \frac{\min_{W\in\mathcal{O}_d} \|\widehat{X}W - \widehat{Y}\|_F}{\sqrt{d\gamma^{-1}(\mathbf A)} + \sqrt{d\gamma^{-1}(\mathbf B)}}.
\end{align*}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\|\widehat{X}W - \widehat{Y}\|_F\)</span> is the Frobenius distance between the estimated latent positions (after adjusting for the orthogonal non-identifiability).
This distance compares how similar the two latent positions are, and thus, it is natural to think that larger values of this distance will give more evidence agains the null hypothesis. In addition to this, the test statistic incorporates a normalizing constant of the form <span class="math notranslate nohighlight">\(\sqrt{d\gamma^{-1}(\mathbf A)} + \sqrt{d\gamma^{-1}(\mathbf B)}\)</span>. Here <span class="math notranslate nohighlight">\(\sigma_1(\mathbf A) \geq \ldots\geq \sigma_n(\mathbf A)\geq 0\)</span> denote the singular values of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (similarly for <span class="math notranslate nohighlight">\(\mathbf B\)</span>),  <span class="math notranslate nohighlight">\(\delta(\mathbf A) = \max_{i\in[n]}\sum_{j=1}^n\mathbf A_{ij}\)</span> denotes the largest observed degree of the graph, and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\gamma(\mathbf A):=\frac{\sigma_d(\mathbf A) - \sigma_{d+1}(\mathbf A)}{\delta(\mathbf A)}
\end{align*}\]</div>
<p>is a constant that standardizes the test statistic. It can be shown that, under appropriate regularity conditions that this test statistic will go to zero as <span class="math notranslate nohighlight">\(n\)</span> goes to infinity under the null hypothesis, and will diverge with <span class="math notranslate nohighlight">\(n\)</span> for some specific alternatives. Thus, <span class="math notranslate nohighlight">\(\mathbf t\)</span> provides a way to construct a consistent test for the hypothesis testing problem described above.</p>
</section>
</section>
<section id="theory-for-multiple-network-models">
<h2><span class="section-number">12.2.3. </span>Theory for multiple network models<a class="headerlink" href="#theory-for-multiple-network-models" title="Permalink to this heading">#</a></h2>
<p>Models for multiple network data often assume that there is a known one-to-one correspondence between the vertices of the graphs. If this correspondence is unknown, an estimate can be obtained via graph matching, which you learned about in <a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html#ch8-gm"><span class="std std-numref">Section 7.3</span></a>. Once the vertices are correctly matched, models for multiple networks exploit the shared structure across the graphs to obtain accurate estimates. In this section we review the theoretical challenges on these circumstances. You can learn more about the problem space across a bevy of excellent academic survey papers such as <span id="id6">[<a class="reference internal" href="#id33" title="D. Conte, P. Foggia, C. Sansone, and M. Vento. THIRTY YEARS OF GRAPH MATCHING IN PATTERN RECOGNITION. Int. J. Pattern Recognit. Artif. Intell., 18(03):265–298, May 2004. doi:10.1142/S0218001404003228.">8</a>]</span>, <span id="id7">[<a class="reference internal" href="#id34" title="Pasquale Foggia, Gennaro Percannella, and Mario Vento. GRAPH MATCHING AND LEARNING IN PATTERN RECOGNITION IN THE LAST 10 YEARS. Int. J. Pattern Recognit. Artif. Intell., 28(01):1450001, October 2013. doi:10.1142/S0218001414500013.">9</a>]</span>, or <span id="id8">[<a class="reference internal" href="#id35" title="Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng, Hongyuan Zha, and Xiaokang Yang. A Short Survey of Recent Advances in Graph Matching. In ICMR '16: Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval, pages 167–174. Association for Computing Machinery, New York, NY, USA, June 2016. doi:10.1145/2911996.2912035.">10</a>]</span> for more details.</p>
<section id="graph-matching-for-correlated-networks">
<h3><span class="section-number">12.2.3.1. </span>Graph Matching for Correlated Networks<a class="headerlink" href="#graph-matching-for-correlated-networks" title="Permalink to this heading">#</a></h3>
<p>Given a pair of network adjacency matrices <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\mathbf B\)</span> with <span class="math notranslate nohighlight">\(n\)</span> vertices each (but possibly permuted), the graph matching problem seeks to <em>align</em> the graphs by identifying the correct correspondence between their vertices. This can be done by identifying a permutation matrix <span class="math notranslate nohighlight">\(\mathbf P\)</span> of size <span class="math notranslate nohighlight">\(n\times n\)</span> that makes the graphs <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\mathbf P^\top \mathbf B \mathbf P\)</span> similar.
There are many methods to perform graph matching, and here we focus on the following optimization problem (which is reviewed more carefully in Section <a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html#ch8-gm"><span class="std std-numref">Section 7.3</span></a>):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\widehat{\mathbf P} =\text{argmin}_P \|\mathbf A - \mathbf P^\top \mathbf B \mathbf P\|_F^2.
\end{equation*}\]</div>
<p>The accuracy in matching the vertices across the graphs depends on this estimated permutation matrix <span class="math notranslate nohighlight">\(\widehat{\mathbf P}\)</span>. Denote by <span class="math notranslate nohighlight">\(\mathbf P^\ast\)</span> to the true permutation matrix that aligns the two graphs, which is the object we would like to estimate. In principle, if there is a unique solution that gives <span class="math notranslate nohighlight">\(\mathbf A = (\mathbf P^\ast)^\top \mathbf B \mathbf P^\ast\)</span>, the two networks are isomorphic (i.e., equal up to some  permutation of the vertices), and matching the vertices across the networks is possible. However, in the presence of  noise (which is typically the case), the graphs are not isomorphic, and hence the solution to the optimization problem above becomes more relevant.</p>
<p>A body of literature has studied the feasibility of finding the correct matching under different random network models, including correlated Erdös-Rényi and Bernoulli networks. In this section we review some of the results for the correlated Erdös-Rényi (ER) model described in <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html#ch5-er"><span class="std std-numref">Section 4.1</span></a> and developed in <span id="id9">[<a class="reference internal" href="../../representations/ch5/multi-network-models.html#id30" title="Vince Lyzinski, Donniell E. Fishkind, and Carey E. Priebe. Seeded graph matching for correlated Erdös-Rényi graphs. J. Mach. Learn. Res., 15(1):3513–3540, January 2014. doi:10.5555/2627435.2750357.">4</a>]</span>.</p>
<p>The correlated ER model has only two parameters, namely, the edge density and the correlation across a pair of graphs. This two parameters are crucial in understanding the feasibility of solving the graph matching problem. On the one hand, the edge density controls the amount of information that is present on each graph. If the density is very close to zero, the information available to match the graphs is low, which makes the problem harder. On the other hand, the correlation paraameter controls the level of similarity across the graphs. A large correlation value facilitates matching the graphs, as the edges exhibit similar or exactly the same patterns in both graphs.  Formally, given parameters <span class="math notranslate nohighlight">\(q \in(0, 1)\)</span> and <span class="math notranslate nohighlight">\(\rho\in[0,1]\)</span>, the <span class="math notranslate nohighlight">\(n\times n\)</span> adjacency matrices <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\mathbf B\)</span> are distributed as correlated ER random graphs if each graph is marginally distributed as an ER of the form <span class="math notranslate nohighlight">\(\mathbf A\sim ER_n(q_n)\)</span>, <span class="math notranslate nohighlight">\(\mathbf B\sim ER_n(q_n)\)</span>, but the edge pairs satisfy <span class="math notranslate nohighlight">\(\text{Corr}(\mathbf A_{ij},\mathbf B_{ij})=\rho\)</span>. In particular, if <span class="math notranslate nohighlight">\(\rho=0\)</span>, then the graphs are just independent realizations of an ER network but there is no common structure between them. On the contrary, if <span class="math notranslate nohighlight">\(\rho=1\)</span>, then the graphs are isomorphic.</p>
<p>Having defined the model, the next question is whether the solution of the graph matching optimization problem (defined in <a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html#ch8-gm"><span class="std std-numref">Section 7.3</span></a>) recover the correct solution. In the correlated ER model, it can be shown that this is possible when the correlation between the networks and the edge probability that are not too small. Formally, the conditions require that <span class="math notranslate nohighlight">\(\rho\geq c_1\sqrt{\frac{\log n}{n}}\)</span> and <span class="math notranslate nohighlight">\(q\geq c_2 \frac{\log n }{n}\)</span>  (here, <span class="math notranslate nohighlight">\(c_1\)</span> and <span class="math notranslate nohighlight">\(c_2\)</span> are just some positive fixed constants). This conditions guarantee that the correlation across the graphs is large enough, so there is enough shared information, while the edge density needs to be sufficiently away from zero and one, to ensure that there are enough edges within each graph. Under these conditions, it can be shown that, with high probability, the correct solution of the graph matching problem is obtained if <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large.</p>
</section>
<section id="joint-spectral-embeddings">
<h3><span class="section-number">12.2.3.2. </span>Joint spectral embeddings<a class="headerlink" href="#joint-spectral-embeddings" title="Permalink to this heading">#</a></h3>
</section>
<section id="omnibus-embedding-omni">
<h3><span class="section-number">12.2.3.3. </span>Omnibus Embedding (omni)<a class="headerlink" href="#omnibus-embedding-omni" title="Permalink to this heading">#</a></h3>
<p>The omnibus embedding described in <a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html#ch6-multinet"><span class="std std-numref">Section 5.4</span></a> jointly estimates the latent positions under the joint random dot product network (<span class="math notranslate nohighlight">\(JRDPG\)</span>) model, which is discussed in <a class="reference internal" href="../../representations/ch5/multi-network-models.html#ch5-multi"><span class="std std-numref">Section 4.6</span></a>. Briefly, the model is <span class="math notranslate nohighlight">\((\mathbf A^{(1)}, \ldots, \mathbf A^{(m)})\sim JRDPG(\mathbf X_n)\)</span>, and the rows of <span class="math notranslate nohighlight">\(\mathbf X_n\in\mathbb R^{n\times d}\)</span> are an i.i.d. sample from some distribution <span class="math notranslate nohighlight">\(F\)</span>. Let <span class="math notranslate nohighlight">\(\widehat{\mathbf{O}}\in\mathbb R^{mn\times mn}\)</span> be the omnibus embedding of <span class="math notranslate nohighlight">\(\mathbf A^{(1)}, \ldots, \mathbf A^{(m)}\)</span> and <span class="math notranslate nohighlight">\(\widehat{\mathbf Z} = ASE(\mathbf{O})\in\mathbb R^{mn\times d}\)</span>.
Under this setting, it can be shown that the rows of <span class="math notranslate nohighlight">\(\widehat{\mathbf Z}_n\)</span> are a consistent estimator of the latent positions of each individual network  as <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>, and that:</p>
<div class="amsmath math notranslate nohighlight" id="equation-770e959c-8ab4-46de-a3cd-7859a1a83c69">
<span class="eqno">(12.4)<a class="headerlink" href="#equation-770e959c-8ab4-46de-a3cd-7859a1a83c69" title="Permalink to this equation">#</a></span>\[\begin{equation}
\max_{i\in[n],j\in[m]}\|(\widehat{\mathbf Z}_n)_{(j-1)n + i} - \mathbf W_n(\mathbf X_n)_{i}\| \leq \frac{C\sqrt{m}\log(mn)}{\sqrt{n}}. \label{eq:OMNI-consistency}    
\end{equation}\]</div>
<p>Furthermore, a central limit theorem for the rows of the omnibus embedding  asserts that:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d1fa9b6-3d42-4f79-a47b-5140da288f15">
<span class="eqno">(12.5)<a class="headerlink" href="#equation-8d1fa9b6-3d42-4f79-a47b-5140da288f15" title="Permalink to this equation">#</a></span>\[\begin{equation}
\lim_{n\rightarrow\infty} \mathbb{P}\left\{\sqrt{n}\left(\mathbf W_n(\widehat{\mathbf Z}_n)_{(j-1)n + i} - (\mathbf X_n)_i\right)\leq \mathbf{z}\right\}  = \int_{\mathcal{X}}\Phi(\mathbf{z}, \widehat{\mathbf{\Sigma}}(\mathbf{x}))\  dF(\mathbf{x}),\label{eq:thm-OMNI-CLT}
\end{equation}\]</div>
<p>for some covariance matrix <span class="math notranslate nohighlight">\(\widehat{\Sigma}(\mathbf{x})\)</span>. For more details, check out the original paper at <span id="id10">[<a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html#id30" title="Keith Levin, A. Athreya, M. Tang, V. Lyzinski, and C. Priebe. A Central Limit Theorem for an Omnibus Embedding of Multiple Random Dot Product Graphs. 2017 IEEE International Conference on Data Mining Workshops (ICDMW), 2017. URL: https://www.semanticscholar.org/paper/A-Central-Limit-Theorem-for-an-Omnibus-Embedding-of-Levin-Athreya/fdc658ee1a25c511d7da405a9df7b30b613e8dc8.">2</a>]</span>.</p>
</section>
<section id="multiple-adjacency-spectral-embedding-mase">
<h3><span class="section-number">12.2.3.4. </span>Multiple adjacency spectral embedding (MASE)<a class="headerlink" href="#multiple-adjacency-spectral-embedding-mase" title="Permalink to this heading">#</a></h3>
<p>The <span class="math notranslate nohighlight">\(COSIE\)</span> model described in <a class="reference internal" href="../../representations/ch5/multi-network-models.html#ch5-multi"><span class="std std-numref">Section 4.6</span></a> gives a joint model that characterizes the distribution of multiple networks with expected probability matrices that share the same common invariant subspace. The <span class="math notranslate nohighlight">\(MASE\)</span> algorithm in <a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html#ch6-multinet"><span class="std std-numref">Section 5.4</span></a> is a consistent estimator for this common invariant subspace, and results in asymptotically normally estimators for the individual symmetric matrices. Specifically, let <span class="math notranslate nohighlight">\(\mathbf V_n\in\mathbb R^{n\times d}\)</span> be
a sequence of orthonormal matrices and <span class="math notranslate nohighlight">\(\mathbf R^{(1)}_n, \ldots, \mathbf R^{(m)}_n\in\mathbb R^{d\times d}\)</span> a sequence of score matrices such that <span class="math notranslate nohighlight">\(\mathbf{P}^{(l)}_n=\mathbf V_n\mathbf R^{(l)}_n\mathbf V_n^\top\in[0,1]^{n\times n} \)</span>, <span class="math notranslate nohighlight">\((\mathbf A_n^{(1)}, \ldots, \mathbf A_n^{(m)})\sim COSIE(\mathbf V_n;, \mathbf R^{(1)}_n, \ldots, \mathbf R^{(m)}_n)\)</span>, and <span class="math notranslate nohighlight">\(\widehat{\mathbf V}, \widehat{\mathbf R}^{(1)}_n, \ldots, \widehat{\mathbf R}^{(1)}_n\)</span> be the estimators obtained by <span class="math notranslate nohighlight">\(MASE\)</span>. Under appropriate regularity conditions, the estimate for <span class="math notranslate nohighlight">\(\mathbf V\)</span> is consistent as <span class="math notranslate nohighlight">\(n,m\rightarrow\infty\)</span>, and there exists some constant <span class="math notranslate nohighlight">\(C&gt;0\)</span> such that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}\left[\min_{\mathbf W\in\mathcal{O}_d}\|\widehat{\mathbf V}-\mathbf V\mathbf W\|_F\right] \leq C\left(\sqrt{\frac{1}{mn}} + {\frac{1}{n}}\right). \label{eq:theorem-bound}
\end{align*}\]</div>
<p>In addition, the entries of <span class="math notranslate nohighlight">\(\widehat{\mathbf{R}}^{(l)}_n\)</span>, <span class="math notranslate nohighlight">\(l\in[m]\)</span> are asymptotically normally distributed. Namely, there exists a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(\mathbf W\)</span> such that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{1}{\sigma_{l,j,k}}\left(\widehat{\mathbf R}^{(l)}_n - \mathbf W^\top\mathbf R^{(l)}_n\mathbf W + \mathbf H_m^{(l)}\right)_{jk} \overset{d}{\rightarrow} \mathcal{N}(0, 1),
\end{align*}\]</div>
<p>as <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>, where:
<span class="math notranslate nohighlight">\(\mathbb{E}[\|\mathbf H_m^{(l)}\|]=O\left(\frac{d}{\sqrt{m}}\right)\)</span> and <span class="math notranslate nohighlight">\(\sigma^2_{l,j,k} = O(1)\)</span>.
For more details about the <span class="math notranslate nohighlight">\(MASE\)</span> algorithm, check out <span id="id11">[<a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html#id31" title="Jesús Arroyo, Avanti Athreya, Joshua Cape, Guodong Chen, Carey E. Priebe, and Joshua T. Vogelstein. Inference for Multiple Heterogeneous Networks with a Common Invariant Subspace. Journal of Machine Learning Research, 22(142):1–49, 2021. URL: https://jmlr.org/papers/v22/19-558.html.">3</a>]</span>.</p>
</section>
</section>
<section id="references">
<h2><span class="section-number">12.2.4. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<div class="docutils container" id="id12">
<div class="citation" id="id59" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Avanti Athreya, Donniell E. Fishkind, Minh Tang, Carey E. Priebe, Youngser Park, Joshua T. Vogelstein, Keith Levin, Vince Lyzinski, and Yichen Qin. Statistical inference on random dot product graphs: a survey. <em>J. Mach. Learn. Res.</em>, 18(1):8393–8484, January 2017. <a class="reference external" href="https://doi.org/10.5555/3122009.3242083">doi:10.5555/3122009.3242083</a>.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">2</a><span class="fn-bracket">]</span></span>
<p>Keith Levin, A. Athreya, M. Tang, V. Lyzinski, and C. Priebe. A Central Limit Theorem for an Omnibus Embedding of Multiple Random Dot Product Graphs. <em>2017 IEEE International Conference on Data Mining Workshops (ICDMW)</em>, 2017. URL: <a class="reference external" href="https://www.semanticscholar.org/paper/A-Central-Limit-Theorem-for-an-Omnibus-Embedding-of-Levin-Athreya/fdc658ee1a25c511d7da405a9df7b30b613e8dc8">https://www.semanticscholar.org/paper/A-Central-Limit-Theorem-for-an-Omnibus-Embedding-of-Levin-Athreya/fdc658ee1a25c511d7da405a9df7b30b613e8dc8</a>.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">3</a><span class="fn-bracket">]</span></span>
<p>Jesús Arroyo, Avanti Athreya, Joshua Cape, Guodong Chen, Carey E. Priebe, and Joshua T. Vogelstein. Inference for Multiple Heterogeneous Networks with a Common Invariant Subspace. <em>Journal of Machine Learning Research</em>, 22(142):1–49, 2021. URL: <a class="reference external" href="https://jmlr.org/papers/v22/19-558.html">https://jmlr.org/papers/v22/19-558.html</a>.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">4</a><span class="fn-bracket">]</span></span>
<p>Vince Lyzinski, Donniell E. Fishkind, and Carey E. Priebe. Seeded graph matching for correlated Erdös-Rényi graphs. <em>J. Mach. Learn. Res.</em>, 15(1):3513–3540, January 2014. <a class="reference external" href="https://doi.org/10.5555/2627435.2750357">doi:10.5555/2627435.2750357</a>.</p>
</div>
<div class="citation" id="id76" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Minh Tang, Avanti Athreya, Daniel L. Sussman, Vince Lyzinski, Youngser Park, and Carey E. Priebe. A Semiparametric Two-Sample Hypothesis Testing Problem for Random Graphs. <em>J. Comput. Graph. Stat.</em>, 26(2):344–354, April 2017. <a class="reference external" href="https://doi.org/10.1080/10618600.2016.1193505">doi:10.1080/10618600.2016.1193505</a>.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">6</a><span class="fn-bracket">]</span></span>
<p>Roman Vershynin. <em>High-Dimensional Probability: An Introduction with Applications in Data Science</em>. Cambridge University Press, Cambridge, England, UK, September 2018. ISBN 978-1-10823159-6. <a class="reference external" href="https://doi.org/10.1017/9781108231596">doi:10.1017/9781108231596</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">7</a><span class="fn-bracket">]</span></span>
<p>Daniel L. Sussman, Minh Tang, Donniell E. Fishkind, and Carey E. Priebe. A Consistent Adjacency Spectral Embedding for Stochastic Blockmodel Graphs. <em>J. Am. Stat. Assoc.</em>, 107(499):1119–1128, September 2012. <a class="reference external" href="https://doi.org/10.1080/01621459.2012.699795">doi:10.1080/01621459.2012.699795</a>.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">8</a><span class="fn-bracket">]</span></span>
<p>D. Conte, P. Foggia, C. Sansone, and M. Vento. THIRTY YEARS OF GRAPH MATCHING IN PATTERN RECOGNITION. <em>Int. J. Pattern Recognit. Artif. Intell.</em>, 18(03):265–298, May 2004. <a class="reference external" href="https://doi.org/10.1142/S0218001404003228">doi:10.1142/S0218001404003228</a>.</p>
</div>
<div class="citation" id="id34" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">9</a><span class="fn-bracket">]</span></span>
<p>Pasquale Foggia, Gennaro Percannella, and Mario Vento. GRAPH MATCHING AND LEARNING IN PATTERN RECOGNITION IN THE LAST 10 YEARS. <em>Int. J. Pattern Recognit. Artif. Intell.</em>, 28(01):1450001, October 2013. <a class="reference external" href="https://doi.org/10.1142/S0218001414500013">doi:10.1142/S0218001414500013</a>.</p>
</div>
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">10</a><span class="fn-bracket">]</span></span>
<p>Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng, Hongyuan Zha, and Xiaokang Yang. A Short Survey of Recent Advances in Graph Matching. In <em>ICMR '16: Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</em>, pages 167–174. Association for Computing Machinery, New York, NY, USA, June 2016. <a class="reference external" href="https://doi.org/10.1145/2911996.2912035">doi:10.1145/2911996.2912035</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./appendix/ch13"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="mle-theory.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12.1. </span>Maximum Likelihood Estimate Theory</p>
      </div>
    </a>
    <a class="right-next"
       href="../ch14/ch14.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Applications (Extended)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#disclaimer-about-classical-statistical-asymptotic-theory">12.2.1. Disclaimer about classical statistical asymptotic theory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjacency-spectral-embedding">12.2.2. Adjacency spectral embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-error-of-the-adjacency-spectral-embedding">12.2.2.1. Statistical error of the adjacency spectral embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency">12.2.2.1.1. Consistency</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#asymptotic-normality">12.2.2.1.2. Asymptotic normality</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding">12.2.2.2. Example: Erdős-Rényi graphs and the adjacency spectral embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-two-graph-hypothesis-testing">12.2.2.3. Application: two-graph hypothesis testing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theory-for-multiple-network-models">12.2.3. Theory for multiple network models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-matching-for-correlated-networks">12.2.3.1. Graph Matching for Correlated Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-spectral-embeddings">12.2.3.2. Joint spectral embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#omnibus-embedding-omni">12.2.3.3. Omnibus Embedding (omni)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-adjacency-spectral-embedding-mase">12.2.3.4. Multiple adjacency spectral embedding (MASE)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">12.2.4. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>