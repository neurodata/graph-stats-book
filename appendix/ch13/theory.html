
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Spectral Method Theory &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What is network machine learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why do we study networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">
     1.3. Types of Network Machine Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.4. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">
     1.5. Challenges of Network Machine Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.4. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.5. Fine-Tune your Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.6. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch4/ch4.html">
   3. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/matrix-representations.html">
     3.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/properties-of-networks.html">
     3.2. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/network-representations.html">
     3.3. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/regularization.html">
     3.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch5/ch5.html">
   4. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">
     4.1. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">
     4.2. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">
     4.3. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">
     4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_SIEM.html">
     4.5. Structured Independent Edge Model (SIEM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/multi-network-models.html">
     4.6. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/models-with-covariates.html">
     4.7. Network Models with Network Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch6/ch6.html">
   5. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">
     5.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/why-embed-networks.html">
     5.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/spectral-embedding.html">
     5.3. Spectral embedding methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">
     5.4. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">
     5.5. Joint Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch7/ch7.html">
   6. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/community-detection.html">
     6.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/testing-differences.html">
     6.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/model-selection.html">
     6.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/vertex-nomination.html">
     6.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/out-of-sample.html">
     6.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   7. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html">
     7.1. Latent Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/significant-communities.html">
     7.2. Two-sample hypothesis testing in SBMs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html">
     7.3. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/multiple-vertex-nomination.html">
     7.4. Vertex Nomination For Two Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   8. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/anomaly-detection.html">
     8.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-edges.html">
     8.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-vertices.html">
     8.3. Testing for Significant Vertices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Next Steps
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../next/ch10/ch10.html">
   9. Where do we go from here?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../next/ch10/random-walk-diffusion-methods.html">
     9.1. Random walk and diffusion-based methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../next/ch10/gnn.html">
     9.2. Graph Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch11/ch11.html">
   10. Representations (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch11/alt-reps.html">
     10.1. Alternative Network Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch12/ch12.html">
   11. Network Model Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/background.html">
     11.1. Background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/foundation.html">
     11.2. Foundation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/ers.html">
     11.3. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/sbms.html">
     11.4. Stochastic Block Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/rdpgs.html">
     11.5. RDPGs and more general network models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ch13.html">
   12. Learning Representations Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="mle-theory.html">
     12.1. Maximum Likelihood Estimate Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="spectral-theory.html">
     12.2. Spectral Method Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch14/ch14.html">
   13. Applications (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch14/hypothesis.html">
     13.1. Hypothesis Testing with coin flips
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch14/unsupervised.html">
     13.2. Unsupervised learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch14/bayes.html">
     13.3. Bayes Plugin Classifier
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/network_machine_learning_in_python/appendix/ch13/theory.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/neurodata/graph-stats-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fappendix/ch13/theory.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/appendix/ch13/theory.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/appendix/ch13/theory.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disclaimer-about-classical-statistical-asymptotic-theory">
   Disclaimer about classical statistical asymptotic theory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adjacency-spectral-embedding">
   Adjacency spectral embedding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setting">
     Setting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#simulation-setting">
       Simulation setting
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consistency-of-estimation-of-the-latent-position-matrix">
     Consistency of estimation of the latent position matrix
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exploring-this-result-via-simulation">
       Exploring this result via simulation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-error-of-the-adjacency-spectral-embedding">
     Statistical error of the adjacency spectral embedding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-setting">
       Example setting
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#consistency">
       Consistency
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#asymptotic-normality">
       Asymptotic normality
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding">
     Example: Erdős-Rényi graphs and the adjacency spectral embedding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applications-two-graph-hypothesis-testing">
     Applications: two-graph hypothesis testing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory-for-multiple-network-models">
   Theory for multiple network models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-matching-for-correlated-networks">
     Graph Matching for Correlated Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#joint-spectral-embeddings">
     Joint spectral embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omnibus-embedding-omni">
     Omnibus Embedding (omni)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-adjacency-spectral-embedding-mase">
     Multiple adjacency spectral embedding (MASE)
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Spectral Method Theory</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disclaimer-about-classical-statistical-asymptotic-theory">
   Disclaimer about classical statistical asymptotic theory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adjacency-spectral-embedding">
   Adjacency spectral embedding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setting">
     Setting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#simulation-setting">
       Simulation setting
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consistency-of-estimation-of-the-latent-position-matrix">
     Consistency of estimation of the latent position matrix
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exploring-this-result-via-simulation">
       Exploring this result via simulation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-error-of-the-adjacency-spectral-embedding">
     Statistical error of the adjacency spectral embedding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-setting">
       Example setting
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#consistency">
       Consistency
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#asymptotic-normality">
       Asymptotic normality
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding">
     Example: Erdős-Rényi graphs and the adjacency spectral embedding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applications-two-graph-hypothesis-testing">
     Applications: two-graph hypothesis testing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory-for-multiple-network-models">
   Theory for multiple network models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-matching-for-correlated-networks">
     Graph Matching for Correlated Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#joint-spectral-embeddings">
     Joint spectral embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omnibus-embedding-omni">
     Omnibus Embedding (omni)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-adjacency-spectral-embedding-mase">
     Multiple adjacency spectral embedding (MASE)
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="spectral-method-theory">
<span id="app-spectral-theory"></span><h1>Spectral Method Theory<a class="headerlink" href="#spectral-method-theory" title="Permalink to this headline">#</a></h1>
<p>We’ve tried to present this material so far in a manner which is as easy to understand as possible with a distant understanding of probability/statistics and a working knowledge of machine learning. Unfortunately, there is really no way around it now, so this section is about to get hairy mathematically. To understand this section properly, you should have an extremely firm understanding of linear algebra, and more than likely, should have a working knowledge of matrix analysis and multivariate probability theory. While we’ve already seen some concentration inequalities in the last section (Chebyshev’s inequality) you should have a working knowledge of how this term can be extended to random vectors and matrices before proceeding. Before taking on this section, we would recommend checking out the excellent primer by Roman Vershynin, <a class="reference external" href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">High Dimensional Probability</a>, which should get you a good foundation to understand many of the results we will take a look at here. We aren’t going to prove many of these results; if you want more details, please check out the <a class="reference external" href="https://jmlr.org/papers/volume18/17-448/17-448.pdf">excellent paper on Random Dot Product Graphs</a> by Avanti Athreya and her research team from 2017.</p>
<p>Buckle up!</p>
<section id="disclaimer-about-classical-statistical-asymptotic-theory">
<h2>Disclaimer about classical statistical asymptotic theory<a class="headerlink" href="#disclaimer-about-classical-statistical-asymptotic-theory" title="Permalink to this headline">#</a></h2>
<p>While in classic statistics there is a large literature that derives the large sample properties of an estimator, these concepts are more challenging in network analysis for multiple reasons. To start with, the very basic concept of sample size is not particularly clear. We often associate sample size with the number of observations, which are usually assumed to be independent from each other (for example, think of the number of poll participants to estimate polling preferences). In a network, having independent observations is no longer possible in the same way since all we observe are edges, and they are related to some type of interaction between two vertices. We therefore often assume that the sampled units are the vertices. However, everytime a new vertex is added, a new set of interactions with all the existing vertices is added to the model, which often results in the need of including more parameters, leading to the second important challenge in studying networks. A body of new literature has addressed some of these challenges for the models and estimators introduced in the previous sections, and we review some of these results here.</p>
</section>
<section id="adjacency-spectral-embedding">
<h2>Adjacency spectral embedding<a class="headerlink" href="#adjacency-spectral-embedding" title="Permalink to this headline">#</a></h2>
<section id="setting">
<h3>Setting<a class="headerlink" href="#setting" title="Permalink to this headline">#</a></h3>
<p>In the following sections, we summarize some of the main results in the literature about spectral embeddings. A more in-deep review of this results is presented  in Dr. Athreya’s paper. In this section, we review some theoretical properties for the adjacency spectral embedding (ASE), introduced in <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-numref">Section 5.3</span></a>.</p>
<p>The first results we consider have is about how well we are able to estimate latent position matrices, and behaviors of these estimates. As we have observed before in the section on the <a class="reference internal" href="ase.html#app-ch13-ase"><span class="std std-ref">adjacency spectral embedding</span></a>, the random latent position matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> of the random network <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> encodes important vertex properties, such as community assignments, or more generally, a latent geometry that characterizes the probabilites of the edges. As such, it is fundamental to understand how close the true and the estimated latent positions (obtained from the ASE) are from each other. The answer depends on several factors that we review here.</p>
<p>For this, we will focus on the <em>a posteriori</em> RDPG, from <a class="reference internal" href="../ch12/rdpgs.html#app-ch12-rdpg"><span class="std std-numref">Section 11.5</span></a>. This is similar to the RDPG that you learned about in <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html#ch5-rdpg"><span class="std std-numref">Section 4.3</span></a> in the main content of the book, albeit with the additional consideration that the latent positions are random, too. In this case, the random adjacency matrix <span class="math notranslate nohighlight">\(\mathbf A^{(n)}\)</span> with <span class="math notranslate nohighlight">\(n\)</span> vertices is sampled from the <em>a posteriori</em> RDPG model with distribution <span class="math notranslate nohighlight">\(F\)</span> if the latent position matrix <span class="math notranslate nohighlight">\(\mathbf X^{(n)}\)</span> takes values in <span class="math notranslate nohighlight">\(\mathbb R^{n \times d}\)</span> and has rows <span class="math notranslate nohighlight">\(\vec{\mathbf }x_i\)</span> which are independent and identically distributed according to <span class="math notranslate nohighlight">\(F\)</span>. In general, it is going to be desirable if <span class="math notranslate nohighlight">\(F\)</span> is an inner-product distribution with support on the intersection of the unit ball and the non-negative orthant. We write the random latent position matrix as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}    X = \begin{cases}
    \leftarrow &amp; \vec x_1^\top &amp; \rightarrow \\
    &amp; \vdots &amp; \\
    \leftarrow &amp; \vec x_n^\top &amp; \rightarrow 
    \end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\vec x_i\)</span> is the <span class="math notranslate nohighlight">\(d\)</span>-dimensional random latent position for node <span class="math notranslate nohighlight">\(i\)</span>. Under this formulation, we saw that the entries of the random adjacency matrix <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> are, given that <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i\)</span> takes the value <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec{\mathbf x}_j\)</span> takes the value <span class="math notranslate nohighlight">\(\vec y\)</span>, sampled independently from <span class="math notranslate nohighlight">\(Bern(\vec x^\top \vec y)\)</span>, for all <span class="math notranslate nohighlight">\(i &lt; j\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> for all <span class="math notranslate nohighlight">\(j &lt; i\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>.</p>
<section id="simulation-setting">
<span id="app-spectral-theory-sim"></span><h4>Simulation setting<a class="headerlink" href="#simulation-setting" title="Permalink to this headline">#</a></h4>
<p>The ER model, which you learned about in <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html#ch5-er"><span class="std std-numref">Section 4.1</span></a> is the simplest example of a Random Dot Product Graph, in which all edge probabilities are the same. We will use this model to derive concrete expressions for the equations in the theory we have presented. Suppose that <span class="math notranslate nohighlight">\(\mathbf A^{(n)}\)</span> is a random network with a <span class="math notranslate nohighlight">\(ER_n(p)\)</span> distribution, for some <span class="math notranslate nohighlight">\(p\in(0,1)\)</span>. We can write the distribution of this network using a unidimensional latent position matrix <span class="math notranslate nohighlight">\(\vec{\mathbf x} = (\mathbf x_1, \ldots, \mathbf x_n)^\top\)</span>, with all entries equal to <span class="math notranslate nohighlight">\(\sqrt{p}\)</span> with probability one, since this will give:</p>
<div class="math notranslate nohighlight">
\[Pr(\mathbf a_{ij} = 1 | \mathbf x_i = \sqrt{p}, \mathbf x_j = \sqrt{p}) = \sqrt{p}^2 = p.\]</div>
<p>Thus, the ER graph <span class="math notranslate nohighlight">\(ER_n(p)\)</span> can be thought as a RDPG with distribution of the latent positions <span class="math notranslate nohighlight">\(F=\delta_{\sqrt{p}}\)</span>, where <span class="math notranslate nohighlight">\(\delta_{\sqrt{p}}\)</span> is the Dirac delta distribution that only assigns positive probability to the mass point <span class="math notranslate nohighlight">\(\sqrt{p}\)</span>. It is worth noting that from this representation we can already notice a non-identifiability in the model, as both <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(-\mathbf{x}\)</span> will result in the same edge probabilities. For practical purposes, it does not matter which of these two representations we recover.</p>
<p>We’ll explore the results of ASE in the context of this extremely simple simulation environment.</p>
</section>
</section>
<section id="consistency-of-estimation-of-the-latent-position-matrix">
<h3>Consistency of estimation of the latent position matrix<a class="headerlink" href="#consistency-of-estimation-of-the-latent-position-matrix" title="Permalink to this headline">#</a></h3>
<p>For the consistency result, for each random network <span class="math notranslate nohighlight">\(\mathbf A^{(n)}\)</span>, the random network will be <span class="math notranslate nohighlight">\(RDPG_n\left(X^{(n)}\right)\)</span>. The idea here for this result is that, as <span class="math notranslate nohighlight">\(n\)</span> increases, we get better and better at estimating the latent position matrix <span class="math notranslate nohighlight">\(\mathbf X^{(n)}\)</span> using the adjacency spectral embedding. This is extremely similar to the consistency result that we saw with the MLE in the previous section, with the slight caveat that the thing we are estimating is random, too. Formally, our problem is showing that as the number of nodes in the network increases, we want the estimate of the latent position matrix <span class="math notranslate nohighlight">\(\widehat{\mathbf X}^{(n)}=ASE(\mathbf A)\in\mathbb R^{n\times d}\)</span> to be arbitrarily close to the random latent position matrix <span class="math notranslate nohighlight">\(\mathbf X^{(n)}\)</span>.</p>
<p>When we want to compare the estimates and the true latent positions, we face an unavoidable problem: the parameters of a random dot product graph, namely, the latent positions, are <em>not identifiable</em>, which you learned about back in <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral-nonidentifiable"><span class="std std-numref">Section 5.3.6.1</span></a>. In other words, given a matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, it is possible to find another matrix <span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^d\)</span> that produces exactly the same edge probability matrix, that is, <span class="math notranslate nohighlight">\(\mathbf{P} = \mathbf{X} \mathbf{X}^\top = \mathbf{Y}\mathbf{Y}^\top\)</span>. For instance, changing the signs of the columns of <span class="math notranslate nohighlight">\(\mathbf X\)</span> does not change the inner products of their rows, and thus, the matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> remains unaffected to these type of transformations. In general,  as long as the columns of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are linearly independent, all the possible matrices <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> that satisfy the previous relation are equal up to an orthogonal transformation. In other words, there exists an orthogonal matrix <span class="math notranslate nohighlight">\({W}\)</span> of size <span class="math notranslate nohighlight">\(d\times d\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{Y} =\mathbf{X}\mathbf {W}\)</span>. Here we need to make a technical assumption  to guarantee that this condition (that the columns of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are linearly independent) holds with high probability. In particular, we will  assume that the second moment matrix <span class="math notranslate nohighlight">\(\Delta = \mathbb{E}[\vec{\mathbf x}_i\vec{\mathbf x}_i^\top]\in\mathbb R^{d\times d}\)</span> has non-zero eigenvalues. This condition simplifies the type of non-identifiabilities we may face, since the probability of having correlated columns in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> would be very small if <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large.</p>
<p>The typical distance between the estimated latent positions <span class="math notranslate nohighlight">\(\widehat{\mathbf X}^{(n)}\)</span> and the latent positions <span class="math notranslate nohighlight">\(\mathbf X^{(n)}\)</span> can be quantified explicitly in terms of the sample size <span class="math notranslate nohighlight">\(n\)</span>, dimension of the latent positions <span class="math notranslate nohighlight">\(d\)</span> and a constant <span class="math notranslate nohighlight">\(C_F\)</span> that only depends on the particular distribution of the latent positions <span class="math notranslate nohighlight">\(F\)</span>. In particular, with probability tending to one as <span class="math notranslate nohighlight">\(n\)</span> goes to infinity, it holds that the largest distance between the true and estimated latent positions satisfies:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e495e3f7-5752-4c1a-8297-82dabe363e1f">
<span class="eqno">()<a class="headerlink" href="#equation-e495e3f7-5752-4c1a-8297-82dabe363e1f" title="Permalink to this equation">#</a></span>\[\begin{equation}
\max_{i\in[n]}\|\widehat{\vec{\mathbf x}}_i - \mathbf W{\vec{\mathbf x}}_i\|_2 \leq \frac{C_Fd^{1/2}\log^2 n}{\sqrt{n}}.
\end{equation}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf W\)</span>  is a <span class="math notranslate nohighlight">\(d \times d\)</span> random rotation matrix matrix (which depends on the specific realizations of <span class="math notranslate nohighlight">\(\widehat{\mathbf X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>) that accounts for the non-identifiability of the latent positions we mentioned before. In other words, the previous equation shows that the true and estimated latent positions (after a proper rotation) are uniformly close to each other, and they get closer as the number of nodes in the network <span class="math notranslate nohighlight">\(n\)</span> grows. The error rate also depends on the dimension of the model <span class="math notranslate nohighlight">\(d\)</span>, since as <span class="math notranslate nohighlight">\(d\)</span> gets larger, the dimension of the parameter space increases and the estimation error also increases.</p>
<section id="exploring-this-result-via-simulation">
<h4>Exploring this result via simulation<a class="headerlink" href="#exploring-this-result-via-simulation" title="Permalink to this headline">#</a></h4>
<p>Let’s now show this result using <code class="docutils literal notranslate"><span class="pre">graspologic</span></code> in the context of the simulation setting we discussed in
<code class="xref std std-numref docutils literal notranslate"><span class="pre">app:spectral:theory:setting</span></code>.</p>
<p>The adjacency spectral embedding of the ER graph <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the unidimensional vector <span class="math notranslate nohighlight">\(\widehat{x} = \widehat{\lambda}\widehat{\mathbf{v}}\)</span>, where  <span class="math notranslate nohighlight">\(\widehat{\mathbf{v}}\in\mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(\widehat{\lambda}\in\mathbb{R}\)</span> are the leading eigenvector and eigenvalue of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. The adjacency spectral embedding of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is simply given by <span class="math notranslate nohighlight">\(\widehat{\mathbf{x}} = ASE(\mathbf{A}) = \sqrt{\widehat{\lambda}}\widehat{\mathbf{v}}\)</span>. According to the theory presented before, we have that the error in estimating <span class="math notranslate nohighlight">\({\mathbf{x}}\)</span> from <span class="math notranslate nohighlight">\(\widehat{\mathbf{x}}\)</span> should decrease as the size of the network increases. In particular, we have that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\max_{i\in[n]}|\widehat{\mathbf x}_i - \sqrt{p}w_n| \leq \frac{C\log^2 n}{\sqrt{n}},
\end{align*}\]</div>
<p>with probability tending to <span class="math notranslate nohighlight">\(1\)</span> as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>. Here, <span class="math notranslate nohighlight">\(w_n\)</span> plays the role of  a one-dimensional orthogonal matrix that accounts for the non-identifiability of the model, or in other words, <span class="math notranslate nohighlight">\(w_n\)</span> is either equal to <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(-1\)</span>, which comes from the sign ambiguity of the eigenvectors.</p>
<p>So, we’ll show this as follows. For each <span class="math notranslate nohighlight">\(n\)</span>, we’ll sample an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> network, where <span class="math notranslate nohighlight">\(n\)</span> is either <span class="math notranslate nohighlight">\(10\)</span>, <span class="math notranslate nohighlight">\(50\)</span>, or <span class="math notranslate nohighlight">\(200\)</span>. We’ll spectrally embed the network we obtain, rotate the embedding to reorient it with the true latent positions <span class="math notranslate nohighlight">\(X\)</span>, and then compute the maximum distance between any <span class="math notranslate nohighlight">\(\hat{x}_i\)</span> and <span class="math notranslate nohighlight">\(\sqrt{p}\)</span>. We’ll repeat this <span class="math notranslate nohighlight">\(20\)</span> times for each choice of <span class="math notranslate nohighlight">\(n\)</span>, since the result that we obtain here is a probabilistic result, and show that the error margin is decreasing as <span class="math notranslate nohighlight">\(n\)</span> grows. Finally, we’ll verify that there is a constant <span class="math notranslate nohighlight">\(C\)</span> where this result holds across all of the values of <span class="math notranslate nohighlight">\(n\)</span> that we try.</p>
</section>
</section>
<section id="statistical-error-of-the-adjacency-spectral-embedding">
<h3>Statistical error of the adjacency spectral embedding<a class="headerlink" href="#statistical-error-of-the-adjacency-spectral-embedding" title="Permalink to this headline">#</a></h3>
<section id="example-setting">
<h4>Example setting<a class="headerlink" href="#example-setting" title="Permalink to this headline">#</a></h4>
<p>When we’re reviewing results about the adjacency spectral embedding, we’ll do this with some simple simulations.</p>
</section>
<section id="consistency">
<h4>Consistency<a class="headerlink" href="#consistency" title="Permalink to this headline">#</a></h4>
<p>We can see this result in practice fairly simply using <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>. The process is as follows. We’re going to consider only the simplest possible case here, which is when the random latent positions are just fixed. First, we’re going to sample an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> network, where <span class="math notranslate nohighlight">\(p=0.5\)</span>. This is equivalent to an RDPG with <span class="math notranslate nohighlight">\(d=1\)</span>, where <span class="math notranslate nohighlight">\(x_i = \sqrt{p}\)</span> for all <span class="math notranslate nohighlight">\(i \in [n]\)</span>. Since there is only <span class="math notranslate nohighlight">\(1\)</span> dimension, the only rotation in <span class="math notranslate nohighlight">\(1\)</span> dimension is a sign flip, which means that we can align the estimated latent positions <span class="math notranslate nohighlight">\(\hat X\)</span> with the true latent positions <span class="math notranslate nohighlight">\(X\)</span> by just checking the signs.</p>
<p>Next, once we rotate the estimates, we simply compute the maximum difference between the estimated</p>
<p>We’ll repeat t
his <span class="math notranslate nohighlight">\(30\)</span> times for a given value of <span class="math notranslate nohighlight">\(n\)</span>, and we will repeat for <span class="math notranslate nohighlight">\(n \in \{20, 50, 100\}\)</span>. Let’s take a look at how we do this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="asymptotic-normality">
<h4>Asymptotic normality<a class="headerlink" href="#asymptotic-normality" title="Permalink to this headline">#</a></h4>
<p>A further result on the asymptotic properties of the <code class="docutils literal notranslate"><span class="pre">ASE</span></code> concerns to the distribution of the estimation error, that is, the difference between the estimated latent position <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf x}}_i\)</span> (for a given node <span class="math notranslate nohighlight">\(i\)</span>) and the true parameter <span class="math notranslate nohighlight">\(\widehat{\vec{\mathbf x}}_i\)</span>. Because the estimator is consistent, this difference shrinks with <span class="math notranslate nohighlight">\(n\)</span> (after the proper orthogonal rotation), but knowing this fact is not enough if one desires to quantify their difference more carefully. This is important, as some statistical tasks, such as hypothesis testing or confidence interval estimation, require to know distributional properties of the estimation error.</p>
<p>Distributional results on the rows of the adjacency spectral embedding show that the error in estimating the true latent positions converge to a type of multivariate normal normal distribution as the size of the network grows. In particular, it has been shown that the
difference between <span class="math notranslate nohighlight">\(\widehat{\mathbf{X}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, after a proper orthogonal transformation <span class="math notranslate nohighlight">\(\mathbf{W}_n\)</span>, converge to a mixture of multivariate normal distributions. Formally, we can write the multivariate cumulative distribution function for any given vector <span class="math notranslate nohighlight">\(\mathbf{z}\in\mathbb R^{d}\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-85915bf1-f681-4b77-9e0f-7241190ce2de">
<span class="eqno">()<a class="headerlink" href="#equation-85915bf1-f681-4b77-9e0f-7241190ce2de" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathbb{P}\left(\sqrt{n}\left(\widehat{\mathbf X}_i - \mathbf W_n \mathbf X_i\right)\leq \mathbf{z} \right) \approx \int_{\mathbb{R}^d}\Phi(\mathbf{z}, \mathbf{\Sigma}(\mathbf{x}))\  dF(\mathbf{x}),
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(\cdot, \mathbf{\Sigma}(\mathbf{x}))\)</span> is the cumulative distribution function of a <span class="math notranslate nohighlight">\(d\)</span>-dimensional multivariate normal distribution with mean zero and a covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\mathbf{x})\in\mathbb R^{d\times d}\)</span>. It is worth noting that the integral on the right hand side operates over all possible values of <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^d\)</span>, and it is integrated with respect to the distribution of the latent positions, which are sample independently from <span class="math notranslate nohighlight">\(F\)</span>. The covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\mathbf{x})\)</span> depends on a specific value of a latent position <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^d\)</span>, and is given by
$<span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\mathbf{x}) = \mathbf{\Delta}^{-1}\mathbb{E}[\mathbf{X}_1\mathbf{X}_1^\top(\mathbf{x}^\top\mathbf{X}_1 - (\mathbf{x}^\top\mathbf{X}_1)^2) ]\mathbf{\Delta}^{-1}. \)</span><span class="math notranslate nohighlight">\(
For certain classes of distributions, it is possible to simplify this expression further to get a more specific form of the asymptotic distribution of \)</span>\widehat{\mathbf{X}}_i$. We present an example later on this section.</p>
<p>The two results presented on this section constitute the basic properties of the adjacency spectral embedding under the RDPG model, but existing results are not limited to this particular method and model. Extensions of these results to a broader class of network models have been already developed (included the so-called <em>generalized random dot product graphs</em>). Analogous results also exist for other type of embedding methodologies. For instance, the Laplacian spectral embedding (<span class="math notranslate nohighlight">\(LSE\)</span>)  (see <a class="reference external" href="#link?">Section 6.3.10.2</a>) possess analogous properties to the ASE (namely, consistent estimates with known asymptotic distribution), although the particular form of this theorem is a bit more complicated. The study of different network models and embedding methods is still a research area under development, and new results keep coming every day.</p>
</section>
</section>
<section id="example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding">
<h3>Example: Erdős-Rényi graphs and the adjacency spectral embedding<a class="headerlink" href="#example-erdos-renyi-graphs-and-the-adjacency-spectral-embedding" title="Permalink to this headline">#</a></h3>
<p>Similarly, we can also obtain an expression for the asymptotic distribution of the difference between the true and estimated positions. First, notice that since <span class="math notranslate nohighlight">\(F\)</span> has only one point mass at <span class="math notranslate nohighlight">\(\sqrt{p}\)</span>, we can evaluate integrals and expections with respect to this distribution, resulting in
$<span class="math notranslate nohighlight">\(\int_{\mathbb{R}^d}\Phi(\mathbf{z}, \mathbf{\Sigma}(\mathbf{x}))\  dF(\mathbf{x}) = \Phi(\mathbf{z}, \mathbf{\Sigma}(\sqrt{p}).\)</span><span class="math notranslate nohighlight">\(
The exact form of the covariance term \)</span>\mathbf{\Sigma}(\mathbf{x})<span class="math notranslate nohighlight">\( can be obtained from the second moment matrix \)</span>\mathbf{\Delta} = \mathbb{E}[\mathbf{x}_1^2] = p<span class="math notranslate nohighlight">\(, and it is given by
\)</span><span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\mathbf{x}) = \frac{1}{p^2}\mathbb{E}[p(\mathbf{x}\sqrt{p} - (\mathbf{x}\sqrt{p})^2)].\)</span><span class="math notranslate nohighlight">\(
Combining these results, we get that the limiting distribution of the difference between \)</span>\widehat{\mathbf{x}}<span class="math notranslate nohighlight">\( and \)</span>\mathbf{x}<span class="math notranslate nohighlight">\( satisfies
\)</span><span class="math notranslate nohighlight">\(\sqrt{n}\left(\widehat{\mathbf{x}}_i - \sqrt{p}\right)\ \rightarrow\  N(0, 1-p)\quad\quad\text{for each vertex }i=1, \ldots, n.\)</span><span class="math notranslate nohighlight">\(
The asymptotic normality result is illustrated in the simulation setting below. In particular, observe that, as the size of theg graph \)</span>n<span class="math notranslate nohighlight">\( increases, the distribution of the entries of \)</span>\mathbf{x}$ resembles the normal distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># need to include simulations here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="applications-two-graph-hypothesis-testing">
<h3>Applications: two-graph hypothesis testing<a class="headerlink" href="#applications-two-graph-hypothesis-testing" title="Permalink to this headline">#</a></h3>
<p>The results previously discussed demonstrate that the true and estimated latent positions are close to each other, and in fact, their distance gets smaller as <span class="math notranslate nohighlight">\(n\)</span> increases. As such, the ASE provides an accurate estimator of the latent positions. This result justifies the use of <span class="math notranslate nohighlight">\(\mathbf {\hat X}\)</span> in place of <span class="math notranslate nohighlight">\(\mathbf X\)</span> for subsequent inference tasks, such as community detection, vertex nomination or classification (see Sections ??-?? for a more thorough discussion on these topics). The theoretical results for the ASE have multiple implications. One of those  is that the estimated latent positions carry almost the same information as the true latent positions, and we can even quantify how different they are. This is particularly useful for performing statistical inference tasks about vertex properties. Here we consider one of these tasks: two-graph hypothesis testing.</p>
<p>Comparing the distribution of two populations is a frequent problem in statistics and across multiple domains. In classical statistics, a typical strategy to perform this task is to compare the mean of two populations by using an appropriate test statistic. Theoretical results on the distribution of this statistic (either exact or asymptotic) are then used to derive a measure of uncertainty for this problem (such as p-values or confidence intervals). Similarly, when comparing two observed graphs, we may wonder whether they were generated by the same mechanism. The results discussed before have been used to develop valid statistical tests for   two-network hypothesis testing questions.</p>
<p>A network hypothesis test for the equivalence between the latent positions of the vertices of a pair of networks with aligned vertices can be constructed by using the estimates of the latent positions. Formally, let <span class="math notranslate nohighlight">\(\mathbf X, \mathbf Y\in\mathbb R^{n\times d}\)</span> be the latent position matrices, and define
<span class="math notranslate nohighlight">\(\mathbf A\sim RDPG(\mathbf X)\)</span>, <span class="math notranslate nohighlight">\(\mathbf B\sim RDPG(Y)\)</span> as independent random adjacency matrices. We can test whether the two networks have the same distribution by comparing their latent positions via a hypothesis test of the form</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{H}_0:\mathbf X =_{\mathbf W} \mathbf Y\quad\quad\quad \text{ vs.}\quad\quad\quad \mathcal{H}_a:\mathbf X\neq_{\mathbf W} \mathbf Y,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf X =_{\mathbf W}\mathbf Y\)</span> denotes that <span class="math notranslate nohighlight">\(\mathbf X\)</span> and <span class="math notranslate nohighlight">\(\mathbf Y\)</span> are equivalent up to an orthogonal transformation <span class="math notranslate nohighlight">\(\mathbf W\in\mathcal{O}_d\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{O}_d\)</span> is the set of <span class="math notranslate nohighlight">\(d\times d\)</span> orthogonal matrices. Since we do not have access to the true latent positions, we can use the estimates <span class="math notranslate nohighlight">\(\widehat{\mathbf X}\)</span> and <span class="math notranslate nohighlight">\(\widehat{\mathbf Y}\)</span> to construct a test statsistic. This test statistic is defined as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
T : = \frac{\min_{\mathbf W\in\mathcal{O}_d} \|\widehat{\mathbf X}\mathbf W - \widehat{\mathbf Y}\|_F}{\sqrt{d\gamma^{-1}(\mathbf A)} + \sqrt{d\gamma^{-1}(\mathbf B)}}.
\end{align*}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\|\widehat{\mathbf X}\mathbf W - \widehat{\mathbf Y}\|_F\)</span> is the Frobenius distance between the estimated latent positions (after adjusting for the orthogonal non-identifiability).
This distance compares how similar the two latent positions are, and thus, it is natural to think that larger values of this distance will give more evidence agains the null hypothesis. In addition to this, the test statistic incorporates a normalizing constant of the form <span class="math notranslate nohighlight">\(\sqrt{d\gamma^{-1}(\mathbf A)} + \sqrt{d\gamma^{-1}(\mathbf B)}\)</span>. Here <span class="math notranslate nohighlight">\(\sigma_1(\mathbf A) \geq \ldots\geq \sigma_n(\mathbf A)\geq 0\)</span> denote the singular values of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (similarly for <span class="math notranslate nohighlight">\(\mathbf B\)</span>),  <span class="math notranslate nohighlight">\(\delta(\mathbf A) = \max_{i\in[n]}\sum_{j=1}^n\mathbf A_{ij}\)</span> denotes the largest observed degree of the graph, and
$<span class="math notranslate nohighlight">\(\gamma(\mathbf A):=\frac{\sigma_d(\mathbf A) - \sigma_{d+1}(\mathbf A)}{\delta(\mathbf A)}\)</span><span class="math notranslate nohighlight">\(
is a constant that standardizes the test statistic. It can be shown that, under appropriate regularity conditions that this test statistic will go to zero as \)</span>n<span class="math notranslate nohighlight">\( goes to infinity under the null hypothesis, and will diverge with \)</span>n<span class="math notranslate nohighlight">\( for some specific alternatives. Thus, \)</span>T$ provides a way to construct a consistent test for the hypothesis testing problem described above.</p>
</section>
</section>
<section id="theory-for-multiple-network-models">
<h2>Theory for multiple network models<a class="headerlink" href="#theory-for-multiple-network-models" title="Permalink to this headline">#</a></h2>
<p>Models for multiple network data often assume that there is a known one-to-one correspondence between the vertices of the graphs. If this correspondence is unknown, an estimate can be obtained via graph matching. Once the vertices are correctly matched, models for multiple networks exploit the shared structure across the graphs to obtain accurate estimates. In this section we review the theoretical challenges on these circumstances.</p>
<section id="graph-matching-for-correlated-networks">
<h3>Graph Matching for Correlated Networks<a class="headerlink" href="#graph-matching-for-correlated-networks" title="Permalink to this headline">#</a></h3>
<p>Given a pair of network adjacency matrices <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\mathbf B\)</span> with <span class="math notranslate nohighlight">\(n\)</span> vertices each (but possibly permuted), the graph matching problem seeks to <em>align</em> the graphs by identifying the correct correspondence between their vertices. This can be done by identifying a permutation matrix <span class="math notranslate nohighlight">\(\mathbf P\)</span> of size <span class="math notranslate nohighlight">\(n\times n\)</span> that makes the graphs <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\mathbf P^\top \mathbf B \mathbf P\)</span> similar.
There are many methods to perform grapph matching, and here we focus on the following optimization problem (which is reviewed more carefully in Section ????):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\widehat{\mathbf P} =\text{argmin}_P \|\mathbf A - \mathbf P^\top \mathbf B \mathbf P\|_F^2.
\end{equation*}\]</div>
<p>The accuracy in matching the vertices across the graphs depends on this estimated permutation matrix <span class="math notranslate nohighlight">\(\widehat{\mathbf P}\)</span>. Denote by <span class="math notranslate nohighlight">\(\mathbf P^\ast\)</span> to the true permutation matrix that aligns the two graphs, which is the object we would like to estimate. In principle, if there is a unique solution that gives <span class="math notranslate nohighlight">\(\mathbf A = (\mathbf P^\ast)^\top \mathbf B \mathbf P^\ast\)</span>, the two networks are isomorphic (i.e., equal up to some  permutation of the vertices), and matching the vertices across the networks is possible. However, in the presence of  noise (which is typically the case), the graphs are not isomorphic, and hence the solution to the optimization problem above becomes more relevant.</p>
<p>A body of literature has studied the feasibility of finding the correct matching under different random network models, including correlated Erdös-Rényi and Bernoulli networks. In this section we review some of the results for the correlated Erdös-Rényi (ER) model described in <a class="reference external" href="#link?">Section 5.5</a>.</p>
<p>The correlated ER model has only two parameters, namely, the edge density and the correlation across a pair of graphs. This two parameters are crucial in understanding the feasibility of solving the graph matching problem. On the one hand, the edge density controls the amount of information that is present on each graph. If the density is very close to zero, the information available to match the graphs is low, which makes the problem harder. On the other hand, the correlation paraameter controls the level of similarity across the graphs. A large correlation value facilitates matching the graphs, as the edges exhibit similar or exactly the same patterns in both graphs.  Formally, given parameters <span class="math notranslate nohighlight">\(q \in(0, 1)\)</span> and <span class="math notranslate nohighlight">\(\rho\in[0,1]\)</span>, the <span class="math notranslate nohighlight">\(n\times n\)</span> adjacency matrices <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\mathbf B\)</span> are distributed as correlated ER random graphs if each graph is marginally distributed as an ER of the form <span class="math notranslate nohighlight">\(\mathbf A\sim ER_n(q_n)\)</span>, <span class="math notranslate nohighlight">\(\mathbf B\sim ER_n(q_n)\)</span>, but the edge pairs satisfy <span class="math notranslate nohighlight">\(\text{Corr}(\mathbf A_{ij},\mathbf B_{ij})=\rho\)</span>. In particular, if <span class="math notranslate nohighlight">\(\rho=0\)</span>, then the graphs are just independent realizations of an ER network but there is no common structure between them. On the contrary, if <span class="math notranslate nohighlight">\(\rho=1\)</span>, then the graphs are isomorphic.</p>
<p>Having defined the model, the next question is whether the solution of the graph matching optimization problem (defined in <a class="reference external" href="#link?">Section 9.3</a>) recover the correct solution. In the correlated ER model, it can be shown that this is possible when the correlation between the networks and the edge probability that are not to small. Formally, the conditions require that <span class="math notranslate nohighlight">\(\rho\geq c_1\sqrt{\frac{\log n}{n}}\)</span> and <span class="math notranslate nohighlight">\(q\geq c_2 \frac{\log n }{n}\)</span>  (here, <span class="math notranslate nohighlight">\(c_1\)</span> and <span class="math notranslate nohighlight">\(c_2\)</span> are just some positive fixed constants). This conditions guarantee that the correlation across the graphs is large enough, so there is enough shared information, while the edge density needs to be sufficiently away from zero and one, to ensure that there are enough edges within each graph. Under these conditions, it can be shown that, with high probability, the correct solution of the graph matching problem is obtained if <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large.</p>
</section>
<section id="joint-spectral-embeddings">
<h3>Joint spectral embeddings<a class="headerlink" href="#joint-spectral-embeddings" title="Permalink to this headline">#</a></h3>
</section>
<section id="omnibus-embedding-omni">
<h3>Omnibus Embedding (omni)<a class="headerlink" href="#omnibus-embedding-omni" title="Permalink to this headline">#</a></h3>
<p>The omnibus embedding described in [Section 6.7] jointly estimates the latent positions under the joint random dot product network (<span class="math notranslate nohighlight">\( JRDPG\)</span>) model, where <span class="math notranslate nohighlight">\((\mathbf A^{(1)}, \ldots, \mathbf A^{(m)})\sim JRDPG(\mathbf X_n)\)</span>, and the rows of <span class="math notranslate nohighlight">\(\mathbf X_n\in\mathbb R^{n\times d}\)</span> are an i.i.d. sample from some distribution <span class="math notranslate nohighlight">\(F\)</span>. Let <span class="math notranslate nohighlight">\(\widehat{\mathbf{O}}\in\mathbb R^{mn\times mn}\)</span> be the omnibus embedding of <span class="math notranslate nohighlight">\(\mathbf A^{(1)}, \ldots, \mathbf A^{(m)}\)</span> and <span class="math notranslate nohighlight">\(\widehat{\mathbf Z} = ASE(\mathbf{O})\in\mathbb R^{mn\times d}\)</span>.
Under this setting, it can be shown that the rows of <span class="math notranslate nohighlight">\(\widehat{\mathbf Z}_n\)</span> are a consistent estimator of the latent positions of each individual network  as <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>, and that:</p>
<div class="amsmath math notranslate nohighlight" id="equation-67a7be5a-bc39-4c2c-b01c-1104145dad51">
<span class="eqno">()<a class="headerlink" href="#equation-67a7be5a-bc39-4c2c-b01c-1104145dad51" title="Permalink to this equation">#</a></span>\[\begin{equation}
\max_{i\in[n],j\in[m]}\|(\widehat{\mathbf Z}_n)_{(j-1)n + i} - \mathbf W_n(\mathbf X_n)_{i}\| \leq \frac{C\sqrt{m}\log(mn)}{\sqrt{n}}. \label{eq:OMNI-consistency}    
\end{equation}\]</div>
<p>Furthermore, a central limit theorem for the rows of the omnibus embedding  asserts that:</p>
<div class="amsmath math notranslate nohighlight" id="equation-35100c92-6f2b-462a-a05d-d70656551687">
<span class="eqno">()<a class="headerlink" href="#equation-35100c92-6f2b-462a-a05d-d70656551687" title="Permalink to this equation">#</a></span>\[\begin{equation}
\lim_{n\rightarrow\infty} \mathbb{P}\left\{\sqrt{n}\left(\mathbf W_n(\widehat{\mathbf Z}_n)_{(j-1)n + i} - (\mathbf X_n)_i\right)\leq \mathbf{z}\right\}  = \int_{\mathcal{X}}\Phi(\mathbf{z}, \widehat{\mathbf{\Sigma}}(\mathbf{x}))\  dF(\mathbf{x}),\label{eq:thm-OMNI-CLT}
\end{equation}\]</div>
<p>for some covariance matrix <span class="math notranslate nohighlight">\(\widehat{\Sigma}(\mathbf{x})\)</span>.</p>
</section>
<section id="multiple-adjacency-spectral-embedding-mase">
<h3>Multiple adjacency spectral embedding (MASE)<a class="headerlink" href="#multiple-adjacency-spectral-embedding-mase" title="Permalink to this headline">#</a></h3>
<p>The <span class="math notranslate nohighlight">\(COSIE\)</span> model described in <a class="reference external" href="#link?">Section 5.5</a> gives a joint model that characterizes the distribution of multiple networks with expected probability matrices that share the same common invariant subspace. The <span class="math notranslate nohighlight">\(MASE\)</span> algorithm (see <a class="reference external" href="#link?">Section 5.5</a>) is a consistent estimator for this common invariant subspace, and results in asymptotically normally estimators for the individual symmetric matrices. Specifically, let <span class="math notranslate nohighlight">\(\mathbf V_n\in\mathbb R^{n\times d}\)</span> be
a sequence of orthonormal matrices and <span class="math notranslate nohighlight">\(\mathbf R^{(1)}_n, \ldots, \mathbf R^{(m)}_n\in\mathbb R^{d\times d}\)</span> a sequence of score matrices such that <span class="math notranslate nohighlight">\(\mathbf{P}^{(l)}_n=\mathbf V_n\mathbf R^{(l)}_n\mathbf V_n^\top\in[0,1]^{n\times n} \)</span>, <span class="math notranslate nohighlight">\((\mathbf A_n^{(1)}, \ldots, \mathbf A_n^{(m)})\sim COSIE(\mathbf V_n;, \mathbf R^{(1)}_n, \ldots, \mathbf R^{(m)}_n)\)</span>, and <span class="math notranslate nohighlight">\(\widehat{\mathbf V}, \widehat{\mathbf R}^{(1)}_n, \ldots, \widehat{\mathbf R}^{(1)}_n\)</span> be the estimators obtained by <span class="math notranslate nohighlight">\(MASE\)</span>. Under appropriate regularity conditions, the estimate for <span class="math notranslate nohighlight">\(\mathbf V\)</span> is consistent as <span class="math notranslate nohighlight">\(n,m\rightarrow\infty\)</span>, and there exists some constant <span class="math notranslate nohighlight">\(C&gt;0\)</span> such that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}\left[\min_{\mathbf W\in\mathcal{O}_d}\|\widehat{\mathbf V}-\mathbf V\mathbf W\|_F\right] \leq C\left(\sqrt{\frac{1}{mn}} + {\frac{1}{n}}\right). \label{eq:theorem-bound}
\end{align*}\]</div>
<p>In addition, the entries of <span class="math notranslate nohighlight">\(\widehat{\mathbf{R}}^{(l)}_n\)</span>, <span class="math notranslate nohighlight">\(l\in[m]\)</span> are asymptotically normally distributed. Namely, there exists a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(\mathbf W\)</span> such that:
$<span class="math notranslate nohighlight">\(\frac{1}{\sigma_{l,j,k}}\left(\widehat{\mathbf R}^{(l)}_n - \mathbf W^\top\mathbf R^{(l)}_n\mathbf W + \mathbf H_m^{(l)}\right)_{jk} \overset{d}{\rightarrow} \mathcal{N}(0, 1), \)</span><span class="math notranslate nohighlight">\(
as \)</span>n\rightarrow\infty<span class="math notranslate nohighlight">\(, where:
\)</span>\mathbb{E}[|\mathbf H_m^{(l)}|]=O\left(\frac{d}{\sqrt{m}}\right)<span class="math notranslate nohighlight">\( and \)</span>\sigma^2_{l,j,k} = O(1)$.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./appendix/ch13"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>