
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Finding singular vectors With singular value decomposition &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What is network machine learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why do we study networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">
     1.3. Types of Network Machine Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">
     1.4. Challenges of Network Machine Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.4. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.5. Fine-Tune your Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.6. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch4/ch4.html">
   3. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/matrix-representations.html">
     3.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/properties-of-networks.html">
     3.2. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/network-representations.html">
     3.3. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/regularization.html">
     3.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch5/ch5.html">
   4. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">
     4.1. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">
     4.2. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">
     4.3. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">
     4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_DCSBM.html">
     4.5. Degree-Corrected Stochastic Block Model (DCSBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_SIEM.html">
     4.6. Structured Independent Edge Model (SIEM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/multi-network-models.html">
     4.7. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/models-with-covariates.html">
     4.8. Network Models with Network Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch6/ch6.html">
   5. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">
     5.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/why-embed-networks.html">
     5.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/spectral-embedding.html">
     5.3. Spectral embedding methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">
     5.4. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">
     5.5. Joint Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch7/ch7.html">
   6. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/community-detection.html">
     6.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/testing-differences.html">
     6.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/model-selection.html">
     6.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/vertex-nomination.html">
     6.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/out-of-sample.html">
     6.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   7. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html">
     7.1. Latent Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/significant-communities.html">
     7.2. Two-sample hypothesis testing in SBMs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html">
     7.3. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/multiple-vertex-nomination.html">
     7.4. Vertex Nomination For Two Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   8. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/anomaly-detection.html">
     8.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-edges.html">
     8.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-vertices.html">
     8.3. Testing for Significant Vertices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Next Steps
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../next/ch10/ch10.html">
   9. Where do we go from here?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../next/ch10/random-walk-diffusion-methods.html">
     9.1. Random walk and diffusion-based methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../next/ch10/gnn.html">
     9.2. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../next/ch10/sparsity.html">
     9.3. Network Sparsity
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch11/ch11.html">
   10. Representations (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch11/alt-reps.html">
     10.1. Alternative Network Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch12/ch12.html">
   11. Network Model Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/background.html">
     11.2. Background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/foundation.html">
     11.3. Foundation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/ers.html">
     11.4. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/sbms.html">
     11.5. Stochastic Block Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch12/rdpgs.html">
     11.6. RDPGs and more general network models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ch13.html">
   12. Learning Representations Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="mle-theory.html">
     12.1. Maximum Likelihood Estimate Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="spectral-theory.html">
     12.2. Spectral Method Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch14/ch14.html">
   13. Applications (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch14/hypothesis.html">
     13.1. Hypothesis Testing with coin flips
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch14/unsupervised.html">
     13.2. Unsupervised learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch14/bayes.html">
     13.3. Bayes Plugin Classifier
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/neurodata/graph-stats-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fappendix/ch13/lse.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/appendix/ch13/lse.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/appendix/ch13/lse.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-svd-to-the-laplacian">
   Applying the svd to the Laplacian
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-svd-as-a-sum-of-rank-1-matrices">
   The
   <code class="docutils literal notranslate">
    <span class="pre">
     svd
    </span>
   </code>
   as a sum of rank-1 matrices
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-laplacian-as-a-sum-of-rank-1-matrices">
     The Laplacian as a sum of rank
     <span class="math notranslate nohighlight">
      \(1\)
     </span>
     matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#critical-properties-of-the-svd">
   Critical Properties of the
   <code class="docutils literal notranslate">
    <span class="pre">
     svd
    </span>
   </code>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-symmetric-laplacian-left-right-singular-vector-equivalence">
     The symmetric Laplacian left/right singular vector equivalence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#you-can-approximate-your-symmetric-laplacian-by-only-summing-a-few-of-the-low-rank-matrices">
     You can approximate your symmetric Laplacian by only summing a few of the low-rank matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approximating-becomes-extremely-useful-when-you-have-a-bigger-now-regularized-laplacian">
     Approximating becomes extremely useful when you have a bigger (now regularized) Laplacian
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-the-svd-to-understand-the-estimated-latent-position-matrix">
   Using the
   <code class="docutils literal notranslate">
    <span class="pre">
     svd
    </span>
   </code>
   to understand the estimated latent position matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-matrix-rank-helps-you-understand-spectral-embedding">
   How Matrix Rank Helps you Understand Spectral Embedding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-the-dad-laplacian">
     Why the DAD Laplacian?
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Finding singular vectors With singular value decomposition</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-svd-to-the-laplacian">
   Applying the svd to the Laplacian
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-svd-as-a-sum-of-rank-1-matrices">
   The
   <code class="docutils literal notranslate">
    <span class="pre">
     svd
    </span>
   </code>
   as a sum of rank-1 matrices
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-laplacian-as-a-sum-of-rank-1-matrices">
     The Laplacian as a sum of rank
     <span class="math notranslate nohighlight">
      \(1\)
     </span>
     matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#critical-properties-of-the-svd">
   Critical Properties of the
   <code class="docutils literal notranslate">
    <span class="pre">
     svd
    </span>
   </code>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-symmetric-laplacian-left-right-singular-vector-equivalence">
     The symmetric Laplacian left/right singular vector equivalence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#you-can-approximate-your-symmetric-laplacian-by-only-summing-a-few-of-the-low-rank-matrices">
     You can approximate your symmetric Laplacian by only summing a few of the low-rank matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approximating-becomes-extremely-useful-when-you-have-a-bigger-now-regularized-laplacian">
     Approximating becomes extremely useful when you have a bigger (now regularized) Laplacian
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-the-svd-to-understand-the-estimated-latent-position-matrix">
   Using the
   <code class="docutils literal notranslate">
    <span class="pre">
     svd
    </span>
   </code>
   to understand the estimated latent position matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-matrix-rank-helps-you-understand-spectral-embedding">
   How Matrix Rank Helps you Understand Spectral Embedding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-the-dad-laplacian">
     Why the DAD Laplacian?
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="finding-singular-vectors-with-singular-value-decomposition">
<span id="app-ch13-lse"></span><h1>Finding singular vectors With singular value decomposition<a class="headerlink" href="#finding-singular-vectors-with-singular-value-decomposition" title="Permalink to this headline">#</a></h1>
<p>In this section, we’ll do a more thorough overview of the <code class="docutils literal notranslate"><span class="pre">svd</span></code> as it relates to the Laplacian Spectral Embedding, or <code class="docutils literal notranslate"><span class="pre">LSE</span></code>, that you learned about in <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-numref">Section 5.3</span></a>. Let’s start off by just obtaining a simulation with <span class="math notranslate nohighlight">\(20\)</span> nodes:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="c1"># Make network</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="n">A</span><span class="p">,</span> <span class="n">zs</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">zs</span> <span class="o">=</span> <span class="n">zs</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># to make 1s and 2s instead of 0s and 1s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">5</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1"># Make network</span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> 
<span class="g g-Whitespace">      </span><span class="mi">6</span>               <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">A</span><span class="p">,</span> <span class="n">zs</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">zs</span> <span class="o">=</span> <span class="n">zs</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># to make 1s and 2s instead of 0s and 1s</span>

<span class="ne">NameError</span>: name &#39;np&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">zs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;A, a sample from an $SBM_n(z, B)$ Simulation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/lse_3_0.png" src="../../_images/lse_3_0.png" />
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">svd</span></code> is a way to decompose a matrix into three distinct new matrices – in our case, the matrix will be the Laplacian we just built. These three new matrices correspond to the singular vectors and singular values of the original matrix: the algorithm will collect all of the singular vectors as columns of one matrix, and the singular values as the diagonals of another matrix.</p>
<p>This is pretty similar to another linear algebra concept that will come in handy, called the eigendecomposition. If you remember from linear algebra, the eigenvalue decomposition was the decomposition:</p>
<p>Unlike the eigendecomposition, which does not <em>necessarily</em> exist for a square matrix <span class="math notranslate nohighlight">\(X\)</span>, the singular value decomposition <em>always</em> exists.</p>
<section id="applying-the-svd-to-the-laplacian">
<h2>Applying the svd to the Laplacian<a class="headerlink" href="#applying-the-svd-to-the-laplacian" title="Permalink to this headline">#</a></h2>
<p>For the Laplacian Spectral Embedding, the reason that we care about this was discussed in <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-numref">Section 5.3</span></a>, the spectral embedding. Let’s take the Laplacian and show its <code class="docutils literal notranslate"><span class="pre">svd</span></code>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">to_laplacian</span>

<span class="n">L</span> <span class="o">=</span> <span class="n">to_laplacian</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">form</span><span class="o">=</span><span class="s2">&quot;DAD&quot;</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">hermitian</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">Normalize</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">GraphColormap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># First axis (Laplacian)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$L$&quot;</span><span class="p">)</span>

<span class="c1"># Second axis (=)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> 
            <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Third axis (U)</span>
<span class="n">U_ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U$&quot;</span><span class="p">)</span>
<span class="n">U_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Columns of left singular vectors&quot;</span><span class="p">)</span>

<span class="c1"># Third axis (s)</span>
<span class="n">E_ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\Sigma$&quot;</span><span class="p">)</span>
<span class="n">E_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Singular values on diagonal&quot;</span><span class="p">)</span>

<span class="c1"># Fourth axis (V^T)</span>
<span class="n">Ut_ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">Vt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$V^T$&quot;</span><span class="p">)</span>
<span class="n">Ut_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Rows of right singular vectors&quot;</span><span class="p">)</span>

<span class="c1"># Colorbar</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">color</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Decomposing your simple Laplacian into singular values/vectors with SVD&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/lse_6_0.png" src="../../_images/lse_6_0.png" />
</div>
</div>
</section>
<section id="the-svd-as-a-sum-of-rank-1-matrices">
<h2>The <code class="docutils literal notranslate"><span class="pre">svd</span></code> as a sum of rank-1 matrices<a class="headerlink" href="#the-svd-as-a-sum-of-rank-1-matrices" title="Permalink to this headline">#</a></h2>
<p>This expression <span class="math notranslate nohighlight">\(heatmap U\Sigma V^\top\)</span> is a little bit complicated, so we will simplify it down a little bit here: as it turns out, this “complicated” looking matrix multiplication is actually pretty straightforward because <span class="math notranslate nohighlight">\(\Sigma\)</span> is diagonal. You can write this equation down like this, which is much more understandable:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X &amp;= \sum_{i = 1}^n \sigma_i \vec u_i \vec v_i^\top = \sum_{i = 1}^n \sigma_i\begin{bmatrix}
        \uparrow \\ \vec u_i \\ \downarrow
    \end{bmatrix}\begin{bmatrix}
        \leftarrow &amp; \vec v_i^\top &amp; \rightarrow
    \end{bmatrix}
\end{align*}\]</div>
<p>Let’s try to interpret what this intimidating sum really is saying to us. To start with, for each term, you have the left singular vector, <span class="math notranslate nohighlight">\(\vec u_i\)</span>. By matrix multiplication, taking the product of <span class="math notranslate nohighlight">\(\vec u_i\)</span> with <span class="math notranslate nohighlight">\(\vec v_i^\top\)</span> gives you this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec u_i \vec v_i^\top &amp;= \begin{bmatrix}
    \uparrow &amp;  &amp; \uparrow \\
    v_{i1}\vec u_i &amp; ... &amp; v_{in}\vec u_i \\
    \downarrow &amp;  &amp; \downarrow
    \end{bmatrix}
\end{align*}\]</div>
<p>So basically, you have a matrix with <span class="math notranslate nohighlight">\(n\)</span> columns, all of which are multiples of the vector <span class="math notranslate nohighlight">\(\vec u_i\)</span>. What multiple? Well, that is what <span class="math notranslate nohighlight">\(\vec v_i\)</span> tells us: the “amount” of <span class="math notranslate nohighlight">\(\vec u_i\)</span> in a particular column <span class="math notranslate nohighlight">\(j\)</span> is indicated by <span class="math notranslate nohighlight">\(v_{ij}\)</span>. This gives you a matrix, <span class="math notranslate nohighlight">\(\vec u_i\vec v_i^\top\)</span>, which <em>also</em> has <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns, where every column is a multiple of <span class="math notranslate nohighlight">\(\vec u_i\)</span>. This introduces you to our next important property: <em>matrix rank</em>.</p>
<p>Since every column of <span class="math notranslate nohighlight">\(\vec u_i \vec v_i^\top\)</span> is a multiple of <span class="math notranslate nohighlight">\(\vec u_i\)</span>, it’s pretty clear that the resulting matrix <span class="math notranslate nohighlight">\(\vec u_i \vec v_i^\top\)</span> is low rank (it is rank <span class="math notranslate nohighlight">\(1\)</span>, since each column is a multiple of the column <span class="math notranslate nohighlight">\(\vec u_i\)</span>). Finally, you just take this <span class="math notranslate nohighlight">\(n\)</span> row and <span class="math notranslate nohighlight">\(n\)</span> column matrix, and you multiply the whole thing by <span class="math notranslate nohighlight">\(\sigma_i\)</span>. So as it turns out, <span class="math notranslate nohighlight">\(X\)</span> is equal to a weighted sum of rank-<span class="math notranslate nohighlight">\(1\)</span> matrices, where the weights are all given to you by the singular values:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
X &amp;= \sum_{i = 1}^n \sigma_i \vec u_i \vec v_i^\top = \sigma_1\begin{bmatrix}
        \uparrow \\ \vec u_1 \\ \downarrow
    \end{bmatrix}\begin{bmatrix}
        \leftarrow &amp; \vec v_1^\top &amp; \rightarrow
    \end{bmatrix} + ... + \sigma_n\begin{bmatrix}
        \uparrow \\ \vec u_n \\ \downarrow
    \end{bmatrix}\begin{bmatrix}
        \leftarrow &amp; \vec v_n^\top &amp; \rightarrow
    \end{bmatrix}
\end{align*}\]</div>
<section id="the-laplacian-as-a-sum-of-rank-1-matrices">
<h3>The Laplacian as a sum of rank <span class="math notranslate nohighlight">\(1\)</span> matrices<a class="headerlink" href="#the-laplacian-as-a-sum-of-rank-1-matrices" title="Permalink to this headline">#</a></h3>
<p>Let’s express this operation using the Laplacian, by looking at the first six rank-1 matrices, and looking at the sum of all of the rank-1 matrices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_nodes</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">low_rank_matrices</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">low_rank_matrices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">U</span><span class="p">[:,[</span><span class="n">i</span><span class="p">]])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Vt</span><span class="p">[[</span><span class="n">i</span><span class="p">],:]))</span>
<span class="n">Lsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">low_rank_matrices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.gridspec</span> <span class="kn">import</span> <span class="n">GridSpec</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">ax_laplacian</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:])</span>

<span class="c1"># Plot low-rank matrices</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">])</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;$\sigma_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> u_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> v_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">^T$&quot;</span>
        <span class="n">heatmap</span><span class="p">(</span><span class="n">low_rank_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        
<span class="c1"># Plot Laplacian</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Lsum</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax_laplacian</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$L = \sum_{i = 1}^n \sigma_i u_i v_i^T$&quot;</span><span class="p">)</span>

<span class="c1"># # Colorbar</span>
<span class="n">cax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">.04</span><span class="p">,</span> <span class="mf">.8</span><span class="p">])</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Lsum</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Lsum</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">color</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">use_gridspec</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>


<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;You can recreate your simple Laplacian by summing all the low-rank matrices&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/lse_9_0.png" src="../../_images/lse_9_0.png" />
</div>
</div>
</section>
</section>
<section id="critical-properties-of-the-svd">
<h2>Critical Properties of the <code class="docutils literal notranslate"><span class="pre">svd</span></code><a class="headerlink" href="#critical-properties-of-the-svd" title="Permalink to this headline">#</a></h2>
<p>To get a little deeper in the weeds, we’re also going to repeatedly make use of a property of the Singular Value Decomposition that will be very handy for this section and the next section:</p>
<p>For more technical and generalized details on how <code class="docutils literal notranslate"><span class="pre">svd</span></code> works, or for explicit proofs, we would recommend a Linear Algebra textbook such as <code class="xref std std-numref docutils literal notranslate"><span class="pre">Axler</span></code> or <code class="xref std std-numref docutils literal notranslate"><span class="pre">Trefethen1997</span></code>. We’ll look at the <code class="docutils literal notranslate"><span class="pre">svd</span></code> with a bit more detail here in the specific case where you start with a matrix which is square, symmetric, and has real singular values. You can see this fact by noting that if <span class="math notranslate nohighlight">\(X\)</span> is symmetric with the eigendecomposition <span class="math notranslate nohighlight">\((Q, \Lambda)\)</span>, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X = Q\Lambda Q^\top &amp;= \sum_{i = 1}^n \vec q_i \lambda_i \vec q_i^\top =  \sum_{i = 1}^n \vec q_i |\lambda_i|\sign(\lambda_i) \vec q_i^\top.
\end{align*}\]</div>
<p>This looks really similar to the equation we saw above for the singular value decomposition, and in fact it shows us something really interesting: a singular value decomposition <em>almost</em> gives us an eigendecomposition of <span class="math notranslate nohighlight">\(X\)</span>. If we were to take the left singular vectors <span class="math notranslate nohighlight">\(\vec u_i = \vec q_i\)</span>, the singular values <span class="math notranslate nohighlight">\(|\lambda_i|\)</span>, and the right singular vectors <span class="math notranslate nohighlight">\(\vec v_i = \sign(\lambda_i) \vec q_i\)</span>, we have a singular value decomposition of <span class="math notranslate nohighlight">\(X\)</span>. Likewise, (somewhat more interestingly), if we have a singular value decomposition of <span class="math notranslate nohighlight">\(X\)</span> as <span class="math notranslate nohighlight">\((U, \Sigma, V)\)</span>, then the columns of <span class="math notranslate nohighlight">\(U\)</span> are also eigenvectors of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<section id="the-symmetric-laplacian-left-right-singular-vector-equivalence">
<h3>The symmetric Laplacian left/right singular vector equivalence<a class="headerlink" href="#the-symmetric-laplacian-left-right-singular-vector-equivalence" title="Permalink to this headline">#</a></h3>
<p>Next, we’ll see that the left and right singular vectors are equivalent for the Laplacian of the diagonally-augmented adjacency matrix for the non-zero singular values (which in this case, is all of them, as <span class="math notranslate nohighlight">\(L\)</span> is <em>positive definite</em>). We take a look at the columns of <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, as well as their difference. We will quantify the difference between the first four columns of <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> using the <em>Frobenius norm</em> of the difference between them. This quantity, abbreviated <span class="math notranslate nohighlight">\(||U - V||_F\)</span>, has a value greater than <span class="math notranslate nohighlight">\(0\)</span> if the two matrices have any entries not in common:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">U</span> <span class="o">-</span> <span class="n">Vt</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 1.          0.67458987  0.21337895  0.18851716  0.15410462  0.11674234
 -0.37030786 -0.35020252 -0.32866122 -0.2711415  -0.24919356 -0.21174216
 -0.16502093 -0.15438362 -0.12292021  0.02157577  0.00729992 -0.02987015
 -0.06635255 -0.05641237]
[1.         0.67458987 0.37030786 0.35020252 0.32866122 0.2711415
 0.24919356 0.21337895 0.21174216 0.18851716 0.16502093 0.15438362
 0.15410462 0.12292021 0.11674234 0.06635255 0.05641237 0.02987015
 0.02157577 0.00729992]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">cbar_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">.91</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mf">.03</span><span class="p">,</span> <span class="mf">.4</span><span class="p">])</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">U</span><span class="p">,</span> <span class="n">Vt</span><span class="p">])),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">U</span><span class="p">,</span> <span class="n">Vt</span><span class="p">]))</span>

<span class="n">mtxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">U</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Vt</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">U</span> <span class="o">-</span> <span class="n">Vt</span><span class="o">.</span><span class="n">transpose</span><span class="p">()]</span>
<span class="n">title</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Columns of U&quot;</span><span class="p">,</span> <span class="s2">&quot;Columns of V&quot;</span><span class="p">,</span> <span class="s2">&quot;first two columns of U - V, diff = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mtxs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
           <span class="n">cbar_ax</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">i</span> <span class="k">else</span> <span class="n">cbar_ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">color</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([]);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/lse_12_0.png" src="../../_images/lse_12_0.png" />
</div>
</div>
</section>
<section id="you-can-approximate-your-symmetric-laplacian-by-only-summing-a-few-of-the-low-rank-matrices">
<h3>You can approximate your symmetric Laplacian by only summing a few of the low-rank matrices<a class="headerlink" href="#you-can-approximate-your-symmetric-laplacian-by-only-summing-a-few-of-the-low-rank-matrices" title="Permalink to this headline">#</a></h3>
<p>Your expression has simplified down a little bit, but we aren’t quite finished. If the Laplacian <span class="math notranslate nohighlight">\(L\)</span> has the singular value decomposition with matrix <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span>, then you could express <span class="math notranslate nohighlight">\(L\)</span> as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    L &amp;= \sum_{i = 1}^n \sigma_i \vec u_i \vec v_i^\top
\end{align*}\]</div>
<p>But wait, we know that some of these singular vectors are just redundant: their singular value is zero! And for all of the other left and right singular vectors, you knew that the corresponding singular vectors were exactly equivalent So this means that, if the first <span class="math notranslate nohighlight">\(K\)</span> singular values are non-zero, that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    L &amp;= \sum_{i = 1}^K \sigma_i\vec u_i \vec u_i^\top
\end{align*}\]</div>
<p>For each term in this sum, remember we said that you had a rank <span class="math notranslate nohighlight">\(1\)</span> matrix whose columns are multiples of <span class="math notranslate nohighlight">\(\vec u_i\)</span> (where the exact multiplicative factor was given by <span class="math notranslate nohighlight">\(\vec v_i\)</span>, but now is also given by <span class="math notranslate nohighlight">\(\vec u_i\)</span> since <span class="math notranslate nohighlight">\(\vec u_i = \vec v_i\)</span> for these vectors with non-zero singular values) weighted by <span class="math notranslate nohighlight">\(\sigma_i\)</span>. What do we mean by <em>weighted</em> here?</p>
<p>To better understand this term, we’ll use a few facts. Remember that we said that the Laplacian had positive singular values, which means that their sum, <span class="math notranslate nohighlight">\(s = \sum_{i = 1}^n \sigma_i\)</span>, is also positive. Let’s rewrite your expression a tiny bit:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    L &amp;= \sum_{i = 1}^K s\frac{\sigma_i}{s}\vec u_i \vec v_i^\top \\
    &amp;= s\sum_{i = 1}^K \frac{\sigma_i}{s}\vec u_i \vec v_i^\top
\end{align*}\]</div>
<p>All we have done here is used the fact that <span class="math notranslate nohighlight">\(\frac{s}{s} = 1\)</span>, so we basically just <em>pulled out</em> a term of <span class="math notranslate nohighlight">\(s\)</span> from every element.</p>
<p>Now we can start to understand this expression a little bit better. Notice that since the singular values are positive, that the term <span class="math notranslate nohighlight">\(\frac{\sigma_i}{s}\)</span> is going to be a fraction for every singular value/vector (it will be between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>). To take it a step further, the sum of all of these terms will, in fact, be <span class="math notranslate nohighlight">\(1\)</span>! That is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{i = 1}^K \frac{\sigma_i}{s} = 1
\end{align*}\]</div>
<p>This is because you could pull out the common factor <span class="math notranslate nohighlight">\(\frac{1}{s} = \frac{1}{\sum_{i = 1}^n \sigma_i}\)</span> from every element of the sum, and then you are just left with:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{1}{\sum_{i = 1}^K \sigma_i}\sum_{i = 1}^n \sigma_i = 1
\end{align*}\]</div>
<p>So in a sense, the quantity <span class="math notranslate nohighlight">\(\frac{\sigma_i}{s}\)</span> tells you the <em>fraction</em> of <span class="math notranslate nohighlight">\(L\)</span> that is <em>explained</em> by <span class="math notranslate nohighlight">\(\vec u_i \vec u_i^\top\)</span>. This quantity represents how much of <span class="math notranslate nohighlight">\(\vec u_i \vec u_i^\top\)</span> you need to include in order to obtain <span class="math notranslate nohighlight">\(L\)</span>. But there’s another fun fact: remember that the singular values were all ordered in <em>decreasing</em> order! This meant that <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_K\)</span>. So, if you just add <span class="math notranslate nohighlight">\(\frac{1}{s}\)</span> to each of these, since <span class="math notranslate nohighlight">\(s\)</span> is positive, <span class="math notranslate nohighlight">\(\frac{1}{s}\)</span> will be too, and consequently, <span class="math notranslate nohighlight">\(\frac{\sigma_1}{s} \geq \frac{\sigma_2}{s} \geq ... \geq \frac{\sigma_K}{s}\)</span>!</p>
<p>What this means is that to describe <span class="math notranslate nohighlight">\(L\)</span>, you need <em>more</em> of the first few singular vectors than you do of the later singular vectors. These singular vectors will comprise a bigger <em>fraction</em> of <span class="math notranslate nohighlight">\(L\)</span> than the others.</p>
<p>Logically, if you wanted to take <span class="math notranslate nohighlight">\(L\)</span> and form a <em>best</em> representation with only a single rank-<span class="math notranslate nohighlight">\(1\)</span> matrix, wouldn’t it make a lot of sense to take the rank-<span class="math notranslate nohighlight">\(1\)</span> matrix which was the largest fraction of <span class="math notranslate nohighlight">\(L\)</span>? And if you wanted to take <span class="math notranslate nohighlight">\(L\)</span> and form the <em>best</em> representation with a rank-<span class="math notranslate nohighlight">\(2\)</span> matrix, what could you do there?</p>
<p>You will remember we brought up a fact about <span class="math notranslate nohighlight">\(U\)</span>: it was orthonormal, which meant that the columns <span class="math notranslate nohighlight">\(\vec u_i\)</span> were not sums of other columns nor their multiples. This means that to understand a sum of any two different rank-<span class="math notranslate nohighlight">\(1\)</span> matrices <span class="math notranslate nohighlight">\(\sigma_i\vec u_i\vec u_i^\top\)</span> and <span class="math notranslate nohighlight">\(\sigma_j\vec u_j\vec u_j^\top\)</span> for some combination <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> that make up <span class="math notranslate nohighlight">\(L\)</span> be rank-<span class="math notranslate nohighlight">\(2\)</span>: you cannot express <span class="math notranslate nohighlight">\(\vec u_i\)</span> as a multiple of <span class="math notranslate nohighlight">\(\vec u_j\)</span>, and vice-versa.</p>
<p>So then to get the best rank-<span class="math notranslate nohighlight">\(2\)</span> representation of <span class="math notranslate nohighlight">\(L\)</span>, wouldn’t it make sense for you to take the two rank-<span class="math notranslate nohighlight">\(1\)</span> matrices that were the largest and second largest fraction of <span class="math notranslate nohighlight">\(L\)</span>? We think so too! This pattern, coupled with the orthonormality of <span class="math notranslate nohighlight">\(U\)</span>, gives you that <span class="math notranslate nohighlight">\(L_d\)</span> is a rank-<span class="math notranslate nohighlight">\(d\)</span> representation of <span class="math notranslate nohighlight">\(L\)</span> as long as the singular values are non-zero.</p>
<p>As it turns out, for a Laplacian, the best rank-<span class="math notranslate nohighlight">\(d\)</span> representation is the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    L_d &amp;= \sum_{i = 1}^d \sigma_i \vec u_i \vec u_i^\top
\end{align*}\]</div>
<p>When we say best here, we mean the representation where the Frobenius norm of the difference (there’s that term again!) <span class="math notranslate nohighlight">\(||L - L_d||_F\)</span> is at a minimum. This result holds true for <em>any</em> svd of a matrix:</p>
<p>This concept of matrix rank allows us to capture an extremely useful property of the SVD: the first <span class="math notranslate nohighlight">\(d\)</span> entries of each of the matrices <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> will give us the best rank-<span class="math notranslate nohighlight">\(d\)</span> approximation of our original matrix:</p>
<p>In this sense, <span class="math notranslate nohighlight">\(L_d\)</span> is the best rank-<span class="math notranslate nohighlight">\(d\)</span> <em>approximation</em> of the original matrix <span class="math notranslate nohighlight">\(L\)</span>. A similar property holds in general for <em>any</em> svd, with a lot fewer restrictions than we’ve placed here (without needing the left and right singular vectors to be equivalent), but this will suffice intuitionally for your purposes that you need going forward. We have the convenient intuition for the case where we are working with a positive semi-definite matrix that each successive rank-<span class="math notranslate nohighlight">\(1\)</span> matrix we are adding is a lower and lower <em>fraction</em> of <span class="math notranslate nohighlight">\(L\)</span>, which does not quite hold for the more general case of an svd.</p>
<p>This tells you something interesting about spectral embedding: the information in the first few singular vectors of a high rank matrix lets you find a more simple approximation to it. You can take a matrix that’s extremely complicated (high-rank) and project it down to something which is much less complicated (low-rank).</p>
<p>Look below. In each plot, we’re summing more and more of these low-rank matrices. By the time we get to the fourth sum, we’ve totally recreated the original Laplacian, and the remaining terms don’t even matter.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">current</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">new</span> <span class="o">=</span> <span class="n">low_rank_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">current</span> <span class="o">+=</span> <span class="n">new</span>
    <span class="n">heatmap</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$\sum_</span><span class="se">{{</span><span class="s2">i = 1</span><span class="se">}}</span><span class="s2">^</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> \sigma_i u_i u_i^T$&quot;</span><span class="p">)</span>
    
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Each of these is the sum of an </span><span class="se">\n</span><span class="s2">increasing number of low-rank matrices&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="approximating-becomes-extremely-useful-when-you-have-a-bigger-now-regularized-laplacian">
<h3>Approximating becomes extremely useful when you have a bigger (now regularized) Laplacian<a class="headerlink" href="#approximating-becomes-extremely-useful-when-you-have-a-bigger-now-regularized-laplacian" title="Permalink to this headline">#</a></h3>
<p>This becomes even more useful when you have huge networks with thousands of nodes, but only a few communities. It turns out, especially in this situation, you can usually sum a very small number of low-rank matrices and get to an excellent approximation for your network that uses much less information.</p>
<p>Take the network below, for example. It’s generated from a <a class="reference external" href="#ch5:sbm">stochastic block model</a> with 50 nodes total (25 in one community, 25 in another):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">to_laplacian</span>

<span class="c1"># Make network</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span>
<span class="n">Abig</span><span class="p">,</span> <span class="n">labelsbig</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You take its <a class="reference internal" href="../../representations/ch4/matrix-representations.html#ch4-mtx-rep-dad-laplacian"><span class="std std-ref">DAD Laplacian</span></a> (remember that this means <span class="math notranslate nohighlight">\(L = D^{-1/2} A D^{-1/2}\)</span>), decompose it, and sum the first three low-rank matrices that you generated from the singular vector columns.</p>
<p>The result is not exact, but it looks pretty close. And you only needed the information from the first two singular vectors instead of all of the information in your full <span class="math notranslate nohighlight">\(n \times n\)</span> matrix!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Form new laplacian</span>
<span class="n">Lbig</span> <span class="o">=</span> <span class="n">to_laplacian</span><span class="p">(</span><span class="n">Abig</span><span class="p">)</span>

<span class="c1"># decompose</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Ubig</span><span class="p">,</span> <span class="n">sbig</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">Lbig</span><span class="p">)</span>

<span class="n">Ubigk</span> <span class="o">=</span> <span class="n">Ubig</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">d</span><span class="p">];</span> <span class="n">Sbigk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sbig</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">d</span><span class="p">])</span>
<span class="n">low_rank_approximation</span> <span class="o">=</span> <span class="n">Ubigk</span> <span class="o">@</span> <span class="n">Sbigk</span> <span class="o">@</span> <span class="n">Ubigk</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">l2_hm</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">Lbig</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$L$&quot;</span><span class="p">)</span>
<span class="n">l2approx_hm</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">low_rank_approximation</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$L_2 = \sum_{{i = 1}}^</span><span class="si">{2}</span><span class="s2"> \sigma_i u_i u_i^T$&quot;</span><span class="p">)</span>

<span class="n">l2_hm</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Full-rank Laplacian for a 50-node matrix&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">})</span>
<span class="n">l2approx_hm</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Sum of only two low-rank matrices&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">});</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Summing only two low-rank matrices approximates the normalized Laplacian pretty well!&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This is where a lot of the power of an <code class="docutils literal notranslate"><span class="pre">svd</span></code> comes from: you can approximate extremely complicated (high-rank) matrices with extremely simple (low-rank) matrices.</p>
</section>
</section>
<section id="using-the-svd-to-understand-the-estimated-latent-position-matrix">
<h2>Using the <code class="docutils literal notranslate"><span class="pre">svd</span></code> to understand the estimated latent position matrix<a class="headerlink" href="#using-the-svd-to-understand-the-estimated-latent-position-matrix" title="Permalink to this headline">#</a></h2>
<p>So, now we’ve taken your Laplacian <span class="math notranslate nohighlight">\(L\)</span>, and we’ve reduced it down to the most important <span class="math notranslate nohighlight">\(d\)</span> low-rank matrices. You expressed <span class="math notranslate nohighlight">\(L_d\)</span> as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    L_d &amp;= \sum_{i = 1}^d\sigma_i \vec u_i \vec u_i^\top
\end{align*}\]</div>
<p>To keep the fun going, we’re going to back up a step. To do this, we’ll introduce two new matrices, <span class="math notranslate nohighlight">\(\Sigma_d\)</span> and <span class="math notranslate nohighlight">\(U_d\)</span>. In this case, <span class="math notranslate nohighlight">\(\Sigma_d\)</span> is going to be a <span class="math notranslate nohighlight">\(d \times d\)</span> diagonal matrix, whose diagonal entries are the top <span class="math notranslate nohighlight">\(d\)</span> singular values of <span class="math notranslate nohighlight">\(L\)</span>, and <span class="math notranslate nohighlight">\(U_d\)</span> is going to be the <span class="math notranslate nohighlight">\(n \times d\)</span> matrix whose columns are the corresponding top <span class="math notranslate nohighlight">\(d\)</span> singular vectors of <span class="math notranslate nohighlight">\(L\)</span>. These matrices look like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    U_d &amp;= \begin{bmatrix}
    \uparrow &amp; &amp; \uparrow \\
    \vec u_1 &amp; ... &amp; \vec u_d \\
    \downarrow &amp; &amp; \downarrow
    \end{bmatrix},\;\;\; \Sigma_d = \begin{bmatrix}
        \sigma_1 &amp; 0 &amp; ... &amp; 0\\
        0 &amp; \sigma_2 &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sigma_d
    \end{bmatrix}
\end{align*}\]</div>
<p>As it turns out, just like you expressed the product <span class="math notranslate nohighlight">\(U\Sigma V^\top\)</span> as a sum, you can do the reverse here, too: you can express the above sum for <span class="math notranslate nohighlight">\(L_d\)</span> as a matrix product, by writing that <span class="math notranslate nohighlight">\(L_d = U_d \Sigma_d U_d^\top\)</span>. Why does this help us?</p>
<p>You know that the singular values for a Laplacian are non-negative, so they all have a square root. This means you could express <span class="math notranslate nohighlight">\(\sigma_i\)</span> as the product of <span class="math notranslate nohighlight">\(\sqrt{\sigma_i}\sqrt{\sigma_i}\)</span> with itself!</p>
<p>Finally, remember that if you were to multiply two diagonal matrices, the resulting matrix would just be the element-wise product of each diagonal entry. This means you could just write <span class="math notranslate nohighlight">\(\Sigma_d\)</span> like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Sigma_d &amp;= \begin{bmatrix}
        \sqrt{\sigma_1} &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \sqrt{\sigma_2} &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sqrt{\sigma_d}
    \end{bmatrix}\begin{bmatrix}
        \sqrt{\sigma_1} &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \sqrt{\sigma_2} &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sqrt{\sigma_d}
    \end{bmatrix}
\end{align*}\]</div>
<p>You will call the resulting matrix the “square root” matrix of <span class="math notranslate nohighlight">\(\Sigma_d\)</span>, abbreviated <span class="math notranslate nohighlight">\(\sqrt{\Sigma_d}\)</span> which hopefully is named for pretty obvious reasons. So, <span class="math notranslate nohighlight">\(\Sigma_d = \sqrt{\Sigma_d}\sqrt{\Sigma_d}\)</span>. This matrix has <span class="math notranslate nohighlight">\(d \times d\)</span> entries, and is therefore square. Also, notice that all the off-diagonal entries are just <span class="math notranslate nohighlight">\(0\)</span>, which means it’s symmetric too, because of the convenient fact that <span class="math notranslate nohighlight">\(0 = 0\)</span> (and hence, the off-diagonal entries are all equal). Putting this fact together means that <span class="math notranslate nohighlight">\(\sqrt{\Sigma_d} = \sqrt{\Sigma_d}^\top\)</span>, which is the definition of matrix symmetry. So finally, <span class="math notranslate nohighlight">\(\Sigma_d = \sqrt{\Sigma_d}\sqrt{\Sigma_d}^\top\)</span>. This gives you that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    L_d &amp;= U_d \sqrt{\Sigma_d}\sqrt{\Sigma_d}^\top U_d^\top
\end{align*}\]</div>
<p>Or stated another way, if you call <span class="math notranslate nohighlight">\(\hat X_d = U_d \sqrt{\Sigma_d}\)</span>, then <span class="math notranslate nohighlight">\(L_d = \hat X_d \hat X_d^\top\)</span>. This means that the matrix <span class="math notranslate nohighlight">\(X_d\)</span> contains <em>all</em> of the information you need to describe <span class="math notranslate nohighlight">\(L_d\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\hat X_d\)</span> contains all the information you need to study <span class="math notranslate nohighlight">\(L_d\)</span>, why wouldn’t you just study <span class="math notranslate nohighlight">\(\hat X_d\)</span> itself? This is exactly what you do. Because <span class="math notranslate nohighlight">\(\hat X_d\)</span> is so important, you give it a special name: you call <span class="math notranslate nohighlight">\(\hat X_d\)</span> a rank-<span class="math notranslate nohighlight">\(d\)</span> estimate of the <strong>latent positions of the Laplacian</strong>. It looks like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat X_d &amp;= U_d \sqrt{\Sigma_d} = \begin{bmatrix}
        \uparrow &amp; &amp; \uparrow \\
        \sqrt{\sigma_1}\vec u_1 &amp; ... &amp; \sqrt{\sigma_d}\vec u_d \\
        \downarrow &amp; &amp; \downarrow
    \end{bmatrix}
\end{align*}\]</div>
<p>It is rank-<span class="math notranslate nohighlight">\(d\)</span> because it has <span class="math notranslate nohighlight">\(d\)</span> unique columns, called the estimates of the latent dimensions of <span class="math notranslate nohighlight">\(L\)</span>. These columns are the unique vectors <span class="math notranslate nohighlight">\(\vec u_i\)</span> that you needed to best describe <span class="math notranslate nohighlight">\(L\)</span>, and then weighted by just how important they were <span class="math notranslate nohighlight">\(\sqrt{\sigma_i}\)</span>.</p>
</section>
<section id="how-matrix-rank-helps-you-understand-spectral-embedding">
<h2>How Matrix Rank Helps you Understand Spectral Embedding<a class="headerlink" href="#how-matrix-rank-helps-you-understand-spectral-embedding" title="Permalink to this headline">#</a></h2>
<p>Let’s take a closer look at the estimate of the latent positions for for <span class="math notranslate nohighlight">\(50\)</span> node example you just generated. You again embed into two dimensions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Ud</span> <span class="o">=</span> <span class="n">Ubig</span><span class="p">[:,</span> <span class="p">:</span><span class="n">d</span><span class="p">]</span>
<span class="n">Sdsqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sbig</span><span class="p">[:</span><span class="n">d</span><span class="p">]))</span>

<span class="n">Xhat</span> <span class="o">=</span> <span class="n">Ud</span> <span class="o">@</span> <span class="n">Sdsqrt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt{</span><span class="se">\\</span><span class="s2">sigma_1}</span><span class="se">\\</span><span class="s2">vec u_1$&quot;</span><span class="p">,</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt{</span><span class="se">\\</span><span class="s2">sigma_2}</span><span class="se">\\</span><span class="s2">vec u_2$&quot;</span><span class="p">,</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt{</span><span class="se">\\</span><span class="s2">sigma_3}</span><span class="se">\\</span><span class="s2">vec u_3$&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">49</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;26&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Estimate of Latent Position Matrix $</span><span class="se">\\</span><span class="s2">hat X_2$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<p>When you want to study the network, you can look at the estimate of the latent positions of the Laplacian, <span class="math notranslate nohighlight">\(\hat X_d\)</span>. This is called investigating the <strong>Laplacian Spectral Embedding</strong> (or, more succinctly, the <em>LSE</em>) of the network, and is a crucial technique to understanding a network. In particular, the property that you can study here is that the first two dimensions capture the difference between the nodes in community one (the first three nodes) and the nodes in community two (the second three nodes). The nodes in community two all have the first dimension having the same value (light purple) and the nodes in community one all have the second dimension having the same value (also light purple). That the dimensions of the estimated latent positions capture differences between your communities is an important reason the LSE is a crucial technique to understand.</p>
<section id="why-the-dad-laplacian">
<h3>Why the DAD Laplacian?<a class="headerlink" href="#why-the-dad-laplacian" title="Permalink to this headline">#</a></h3>
<p>Remember back in <code class="docutils literal notranslate"><span class="pre">ch4:mat-rep:dad_laplacian</span></code> when we investigated the <code class="docutils literal notranslate"><span class="pre">DAD</span></code> Laplacian as an alternative to the normalized Laplacian? Why don’t we just use the normalized Laplacian instead?</p>
<p>As it turns out, the singular vectors for the <code class="docutils literal notranslate"><span class="pre">DAD</span></code> Laplacian and the normalized Laplacian are identical except for one caveat: they are in <em>reverse</em> order from one another. This means that the singular values/vectors you would want to produce an estimate of the latent position matrix are the first few for the <code class="docutils literal notranslate"><span class="pre">DAD</span></code> Laplacian, but the <em>last</em> few for the normalized Laplacian. This has to do with a number of theoretical results, which are not ultra critical for you to understand at this point in your learning.</p>
<p>So then why does this matter to you, as a budding network scientist? As it turns out, the singular value decomposition is pretty computationally intensive to compute – that is, when the number of nodes in the network are big, we might be waiting a <em>while</em> to compute the full singular value decomposition. However, if we know that we only want <span class="math notranslate nohighlight">\(K\)</span> of the singular values/vectors ahead of time, we don’t need to compute them all: we can just compute the first <span class="math notranslate nohighlight">\(K\)</span> singular values/vectors, and then stop. This means that if we know the number of singular values/vectors that we want before using <code class="docutils literal notranslate"><span class="pre">LSE</span></code>, we can get <em>much</em> faster computational performance by using the <code class="docutils literal notranslate"><span class="pre">DAD</span></code> Laplacian and retaining the first few vectors than we can using the normalized Laplacian and the last few vectors.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./appendix/ch13"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>