

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>13.1. Hypothesis Testing with coin flips &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'appendix/ch14/hypothesis';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13.2. Unsupervised learning" href="unsupervised.html" />
    <link rel="prev" title="13. Applications (Extended)" href="ch14.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../coverpage.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/terminology.html">Terminology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch1/ch1.html">1. The Network Machine Learning Landscape</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">1.1. What is network machine learning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/why-study-networks.html">1.2. Why do we study networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">1.3. Types of Network Machine Learning Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">1.4. Examples of applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">1.5. Challenges of Network Machine Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch2/ch2.html">2. End-to-end Biology Network Machine Learning Project</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/big-picture.html">2.1. Look at the big picture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/get-the-data.html">2.2. Get the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">2.3. Prepare the Data for Network Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/select-and-train.html">2.4. Select and Train a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/fine-tune.html">2.5. Fine-Tune your Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">2.6. Discover and Visualize the Data to Gain Insights</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Representations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch4/ch4.html">3. Properties of Networks as a Statistical Object</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/matrix-representations.html">3.1. Matrix Representations Of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/properties-of-networks.html">3.2. Properties of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/network-representations.html">3.3. Representations of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/regularization.html">3.4. Regularization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch5/ch5.html">4. Why Use Statistical Models?</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">4.1. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">4.2. Stochastic Block Models (SBM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">4.3. Random Dot Product Graphs (RDPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SIEM.html">4.5. Structured Independent Edge Model (SIEM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/multi-network-models.html">4.6. Multiple Network Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/models-with-covariates.html">4.7. Network Models with Network Covariates</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch6/ch6.html">5. Learning Network Representations</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">5.1. Estimating Parameters in Network Models via MLE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/why-embed-networks.html">5.2. Why embed networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/spectral-embedding.html">5.3. Spectral embedding methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">5.4. Multiple-Network Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">5.5. Joint Representation Learning</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch7/ch7.html">6. Applications When You Have One Network</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/community-detection.html">6.1. Community Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/testing-differences.html">6.2. Testing for Differences between Groups of Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/model-selection.html">6.3. Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/vertex-nomination.html">6.4. Single-Network Vertex Nomination</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/out-of-sample.html">6.5. Out-of-sample Embedding</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch8/ch8.html">7. Applications for Two Networks</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html">7.1. Latent Two-Sample Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/significant-communities.html">7.2. Two-sample hypothesis testing in SBMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html">7.3. Graph Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/multiple-vertex-nomination.html">7.4. Vertex Nomination For Two Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch9/ch9.html">8. Applications for Many Networks</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/anomaly-detection.html">8.1. Anomaly Detection For Timeseries of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-edges.html">8.2. Testing for Significant Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-vertices.html">8.3. Testing for Significant Vertices</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Next Steps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../next/ch10/ch10.html">9. Where do we go from here?</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/random-walk-diffusion-methods.html">9.1. Random walk and diffusion-based methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/gnn.html">9.2. Graph Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/sparsity.html">9.3. Network Sparsity</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch11/ch11.html">10. Representations (Extended)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch11/alt-reps.html">10.1. Alternative Network Representations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch12/ch12.html">11. Network Model Theory</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch12/background.html">11.2. Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/foundation.html">11.3. Foundation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/ers.html">11.4. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/sbms.html">11.5. Stochastic Block Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch12/rdpgs.html">11.6. RDPGs and more general network models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch13/ch13.html">12. Learning Representations Theory</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch13/mle-theory.html">12.1. Maximum Likelihood Estimate Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch13/spectral-theory.html">12.2. Spectral Method Theory</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ch14.html">13. Applications (Extended)</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">13.1. Hypothesis Testing with coin flips</a></li>
<li class="toctree-l2"><a class="reference internal" href="unsupervised.html">13.2. Unsupervised learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayes.html">13.3. Bayes Plugin Classifier</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">Graspologic Documentation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/appendix/ch14/hypothesis.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fappendix/ch14/hypothesis.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/appendix/ch14/hypothesis.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Hypothesis Testing with coin flips</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-tell-us-the-chances-of-observing-an-outcome-if-the-null-hypothesis-is-true">13.1.1. <span class="math notranslate nohighlight">\(p\)</span>-values tell us the chances of observing an outcome if the null hypothesis is true</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-sample-tests-with-weighted-networks">13.1.2. Two-sample tests with weighted networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weighted-siem-has-a-vector-of-distribution-functions">13.1.2.1. The weighted SIEM has a vector of distribution functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mann-whitney-wilcoxon-u-test">13.1.2.1.1. The Mann-Whitney Wilcoxon <span class="math notranslate nohighlight">\(U\)</span> Test</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="hypothesis-testing-with-coin-flips">
<span id="app-ch14-hypotest-intro"></span><h1><span class="section-number">13.1. </span>Hypothesis Testing with coin flips<a class="headerlink" href="#hypothesis-testing-with-coin-flips" title="Permalink to this heading">#</a></h1>
<p>To describe how hypothesis testing works from an implementation perspective, we’ll naturally return to coin flips. A gambler has a coin, and asks us to gamble one dollar. If the coin lands on heads, we obtain our dollar back, and an additional dollar. If the coin lands on tails, we lose our dollar. We get to observe <span class="math notranslate nohighlight">\(10\)</span> random coin flips, and determine whether we want to play the game.</p>
<p>As it turns out, if this coin has a probability of landing on heads greater than <span class="math notranslate nohighlight">\(0.5\)</span>, we will, over time, <em>make</em> money if we continue to play the game. If the coin has a probablity of landing on heads less than <span class="math notranslate nohighlight">\(0.5\)</span>, we will, over time, <em>lose</em> money if we continue to play the game. So, if we are going to play the game in a principled way, we want to be really sure that we are going to gain money by playing. How can we use hypothesis testing to use a little bit of math to determine whether we should play or not?</p>
<p>To begin, we need to delineate our hypotheses. Here, our question of interest is, should we play the game? Therefore, the alternative hypothesis is the one where we <em>should</em> play the game, or the coin has a probability greater than <span class="math notranslate nohighlight">\(0.5\)</span>. The null hypothesis is that we should <em>not</em> play the game, or the coin has a probability that is not greaterr than <span class="math notranslate nohighlight">\(0.5\)</span>. If we use <span class="math notranslate nohighlight">\(p\)</span> to denote the probability that the gambler’s coin lands on heads, we write this as <span class="math notranslate nohighlight">\(H_A: p &gt; 0.5\)</span>, against the null hypothesis that <span class="math notranslate nohighlight">\(H_0: p \leq 0.5\)</span>.</p>
<p>Let’s say that we observe <span class="math notranslate nohighlight">\(10\)</span> coin flips, and we see that the coin lands on heads <span class="math notranslate nohighlight">\(6\)</span> times. If we were to estimate the coin’s probability of landing on heads, like we did in <a class="reference internal" href="../ch13/mle-theory.html#app-ch13-mle"><span class="std std-numref">Section 12.1</span></a>, we would get that <span class="math notranslate nohighlight">\(\hat p = \frac{6}{10}\)</span>, or that our estimate <span class="math notranslate nohighlight">\(\hat p = 0.6\)</span>. It’s pretty obvious that <span class="math notranslate nohighlight">\(\hat p\)</span> is greater than <span class="math notranslate nohighlight">\(0.5\)</span>, so we should play the game, right?</p>
<p>The answer is a little more complicated than that. If you take a fair coin (one that is roughly equally likely to land on heads and tails) and flip it enough times, you will probably find a sequence of ten flips with six heads. We are scientists, after all, so we want to be <em>really</em> sure that if we play we are going to make money. In statistical speak, we want to be sure that if we play the game, there is a low chance that we are actually wrong. We want to know exactly what the chances are that, if we were <em>wrong</em>, we could still have seen the estimate <span class="math notranslate nohighlight">\(\hat p\)</span> that we calculated. This is where a concept called a <em>p-value</em> comes into play.</p>
<section id="p-values-tell-us-the-chances-of-observing-an-outcome-if-the-null-hypothesis-is-true">
<h2><span class="section-number">13.1.1. </span><span class="math notranslate nohighlight">\(p\)</span>-values tell us the chances of observing an outcome if the null hypothesis is true<a class="headerlink" href="#p-values-tell-us-the-chances-of-observing-an-outcome-if-the-null-hypothesis-is-true" title="Permalink to this heading">#</a></h2>
<p>To understand the <span class="math notranslate nohighlight">\(p\)</span>-value, we must first introduce a concept called a <em>test statistic</em>. A <strong>test-statistic</strong> is a fancy word for a data-derived quantity that summarizes our data (which is the definition of a <em>statistic</em>) with a single number which will be used for hypothesis testing (and hence, it is a <em>test statistic</em>). The test statistic should capture a property of the data that is relevant to our question of interest (here, the probability that the coin lands on heads), so it makes sense in our example to just use the estimate of the probability that the coin lands on heads as our test statistic. In general, the <strong><span class="math notranslate nohighlight">\(p\)</span>-value</strong> is the probability that we would incorrectly reject the null hypothesis (that is, that the coin is fair) in favor of the alternative hypothesis (that is, that the coin is not fair) if the null hypothesis were really true.</p>
<p>Next, we need to think about what types of values we would see from the test statistic if the null hypothesis were true. If the coin really had a probability of <span class="math notranslate nohighlight">\(0.5\)</span> of landing on heads and we flipped it <span class="math notranslate nohighlight">\(10\)</span> times, there is a non-zero probabaility that we would see all heads and our estimate would have been <span class="math notranslate nohighlight">\(1.0\)</span>, that we would see <span class="math notranslate nohighlight">\(9\)</span> heads and <span class="math notranslate nohighlight">\(1\)</span> tails and our estimate would have been <span class="math notranslate nohighlight">\(0.9\)</span>, so on and so-forth all the way to the outcome where we see only tails and our estimate would have been <span class="math notranslate nohighlight">\(0.0\)</span>. Which one we observed when watching the gambler flip the coin would come down to <em>random chance</em>, so we will use the random variable <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span> to describe this quantity. Specifically, <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span> is a random variable which takes values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> in <span class="math notranslate nohighlight">\(0.1\)</span> probability increments. <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span>’s probability of taking a given increment <span class="math notranslate nohighlight">\(k\)</span> is equal to the probability that we would have estimated the probability to be <span class="math notranslate nohighlight">\(k\)</span> if the null hypothesis (the coin is fair and lands on heads with probability <span class="math notranslate nohighlight">\(0.5\)</span>) were true. <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span> takes each increment with the following probabilities:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">incr</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">/</span><span class="mi">10</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">)]</span>
<span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">i</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">incr</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">incr</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Estimate of probability of landing on heads, $\hat p_0$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability of seeing this estimate if $H_0$ true&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Distribution of $\hat p_0$&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="s2">&quot;Estimate we observed, $\hat p = 0.6$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/695a3428356ddd8ff3a4f643f3cc1a74b44e740698e1778406c4195231974e80.png" src="../../_images/695a3428356ddd8ff3a4f643f3cc1a74b44e740698e1778406c4195231974e80.png" />
</div>
</div>
<p>So, this is a little surprising! Even if the null hypothesis were true and the coin really landed on heads with a probability of <span class="math notranslate nohighlight">\(0.5\)</span>, there is still a pretty good chance we would observe a probability of at least <span class="math notranslate nohighlight">\(0.6\)</span> if we were to flip a fair coin <span class="math notranslate nohighlight">\(10\)</span> times. We describe this quantity using a <span class="math notranslate nohighlight">\(p\)</span>-value, which is the probability of seeing a test statistic at least as extreme as the one we calculated from our data if the null hypothesis were true. Here, the <span class="math notranslate nohighlight">\(p\)</span>-value is the probability that, if the coin were fair, we would produce an estimate of <span class="math notranslate nohighlight">\(0.6\)</span>, <span class="math notranslate nohighlight">\(0.7\)</span>, <span class="math notranslate nohighlight">\(0.8\)</span>, <span class="math notranslate nohighlight">\(0.9\)</span>, or <span class="math notranslate nohighlight">\(1.0\)</span>. This is the sum of all of the heights of the corresponding bars in the above plot, which turns out to be <span class="math notranslate nohighlight">\(0.377\)</span>. This approach is called the <em>binomial test</em>, which we can implement in python as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom_test</span>

<span class="n">nheads</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1"># the number of heads in the experiment</span>
<span class="n">nflips</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># the total number of coin flips in the experiment</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># the probability of the coin under the null hypothesis</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">binom_test</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span> <span class="n">nflips</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s2">&quot;greater&quot;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p-value: 0.377
</pre></div>
</div>
</div>
</div>
<p>This might be enough for you to play the game, but as scientists we usually like to be a little more confident than this when we report something. We use the term the “<span class="math notranslate nohighlight">\(\alpha\)</span> of the test” to describe what an acceptable chance of being incorrect is for us. We can pick <span class="math notranslate nohighlight">\(\alpha\)</span> to be anything we want really so long as decide this threshold before calculating the <span class="math notranslate nohighlight">\(p\)</span>-value. When the <span class="math notranslate nohighlight">\(p\)</span>-value is less than <span class="math notranslate nohighlight">\(\alpha\)</span>, we report that we have evidence in support of rejecting the null hypothesis in favor of the alternative hypothesis. When the <span class="math notranslate nohighlight">\(p\)</span>-value is greater than <span class="math notranslate nohighlight">\(\alpha\)</span>, we report that we have insufficient evidence to rejectthe null hypothesis in favor of the alternative hypothesis.</p>
<p>Usually, scientists choose <span class="math notranslate nohighlight">\(\alpha\)</span> to be <span class="math notranslate nohighlight">\(0.05\)</span>. Intuitively, this corresponds to us stating that, after all of the assumptions that we made throughout the scientific process (things that went into the statistical modelling), we consider an acceptable fraction of outcomes where we incorrectly reject the null hypothesis in favor of the alternative hypothesis to be about <span class="math notranslate nohighlight">\(5\%\)</span> of the tim. In other words, we look for a <span class="math notranslate nohighlight">\(p\)</span>-value that is less than <span class="math notranslate nohighlight">\(0.05\)</span>.</p>
<p>It is worth noting that there are multiple types of statistical tests. In this example, we saw an instance of something called a <em>one-sample test</em>. A <strong>one-sample test</strong> is a statistical test in which we derive conclusions using a single sample of data (hence the term, one-sample). In this case, we only have a single sample, which are the outcomes from each of the coin flips. This is only called a one-sample test, and not a (however many times we flip the coin) sample test because we expect that all of the coin flips are the outcome of identical coins (the same coin is used for each flip).</p>
</section>
<section id="two-sample-tests-with-weighted-networks">
<span id="app-ch14-hypotest-intro-weighted"></span><h2><span class="section-number">13.1.2. </span>Two-sample tests with weighted networks<a class="headerlink" href="#two-sample-tests-with-weighted-networks" title="Permalink to this heading">#</a></h2>
<p>When the networks were unweighted, everything made complete sense in the context of our coin flip regime. For weighted networks, on the other hand, things can go a <em>little</em> bit differently. To understand this question in the context of weighted networks, we’ll return to our discussion on the SIEM, and reformulate it a little bit for the case where the adjacency matrix’s entries take non-binary values (they aren’t just zeros and ones).</p>
<section id="the-weighted-siem-has-a-vector-of-distribution-functions">
<h3><span class="section-number">13.1.2.1. </span>The weighted SIEM has a vector of distribution functions<a class="headerlink" href="#the-weighted-siem-has-a-vector-of-distribution-functions" title="Permalink to this heading">#</a></h3>
<p>For the weighted SIEM, the first parameter is identical to that of the unweighted SIEM: the edge cluster matrix, <span class="math notranslate nohighlight">\(Z\)</span>, which is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix whose entries <span class="math notranslate nohighlight">\(z_{ij}\)</span> indicate which of the <span class="math notranslate nohighlight">\(K\)</span> clusters the <span class="math notranslate nohighlight">\((i, j)\)</span> edge is in.</p>
<p>The second parameter is a little different. Remember for the unweighted case, we had a probability vector, <span class="math notranslate nohighlight">\(\vec p\)</span>, whose entries <span class="math notranslate nohighlight">\(p_k\)</span> indicated the probability of an edge in the <span class="math notranslate nohighlight">\(k^{th}\)</span> edge cluster. How do we extend this concept to edges which are not binary valued? To accomplish this, we introduce disribution functions.</p>
<p>Remember we said that, for a particular edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> where the edge cluster for the <span class="math notranslate nohighlight">\((i,j)\)</span> edge was <span class="math notranslate nohighlight">\(z_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> took a value of one with probability <span class="math notranslate nohighlight">\(p_{z_{ij}}\)</span>, and zero with probability <span class="math notranslate nohighlight">\(1 - p_{z_{ij}}\)</span>. As it turns out, what this says at a deeper level is that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> has a <em>distribution</em> whose parameter is just a probability <span class="math notranslate nohighlight">\(p_{z_{ij}}\)</span>. The distribution, in this case, is the <em>Bernoulli</em> distribution with probabilty <span class="math notranslate nohighlight">\(p_{z_{ij}}\)</span>. However, there are other distributions we could use as well for <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>.</p>
<p>You are probably already familiar with the Normal distribution, which has a mean <span class="math notranslate nohighlight">\(\mu\)</span> and a standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. What you need to know about the Normal distribution is that, if you were to sample a large number of points and then plotted their values on a histogram, you would see “roughly” a Bell-curve type of shape to the tops of the histogram bars. These bars would be centered roughly around the mean value <span class="math notranslate nohighlight">\(\mu\)</span>, the bars would be roughly symmetric (the heights on either side of the mean would be about equal), and that about <span class="math notranslate nohighlight">\(68\%\)</span> of the data would be between <span class="math notranslate nohighlight">\(\mu - \sigma\)</span> and <span class="math notranslate nohighlight">\(\mu + \sigma\)</span> (within one standard deviation). In the below plot, we will simulate <span class="math notranslate nohighlight">\(10,000\)</span> samples <span class="math notranslate nohighlight">\(x_i\)</span> of random variables <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>, which are Normally distributed with a mean of <span class="math notranslate nohighlight">\(0\)</span> and a standard deviation of <span class="math notranslate nohighlight">\(5\)</span>, <span class="math notranslate nohighlight">\(10,000\)</span> samples <span class="math notranslate nohighlight">\(y_i\)</span> of random variables <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> with are Normally disributed with a mean of <span class="math notranslate nohighlight">\(5\)</span> and a standard deviation of <span class="math notranslate nohighlight">\(5\)</span>, and <span class="math notranslate nohighlight">\(10,000\)</span> samples <span class="math notranslate nohighlight">\(z_i\)</span> of random variables <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> which are Normally distributed with a mean of <span class="math notranslate nohighlight">\(0\)</span> and a standard deviation of <span class="math notranslate nohighlight">\(15\)</span>. We will then compute histograms so we can get a sense of how the Normal distribution behaves:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;xi&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="s2">&quot;yi&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="s2">&quot;zi&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="mi">15</span><span class="p">}}</span>
<span class="n">nsim</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">samples</span><span class="o">.</span><span class="n">extend</span><span class="p">([{</span><span class="s2">&quot;Sample&quot;</span><span class="p">:</span> <span class="n">idx</span><span class="p">,</span> <span class="s2">&quot;Value&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">nsim</span><span class="p">,</span> <span class="o">**</span><span class="n">param</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">real_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">real_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Value&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Sample&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;probability&quot;</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">common_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of Points&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/28d79abad276c4af7b7408aaecdcfef01bc276a8904693fda6747266bbc52908.png" src="../../_images/28d79abad276c4af7b7408aaecdcfef01bc276a8904693fda6747266bbc52908.png" />
</div>
</div>
<p>Notice that samples of <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> and <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> differ in the approximate centers of the histograms. This reflects that the mean of the random variables <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> are <span class="math notranslate nohighlight">\(10\)</span> units less than the means of <span class="math notranslate nohighlight">\(\mathbf y_i\)</span>. Further, samples of <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> appear to take a “wider” range of values than samples of <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>, despite the centers looking to be in approximately the same spot (right around zero). This reflect that the standard deviation of the random variables <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> exceed that of the <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> by <span class="math notranslate nohighlight">\(10\)</span> units. We could, in a generic sense, say that the points <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> have distribution <span class="math notranslate nohighlight">\(F_x\)</span>, the points <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> have the distribution <span class="math notranslate nohighlight">\(F_y\)</span>, and the points <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> have the distribution <span class="math notranslate nohighlight">\(F_z\)</span>. Here, <span class="math notranslate nohighlight">\(F_x\)</span> just so happens to be the Normal distribution with mean <span class="math notranslate nohighlight">\(0\)</span> and standard deviation <span class="math notranslate nohighlight">\(5\)</span>, but we don’t need to be that specific when defining the weighted SIEM.</p>
<p>For the weighted SIEM, we will have a length-<span class="math notranslate nohighlight">\(K\)</span> vector <span class="math notranslate nohighlight">\(\vec F\)</span> of distribution functions, where <span class="math notranslate nohighlight">\(F_k\)</span> defines the distribution for the <span class="math notranslate nohighlight">\(k^{th}\)</span> cluster. In this sense, <span class="math notranslate nohighlight">\(F_k\)</span> could be any distribution really; it could be the normal distribution with mean <span class="math notranslate nohighlight">\(\mu_k\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma_k\)</span>, it could be the Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p_k\)</span>, or it could be something more complicated. There are any number of possibilities for what the edge-weight distribution could be.</p>
<p>Unfortunately, we can’t quite use our coin flip example since the weighted SIEM does not necessarily have binary-valued adjacencies. For each entry <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>, we identify the corresponding cluster <span class="math notranslate nohighlight">\(z_{ij}\)</span> that this edge is in. Remember that <span class="math notranslate nohighlight">\(z_{ij}\)</span> takes one of <span class="math notranslate nohighlight">\(K\)</span> possible values. We say that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> has the distribution indicated by the distribution function <span class="math notranslate nohighlight">\(F_{z_{ij}}\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an SIEM random network with <span class="math notranslate nohighlight">\(n\)</span> nodes, the cluster assignment matrix <span class="math notranslate nohighlight">\(Z\)</span>, and the distribution vector <span class="math notranslate nohighlight">\(\vec F\)</span>, we say that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(wSIEM_n(Z, \vec F)\)</span> random network.</p>
<p>Next, let’s look at how one would sample from a weighted SIEM. The procedure below will simulate a sample <span class="math notranslate nohighlight">\(A\)</span> of the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is <span class="math notranslate nohighlight">\(wSIEM_n(Z, \vec F)\)</span>.</p>
<div class="admonition-simulating-a-sample-from-an-wsiem-n-z-vec-f-random-network admonition">
<p class="admonition-title">Simulating a sample from an <span class="math notranslate nohighlight">\(wSIEM_n(Z, \vec F)\)</span> random network</p>
<ol class="arabic simple">
<li><p>Determine a cluster assignment matrix <span class="math notranslate nohighlight">\(Z\)</span> for each edge of the network. Each edge entry <span class="math notranslate nohighlight">\(z_{ij}\)</span> can take one of <span class="math notranslate nohighlight">\(K\)</span> possible values.</p></li>
<li><p>Determine a length-<span class="math notranslate nohighlight">\(K\)</span> distribution vector for each of the <span class="math notranslate nohighlight">\(K\)</span> clusters.</p></li>
<li><p>For each edge <span class="math notranslate nohighlight">\((i, j)\)</span>:</p>
<ul class="simple">
<li><p>Denote <span class="math notranslate nohighlight">\(z_{ij}\)</span> to be the cluster assignment of the potential edge between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>Sample a single sample from the distribution function <span class="math notranslate nohighlight">\(F_{z_{ij}}\)</span>, and record the result as <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>.</p></li>
</ul>
</li>
<li><p>The adjacency matrix we produce, <span class="math notranslate nohighlight">\(A\)</span>, is a sample of an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network.</p></li>
</ol>
</div>
<p>Next, let’s come up with a working example for the weighted SIEM network. Let’s imagine that we have the brain of an individual. The <span class="math notranslate nohighlight">\(n=100\)</span> nodes are areas of the brain. For instance, there is a node of the brain responsible for movement, an area of the brain which is responsible for sight, so on and so forth. As you may be aware, the brain is split into two hemispheres, the left and right hemispheres. In many cases, these areas of the brain have a <em>bilateral analogue</em>, which means that the same area of the brain appears in the left and right hemispheres. While the region of the brain may have a slightly different function in each of the two hemispheres, they tend to work together to perform functions for the individual. For instance, if the left motor area is active when someone is walking, the right motor area will tend to be active too. In this case, our edge weights are how strongly two regions of the brain tend to <em>correlate</em> with one another over the course of the day.</p>
<p>As we learned in the section on the rho-correlated SBM in <a class="reference internal" href="../../representations/ch5/multi-network-models.html#ch5-multi"><span class="std std-numref">Section 4.6</span></a>, correlation is a value between <span class="math notranslate nohighlight">\(-1\)</span> (the two areas are <em>never</em> active together) and <span class="math notranslate nohighlight">\(1\)</span> (the two areas <em>always</em> are active together). If we organize the nodes first by hemisphere (left, then right) and second by the region of the brain, what we would anticipate is that the bilateral areas would tend to be higher correlated than non-bilateral areas. This would be reflected as an “off-diagonal” band (the <em>bilateral band</em>, cluster 2) that tends to have higher values than the other entries of the adjacency matrix (<em>non-bilateral edges</em>, cluster 1). The way we will simulate this is as follows. The non-bilateral edges will have a distribution that is between <span class="math notranslate nohighlight">\(-0.5\)</span> and <span class="math notranslate nohighlight">\(0.5\)</span>, with no particular values being more or less likely. On the other hand, the bilateral band will tend to have higher values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">siem</span>

<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># edges are always present</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># initialize edge clusters Z</span>
<span class="n">band_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">Z</span><span class="p">[</span><span class="n">band_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># set the bilateral edges to cluster 2</span>
<span class="n">Z</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">diag_indices</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># remove the diagonals from edge clusters, since the network is loopless</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">siem</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">edge_comm</span><span class="o">=</span><span class="n">Z</span><span class="p">,</span> <span class="n">wt</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">],</span> 
         <span class="n">wtargs</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;low&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="s2">&quot;high&quot;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">zs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Left&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Right&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))]</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Brain Network&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;qualitative&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Edge Cluster&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/64febc3e6f774ab855ef88c92517120d8965f742790b84e4dbd22db38b944662.png" src="../../_images/64febc3e6f774ab855ef88c92517120d8965f742790b84e4dbd22db38b944662.png" />
</div>
</div>
<p>Notice that the edges in edge-cluster two (the bilateral bands) tend to have higher correlations than the edges in edge-cluster one (the non-bilateral bands). We can visualize this property by looking at a histogram of the two edge clusters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">edge_comm_name</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;Bilateral&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;Non-bilateral&quot;</span><span class="p">}</span>
<span class="k">for</span> <span class="n">clust</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">idx_clust</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Z</span> <span class="o">==</span> <span class="n">clust</span><span class="p">)</span>
    <span class="n">samples</span><span class="o">.</span><span class="n">extend</span><span class="p">([{</span><span class="s2">&quot;Cluster&quot;</span><span class="p">:</span> <span class="s2">&quot;Cluster </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span> <span class="n">edge_comm_name</span><span class="p">[</span><span class="n">clust</span><span class="p">]),</span> <span class="s2">&quot;Correlation&quot;</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">]}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">idx_clust</span><span class="p">)])</span>

<span class="n">real_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">real_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Correlation&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;probability&quot;</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">common_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of Correlations&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/0879e2c925daf9a20fed370e99415e815a61502fe0b1311e666d0d397b0713e5.png" src="../../_images/0879e2c925daf9a20fed370e99415e815a61502fe0b1311e666d0d397b0713e5.png" />
</div>
</div>
<p>Now that we are in a statistical testing mindset, a question you might have is that the correlations for cluster two tend to look much larger than the correlations for cluster one. How do we formalize this relationship? We can’t use the fisher’s exact test, since the fisher’s exact test only worked for the case where our network was unweighted and the adjacency matrix took only binary values (<span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>). What can we do for a weighted network, when the adjacency matrix is not resticted to binary values? To formalize our situation a bit more, let’s return to making some hypotheses about the data. We want to test whether the correlations in the second cluster exceed the correlations in the first cluster. This means that we have an alternative hypothesis <span class="math notranslate nohighlight">\(H_A:\)</span> correrlations in cluster two <span class="math notranslate nohighlight">\(&gt;\)</span> correlations in cluster one. Remember that for two sample testing, the null hypothesis is going to be the opposite, so <span class="math notranslate nohighlight">\(H_0:\)</span> correlations in cluster two <span class="math notranslate nohighlight">\(\leq \)</span> correlations in cluster one. This is the weighted two-sample hypothesis testing problem.</p>
<section id="the-mann-whitney-wilcoxon-u-test">
<h4><span class="section-number">13.1.2.1.1. </span>The Mann-Whitney Wilcoxon <span class="math notranslate nohighlight">\(U\)</span> Test<a class="headerlink" href="#the-mann-whitney-wilcoxon-u-test" title="Permalink to this heading">#</a></h4>
<p>There are a variety of approaches to overcoming the weighted two-sample hypothesis testing problem. A popular approach is known as the two-sample <span class="math notranslate nohighlight">\(t\)</span>-test, which basically means that we assume that the samples are really just normally distributed, with some mean and variance, and then we forget about the actual data after we estimate these means and variances. In so doing, we construct a test statistic by using properties about the normal distribution (e.g., that the mean and the variance are the <em>only</em> useful pieces of information in the data), not the actual data itself. To read more about the two-sample <span class="math notranslate nohighlight">\(t\)</span>-test, we would recommend you check out <span id="id1">[]</span>. Unfortunately, if the data is not well-summarized by a normal distribution, the <span class="math notranslate nohighlight">\(t\)</span>-test tends to be a fairly poor choice for hypothesis testing. Instead, what we tend to use for the weighted two-sample hypothesis testing problem is we turn to what is known as the <span class="math notranslate nohighlight">\(U\)</span>-test. Instead of relying on assumptions that the data is normally distributed, the <span class="math notranslate nohighlight">\(U\)</span>-test does not assume anything about the nature of the distribution itself. The only assumptions that it makes are that the data (in this case, the edges for each cluster) is independent, which falls right in line with the generative description we gave for the SIEM.</p>
<p>You can think of this, intuitively, as the <span class="math notranslate nohighlight">\(t\)</span>-test assuming the data distribution following the “bell shape” of the normal distribution (and therefore, tries to compare two “bell shaped” curves), whereas the <span class="math notranslate nohighlight">\(U\)</span>-test does not make this assumption and instead only uses the data itself. Even when the data is normally distributed, the <span class="math notranslate nohighlight">\(U\)</span>-test still tends to perform almost as well as the <span class="math notranslate nohighlight">\(t\)</span>-test. For this reason, we find the <span class="math notranslate nohighlight">\(U\)</span>-test to be a better procedure than other forms of testing that make heavy assumptions about the distributions of edge weights.</p>
<p>The <span class="math notranslate nohighlight">\(U\)</span>-test is very easy to implement in <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>. We simply fit an SIEM to the data using the <code class="docutils literal notranslate"><span class="pre">SIEMEstimator</span></code> class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">SIEMEstimator</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SIEMEstimator</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>aand then we use the <code class="docutils literal notranslate"><span class="pre">compare</span></code> function to test whether cluster <code class="docutils literal notranslate"><span class="pre">2</span></code> is greater than cluster <code class="docutils literal notranslate"><span class="pre">1</span></code> by specifying <code class="docutils literal notranslate"><span class="pre">alternative=&quot;greater&quot;</span></code> as a <code class="docutils literal notranslate"><span class="pre">methodarg</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">mannwhitneyu</span>

<span class="c1"># perform mann-whitney u-test with alternative hypothesis that </span>
<span class="c1"># correlations of cluster 2 &gt; correlations of cluster 1</span>
<span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">mannwhitneyu</span><span class="p">,</span> <span class="n">methodargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;alternative&quot;</span><span class="p">:</span> <span class="s2">&quot;greater&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p-value: 0.000
</pre></div>
</div>
</div>
</div>
<p>Since the <span class="math notranslate nohighlight">\(p\)</span>-value is much less than <span class="math notranslate nohighlight">\(0.05\)</span> (the print statement cut off at 3 decimal places, but in reality, it is about <span class="math notranslate nohighlight">\(10^{-64}\)</span>, which is <em>really</em> tiny!), with our decision threshold <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, we reject the null in favor of the alternative hypothesis that the correlations in cluster two (the bilateral edges) really are bigger than the correlations in cluster one (the non-bilateral edges).</p>
<p>We could also use a similar, but slightly reworded, hypothesis and get the same answer. Note that saying that the correlations in cluster two are bigger than the correlations in cluster one would be the same as saying that the correlations in cluster one are smaller than the correlations in cluster two. We can reorder the clusters we are testing, and instead specify the alternative hypothesis as <code class="docutils literal notranslate"><span class="pre">&quot;less&quot;</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">mannwhitneyu</span><span class="p">,</span> <span class="n">methodargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;alternative&quot;</span><span class="p">:</span> <span class="s2">&quot;less&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p-value: 0.000
</pre></div>
</div>
</div>
</div>
<p>Which would obviously give us the same answer.</p>
<p>If you instead wanted to use a <span class="math notranslate nohighlight">\(t\)</span>-test or a different testing approach, you can do that very easily with graspologic, too. Just pass your testing strategy using the <code class="docutils literal notranslate"><span class="pre">method</span></code> argument, and pass additional arguments for the testing strategy to the <code class="docutils literal notranslate"><span class="pre">methodargs</span></code> dictionary. The only requirement is that the testing strategy takes two unnamed arguments, where the first argument is the first cluster of data, and the second argument is the second cluster of data. Here, we will be using <code class="docutils literal notranslate"><span class="pre">ttest_ind</span></code> <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html">from scipy</a>. We want to assume as little as possible about the data, so we will specify that the variances might not necessarily be equal as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">ttest_ind</span>

<span class="c1"># perform mann-whitney u-test with alternative hypothesis that </span>
<span class="c1"># correlations of cluster 2 &gt; correlations of cluster 1</span>
<span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">ttest_ind</span><span class="p">,</span> <span class="n">methodargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;alternative&quot;</span><span class="p">:</span> <span class="s2">&quot;greater&quot;</span><span class="p">,</span> <span class="s2">&quot;equal_var&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;t statistic: </span><span class="si">{:.2f}</span><span class="s2">, p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>t statistic: 64.59, p-value: 0.000
</pre></div>
</div>
</div>
</div>
<p>And again we get a <span class="math notranslate nohighlight">\(p\)</span>-value that is much less than our decision threshold, so again we reject the null in favor of the alternative hypothesis that the correlations in cluster two exceed the correlations in cluster one.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./appendix/ch14"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ch14.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Applications (Extended)</p>
      </div>
    </a>
    <a class="right-next"
       href="unsupervised.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13.2. </span>Unsupervised learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-tell-us-the-chances-of-observing-an-outcome-if-the-null-hypothesis-is-true">13.1.1. <span class="math notranslate nohighlight">\(p\)</span>-values tell us the chances of observing an outcome if the null hypothesis is true</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-sample-tests-with-weighted-networks">13.1.2. Two-sample tests with weighted networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weighted-siem-has-a-vector-of-distribution-functions">13.1.2.1. The weighted SIEM has a vector of distribution functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mann-whitney-wilcoxon-u-test">13.1.2.1.1. The Mann-Whitney Wilcoxon <span class="math notranslate nohighlight">\(U\)</span> Test</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>