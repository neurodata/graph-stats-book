

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>11.2. Background &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'appendix/ch12/background';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="11.3. Foundation" href="foundation.html" />
    <link rel="prev" title="11. Network Model Theory" href="ch12.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../coverpage.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../introduction/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/terminology.html">Terminology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch1/ch1.html">1. The Network Machine Learning Landscape</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">1.1. What is network machine learning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/why-study-networks.html">1.2. Why do we study networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">1.3. Types of Network Machine Learning Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">1.4. Examples of applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">1.5. Challenges of Network Machine Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../foundations/ch2/ch2.html">2. End-to-end Biology Network Machine Learning Project</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/big-picture.html">2.1. Look at the big picture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/get-the-data.html">2.2. Get the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">2.3. Prepare the Data for Network Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/select-and-train.html">2.4. Select and Train a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/fine-tune.html">2.5. Fine-Tune your Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">2.6. Discover and Visualize the Data to Gain Insights</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Representations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch4/ch4.html">3. Properties of Networks as a Statistical Object</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/matrix-representations.html">3.1. Matrix Representations Of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/properties-of-networks.html">3.2. Properties of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/network-representations.html">3.3. Representations of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch4/regularization.html">3.4. Regularization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch5/ch5.html">4. Why Use Statistical Models?</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">4.1. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">4.2. Stochastic Block Models (SBM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">4.3. Random Dot Product Graphs (RDPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/single-network-models_SIEM.html">4.5. Structured Independent Edge Model (SIEM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/multi-network-models.html">4.6. Multiple Network Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch5/models-with-covariates.html">4.7. Network Models with Network Covariates</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../representations/ch6/ch6.html">5. Learning Network Representations</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">5.1. Estimating Parameters in Network Models via MLE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/why-embed-networks.html">5.2. Why embed networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/spectral-embedding.html">5.3. Spectral embedding methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">5.4. Multiple-Network Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">5.5. Joint Representation Learning</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch7/ch7.html">6. Applications When You Have One Network</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/community-detection.html">6.1. Community Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/testing-differences.html">6.2. Testing for Differences between Groups of Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/model-selection.html">6.3. Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/vertex-nomination.html">6.4. Single-Network Vertex Nomination</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch7/out-of-sample.html">6.5. Out-of-sample Embedding</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch8/ch8.html">7. Applications for Two Networks</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html">7.1. Latent Two-Sample Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/significant-communities.html">7.2. Two-sample hypothesis testing in SBMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html">7.3. Graph Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch8/multiple-vertex-nomination.html">7.4. Vertex Nomination For Two Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../applications/ch9/ch9.html">8. Applications for Many Networks</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/anomaly-detection.html">8.1. Anomaly Detection For Timeseries of Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-edges.html">8.2. Testing for Significant Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../applications/ch9/significant-vertices.html">8.3. Testing for Significant Vertices</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Next Steps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../next/ch10/ch10.html">9. Where do we go from here?</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/random-walk-diffusion-methods.html">9.1. Random walk and diffusion-based methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../next/ch10/gnn.html">9.2. Graph Neural Networks</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch11/ch11.html">10. Representations (Extended)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch11/alt-reps.html">10.1. Alternative Network Representations</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ch12.html">11. Network Model Theory</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">11.2. Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="foundation.html">11.3. Foundation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ers.html">11.4. Erdös-Rényi (ER) Random Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="sbms.html">11.5. Stochastic Block Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="rdpgs.html">11.6. RDPGs and more general network models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch13/ch13.html">12. Learning Representations Theory</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch13/mle-theory.html">12.1. Maximum Likelihood Estimate Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch13/spectral-theory.html">12.2. Spectral Method Theory</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ch14/ch14.html">13. Applications (Extended)</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../ch14/hypothesis.html">13.1. Hypothesis Testing with coin flips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch14/unsupervised.html">13.2. Unsupervised learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ch14/bayes.html">13.3. Bayes Plugin Classifier</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">Graspologic Documentation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/appendix/ch12/background.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fappendix/ch12/background.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/appendix/ch12/background.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Background</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notational-tendencies">11.2.1. Notational Tendencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background-in-probability-distributions-and-distribution-functions">11.2.2. Background in probability distributions and distribution functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abuses-of-notation">11.2.3. Abuses of notation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="background">
<span id="app-ch12-background"></span><h1><span class="section-number">11.2. </span>Background<a class="headerlink" href="#background" title="Permalink to this heading">#</a></h1>
<p>To understand this section, there are some important background points you should be get some familiarity with. First, you should be readily familiar with all of the mathematical operations and concepts from probability and statistics which are explained directly in the Terminology. We will introduce some of these background points, and the specific notations we will use for this section here. It is often the case that when you read probability or statistics books, that different instructors or professors will use different notations that will differ. We find these differences to be cumbersome, so we’re going to hopefully outline most of the common ones that people have multiple notations for here. They are:</p>
<section id="notational-tendencies">
<h2><span class="section-number">11.2.1. </span>Notational Tendencies<a class="headerlink" href="#notational-tendencies" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Extrapolatory ellipses: We might often summarize a sequence of natural numbers with ellipses; e.g., <span class="math notranslate nohighlight">\(\{1, ..., n\}\)</span>. All the ellipses mean is to just continue indexing in the meantime, until you reach the last index in the sequence. For instance, in the above sequence, the ellipses stand for <span class="math notranslate nohighlight">\(4\)</span>, <span class="math notranslate nohighlight">\(5\)</span>, <span class="math notranslate nohighlight">\(6\)</span>, … all the way up to <span class="math notranslate nohighlight">\(n-1\)</span>. These ellipses have the same interpretation in either vectors or matrices; just continue the numbering pattern upwards basically.</p></li>
<li><p>Shorthand for sequences of natural numbers: We will denote a sequence of natural numbers that goes from <span class="math notranslate nohighlight">\(1\)</span> up to <span class="math notranslate nohighlight">\(n\)</span> using the notation <span class="math notranslate nohighlight">\([n]\)</span>. Stated another way, <span class="math notranslate nohighlight">\([n] = \{1, ..., n\}\)</span>.</p></li>
<li><p>Useful numerical spaces: We will often use a number of numerical spaces in this book. The common ones will tend to be accented with a double bold-faced print. They are the natural numbers (denoted <span class="math notranslate nohighlight">\(\mathbb N\)</span>), the integers (denoted <span class="math notranslate nohighlight">\(\mathbb Z\)</span>), the non-negative integers (denoted <span class="math notranslate nohighlight">\(\mathbb Z_{\geq 0}\)</span>), the positive integers (denoted <span class="math notranslate nohighlight">\(\mathbb Z_+\)</span>), and the real numbers (denoted <span class="math notranslate nohighlight">\(\mathbb R\)</span>).</p></li>
<li><p>Shorthand for objects which can be (arbitrary) values from a particular numerical space: it will often be the case that in describing a network model, our description applies regardless of what the value is for a particular number in that description. For this reason, we tend to use notation to denote the arbitrariness of this choice. We will use the notation <span class="math notranslate nohighlight">\(x \in \mathcal S\)</span> to denote that the value <span class="math notranslate nohighlight">\(x\)</span> (which could be a scalar, a vector, or a matrix) has values which can be described by the numerical space captured by <span class="math notranslate nohighlight">\(\mathcal S\)</span>. For instance, <span class="math notranslate nohighlight">\(x \in \mathbb R\)</span> means that <span class="math notranslate nohighlight">\(x\)</span> is an arbitrary real number. <span class="math notranslate nohighlight">\(\vec x \in \mathbb R^d\)</span> means that <span class="math notranslate nohighlight">\(\vec x\)</span> is an arbitrary vector with <span class="math notranslate nohighlight">\(d\)</span> elements, where each element is an arbitrary real number; e.g., <span class="math notranslate nohighlight">\(x_i \in \mathbb R\)</span>. Another common vector representation we will see is <span class="math notranslate nohighlight">\(\vec x \in [K]^d\)</span> or <span class="math notranslate nohighlight">\(\vec x \in \{1,..., K\}^d\)</span>, which in both cases, means that each element <span class="math notranslate nohighlight">\(x_i\)</span> is an arbitrary natural number that is at most <span class="math notranslate nohighlight">\(K\)</span>. <span class="math notranslate nohighlight">\(X \in \mathbb R^{r \times c}\)</span> means that <span class="math notranslate nohighlight">\(X\)</span> is an arbitrary vector with <span class="math notranslate nohighlight">\(r\)</span> rows and <span class="math notranslate nohighlight">\(c\)</span> columns, where each element is an arbitrary real number; e.g., <span class="math notranslate nohighlight">\(x_{ij} \in \mathbb R\)</span>.</p></li>
<li><p>Vector in-line notation: with <span class="math notranslate nohighlight">\(\vec x\)</span> as a vector, we might sometimes resort to describing <span class="math notranslate nohighlight">\(\vec x\)</span> using an in-line notation which directly captures its dimensionality. We might say something like, <span class="math notranslate nohighlight">\(\vec x = (x_i)_{i = 1}^d\)</span>, which just means that <span class="math notranslate nohighlight">\(\vec x\)</span> looks like this:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x = \begin{bmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_d
    \end{bmatrix}
\end{align*}\]</div>
<ol class="arabic simple" start="6">
<li><p>Matrix in-line notation: with <span class="math notranslate nohighlight">\(X\)</span> as a matrix, we might similarly describe <span class="math notranslate nohighlight">\(X\)</span> using in-line notation which captures its number of rows and columns, with something like <span class="math notranslate nohighlight">\(X = (x_{ij})_{i \in [r], j \in [c]}\)</span> or <span class="math notranslate nohighlight">\(\left((x_{ij})_{j = 1}^c\right)_{i = 1}^r\)</span>. What this means is that we first “unroll” <span class="math notranslate nohighlight">\(X\)</span> <em>across</em> the dimension being indexed by <span class="math notranslate nohighlight">\(j\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(c\)</span>, and then <em>down</em> the dimension being indexed by <span class="math notranslate nohighlight">\(i\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(r\)</span>. In this sense, <span class="math notranslate nohighlight">\(X\)</span> would be a matrix with <span class="math notranslate nohighlight">\(r\)</span> rows and <span class="math notranslate nohighlight">\(c\)</span> columns. It would look like this:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X &amp;= \begin{bmatrix}
        x_{11} &amp; ... &amp; x_{1c} \\
        \vdots &amp; \ddots &amp; \vdots \\
        x_{r1} &amp; ... &amp; x_{rc}
    \end{bmatrix}
\end{align*}\]</div>
</section>
<section id="background-in-probability-distributions-and-distribution-functions">
<h2><span class="section-number">11.2.2. </span>Background in probability distributions and distribution functions<a class="headerlink" href="#background-in-probability-distributions-and-distribution-functions" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Distribution functions for random variables: <span class="math notranslate nohighlight">\(\mathbf x \sim F\)</span>, which basically can be read as, the random variable (which is denoted with a bold-faced <span class="math notranslate nohighlight">\(\mathbf x\)</span>) has a distribution which is delineated by the distribution function <span class="math notranslate nohighlight">\(F\)</span>.” For instance, if <span class="math notranslate nohighlight">\(\mathbf x \sim Bern(p)\)</span>, this means that the random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> has a distribution which can be described by a Bernoulli random variable with probability <span class="math notranslate nohighlight">\(p\)</span>. Stated mathematically, <span class="math notranslate nohighlight">\(Pr(\mathbf x = x) = \begin{cases}p &amp; x = 1 \\1 - p &amp; x = 0\end{cases}\)</span>. In this case, what this means is that realizations of <span class="math notranslate nohighlight">\(\mathbf x\)</span>, denoted by <span class="math notranslate nohighlight">\(x\)</span> (no bold-face), are like a coin flip which lands on heads with probability <span class="math notranslate nohighlight">\(p\)</span> and tails with probability <span class="math notranslate nohighlight">\(1 - p\)</span>, where “heads” is akin to <span class="math notranslate nohighlight">\(x\)</span> having a value of <span class="math notranslate nohighlight">\(1\)</span>, and “tails” is akin to <span class="math notranslate nohighlight">\(x\)</span> having a value of <span class="math notranslate nohighlight">\(0\)</span>.</p>
<ul class="simple">
<li><p>We believe that familiarity with the Bernoulli distribution with probability parameter <span class="math notranslate nohighlight">\(p\)</span> denoted <span class="math notranslate nohighlight">\(Bern(p)\)</span>, the Categorical (Multinoulli) distribution with probability vector <span class="math notranslate nohighlight">\(\vec p\)</span>, the Normal distribution <span class="math notranslate nohighlight">\(\mathcal N(\mu, \sigma^2)\)</span>, and the Uniform distribution <span class="math notranslate nohighlight">\(Unif(a, b)\)</span> with a minimum at <span class="math notranslate nohighlight">\(a\)</span> and a maximum at <span class="math notranslate nohighlight">\(b\)</span> would be valuable to look at. If you are already familiar with these words but forget exactly what they mean, we will describe them in-line as necessary, as well.</p></li>
</ul>
</li>
<li><p>Distribution functions for random vectors: <span class="math notranslate nohighlight">\(\mathbf {\vec x} \sim F\)</span>, which can be read as, “the random vector <span class="math notranslate nohighlight">\(\mathbf x\)</span> has a distribution function which is delineated by the distribution function <span class="math notranslate nohighlight">\(F\)</span>”. In most cases, we will assume some level of statistical independence for random vectors, which basically means that instead of describing <span class="math notranslate nohighlight">\(\mathbf{\vec x}\)</span> itself directly as having a distribution, we can for our purposes describe each individual element of <span class="math notranslate nohighlight">\(\mathbf {\vec x}\)</span>, written <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>, as having a distribution. This will reduce cumbersome descriptions of distribution functions for random vectors to simpler distributions functions for random variables.</p>
<ul class="simple">
<li><p>This brings us to an important aside. When we are working with vectors (either random or fixed), you might see us use the fancy word “dimensions” to describe individual elements of these vectors. The <span class="math notranslate nohighlight">\(i^{th}\)</span> dimension of a scalar vector <span class="math notranslate nohighlight">\(\vec x\)</span> is just the <span class="math notranslate nohighlight">\(i^{th}\)</span> element of that vector; e.g., <span class="math notranslate nohighlight">\(x_i\)</span>, which is a scalar. The <span class="math notranslate nohighlight">\(i^{th}\)</span> dimension of a random vector <span class="math notranslate nohighlight">\(\mathbf {\vec x}\)</span> is similar, where <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> is a random variable.</p></li>
</ul>
</li>
<li><p>Distribution functions for random matrices: <span class="math notranslate nohighlight">\(\mathbf X \sim F\)</span>, which can be read as, “the random matrix <span class="math notranslate nohighlight">\(\mathbf x\)</span> has a distribution function which is delineated by the distribution function <span class="math notranslate nohighlight">\(F\)</span>”. Here, we will also tend to assume statistical independence for random matrices, which again means that instead of describing <span class="math notranslate nohighlight">\(\mathbf X\)</span>, we can just look at the individual elements of <span class="math notranslate nohighlight">\(\mathbf X\)</span>, denoted <span class="math notranslate nohighlight">\(\mathbf x_{ij}\)</span>, as having distributions.</p>
<ul class="simple">
<li><p>There is one important exception to this, which will arise for the <em>a posteriori</em> Random Dot Product Graph, which will use something called <strong>inner-product distributions</strong>. In this case, instead of describing <span class="math notranslate nohighlight">\(\mathbf X\)</span> itself, we will describe a family of distributions for random vectors, which comprise the rows of <span class="math notranslate nohighlight">\(\mathbf X\)</span>. We will try our best to explain these in an intuitive way without going outside of the scope of a graduate understanding of statistics.</p></li>
</ul>
</li>
<li><p>Parametrized functions: In statistics, there is a concept called a <strong>parameter</strong>. A parameter is a number, or a set of numbers, which uniquely defines the behavior of something. In our case, we will often be concerned with parametrized random variables, such as <span class="math notranslate nohighlight">\(\mathbf x \sim Bern(p)\)</span>, which states that <span class="math notranslate nohighlight">\(\mathbf x\)</span> is a random variable which is described by the Bernoulli distribution with <em>parameter</em> <span class="math notranslate nohighlight">\(p\)</span>. In this case, in a sense, the “<span class="math notranslate nohighlight">\(p\)</span>” is static, in that for our particular random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> that we are talking about, <span class="math notranslate nohighlight">\(p\)</span> itself isn’t going to change. However, when we talk about realizations of <span class="math notranslate nohighlight">\(\mathbf x\)</span> (which are <span class="math notranslate nohighlight">\(0\)</span>s and <span class="math notranslate nohighlight">\(1\)</span>s) these realizations can, and will, change. In this sense, when we take probability statements about <span class="math notranslate nohighlight">\(\mathbf x\)</span>, such as <span class="math notranslate nohighlight">\(Pr(\mathbf x = 1, p) = p\)</span> or <span class="math notranslate nohighlight">\(Pr(\mathbf x = 0, p) = 1 - p\)</span>, the probability is a function of <em>both</em> the parameter <span class="math notranslate nohighlight">\(p\)</span> <em>and</em> the value <span class="math notranslate nohighlight">\(x\)</span> which <span class="math notranslate nohighlight">\(\mathbf x\)</span> takes. Simultaneously, however, when we study <span class="math notranslate nohighlight">\(\mathbf x\)</span>, the <span class="math notranslate nohighlight">\(p\)</span> isn’t going to change for a given <span class="math notranslate nohighlight">\(\mathbf x\)</span>. For this reason, we explicitly delineate this difference by instead dropping the <span class="math notranslate nohighlight">\(p\)</span> down as  a subscript; e.g., <span class="math notranslate nohighlight">\(Pr(\mathbf x = x, p)\)</span> will instead be denoted as <span class="math notranslate nohighlight">\(Pr_p(\mathbf x = x)\)</span>. This makes explicit that <span class="math notranslate nohighlight">\(p\)</span> is a parameter of the distribution of <span class="math notranslate nohighlight">\(\mathbf x\)</span>, and not something that is realized (or changing) for different realizations of <span class="math notranslate nohighlight">\(\mathbf x\)</span>.</p></li>
<li><p>Arbitrary sets of parameters: When we describe random variables very generally, it is often the case that we want to be as unrestrictive as possible. For instance, if we are describing a generic random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> which could have a Bernoulli distribution with a parameter <span class="math notranslate nohighlight">\(p\)</span> <em>or</em> a Normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, we have different sets of parameters depending on which distribution <span class="math notranslate nohighlight">\(\mathbf x\)</span> has. When a random variable could have different parameters, we will often describe the parameters using the notation <span class="math notranslate nohighlight">\(\theta\)</span>, which is just an arbitrary parameter set. For instance, in the example we gave, <span class="math notranslate nohighlight">\(\theta\)</span> could be the set <span class="math notranslate nohighlight">\(\{p\}\)</span> or the set <span class="math notranslate nohighlight">\(\{\mu, \sigma^2\}\)</span>, so we will just describe <span class="math notranslate nohighlight">\(\mathbf x\)</span> in terms of the generic parameter <span class="math notranslate nohighlight">\(\theta\)</span> instead of cumbersomely writing that the parameter set can be different every time. In this sense, <span class="math notranslate nohighlight">\(\theta\)</span> will denote an arbitrary set of parameters for a random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span>, a random vector <span class="math notranslate nohighlight">\(\mathbf{\vec x}\)</span>, or a random matrix <span class="math notranslate nohighlight">\(\mathbf X\)</span>.</p></li>
</ol>
</section>
<section id="abuses-of-notation">
<h2><span class="section-number">11.2.3. </span>Abuses of notation<a class="headerlink" href="#abuses-of-notation" title="Permalink to this heading">#</a></h2>
<p>In statistical work, there is a common problem experienced called an “abuse” of notation. What this means is a particular notation choice which takes on multiple meanings. These can really complicate things for your understanding, if you see a notation defined and used as one particular thing, and then redefined and reused as another particular thing. We will use one particular abuse of notation fairly regularly. As it turns out, if we have a random variable, random vector, or random matrix, a unique distribution can be delineated <em>entirely</em> by its cumulative distribution. This is an important aside, since cumulative distribution functions aren’t distributions <em>themselves</em>, but they each equivalently delineate unique distributions. To make this description explicit, let’s say that <span class="math notranslate nohighlight">\(\mathbf x \sim \mathcal N(\mu, \sigma^2)\)</span>, which means that <span class="math notranslate nohighlight">\(\mathbf x\)</span> has the Normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. This statement is explicit, and will not stop being true. But, equivalently, we might use an <em>abuse</em> of notation, by defining the cumulative distribution function:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    F_{\mu, \sigma^2}(x) &amp;= \int_{-\infty}^x f_{\mu, \sigma^2}(x) \text{d}x
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{\mu, \sigma^2}(x)\)</span> is the probability density for the normal distribution at the value <span class="math notranslate nohighlight">\(x\)</span> with mean <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. For a given choice of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>, <span class="math notranslate nohighlight">\(F_{\mu, \sigma^2}(x)\)</span> has a unique value over the range of values which <span class="math notranslate nohighlight">\(x\)</span> could take. In this sense, if we were to say that <span class="math notranslate nohighlight">\(\mathbf x \sim F_{\mu, \sigma^2}\)</span>, we have kind of “abused” the notation of the cumulative distribution function, in that the cumulative distribution function is not <em>itself</em> a description for a random variable. However, since <span class="math notranslate nohighlight">\(F_{\mu,\sigma^2}\)</span> is unique for a <span class="math notranslate nohighlight">\(\mathcal N(\mu, \sigma^2)\)</span> random variable, it “does the job” for us, and is “clear enough” for our purposes. The reason that we do this is, sometimes we might want to leave it <em>totally generic</em> as to the type of distribution that our random variable is described as. For instance, we could say <span class="math notranslate nohighlight">\(\mathbf x \sim F\)</span>, which just means that <span class="math notranslate nohighlight">\(\mathbf x\)</span> is a random variable with an arbitrary cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span>. This leaves it generic to us the specifics of <span class="math notranslate nohighlight">\(F\)</span>, including the parameter choices that could be chosen, or the behavior of the specific family of random variables that it would define. This will come up when we study <em>inner product distributions</em> below.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./appendix/ch12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="ch12.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Network Model Theory</p>
      </div>
    </a>
    <a class="right-next"
       href="foundation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11.3. </span>Foundation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notational-tendencies">11.2.1. Notational Tendencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background-in-probability-distributions-and-distribution-functions">11.2.2. Background in probability distributions and distribution functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abuses-of-notation">11.2.3. Abuses of notation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>