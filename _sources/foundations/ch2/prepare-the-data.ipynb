{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch2:prepare)=\n",
    "# Prepare the Data for Network Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's time for us to prepare our networks for machine learning algorithms. Like before, you are going to try to capture most of these with functions. This is because:\n",
    "1. Functions will make the useful data preparation code that we write usable on new networks,\n",
    "2. You will gradually build libraries of utility functions that we can prepare together into packages of their own or recycle for future projects,\n",
    "3. You can use modularize these functions into other parts of your data pipeline before it gets to your algorithm, to keep a lean module-oriented design,\n",
    "4. You can easily try different transformations of the data and evaluate which ones tend to work best.\n",
    "\n",
    "First, let's re-load the data that we have read in from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import graspologic as gp\n",
    "\n",
    "AWSND_ROOT = \"https://open-neurodata.s3.amazonaws.com/m2g/Diffusion/BNU1-8-27-20-m2g-native-csa-det/\"\n",
    "DWI_URL = os.path.join(AWSND_ROOT, \n",
    "                       \"sub-0025864/ses-1/connectomes/AAL_space-MNI152NLin6_res-2x2x2/\")\n",
    "DWI_PATH = os.path.join(\"datasets\", \"dwi\")\n",
    "DWI_NAME = \"sub-0025864_ses-1_dwi_AAL_space-MNI152NLin6_res-2x2x2_connectome.csv\"\n",
    "\n",
    "def fetch_dwi_data(dwi_url=DWI_URL, dwi_path=DWI_PATH, dwi_name=DWI_NAME):\n",
    "    local_path = os.path.join(dwi_path, dwi_name)\n",
    "    if not os.path.isdir(dwi_path):\n",
    "        os.makedirs(dwi_path)\n",
    "    csv_url = os.path.join(dwi_url, dwi_name)\n",
    "    urllib.request.urlretrieve(csv_url, local_path)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = fetch_dwi_data()\n",
    "A = gp.utils.import_edgelist(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Most network machine learning algorithms cannot work with a node which is *isolated*, a term we will learn in [Chapter 4](#link?) which means that the node has no edges. Let's start with fixing this. We can remove isolated nodes from the network as follows:\n",
    "1. Compute the number of nodes each node connects to. This consists of summing the matrix along the rows (or columns). The network is *undirected*, a property you will learn in [properties of networks](ch4:prop-net), which means that if a node can communicate with another node, that other node can also communicate with that node\n",
    "2. Identify any nodes which are connected to zero nodes along either the rows or columns. These are the *isolated* nodes.\n",
    "3. Remove the isolated nodes from the adjacency matrix.\n",
    "\n",
    "Let's see how this works in practice. We begin by first taking the row sums of each node, which tells us how many nodes that each node is connected to. Next, we remove all nodes with are not connected to any other nodes (the row and column sum are both zero) from both the adjacency matrix and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_isolates(A):\n",
    "    \"\"\"\n",
    "    A function which removes isolated nodes from the \n",
    "    adjacency matrix A and the labels.\n",
    "    \"\"\"\n",
    "    degree = A.sum(axis=0)  # sum along the rows to obtain the node degree\n",
    "    out_degree = A.sum(axis=1)\n",
    "    A_purged = A[~(degree == 0),:]\n",
    "    A_purged = A_purged[:,~(degree == 0)]\n",
    "    print(\"Purging {:d} nodes...\".format((degree == 0).sum()))\n",
    "    return A_purged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = remove_isolates(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So no isolated nodes were found, and consequently no nodes were purged. Great! What else can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To streamline the process of cleaning up the raw data, you will often need to write custom data cleaners. You will want your cleaners to work seamlessly with `sklearn`'s functions, such as pipelines, and will require you to only implement three class methods: `fit()`, `transform()`, `fit_transform()`. By adding `TransformerMixin` as a base class, we do not even have to implement the third one! If we use `BaseEstimator` as a base class, we will also obtain `get_params()` and `set_params()`, which will be useful for hyperparameter tuning steps later on. For example, here is an example cleaner class which purges the adjacency matrix of isolates and remaps the categorical labels to numbers. Note that a key step to implementing this all as cleanly as possible is that the inputs, an adjacency matrix and a vector of node labels, are passed in as a *single* tuple object. This is because `sklearn` anticipates that the return arguments from calls of `transform()` can be passed sequentially to one another, which we will see later on when we try to string several of these transformers together into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class CleanData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        (A) = X\n",
    "        Acleaned = remove_isolates(A)\n",
    "        self.A_ = Acleaned\n",
    "        return self.A_\n",
    "    \n",
    "data_cleaner = CleanData()\n",
    "A_clean = data_cleaner.transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge weight transformations\n",
    "\n",
    "One of the most important transformations that we will come across in network machine learning is called *edge-weight transformation*. Many networks you enounter, such as the human diffusion connectome, will have edge weights which do not just take values of 1 or 0 (edge or no edge, a *binary* network); rather, many of the networks you come across may have discrete-weighted edges (the edges take non-negative inter values, such as 0, 1, 2, 3, ...), or decimal-weight edges (the edges take values like 0, 0.1234, 0.234, 2.4234, ...). For a number of reasons discussed later in [Regularization](ch4:regularization), this is often not really a desirable characteristic.  The edges in a network might be error prone, and it might only be desirable to capture one (or a few) properties about the edge weights, rather than just leave them in their raw values. Further, a lot of the techniques we come across throughout this book might not even *work* on networks which are not binary. For this reason, we need to get accustomed to transforming the edge weights to take new sets of values.\n",
    "\n",
    "There are two common approaches to transform edge weights: the first is called binarization (set all of the edges to take a value of 0 or 1), and the second is called an ordinal transformation. \n",
    "\n",
    "### Binarization of edges \n",
    "\n",
    "Binarization is quite simple: the edges in the raw network take non-binary values (values other than just 0s and 1s), and you need them to be 0s and 1s for your algorithm. How do you solve this? \n",
    "\n",
    "The simplest thing to do is usually to just look at which edges take a value of zero, and keep them as zero, and then look at all of the edges which take a non-zero value, and set them to one. In effect, what this does is it just takes the original non-binary network, and converts it to a binary one. Let's take a look at how we can implement this using `graspologic`. We first look at the network before binarization, and then after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_bin = gp.utils.binarize(A_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from graphbook_code import heatmap\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(18, 6))\n",
    "heatmap(A_clean, ax=ax[0], title=\"Weighted Human Connectome\")\n",
    "heatmap(A_bin, ax=ax[1], title=\"Binary Human Connectome\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That heatmap looks a whole lot different, particularly in the top left. What happened was that edges in the weighted human connectome have very small edge weights in the upper left corner, which *almost* look like they are zero. But when we binarize the network, we see that this is no longer the case: all of the edge weights which are non-zero took a value of one (and are dark purple) and all of the edge weights which are zero stay at zero (and are white). \n",
    "\n",
    "Another way we could have normalized these edge weights is through something called a *pass to ranks*. Through a pass to ranks, the edge weights are discarded entirely, with one exception: the edges which are non-zero are first ranked, from smallest to largest, with the largest item having a rank of one, and the smallest item having a rank of $\\frac{1}{\\text{number of non-zero edges}}$. This is called an *ordinal transformation*, in that it preserves the *orders* of the edge-weights, but discards all other information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ptr = gp.utils.pass_to_ranks(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we plot the resulting connectome, before and after passing to ranks, as heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(18, 6))\n",
    "heatmap(A_clean, ax=ax[0], title=\"Weighted human connectome\")\n",
    "heatmap(A_bin, ax=ax[1], title=\"Binary human connectome\")\n",
    "heatmap(A_ptr, ax=ax[2], title=\"Ranked human connectome\", vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shifted the histogram of edge-weights, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(10, 10))\n",
    "sns.histplot(A_clean[A_clean > 0].flatten(), ax=ax[0]);\n",
    "ax[0].set_xlabel(\"Edge weight\")\n",
    "ax[0].set_title(\"Histogram of human connectome non-zero edge weights\");\n",
    "\n",
    "sns.histplot(A_ptr[A_ptr > 0].flatten(), ax=ax[1]);\n",
    "ax[1].set_xlabel(\"ptr(Edge weight)\")\n",
    "ax[1].set_title(\"Histogram of human connectome, passed-to-ranks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the desirable property that it bounds the network's edge weights to be between $0$ and $1$, as we can see above, which is often crucial if we seek to compare two or more networks and the edge weights are relative in magnitude (an edge's weight might mean something in relation to another edge's weight in that same network, but an edge's weight means nothing in relation to another edge's weight in a separate network). Further, passing to ranks is not very susceptible to outliers, as we will see in later chapters. \n",
    "\n",
    "Again, we will turn the edge-weight transformation step into its own class, much like we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        A = X\n",
    "        A_scaled = gp.utils.pass_to_ranks(A)\n",
    "        return (A_scaled)\n",
    "    \n",
    "feature_scaler = FeatureScaler()\n",
    "A_cleaned_scaled = feature_scaler.transform(A_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation pipelines\n",
    "\n",
    "As you can see, there are a number of data transformations that need to be executed to prepare network data for machine learning algorithms. One thing that might be desirable is to develop a pipeline which automates the data preparation process for you. We will perform this using the `Pipeline` class from `sklearn`. The `Pipeline` class can help us apply sequences of transformations. Here is a simple pipeline for doing all of the steps we have performed so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('cleaner', CleanData()),\n",
    "    ('scaler', FeatureScaler()),\n",
    "])\n",
    "\n",
    "xfm_dat = num_pipeline.fit_transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline class takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers, which implement the `fit_transform()` method. In our case, this is handled directly by the `TransformerMixin` base class.\n",
    "\n",
    "When you call the `fit_transform()` method of the numerical pipeline, it calls the `fit_transform()` method on each of the transformers, and passes the output of each call as the parameter to the next call, until it reaches the final estimator, for which it just calls the `fit()` method. \n",
    "\n",
    "Next, we'll see the real handiness of the `Pipeline` module. The reason we went to lengths to define a pipeline was that we wanted to have an easily reproducible procedure that we could efficiently apply to new datasets. We'll see how we can do that using two human connectomes, the one we have been studying so far and an additional subject's data, which we perform below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_xfm_dat1 = num_pipeline.fit_transform(A)\n",
    "\n",
    "DWI_URL2 = os.path.join(AWSND_ROOT, \n",
    "                       \"sub-0025865/ses-1/connectomes/AAL_space-MNI152NLin6_res-2x2x2/\")\n",
    "DWI_NAME2 = \"sub-0025865_ses-1_dwi_AAL_space-MNI152NLin6_res-2x2x2_connectome.csv\"\n",
    "local_path2 = fetch_dwi_data(dwi_url=DWI_URL2, dwi_name=DWI_NAME2)\n",
    "A_sub2 = gp.utils.import_edgelist(local_path2)\n",
    "A_xfm_dat2 = num_pipeline.fit_transform(A_sub2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the mushroom bodies, after transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "heatmap(A_xfm_dat1, title=\"Connectome 1, Preprocessed\", ax=axs[0], vmin=0, vmax=1)\n",
    "heatmap(A_xfm_dat2, title=\"Connectome 2, Preprocessed\", ax=axs[1], vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
