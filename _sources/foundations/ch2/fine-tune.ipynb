{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch2:finetune)=\n",
    "# Fine-Tune your Model\n",
    "\n",
    "Now that you have figured out an appropriate way to represent your network to learn from it, and you have learned how to train an algorithm to learn from it, it's time to tune things up a little bit. Let's start with the code we've gotten together so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from graspologic.utils import import_edgelist, pass_to_ranks\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob\n",
    "\n",
    "# the AWS bucket the data is stored in\n",
    "BUCKET_ROOT = \"open-neurodata\"\n",
    "parcellation = \"Schaefer400\"\n",
    "FMRI_PREFIX = \"m2g/Functional/BNU1-11-12-20-m2g-func/Connectomes/\" + parcellation + \"_space-MNI152NLin6_res-2x2x2.nii.gz/\"\n",
    "FMRI_PATH = os.path.join(\"datasets\", \"fmri\")  # the output folder\n",
    "DS_KEY = \"abs_edgelist\"  # correlation matrices for the networks to exclude\n",
    "\n",
    "def fetch_fmri_data(bucket=BUCKET_ROOT, fmri_prefix=FMRI_PREFIX,\n",
    "                    output=FMRI_PATH, name=DS_KEY):\n",
    "    \"\"\"\n",
    "    A function to fetch fMRI connectomes from AWS S3.\n",
    "    \"\"\"\n",
    "    # check that output directory exists\n",
    "    if not os.path.isdir(FMRI_PATH):\n",
    "        os.makedirs(FMRI_PATH)\n",
    "    # start boto3 session anonymously\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    # obtain the filenames\n",
    "    bucket_conts = s3.list_objects(Bucket=bucket, \n",
    "                    Prefix=fmri_prefix)[\"Contents\"]\n",
    "    for s3_key in bucket_conts:\n",
    "        # get the filename\n",
    "        s3_object = s3_key['Key']\n",
    "        # verify that we are grabbing the right file\n",
    "        if name not in s3_object:\n",
    "            op_fname = os.path.join(FMRI_PATH, str(s3_object.split('/')[-1]))\n",
    "            if not os.path.exists(op_fname):\n",
    "                s3.download_file(bucket, s3_object, op_fname)\n",
    "\n",
    "def read_fmri_data(path=FMRI_PATH):\n",
    "    \"\"\"\n",
    "    A function which loads the connectomes as adjacency matrices.\n",
    "    \"\"\"\n",
    "    # import edgelists with graspologic\n",
    "    # edgelists will be all of the files that end in a csv\n",
    "    networks = [import_edgelist(fname) for fname in glob.glob(os.path.join(path, \"*.csv\"))]\n",
    "    return networks\n",
    "\n",
    "def remove_isolates(A):\n",
    "    \"\"\"\n",
    "    A function which removes isolated nodes from the \n",
    "    adjacency matrix A.\n",
    "    \"\"\"\n",
    "    degree = A.sum(axis=0)  # sum along the rows to obtain the node degree\n",
    "    out_degree = A.sum(axis=1)\n",
    "    A_purged = A[~(degree == 0),:]\n",
    "    A_purged = A_purged[:,~(degree == 0)]\n",
    "    print(\"Purging {:d} nodes...\".format((degree == 0).sum()))\n",
    "    return A_purged\n",
    "\n",
    "class CleanData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"Cleaning data...\")\n",
    "        Acleaned = remove_isolates(X)\n",
    "        A_abs_cl = np.abs(Acleaned)\n",
    "        self.A_ = A_abs_cl\n",
    "        return self.A_\n",
    "    \n",
    "class FeatureScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"Scaling edge-weights...\")\n",
    "        A_scaled = pass_to_ranks(X)\n",
    "        return (A_scaled)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('cleaner', CleanData()),\n",
    "    ('scaler', FeatureScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "fetch_fmri_data()\n",
    "\n",
    "As_raw = read_fmri_data()\n",
    "with contextlib.redirect_stdout(None):\n",
    "    As = np.stack([num_pipeline.fit_transform(A) for A in As_raw], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section, you learned how you can take one of the networks, and use embeddings combined with various clustering techniques to learn about latent structure in your data.\n",
    "\n",
    "However, there's a big caveat: your colleague sent you over a hundred networks, and you ignored all but one of them! Surely, there's something that you can learn from all of them, right?\n",
    "\n",
    "Fortunately, when you have a multiple network problem, there are plenty of approaches that you can use to learn from all of them simultaneously. Let's break down how we can approach this now.\n",
    "\n",
    "So, you know that you want to produce a representation of all of your networks. These networks all have the same nodes, which are the different areas of the brain. For all intents and purposes, you can assume that these different nodes mean the same thing across all of the different people, even if they are different based on each individual. What you want to learn is whether there is some *shared* structure across all of the different networks present in the nodes. To do this, you are going to want to be able to take *all* of your networks, and produce an embedding in which you can look at each *node* as its own object. Does anything exist to help you?\n",
    "\n",
    "Sure does. As you will learn, a particular representation called [MASE](ch6:multinet:mase) does just this. It allows you to take many networks, and learn a single representation for the nodes across all of the networks. This representation, in particular, is going to effectively *borrow strength* from all of the networks you pass in, so you won't have to worry about whether you are just ignoring all of the networks but one like you did before. Let's see what MASE can do for us here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.embed import MultipleASE\n",
    "\n",
    "embedding = MultipleASE().fit_transform(As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.plot import pairplot\n",
    "\n",
    "pairplot(embedding, title=\"Multiple spectral embedding of all connectomes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty different! In particular, many of the plots look quite a bit more \"blobby\" than they did when we only looked at a single network. Let's take a look at what happens when we apply our clustering to this embedding instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.cluster import AutoGMMCluster\n",
    "\n",
    "labels = AutoGMMCluster(max_components=10).fit_predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot(embedding, labels=labels,\n",
    "         title=\"Multiple spectral embedding of all connectomes\", \n",
    "         legend_name=\"Predicted Clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what do these predicted clusters mean?\n",
    "\n",
    "Let's see if we can find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
