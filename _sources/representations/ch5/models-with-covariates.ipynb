{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Models with Covariates\n",
    "\n",
    "To conclude our discussion on network models, we will turn to a common problem in machine learning known as the classification problem. For each piece of data $x_i$ in our sample, we might often have another piece of information $y_i$ associated with this data which we think is associated the data $x_i$ in some way. This additional information $y_i$ is known as the **class** of the $i^{th}$ item, and is a *categorical variable* which takes values between $1$ and $Y$, where $Y$ is the total number of possible classes in our experiment. Consider the case where we have the heights of a selection of people and aliens for a hypothetical sample. The data $x_i$ is the height of each individual $i$. $y_i$ indicates whether each individual is a person (1) or an alien (2). $y_i$ is a categorical variable because we chose people to be $1$ and aliens to be $2$ arbitrarily, and the total number of classes $Y$ is $2$. Our question of interest is the extent to which we can *predict* the class for each individual (person or alien) using only their height $x_i$. \n",
    "\n",
    "Now let's imagine that we have a collection of networks representing the brains of $100$ individuals. These brains belong to either an alien or a human. Each network has $5$ nodes, representing the lobes of the brain: the occipital, the temporal, the parietal, the frontal, and the insula. Edges represent whether pairs of lobes are connected to one another. In this case, we have observed pairs of data $(A^{(m)}, y_m)$, for $m$ from $1$ to $M=100$. Each adjacency matrix $A^{(m)}$ is a $5 \\times 5$ matrix, and the indicator variable $y_m$ takes the value $1$ if the $m^{th}$ individual is a human, or $2$ if the $m^{th}$ individual is an alien. We seek to characterize the alien and human brains in such a way that \n",
    "\n",
    "Remember that to devise a statistical model, we suppose each piece of data in our sample is an observation of a corresponding random variable. When we were dealing with multiple network models, this meant that for each network $A^{(m)}$, that there was a corresponding random network $\\mathbf A^{(m)}$. Here, for each data pair $(A^{(m)}, y_m)$, there exists a corresponding random network $\\mathbf A^{(m)}$ and a corresponding random class $\\mathbf y_m$, where $(A^{(m)}, y_m)$ is a realization of $(\\mathbf A^{(m)}, \\mathbf y_m)$. So, for our multiple network model with covariates, we seek a model which describes both $\\mathbf A^{(m)}$ and $y_m$. We accomplish this through the signal subnetwork model.\n",
    "\n",
    "## Signal Subnetwork Model\n",
    "\n",
    "As it turns out, these aliens are remarkably similar to the humans, except for one piece of information: the connections between the frontal lobe and all other lobes for the aliens have a much higher chance of being connected. In other words, the **subnetwork** comprised of edges incident the frontal lobe carry the *signal disparity* between human and alien brains. To begin, we turn to the signal subnetwork (SSN) model.\n",
    "\n",
    "For the SSN model, the core idea is that for each edge in the network, the probability of an edge existing (or not existing) is either the same, or different, between the two classes. We capture this idea using the **signal subnetwork**. For an edge $(i, j)$ for classes $y$ (either $0$ or $1$), we will use the notation $p_{ij}^y$ to denote the probability of an edge existing in class $y$. \n",
    "\n",
    "### Signal Subnetwork\n",
    "\n",
    "The **signal subnetwork** is a collection of edges $\\mathcal S$, which has edges $(i,j)$ where $i$ and $j$ are nodes in the network between $1$ and $n$, such that the following two conditions hold:\n",
    "\n",
    "1. For each edge which is in the signal subnetwork, the probability of an edge existing differs between classes $0$ and $1$. That is, if an edge $(i, j)$ is in the signal subnetwork $\\mathcal S$, then there exist two classes $y$ and $y'$ where $p_{ij}^{y} \\neq p_{ij}^{y'}$.\n",
    "2. For each edge which is *not* in the signal subnetwork, the probabilitty of an edge existing is the *same* between classes $0$ and $1$. That is, if an edge $(i, j)$ is not in the signal subnetwork $\\mathcal S$, then $p_{ij}^0 = p_{ij}^1$. For this reason, if an edge is not in the signal subnetwork, we will use the term $p_{ij} = p_{ij}^0 = p_{ij}^1 = ... = p_{ij}^Y$. \n",
    "\n",
    "This sounds a little complex, but it's really quite simple: the idea is just that the signal subnetwork is keeping track of the edges which have different probabilities for any pair of classes.\n",
    "\n",
    "Now that we are familiar with the signal subnetwork $\\mathcal S$, we can formally define the signal subnetwork model. For each random pair $(\\mathbf A^{(m)}, y_m)$ of our $M$ total pairs, we first obtain a \"class assignment\" die with $Y$ total sides. For a given face of the dice $y$, the probability that the dice lands on side $Y$ is $\\pi_y$. We flip the class assignment die, and if it lands on side $y$, then $\\mathbf y_m$ takes the value $y$. Next, for each edge $(i,j)$ which is not in the signal subnetwork $\\mathcal S$, we obtain a \"non-signal\" coin which has a probability of $p_{ij}$ of landing on heads and $1 - p_{ij}$ of landing on tails. The edge $\\mathbf a_{ij}$ exists if the coin lands on heads and does not exist if the coin lands on tails. Finally, for each edge $(i, j)$ which is in the signal subnetwork $\\mathcal S$, we check which class $\\mathbf y_m$ indicates. If $\\mathbf y_m$ is $y$, we obtain a \"signal\" coin which has a probability of $p_{ij}^y$ of landing on heads, and a probability of $1 - p_{ij}^y$ of landing on tails. The edge $\\mathbf a_{ij}$ exists if the coin lands on heads and does not exist if the coin lands on tails. In summary, say that a collection of random network/covariate pairs $\\left\\{(\\mathbf A^{(1)}, y_1), ..., (\\mathbf A^{(M)}, y_M)\\right\\}$ with $n$ nodes is $SSN_n(\\pi_1, ..., \\pi_Y, P^1, ..., P^Y, \\mathcal S)$ if the following two conditions hold:\n",
    "1. conditional on the class $\\mathbf y_m$ being $y$, then $\\mathbf A^{(m)}$ is $IER_n(P^y)$.\n",
    "2. For every edge $(i, j)$ which is in the signal subnetwork $\\mathcal S$, then there exist at least two classes $y$ and $y'$ where $p_{ij}^y \\neq p_{ij}^{y'}$.\n",
    "2. For every edge $(i, j)$ which is not in the signal subnetwork $\\mathcal S$, then every edge probability $p_{ij}=p_{ij}^1 =...= p_{ij}^Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
