{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Parameters in Network Models via Spectral Methods\n",
    "\n",
    "\n",
    "Next up, you might think intuitively we would jump to the a posteriori Stochastic Block Model, but as we will see in a second, estimation for an a posteriori Stochastic Block Model is, in fact, additional steps on top of how we estimate parameters for an *a priori* Random Dot Product Graph (RDPG). \n",
    "\n",
    "## Why don't we just use MLE?*\n",
    "\n",
    "The a posteriori Stochastic Block Model has a pair of parameters, the block matrix, $B$, and the community probability vector, $\\vec \\pi$. If you are keeping up with the log-likelihood derivations in the single network models section, you will recall that the log-likelihood for an a posteriori Stochastic Block Model, we obtain that:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(A) &= \\sum_{\\vec \\tau \\in \\mathcal T} \\prod_{k = 1}^K \\left[\\pi_k^{n_k}\\cdot \\prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "That expression, it turns out, is a lot more complicated than what we had to deal with for the *a priori* Stochastic Block Model. Taking the log gives us that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log \n",
    "    \\mathbb P_\\theta(A) &= \\log\\left(\\sum_{\\vec \\tau \\in \\mathcal T} \\prod_{k = 1}^K \\left[\\pi_k^{n_k}\\cdot \\prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\\right]\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Whereas the log of a product of terms is the sum of the logs of the terms, no such easy simplification exists for the log of a *sum* of terms. This means that we will have to get a bit creative here. Instead, we will turn first to the *a priori* Random Dot Product Graph, and then figure out how to estimate parameters from an *a posteriori* SBM using that.\n",
    "\n",
    "## *a priori* Random Dot Product Graph\n",
    "\n",
    "The *a priori* Random Dot Product Graph has a single parameter, $X \\in \\mathbb R^{n \\times d}$, which is a real matrix with $n$ rows (one for each node) and $d$ columns (one for each latent dimension). We estimate $X$ extremely simply for a realization $A$ of a random network $\\pmb A$ which is characterized using the *a priori* Random Dot Product Graph.\n",
    "\n",
    "In order to produce an estimate of $X$, we also need to know the number of latent dimensions of $\\pmb A$, $d$. We might have a reasonable ability to \"guess\" what $d$ is ahead of time, but this will often not be the case. For this reason, we can instead estimate $d$ using $\\hat d$ [cite ZG2]. $\\hat d$ represents an estimate of $d$, which is selected on the basis of \"elbow picking\", as described in the section on spectral embedding. The $\\hat \\cdot$ symbol just means that $\\hat d$ is an estimate of the number of latent dimensions $d$, and not necessarily the *actual* number of latent dimensions. The estimate of $X$ is produced by using the [Adjacency Spectral Embedding](link?), by embedding the observed network $A$ into $d$ (if the number of latent dimensions is known) or $\\hat d$ (if the number of latent dimensions is not known) dimensions.\n",
    "\n",
    "Let's try an example of an *a priori* RDPG. We will use the same example that we used in the single network models section, where we defined the latent position matrix $X$ as follows. Let's assume that we have $60$ people who live along a very long road that is $60$ miles long, and each person is $1$ of a mile apart. The nodes of our network represent the people who live along our assumed street. If two people are closer to one another, it might make sense to think that they have a higher probability of being friends. If two people are neighbors, we think that they will have a very high probability of being friends (almost $1$) and when people are very far apart, we think that they will have a very low probability of being friends (almost $0$). We define $X$ to have rows given by:\n",
    "\\begin{align*}\n",
    "    \\vec x_i = \\begin{bmatrix}\n",
    "        \\left(\\frac{60 - i}{60}\\right)^2 \\\\\n",
    "        \\left(\\frac{i}{60}\\right)^2\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "In this case, since each $\\vec x_i$ is $2$-dimensional, the number of latent dimensions in $X$ is $d=2$. Let's simulate an example network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graspologic.simulations import rdpg\n",
    "from graphbook_code import draw_multiplot\n",
    "from graspologic.plot import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 60  # the number of nodes in our network\n",
    "\n",
    "# design the latent position matrix X according to \n",
    "# the rules we laid out previously\n",
    "X = np.zeros((n,2))\n",
    "for i in range(0, n):\n",
    "    X[i,:] = [((n - i)/n)**2, (i/n)**2]\n",
    "\n",
    "P = X @ np.transpose(X)\n",
    "\n",
    "np.random.seed(12)\n",
    "A = rdpg(X)\n",
    "\n",
    "draw_multiplot(A, title=\"Simulated RDPG(X)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we fit a `rdpg` model to $A$? We will evaluate the performance of the RDPG estimator agaigraspologic.plot the estimated probability matrix, $\\hat P = \\hat X \\hat X^\\top$, to the true probability matrix, $P = XX^\\top$. We can do this using the `RDPGEstimator` object, provided directly by graspologic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.models import RDPGEstimator\n",
    "\n",
    "model = RDPGEstimator(n_components=2, loops=False)  # number of latent dimensions is 2\n",
    "model.fit(A)\n",
    "Xhat = model.latent_\n",
    "Phat = Xhat @ np.transpose(Xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphbook_code import plot_latents\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "heatmap(Phat,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat P_{RDPG}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "heatmap(P,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$P_{RDPG}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our estimated probability matrix tends to preserve the pattern in the true probability matrix, where the probabilities are highest for pairs of nodes which are closer together, but lower for pairs of nodes which are farther apart. \n",
    "\n",
    "What if we did not know that $d$ was $2$ ahead of time? The RDPG Estimator handles this situation just as well, and we can estimate the number of latent dimensions with $\\hat d$ instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RDPGEstimator(loops=False)  # number of latent dimensions is not known\n",
    "model.fit(A)\n",
    "Phat = model.p_mat_\n",
    "print(\"Fit number of latent dimensions: {}\".format(model.latent_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that choosing the best-fit elbow instead yielded $\\hat d = 3$; that is, the number of latent dimensions are estimated to be $3$. Again, looking at the estimated and true probability matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "heatmap(Phat,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat P_{RDPG}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "heatmap(P,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$P_{RDPG}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which also is a decent estimate of the true probability matrix $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *a posteriori* Stochastic Block Model\n",
    "\n",
    "Finally, we can return to our original goal, which was to estimate the parameters of an *a posteriori* Stochastic Block Model.\n",
    "\n",
    "For the *a posteriori* Stochastic Block Model with $K$-communities, recall that we have two parameters, $\\vec \\pi$ which is a $K$-dimensional probability vector, and $B$ which is the $K \\times K$ block matrix. We observe the network $A$, which is a realization of the random network $\\pmb A$. To estimate $\\vec \\pi$ and $B$, the approach we will take will be to use $A$ to produce a *best guess* as to which community each node of $A$ is from, and then use our *best guesses* as to which community each node is from to learn about $\\vec \\pi$ and $B$.\n",
    "\n",
    "### Number of communities $K$ is known\n",
    "\n",
    "When the number of communities is known, the procedure for fitting an *a posteriori* Stochastic Block Model to a network is relatively straightforward. Let's consider a similar example to the scenario we had above, but with $3$ communities instead of $2$. We will have a block matrix given by:\n",
    "\\begin{align*}\n",
    "    B &= \\begin{bmatrix}\n",
    "        0.8 & 0.2 & 0.2 \\\\\n",
    "        0.2 & 0.8 & 0.2 \\\\\n",
    "        0.2 & 0.2 & 0.8\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "Which is a Stochastic block model in which the within-community edge probability is $0.8$, and exceeds the between-community edge probability of $0.2$. We will let the probability of each node being assigned to different blocks be equal, and we will produce a matrix with $100$ nodes in total. For simulating from the Stochastic Block Model, we actually only need the number of nodes for each community, since none of the mathematical operations we take to learn about $\\vec \\pi$ and $B$ will produce a different answer if we were to reorder the nodes in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import sbm\n",
    "\n",
    "pi_vec = [1/3, 1/3, 1/3]\n",
    "n = 100\n",
    "# sample counts of each community with probability pi, equivalent to\n",
    "# sampling a community for each node individually\n",
    "ns = np.random.multinomial(n, pi_vec, size=1).flatten()\n",
    "\n",
    "B = [[0.8, 0.2, 0.2],\n",
    "     [0.2, 0.8, 0.2],\n",
    "     [0.2, 0.2, 0.8]]\n",
    "\n",
    "np.random.seed(12)\n",
    "A = sbm(n=ns, p = B)\n",
    "\n",
    "# the true community labels\n",
    "y = [0 for i in range(0,ns[0])] + [1 for i in range(0, ns[1])] + [2 for i in range(0, ns[2])]\n",
    "draw_multiplot(A, labels=y, xticklabels=10, yticklabels=10, title=\"Simulated SBM($\\pi$, B)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, however, that we do not *actually* know the community labels of each node in $A$, so this problem is a little more difficult than it might seem. Remember that as we learned in the single network models section, even though the communities eachh node is assigned to *look* obvious, this is an artifact of the ordering of the nodes. In real data, the nodes might not actually be ordered in a manner which makes the community structure as readily apparent.\n",
    "\n",
    "To proceed, we cannot simply use the `SBMEstimator` class like we did previously. This is because the `SBMEstimator` uses node community assignments, which we do not have. Instead, what we will do is turn again to the adjacency spectral embedding, to reduce the observed network $A$ to a an estimated latent position matrix, $\\hat X$. Then, we will use K-Means clustering (or an alternative clustering technique, such as Gaussian Mixture Model) to assign each node's latent position to a particular community. Finally, we will use the communities to which each node is assigned to infer about the block matrix, $B$. We will demonstrate how to use K-means clustering to infer block labels here. We begin by first embedding $A$ to estimate a latent position matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.embed import AdjacencySpectralEmbed\n",
    "\n",
    "ase = AdjacencySpectralEmbed()  # adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking\n",
    "Xhat = ase.fit_transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Plots\n",
    "\n",
    "When embedding a matrix using any embedding techniques of `graspologic`, it is critical to investigate the quality of an embedding. One technique to do so that is particularly useful for uncovering \"latent structure\" (community assignments which are present, but *unknown* by us ahead of time) from a graph we suspect might be well-fit by a Stochastic Block Model is known as a \"pairs plot\". In a pairs plot, we investigate how effectively the embedding \"separates\" nodes within the dataset into individual \"clusters\". We will ultimately exploit these \"clusters\" that appear in the latent positions to generate community assignments for each node. To demonstrate the case where the \"pairs plot\" shows obvious latent community structure, we will use the predicted latent position matrix we just produced, from an adjacency matrix which is a realization of a random network which is truly a Stochastic Block Model. The pairs plot looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.plot import pairplot\n",
    "\n",
    "_ = pairplot(Xhat, title=\"SBM adjacency spectral embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the pairs plot is a `d x d` matrix of plots, where `d` is the total number of features of the matrix for which a pairs plot is being produced. For each off-diagonal plot (the plots with the dots), the $k^{th}$ row and $l^{th}$ column scatter plot has the points $(x_{ik}, x_{il})$ for each node $i$ in the adjacency matrix. Stated another way, the off-diagonal plot is a scatter plot for each node of the $k^{th}$ dimension and the $l^{th}$ dimension of the latent position matrix. That these scatter plots indicate that the points appear to be separated into individual clusters provides evidence that the latent position matrix contains latent community structure from the realized network, and is a sign that we will find reasonable \"guesses\" at community assignments further down the line.\n",
    "\n",
    "The diagonal elements simply represent histograms of the indicated values for the indicated dimension. Higher bars indicate that more points are have weights in that range. For instance, the top-left histogram indicates a histogram of the first latent dimension for all nodes, the middle histogram is a histogram of the second latent dimension for all nodes, so on and so forth.\n",
    "\n",
    "Next, we will show a brief example of what happens when adjacency spectral embedding does not indicate that there is latent community structure. Our example network here will be a realization of a network which is ER, with a probability of $0.5$ for an edge existing between any pair of nodes. As an ER network does not have community structure, we would not expect the pairs plot to show discernable clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import er_np\n",
    "\n",
    "A_er = er_np(n=100, p = 0.5)\n",
    "draw_multiplot(A_er, title=\"ER(0.5)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ase_er = AdjacencySpectralEmbed(n_components=3)  # adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking\n",
    "Xhat_er = ase_er.fit_transform(A_er)\n",
    "\n",
    "_ = pairplot(Xhat_er, title=\"ER adjacency spectral embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the SBM example, the scatter plots for the adjacency spectral embedding of a realization of an ER network no longer show the distinct separability into individual communities. \n",
    "\n",
    "Next, let's return to our SBM example and obtain some predicted community assignments for our points. Since we do not have any information as to which cluster each node is assigned to, we must use an unsupervised clustering method. We will use the `KMeans` function from `sklearn`'s cluster module to do so. Since we know that the SBM has 3 communities, we will use 3 clusters for the KMeans algorithm. The clusters produced by the `KMeans` algorithm will be our \"predicted\" community assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "labels_kmeans = KMeans(n_clusters = 3).fit_predict(Xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have simulated data, we have the benefit of being able to evaluate the quality of our predicted community assignments to the true community assignments. We will use the Adjusted Rand Index (ARI), which is a measure of the clustering accuracy. A high ARI (near $1$) indicates a that the predicted community assignments are good relative the true community assignments, and a low ARI (near $0$) indicates that the predicted community assignments are not good relative the true community assignments. The ARI is agnostic to the names of the different communities, which means that even if the community labels assigned by unsupervised learning do not match the community labels in the true realized network, the ARI is still a legitimate statistic we can investigate. We will look more at the implications of this in the following paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "ari_kmeans = adjusted_rand_score(labels_kmeans, y)\n",
    "print(\"ARI(predicted communities, true communities) = {}\".format(ari_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARI of $1$ indicates that the true communities and the predicted communities are in complete agreement!\n",
    "\n",
    "When using unsupervised learning to learn about labels (such as, in this case, community assignments) for a given set of points (such as, in this case, the latent positions of each of the $n$ *nodes* of our realized network), a truly unsupervised approach knows *nothing* about the true labels for the set of points. This has the implication that the assigned community labels may not make sense in the context of the true labels, or may not align. For instance, a predicted community of $2$ may not mean the same thing as the true community being $2$, since the true community assignments did not have any *Euclidean* relevance to the set of points we clustered. This means that we may have to remap the labels from the unsupervised learning predictions to better match the true labels so that we can do further diagnostics. For this reason, the `graspologic` package offers the `remap_labels` utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.utils import remap_labels\n",
    "\n",
    "labels_kmeans_remap = remap_labels(y, labels_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these remapped labels to understand when `KMeans` is, or is not, producing reasonable labels for our investigation. We begin by first looking at a pairs plot, which now will color the points by their assigned community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot(Xhat,\n",
    "         labels=labels_kmeans_remap,\n",
    "         title=f'KMeans on embedding, ARI: {str(ari_kmeans)[:5]}',\n",
    "         legend_name='Predicted label',\n",
    "         height=3.5,\n",
    "         palette='muted',);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final utility of the pairs plot is that we can investigate which points, if any, the clustering technique is getting wrong. We can do this by looking at the classification error of the nodes to each community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - labels_kmeans_remap  # compute which assigned labels from labels_kmeans_remap differ from the true labels y\n",
    "error = error != 0  # if the difference between the community labels is non-zero, an error has occurred\n",
    "er_rt = np.mean(error)  # error rate is the frequency of making an error\n",
    "\n",
    "palette = {'Right':(0,0.7,0.2),\n",
    "           'Wrong':(0.8,0.1,0.1)}\n",
    "\n",
    "error_label = np.array(len(y)*['Right'])  # initialize numpy array for each node\n",
    "error_label[error] = 'Wrong'  # add label 'Wrong' for each error that is made\n",
    "\n",
    "pairplot(Xhat,\n",
    "         labels=error_label,\n",
    "         title=f'Error from KMeans, Error rate: {str(er_rt)}',\n",
    "         legend_name='Error label',\n",
    "         height=3.5,\n",
    "         palette=palette,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our classification has not made any errors.\n",
    "\n",
    "Next, let's learn about the parameters, $\\vec \\pi$ and $B$. To learn about $\\vec \\pi$ is rather simple. Our \"best guess\" at the probability of a node being assigned to a particular community is simply the fraction of nodes which are assigned to that community by the clustering technique we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un, counts = np.unique(labels_kmeans_remap, return_counts=True)\n",
    "cts = dict(zip(un, counts/np.sum(counts)))\n",
    "for k, v in cts.items():\n",
    "    print(\"pi_{}hat: {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the predicted community assignment probability vector, $\\hat{\\vec\\pi}$, does not exactly match the true community assignment probability vector, $\\vec \\pi = \\begin{bmatrix}\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\end{bmatrix}$. To learn about the block matrix $B$, we can now use the `SBMEstimator` class, with our predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.models import SBMEstimator\n",
    "\n",
    "model = SBMEstimator(directed=False, loops=False)\n",
    "model.fit(A, y=labels_kmeans_remap)\n",
    "Bhat = model.block_p_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "heatmap(Bhat,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat B_{SBM}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "heatmap(np.array(B),\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$B_{SBM}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "heatmap(np.abs(Bhat - np.array(B)),\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$|\\hat B_{SBM} - B_{SBM}|$\",\n",
    "        ax=axs[2])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of communities $K$ is not known\n",
    "\n",
    "In real data, we almost never have the beautiful canonical modular structure obvious to us from a Stochastic Block Model. This means that it is *extremely infrequently* going to be the case that we know exactly how we should set the number of communities, $K$, ahead of time.\n",
    "\n",
    "Let's first remember back to the single network models section, when we took a Stochastic block model with obvious community structure, and let's see what happens when we just move the nodes of the adjacency matrix around. We begin with a similar adjacency matrix to $A$ given above, for the $3$-community SBM example, but with the within and between-community edge probabilities a bit closer together so that we can see what happens when we experience errors. The communities are still slightly apparent, but less so than before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = [[0.7, 0.45, 0.35],\n",
    "     [0.45, 0.7, 0.45],\n",
    "     [0.35, 0.45, 0.7]]\n",
    "np.random.seed(12)\n",
    "A = sbm(n=ns, p = B)\n",
    "\n",
    "# the true community labels\n",
    "y = [0 for i in range(0,ns[0])] + [1 for i in range(0, ns[1])] + [2 for i in range(0, ns[2])]\n",
    "draw_multiplot(A, labels=y, xticklabels=10, yticklabels=10, title=\"Simulated SBM($\\pi$, B)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we permute the nodes around to reorder the realized adjacency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a reordering of the n nodes\n",
    "np.random.seed(123)\n",
    "vtx_perm = np.random.choice(A.shape[0], size=A.shape[0], replace=False)\n",
    "\n",
    "A_permuted = A[tuple([vtx_perm])] [:,vtx_perm]\n",
    "y_perm = np.array(y)[vtx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import heatmap as hm_code \n",
    "from graphbook_code import draw_layout_plot as lp_code\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# heatmap\n",
    "hm = hm_code(\n",
    "    A_permuted,\n",
    "    ax=axs[0],\n",
    "    cbar=False,\n",
    "    color=\"sequential\",\n",
    "    xticklabels=10,\n",
    "    yticklabels=10\n",
    ")\n",
    "\n",
    "# layout plot\n",
    "lp_code(A_permuted, ax=axs[1], pos=None, labels=y_perm, node_color=\"qualitative\")\n",
    "plt.suptitle(\"Simulated SBM($\\pi, B$), reordered vertices\", fontsize=20, y=1.1)\n",
    "    \n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only get to see the adjacency matrix in the *left* panel; the panel in the *right* is constructed by using the true labels (which we do *not* have!). This means that we proceed for statistical inference about the random network underlying our realized network using *only* the heatmap we have at right. It is not immediately obvious that this is the realization of a random network which is an SBM with $3$ communities.\n",
    "\n",
    "Our procedure is *very* similar to what we did previously. We again embed using the \"elbow picking\" technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ase_perm = AdjacencySpectralEmbed()  # adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking\n",
    "Xhat_permuted = ase_perm.fit_transform(A_permuted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine the pairs plot, *just* like we did previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pairplot(Xhat_permuted, title=\"SBM adjacency spectral embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still see that there is some level of latent community structure apparent in the pairs plot. This is evident from, for instance, the plots of Dimension 2 against Dimension 3, where we can see that the latent positions of respective nodes *appear* to be clustering in some way.\n",
    "\n",
    "Next, we have the biggest difference with the approach we took previously. Since we do *not* know the optimal number of clusters $K$ *nor* the true community assignments, we must choose an unsupervised clustering technique which allows us to *compare* clusterings with different choices of clusters. We can again perform this using the `KMeans` algorithm that we used previously. Here, we will compare the quality of a clustering with one number of clusters to the quality of a clustering with a *different* number of clusters using the silhouette score. The optimal clustering is selected to be the clustering which has the largest silhouette score across all attempted numbers of clusters.\n",
    "\n",
    "This feature is implemented automatically in the `KMeansCluster` function of `graspologic`. We will select the number of clusters which maximizes the silhouette score, and will allow at most $10$ clusters total to be produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.cluster import KMeansCluster\n",
    "\n",
    "km_clust = KMeansCluster(max_clusters = 10)\n",
    "km_clust = km_clust.fit(Xhat_permuted);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the silhouette score as a function of the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pandas import DataFrame as df\n",
    "\n",
    "nclusters = range(2, 11)  # graspologic nclusters goes from 2 to max_clusters\n",
    "silhouette = km_clust.silhouette_  # obtain the respective silhouette scores\n",
    "\n",
    "silhouette_df = df({\"Number of Clusters\": nclusters, \"Silhouette Score\": silhouette})  # place into pandas dataframe\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "\n",
    "sns.lineplot(data=silhouette_df,ax=ax, x=\"Number of Clusters\", y=\"Silhouette Score\");\n",
    "ax.set_title(\"Silhouette Analysis of KMeans Clusterings\")\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Silhouette Analysis has indicated the best number of clusters as $3$ (which, is indeed, *correct* since we are performing a simulation where we know the right answer). Next, let's take a look at the pairs plot for the optimal classifier. We begin by producing the predicted labels for each of our nodes, and remapping to the true community assignment labels, exactly as we did previously for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_autokmeans = km_clust.fit_predict(Xhat_permuted)\n",
    "labels_autokmeans = remap_labels(y_perm, labels_autokmeans)\n",
    "\n",
    "ari_kmeans = adjusted_rand_score(labels_autokmeans, y_perm)\n",
    "\n",
    "pairplot(Xhat_permuted,\n",
    "         labels=labels_autokmeans,\n",
    "         title=f'KMeans on embedding, ARI: {str(ari_kmeans)[:5]}',\n",
    "         legend_name='Predicted label',\n",
    "         height=3.5,\n",
    "         palette='muted',);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KMeans` was still able to find relatively stable clusters, which align quite well (ARI of $0.855$, which is not perfect but closer to $1$ than to $0$!) with the true labels! Next, we will look at which points `KMeans` tends to get *wrong* to see if any patterns arise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y_perm - labels_autokmeans  # compute which assigned labels from labels_kmeans_remap differ from the true labels y\n",
    "error = error != 0  # if the difference between the community labels is non-zero, an error has occurred\n",
    "er_rt = np.mean(error)  # error rate is the frequency of making an error\n",
    "\n",
    "palette = {'Right':(0,0.7,0.2),\n",
    "           'Wrong':(0.8,0.1,0.1)}\n",
    "\n",
    "error_label = np.array(len(y)*['Right'])  # initialize numpy array for each node\n",
    "error_label[error] = 'Wrong'  # add label 'Wrong' for each error that is made\n",
    "\n",
    "pairplot(Xhat_permuted,\n",
    "         labels=error_label,\n",
    "         title=f'Error from KMeans, Error rate: {str(er_rt)}',\n",
    "         legend_name='Error label',\n",
    "         height=3.5,\n",
    "         palette=palette,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there do not appear to be any dramatic issues in our clustering which woul suggest systematic errors are present. To infer about $\\vec \\pi$ or $B$, we would proceed exactly as we did previously, by using these labels with the `SBMEstimator` class to perform inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un, counts = np.unique(labels_autokmeans, return_counts=True)\n",
    "cts = dict(zip(un, counts/np.sum(counts)))\n",
    "for k, v in cts.items():\n",
    "    print(\"pi_{}hat: {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SBMEstimator(directed=False, loops=False)\n",
    "model.fit(A_permuted, y=labels_autokmeans)\n",
    "Bhat = model.block_p_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "heatmap(Bhat,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat B_{SBM}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "heatmap(np.array(B),\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$B_{SBM}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "heatmap(np.abs(Bhat - np.array(B)),\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$|\\hat B_{SBM} - B_{SBM}|$\",\n",
    "        ax=axs[2])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which appears very close to the true $B$.\n",
    "\n",
    "```{admonition} a posteriori Stochastic Block Model, Recap\n",
    "We just covered many details about how to perform statistical inference with a realization of a random network which we think can be well summarized by a Stochastic Block Model. For this reason, we will review some of the key things that were covered, to better put them in context:\n",
    "1. We learned that the Adjacency Spectral Embedding is a key algorithm for making sense of networks we believe may be realizations of networks which are well-summarized by Stochastic Block Models, as inference on the the *estimated latent positions* is key for learning about community assignments.\n",
    "2. We learned how unsupervised learning allows us to use the estimated latent positions to learn community assignments for nodes within our realization.\n",
    "3. We learned how to *align* the labels produced by our unsupervised learning technique with true labels in our network, using `remap_labels`.\n",
    "4. We learned how to produce community assignments, regardless of whether we know how many communities may be present in the first place.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
