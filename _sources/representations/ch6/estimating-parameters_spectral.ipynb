{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "executive-officer",
   "metadata": {},
   "source": [
    "# Estimating Parameters for the RDPG\n",
    "\n",
    "In this section, we'll cover a special case of the spectral embedding, which is when we think that the network might be well described by an underlying RDPG. We'll double back on some concepts, such as the singular value decomposition and matrix rank, so that we can explain the importance of these concepts as they relate directly to the probability matrix of an RDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-indicator",
   "metadata": {},
   "source": [
    "For this example, we'll work with the school example we've seen previously. The nodes are 100 students in total, from one of two schools. Here, the first 50 students are from the first school, and the second 50 students are from the second school. The probability of two students who both go to the first school being friends is $0.5$, and the probability of two students who both go to school two being friends will also be $0.5$. If two students go to different schools, their probability of being friends will be $0.2$. The statistical model has parameters which look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspologic as gp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ns1 = 50; ns2 = 50\n",
    "ns = [ns1, ns2]\n",
    "\n",
    "# zvec is a column vector of 50 1s followed by 50 2s\n",
    "# this vector gives the school each of the 100 students are from\n",
    "zvec = np.array([\"S1\" for i in range(0, ns1)] + [\"S2\" for i in range(0, ns2)])\n",
    "\n",
    "# the block matrix\n",
    "B = [[0.5, 0.3], [0.3, 0.5]]\n",
    "\n",
    "# the probability matrix\n",
    "zvec_ohe = np.vstack([[1, 0] for i in range(0, ns1)] + [[0, 1] for i in range(0, ns2)])\n",
    "P = zvec_ohe @ B @ zvec_ohe.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-enterprise",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import cmaps\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "gp.plot.heatmap(np.array(B), ax=axs[0], title=\"Block Matrix $B$\", \n",
    "                xticklabels=[\"S1\", \"S2\"], yticklabels=[\"S1\", \"S2\"],\n",
    "                cbar=False, annot=True, vmin=0, vmax=1, cmap=cmaps[\"sequential\"])\n",
    "gp.plot.heatmap(P, ax=axs[1], inner_hier_labels=zvec,\n",
    "                title=\"Probability Matrix $P$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-monster",
   "metadata": {},
   "source": [
    "The procedure for generating the probability matrix from the block probability matrix that we used above is a linear algebra \"cheat\", but in reality, all that's happening is that we are comparing whether two nodes are in the first or the second school, and then taking the appropriate entry from the block matrix accordingly. The operation is exactly the same as we had in the section on RDPG. For two nodes $i$ and $j$, the probability they are connected is:\n",
    "\\begin{align*}\n",
    "    p_{ji} = p_{ij}, p_{ij} = \\begin{cases}\n",
    "        b_{11} & z_i = 1, z_j = 1 \\\\\n",
    "        b_{12} & z_i = 1, z_j = 2 \\\\\n",
    "        b_{22} & z_i = 2, z_j = 2\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Next, we will use this probability matrix and the corresponding community assignment vector to generate a realization of the stochastic block model we saw above. This is our \"real network\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = gp.simulations.sbm(ns, B, directed=False, loops=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-burner",
   "metadata": {},
   "source": [
    "You can see the adjacency matrix and the probability matrix below. Notice that there are two distrinct blocks in the adjacency matrix, which are shared with the probability matrix: in its upper-left, you can see the edges between the first 50 nodes (the individuals in the first school), and in the bottom right, you can see the edges between the second 50 nodes (the individuals in the second school)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-roots",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "gp.plot.heatmap(P, ax=axs[0], inner_hier_labels=zvec,\n",
    "                title=\"Probability Matrix $P$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);\n",
    "gp.plot.heatmap(A, ax=axs[1], inner_hier_labels=zvec,\n",
    "                title=\"Realization of SBM Random Network, $A$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-gambling",
   "metadata": {},
   "source": [
    "As we can see, both the probability matrix $P$ and the realization of the random network $A$ both have a notable \"structure\", in that it's clear that there are more nodes when both individuals are in the first school (the top left and bottom right \"squares\" of the adjacency matrix and probability matrix have more entries and are darker, respectively) and fewer nodes when both individuals are in different schools (the top right and bottom left \"squares\" of the adjacency matrix have fewer entries and are lighter, respectively). \n",
    "\n",
    "Next, we have the key conceptual leap we need to take: remember that if a network is an SBM, there is also an underlying RDPG that *also* describes that network. When we learned about this in [Chapter 5](#link?), we learned that this was because the RDPG is a more broad statistical model that was more general than the SBM. So, to learn about the SBM which underlies $A$, we could also learn about an RDPG which underlies $A$, too. This will prove critical in later applications sections, such as [Chapter 8](#link?), when we try to decipher which nodes are in which communities of the network, without having them handed to us in this nice organized way.\n",
    "\n",
    "## How are nodes from the same community similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-function",
   "metadata": {},
   "source": [
    "As it turns out, there's a really important property that is shared by nodes in an SBM random network. Remember that the probability matrix $P$ gives the probabilities $p_{ji}$ of each pair of nodes $i$ and $j$ of being connected. We're going to introduce a new piece of notation here, called the *vector of probabilities* for a single node. The **vector of probabilities** for a node $i$ is the quantity:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\vec p_i &= \\begin{bmatrix}\n",
    "        p_{i1} \\\\\n",
    "        \\vdots \\\\\n",
    "        p_{in}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In words, it is basically just the $i^{th}$ row of the probability matrix $P$. Now, what happens when we look at the probability vectors for nodes which are in the same, versus different communities? Here, what we will do is take the probability vectors for students $1$ and $2$, who both attend school one, and compare them to the probability vectors for students $51$ and $52$, who both attend school two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the probability vectors for the students we outlined above\n",
    "Psubset = P[[0, 1, 50, 51],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-publisher",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(13, 3))\n",
    "sns.heatmap(Psubset, ax=ax, vmin=0, center=0, vmax=1,\n",
    "           yticklabels=[\"Student 1\", \"Student 2\", \"Student 51\", \"Student 52\"], cmap=cmaps[\"sequential\"])\n",
    "ax.set_title(\"Probability that student $i$ is friends with student $j$\")\n",
    "ax.set_ylabel(\"Student $i$\")\n",
    "ax.set_xlabel(\"Student $j$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-listing",
   "metadata": {},
   "source": [
    "The probability vectors for students $1$, $2$, $51$, and $52$ are shown as the rows of the above heatmap. What do we notice about the probability vectors for students $1$ and $2$ in comparison to students $51$ and $52$? As it turns out, they are exactly the same, and the probability vectors are identical! In general, what this means is that, for an SBM, *all* of the nodes in a single community have the *exact same* probability vector! If you are interested in some more details as to why this is the case for an SBM, the reason is that the probability vector for a given node is *fully specified* by just knowing the block matrix and the community assignment vector for the nodes in the network. What this means is that there is nothing special about one node versus another node in the same community, in the probability sense.\n",
    "\n",
    "## The low-rank property and the probability matrix\n",
    "\n",
    "Now, what does this mean for *us*? What this means is that the probability matrix has a special property, called the *low-rank* property. We already studied matrix rank, which if we recall, described how many unique row or column vectors we would need to define *all* of the other row and column vectors of a matrix.\n",
    "\n",
    "If an SBM is has $K$ communities, what does this mean about its probability matrix? Well, its probability matrix is *also* exactly rank-$K$! From what we learned about the Laplacian Spectral Embedding, we might reasonably expect that a very similar procedure might produce a similarly interesting result for us, so let's get started. \n",
    "\n",
    "We'll take the svd of the probability matrix of *the random network itself* this time (instead of the Laplacian of a realization of the random network). As we remember, the svd created three matrices, $U$, $\\Sigma$, and $V$, for us, with the property that $U$ had $n$ columns called the left singular vectors, $\\Sigma$ was a diagonal matrix whose entries were the singular values in non-increasing order (each singular value can be at most the previous), and $V$ also had $n$ columns called the right singular vectors.\n",
    "\n",
    "Remember that when we take an svd, we start with looking at the scree plot, which was a plot of the singular values ordered by their index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the singular value decomposition\n",
    "U, s, Vt = np.linalg.svd(P)\n",
    "# U is the matrix whose columns are the left singular vectors\n",
    "# s is the vector whose entries are the singular values\n",
    "# Vt is the matrix whose rows are the right singular vectors\n",
    "# and whose tranpose has columns which are the right singular vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-ferry",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "def plot_scree(svs, title=\"\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(10, 4))\n",
    "    sv_dat = DataFrame({\"Singular Value\": svs, \"Dimension\": range(1, len(svs) + 1)})\n",
    "    sns.scatterplot(data=sv_dat, x=\"Dimension\", y=\"Singular Value\", ax=ax)\n",
    "    ax.set_xlim([0, len(s)])\n",
    "    ax.set_xlabel(\"Dimension\")\n",
    "    ax.set_ylabel(\"Singular Value\")\n",
    "    ax.set_title(title)\n",
    "    ax.axvline(x=2, color='r')\n",
    "    ax.text(x=3, y=15, s=\"Dimension 2\", color='r', size=15)\n",
    "\n",
    "plot_scree(s, title=\"Scree plot of $P$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-going",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "What we see in the scree plot is that it just so happens that for the probability matrix for a $K$-community SBM, the probability matrix had $K$ non-zero singular values! This fact will be important to us later on, so we will highlight this later.\n",
    "\n",
    "Next, we see that $P = U\\Sigma V^\\top$, and that the expression we learned previously, is still true: that $P = \\sum_{i = 1}^n \\vec u_i \\vec v_i^\\top$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix product form of expression for P\n",
    "Psvd = U @ np.diag(s) @ Vt\n",
    "\n",
    "# sum expression for P\n",
    "# remember that U[:,i] is the ith column of U (the ith left singular vector)\n",
    "# and Vt[i,:] is the ith column of V (the ith right singular vector)\n",
    "# and the ith row of Vt (transpose)\n",
    "Psum = np.sum([s[i]*np.array(U[:,[i]]) @ np.array(Vt[[i],:]) for i in range(0, ns1 + ns2)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-hours",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(21, 6))\n",
    " \n",
    "gp.plot.heatmap(P, ax=axs[0],\n",
    "                title=\"$P$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);\n",
    "gp.plot.heatmap(Psvd, ax=axs[1],\n",
    "                title=\"$U\\\\Sigma V^\\\\top$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);\n",
    "gp.plot.heatmap(Psum, ax=axs[2],\n",
    "                title=\"$\\\\sum_{i = 1}^n \\sigma_i u_i v_i^\\\\top$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-consultation",
   "metadata": {},
   "source": [
    "#### The best representation of the probability matrix\n",
    "\n",
    "As it turns out, the relationships and intuition we learned about the Laplacian Spectral Embedding work here, too. Since the probability matrix $P$ is square, symmetric, and has all positive real entries, it is *also* going to have $K$ positive, real singular values, and the rest will all be zero. Further, the first $K$ left and right singular vectors will all be the same! We'll borrow the same notation we used with the section on the Laplacian Spectral Embedding, remembering that this means that we have two matrices, $U_K$ and $\\Sigma_K$, where:\n",
    "\\begin{align*}\n",
    "    U_K &= \\begin{bmatrix}\n",
    "        \\uparrow & & \\uparrow \\\\\n",
    "        \\vec u_1 & ... & \\vec u_K \\\\\n",
    "        \\downarrow & & \\downarrow\n",
    "    \\end{bmatrix},\\;\\;\\;\\Sigma_K &= \\begin{bmatrix}\n",
    "        \\sigma_1 & 0 & ... & 0 \\\\\n",
    "        0 & \\sigma_2 & \\ddots & \\vdots \\\\\n",
    "        \\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "        0 & ... & 0 & \\sigma_K\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "and we learned that $P = U_K\\Sigma_K U_K^\\top$, or stated another way:\n",
    "\\begin{align*}\n",
    "    P &= \\sum_{i = 1}^K \\sigma_i \\vec u_i \\vec u_i^\\top\n",
    "\\end{align*}\n",
    "\n",
    "We can see this using the probability matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "UK = U[:,0:K]; SK = np.diag(s[0:K])\n",
    "\n",
    "# Psymm is UK * SigmaK * UK^transpose\n",
    "Psymm = UK @ SK @ UK.transpose()\n",
    "diff = np.linalg.norm(P - Psymm)  # compute the frobenius difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-resort",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "gp.plot.heatmap(P, ax=axs[0],\n",
    "                title=\"$P$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);\n",
    "gp.plot.heatmap(Psymm, ax=axs[1],\n",
    "                title=\"$U_K \\\\Sigma_K U_K^\\\\top$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);\n",
    "gp.plot.heatmap(P - Psymm, ax=axs[2],\n",
    "                title=\"$P - U_K \\\\Sigma_K U_K^\\\\top$, Frobenius difference = {:.2f}\".format(diff),\n",
    "                cmap=cmaps[\"sequential\"], vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-horror",
   "metadata": {},
   "source": [
    "#### The Square Root Matrix\n",
    "\n",
    "It's been quite a ride; don't fall off just yet! Now we are really in the home stretch! We've saved the easiest part for last. Remember that $\\Sigma_K$ is just a diagonal matrix, whose entries are the singular values $\\sigma_i$ for the first $K$ singular values. As it turns out, since $P$ is positive, these singular values are going to be positive too, which means that we can break each singular value into its square root, as $\\sigma_i = \\sqrt{\\sigma_i}\\sqrt{\\sigma_i}$. \n",
    "\n",
    "Like for the Laplacian Spectral Embedding, this meant we could factor the first $K$ singular value matrix $\\Sigma_K$ into the product of its square root matrix and its transpose, as $\\Sigma_K = \\sqrt{\\Sigma_K}\\sqrt{\\Sigma_K}$.\n",
    "\n",
    "This means that our probability matrix is just $P = U_K \\sqrt{\\Sigma_K}\\sqrt{\\Sigma_K}^\\top U_K^\\top$, which is very similar to what we got for the Laplacian spectral embedding. The difference here is that $P$ *itself* is equal to this quantity, not just a \"reduced rank\" representation of $P$ like we had for the Laplacian spectral embedding.\n",
    "\n",
    "If we let $X = U_K \\sqrt{\\Sigma_K}$, then $P = XX^\\top$. This means that $X$ is a latent position matrix for $P$! If you remember back to the [Section on RDPGs](#link?), this means that we have found the latent position parameter for the corresponding RDPG for our SBM random network!\n",
    "\n",
    "Let's see this using numpy again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKsqrt = np.sqrt(SK)\n",
    "X = UK @ SKsqrt\n",
    "Prdpg = X @ X.transpose()\n",
    "diff = np.linalg.norm(P - Prdpg)  # compute the frobenius difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-glossary",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "gp.plot.heatmap(P, ax=axs[0],\n",
    "                title=\"$P$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);\n",
    "gp.plot.heatmap(Prdpg, ax=axs[1],\n",
    "                title=\"$XX^\\\\top$\", vmin=0, vmax=1, cmap=cmaps[\"sequential\"]);\n",
    "gp.plot.heatmap(P - Prdpg, ax=axs[2],\n",
    "                title=\"$P - XX^\\\\top$, Frobenius difference = {:.2f}\".format(diff),\n",
    "                cmap=cmaps[\"sequential\"], vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-adventure",
   "metadata": {},
   "source": [
    "This matrix $X$, the latent position matrix, for an SBM will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-colleague",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_lpm(D, ax=None, title=\"\", xticks=[], xticklabs=[],\n",
    "            yticks=[], yticklabs=[], cbar=True):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(3,8))\n",
    "    sns.heatmap(D, ax=ax, cbar=cbar)\n",
    "    ax.set_xlabel(\"Latent Dimension\")\n",
    "    ax.set_ylabel(\"Node\")\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xticklabs)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(yticklabs)\n",
    "    ax.set_title(title);\n",
    "\n",
    "plot_lpm(X, xticks=[0.5, 1.5], xticklabs=[\"$\\\\sqrt{\\\\sigma_1}\\\\vec u_1$\", \"$\\\\sqrt{\\\\sigma_2}\\\\vec u_2$\"],\n",
    "        yticks=[0, 51, 99], title=\"Heatmap of Latent Position Matrix $X$\",\n",
    "        yticklabs=[\"1\", \"51\", \"100\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-storm",
   "metadata": {},
   "source": [
    "and is the \"low-rank structure\" which describes the probability matrix. \n",
    "\n",
    "### The latent positions are distinct for each community\n",
    "\n",
    "Now, you will notice a very interesting property about the latent position matrix for an SBM. Remember that for an RDPG (and an SBM is also an RDPG), the latent position vectors for the node $i$ are the *rows* $\\vec x_i$ of the latent position matrix $X$. This means that the latent position vector for node $1$ is the vector $(\\sqrt{\\sigma_1}u_{11}, \\sqrt{\\sigma_2}u_{21})$. What do we notice about the latent positions vector for node $2$? It's *exactly* the same! \n",
    "\n",
    "More generally, the latent position vector for *all* nodes which are in the same community will be identical. This is why if we look across the latent position matrix $X$ above, there are only two unique latent position vectors: there are one unique vector (which is black in the first dimension and red in the second dimension) for the nodes in the first community, and a second unique vector (which is black in the first dimension and beige in the second dimension) for the nodes of the second community.\n",
    "\n",
    "```{admonition} Putting it all together\n",
    "What have we learned so far? What we've learned so far is that, if we have a probability matrix that is symmetric and rank-$K$:\n",
    "1. We can decompose this probability matrix using the singular value decomposition.\n",
    "2. We can ignore singular values/vectors other than the first $K$ of them.\n",
    "3. For the first $K$ singular vectors, the left and right vectors are identical.\n",
    "4. We can decompose the singular value matrix into the product of the square root matrix with its transpose.\n",
    "5. We can express the matrix $P$ using the latent position matrix $X$, which is the product of the first $K$ singular vectors with the first $K$ singular values.\n",
    "This means that we have found a latent position matrix $X$ for the probability matrix using the singular values and singular vectors of $P$, by effectively just discarding the ones that don't matter (and have singular values of $0$). We have succeeded in our goal of finding a much lower rank structure, the latent position matrix $X$, to describe the probability matrix $P$.\n",
    "```\n",
    "\n",
    "If you remember from the section on RDPGs, this probability matrix has the property that each entry $p_{ij} = \\vec x_i \\vec x_j^\\top$.\n",
    "\n",
    "## But wait: we don't have the probability matrix! What do we do?\n",
    "\n",
    "All of the logic we developed above was with respect to the probability matrix, $P$, for a SBM. More generally, this logic extends to the probability matrix $P$ for any RDPG, which is because an RDPG with $d$ latent dimensions will *always* have a probability matrix that is *exactly* rank $d$. If we took the latent position matrix $X$ which had $d$ latent dimensions, and then used the svd to find the $U_d$ and $\\Sigma_d$ where $P = U_d \\Sigma_d U_d^\\top$, we could find another latent position matrix $Y = U_d\\sqrt{\\Sigma_d}$ where $P = YY^\\top$.\n",
    "\n",
    "But, we have a slight issue: when we perform machine learning, we don't know the probability matrix! The probability matrix is a *parameter* of the statistical model itself, it is *not* a function of the sample of data we get. All we have is the adjacency matrix itself, $A$, which is our data! We don't actually know what the underlying probability matrix is! How the heck can we find this low rank structure we want to be able to estimate?\n",
    "\n",
    "As it turns out, if we kept obtaining more and more networks $A$ from the underlying RDPG random network $\\mathbf A$, we would *expect* that the network $A$ we saw would be the probability matrix $P$. We'll explain what we mean by expect here by turning back to our coin flip example. As you remember, we perform a coin flip at each pair of nodes $i$ and $j$ of an RDPG, where the coin lands on heads with probability $\\vec x_i\\vec x_j^\\top$, and lands on tails with probability $1 - \\vec x_i\\vec x_j^\\top$. This means we can *expect* the coin to land on heads with probability $\\vec x_i\\vec x_j^\\top$. In the same sense, we can expect the $(i,j)$ entry of the adjacency matrix $A$ to exist with probability $\\vec x_i \\vec x_j^\\top$. In this sense, the expected value of the adjacency matrix $A$ *is* the probability matrix $P = XX^\\top$. \n",
    "\n",
    "So, since the expected value of the adjacency matrix *is* the probability matrix, what if we were to just embed the adjacency matrix instead? Let's see how this might work. Again, we'll use the singular value decomposition on $A$, and take a look at the scree plot for $A$, and compare it to the scree plot of $P$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "UA, sA, VAt = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-orange",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plot_scree(s, title=\"Scree plot of $P$\", ax=axs[0])\n",
    "plot_scree(sA, title=\"Scree plot of $A$\", ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-northwest",
   "metadata": {},
   "source": [
    "Now that's really funky! The singular values of both $P$ and $A$ tend to fall off around *roughly* the same spot, right around dimension $2$! The singular value of $P$ go directly to $0$, but the singular values for $A$ tend to \"round off\" in the direction of $0$, but it isn't *too* far off!\n",
    "\n",
    "As it turns out, this is no coincidence: the singular values for a network which can be described by an RDPG will tend to \"elbow\" off right around the number of true latent dimensions for the probability matrix of the underlying random network. If the RDPG has $d$ latent dimensions, this will occur right around $d$. For this reason, it is usually a good idea when we think a network might be well described by an RDPG to let the elbow selection algorithm do the work for us, and then take a good look at the scree plot to make sure the number of latent dimensions chosen seems reasonable to us. If we don't know how many latent dimensions to retain, we'll call the number of embedding dimensions $\\hat d$, which just means, \"estimate of the number of latent dimensions\".\n",
    "\n",
    "What does it look like when we use the spectral embedding on $A$? We'll compare the embedding of the adjacency matrix to the latent positions of the probability matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "UAK = UA[:,0:K]; USK = np.diag(np.sqrt(sA[0:K]))\n",
    "Aembedded = UAK @ USK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(8, 10))\n",
    "\n",
    "plot_lpm(X, xticks=[0.5, 1.5], xticklabs=[\"Dimension 1\", \"Dimension 2\"],\n",
    "        yticks=[0, 51, 99], title=\"Heatmap of Latent Position Matrix $X$\",\n",
    "        yticklabs=[\"1\", \"51\", \"100\"], ax=axs[0], cbar=False)\n",
    "\n",
    "plot_lpm(Aembedded, xticks=[0.5, 1.5], xticklabs=[\"Dimension 1\", \"Dimension 2\"],\n",
    "        yticks=[0, 51, 99], title=\"Embedded Adjacency Matrix\",\n",
    "        yticklabs=[\"1\", \"51\", \"100\"], ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-border",
   "metadata": {},
   "source": [
    "Wow! When we take the adjacency matrix and embed it into $2$ dimensions, it doesn't look *identical* to the latent position matrix, but it shares some major patterns with it! In particular, it looks like the second latent dimension for the embedded adjacency matrix tends to capture that the second latent dimension of $X$ has higher values for the first $50$ nodes, and lower values for the second $50$ nodes. For a variety of reasons, we will call this \"embedding of $A$\" an *estimate* of the latent position matrix for the underlying RDPG, which we will denote by $\\hat X$. \n",
    "\n",
    "As we learned in the last section, this entire procedure is automated for us by `graspologic` with the `AdjacencySpectralEmbed()` class, or alternatively, the `RDPGEstimator()`. The `RDPGEstimator()` class just makes clear that we are estimating parameters for an RDPG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdpgest = gp.models.RDPGEstimator()\n",
    "rdpgest = rdpgest.fit(A)\n",
    "Xhat = rdpgest.latent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4, 10))\n",
    "\n",
    "plot_lpm(Xhat, xticks=[0.5, 1.5], xticklabs=[\"Dimension 1\", \"Dimension 2\"],\n",
    "        yticks=[0, 51, 99], title=\"Embedded Adjacency Matrix\",\n",
    "        yticklabs=[\"1\", \"51\", \"100\"], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-hampton",
   "metadata": {},
   "source": [
    "As it turns out, this example here is a good instance of another property of the latent position matrix. Remember that the latent position matrix is the matrix $X$ where $P = XX^\\top$. As it turns out, sometimes the columns of this matrix can get flipped around a little bit, through something called a rotation. You can see this by noticing that it looks a lot like the entries of the estimates of the latent position matrix are positive in one case are negative for the other, and it basically looks like the colorbar just got flipped around on us. The rotation doesn't really matter (yet!), and $P$ still is equal to $XX^\\top$, regardless of how that $X$ is rotated. We'll learn more about rotation matrices in the upcoming section on [Multiple Network Representation Learning](#link?) and in the section on [Two Sample Hypothesis Testing](#link?), but for now, all you need to know is that this \"flippage\" of what's big and small in the latent position matrix is not too important (again, yet!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-incentive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
