{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "choice-removal",
   "metadata": {},
   "source": [
    "# Two-sample hypothesis testing in SBMs\n",
    "\n",
    "In the previous section, we saw a situation where we had two networks which we thought could be effectively characterized with RDPG. Further, we knew that the communities were the same across both networks: that is, we knew that community $1$ in the first network was the same as community $1$ in the second network, so on and so forth all the way up to community $K$. In this situation, we found that we could test whether the latent positions for the underlying RDPGs are the same; that is, whether $H_0: X_1 = X_2R$ against $H_A: H_1 \\neq X_2R$. We called this the two-sample hypothesis test for RDPGs. \n",
    "\n",
    "What if we can take this a step further, however, and we can say that the networks are realizations of SBMs? How can we check whether the block matrices are the same? If you remember from [Chapter 5.4](#link?), we learned that SBMs are also RDPGs. What this means is that, since the SBM is an RDPG, the SBM also has a latent position matrix. Therefore, we could test whether the networks are the same by just using the [two-sample hypothesis test for the RDPG](ch8:twosample). The interpretation of rejecting, or failing to reject, the null hypothesis here was that the latent positions were the same/different across the two networks, and therefore, they share the same probability matrix. This is excellent news, so are we done?\n",
    "\n",
    "Not quite yet; as it turns out, when we think that the networks are realizations of SBMs, there are a lot more interesting questions we can ask about the probabilities that might arise. Specifically, we can deduce many useful ways in which two probability matrices for SBMs might be different, but *still* share similar characteristics.\n",
    "\n",
    "For this example, we will introduce a new scenario. We have two networks which summarize the traffic patterns between $n=100$ towns (represented by the nodes in our network) across $K=3$ states (represented by the communities in our network). The first 45 towns are in New York (the first community), the second 30 towns are in New Jersey (the second community), and the third 25 towns are in Pennsylvania (the third community). The community assignment vector $\\vec z$ looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ns = [45, 30, 25]  # number of students\n",
    "\n",
    "# z is a column vector indicating which state each\n",
    "# town is in\n",
    "z = np.array([1 for i in range(0, ns[0])] + \n",
    "              [2 for i in range(0, ns[1])] +\n",
    "              [3 for i in range(0, ns[2])] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-violation",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "def plot_tau(tau, title=\"\", xlab=\"Node\"):\n",
    "    cmap = matplotlib.colors.ListedColormap([\"blue\", 'red', 'green'])\n",
    "    fig, ax = plt.subplots(figsize=(10,2))\n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap((tau - 1).reshape((1,tau.shape[0])), cmap=cmap,\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_ticks([.35, 1, 1.65])\n",
    "        cbar.set_ticklabels(['1 - New York', '2 - New Jersey', '3 - Pennsylvania'])\n",
    "        ax.set(xlabel=xlab)\n",
    "        ax.set_xticks([.5,44.5, 74.5,99.5])\n",
    "        ax.set_xticklabels([\"1\", \"45\", \"75\", \"100\"])\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "plot_tau(z, title=\"$\\\\vec z$, Community Asssignment Vector\",\n",
    "         xlab=\"Town\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-wednesday",
   "metadata": {},
   "source": [
    "For a month, we measure the number of drivers who commute from one town to the other in a specified time window, and if more than $1,000$ drivers regularly make this commute, we add an edge between the pair of towns. In general, we know that people tend to commute more frequently within their state, so the probabilities that an edge exists between a pair of towns in the same state exceeds the probabilities that an edge exists between a pair of towns which are not in the same state. Now, here's the twist: we have measured the first network between 8 AM and 8 PM (covering the bulk of the work day), and the second network between 8 PM and 8 AM (covering the bulk of night time). We know that a lot of people in New Jersey tend to commute to new York for the work day, so we the probability of an edge existing between a New Jersey town and a New York town are higher during the day than the night. We don't think that driving patterns themselves really change *too* much otherwise, but we do think that the probability of an edge existing is about $50\\%$ higher during the daytime. for all pairs of communities in the network. The block matrices look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bnight = np.array([[.3, .15, .15], [.15, .3, .15], [.15, .15, .3]])\n",
    "Bday = Bnight*1.5  # day time block matarix is 50% more than night\n",
    "\n",
    "# people tend to commute from New Jersey to New York during the day\n",
    "Bday[0, 1] = .4; Bday[1,0] = .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_block(X, title=\"\", blockname=\"State\", blocktix=[0.5, 1.5, 2.5],\n",
    "               blocklabs=[\"1 - New York\", \"2 - New Jersey\", \"3 - Pennsylvania\"],\n",
    "               ax=None):\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=0, vmax=1, annot=True)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=blockname, xlabel=blockname)\n",
    "        ax.set_yticks(blocktix)\n",
    "        ax.set_yticklabels(blocklabs)\n",
    "        ax.set_xticks(blocktix)\n",
    "        ax.set_xticklabels(blocklabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_block(Bnight, title=\"Night Block Matrix\", ax=axs[0])\n",
    "plot_block(Bday, title=\"Day Block Matrix\", ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-classics",
   "metadata": {},
   "source": [
    "We then sample two networks with the above parameters, giving us the following two networks for the day and the night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspologic as gp\n",
    "from graspologic.simulations import sbm\n",
    "\n",
    "Aday = sbm(ns, Bday)\n",
    "Anight = sbm(ns, Bnight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "draw_multiplot(Anight, labels=list(z), title=\"Night Time Adjacency Matrix\");\n",
    "draw_multiplot(Aday, labels=list(z), title=\"Day Time Adjacency Matrix\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-admission",
   "metadata": {},
   "source": [
    "How can we ask whether the block matrices have similarities?\n",
    "\n",
    "## Testing whether the block matrices in an SBM are different\n",
    "\n",
    "Based on what we learned above, we know ahead of time that the block matrices for the SBMs are different. However, how can we actually test this? Well, let's start by being clear about what we mean by \"different\". To make this a little big more mathemattical, we'll introduce some new variables for the block matrices during the day time ($B^{(d)}$) and at night time ($B^{(n)}$) clearly. The block matrices are:\n",
    "\n",
    "\\begin{align*}\n",
    "    B^{(d)} &= \\begin{bmatrix}\n",
    "    b^{(d)}_{11} & b^{(d)}_{12} & b^{(d)}_{13} \\\\\n",
    "    b^{(d)}_{21} & b^{(d)}_{22} & b^{(d)}_{23} \\\\\n",
    "    b^{(d)}_{31} & b^{(d)}_{32} & b^{(d)}_{33}\n",
    "    \\end{bmatrix}; \\;\\;\\; B^{(n)} = \\begin{bmatrix}\n",
    "    b^{(n)}_{11} & b^{(n)}_{12} & b^{(n)}_{13} \\\\\n",
    "    b^{(n)}_{21} & b^{(n)}_{22} & b^{(n)}_{23} \\\\\n",
    "    b^{(n)}_{31} & b^{(n)}_{32} & b^{(n)}_{33}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "The hypothesis we want to test is the null hypothesis that the block matrices are the same, $H_0: B^{(d)} = B^{(n)}$, against the alternative hypothesis that the block matrices are different, $H_A: B^{(d)} \\neq B^{(n)}$. For a matrix, remember that two matrices are equal if all of the entries are identical, and two matrices are unequal if at least one of the entries are unequal. We can rerformulate the null and alternative hypotheses with this logic.\n",
    "\n",
    "For the null hypothesis, $H_0: B^{(d)} = B^{(n)}$, the statement is therefore equivalent to saying that for all pairs of communities $k$ and $l$, $b^{(d)}_{kl} = b^{(n)}_{kl}$. We will write each of these statements down as individual hypotheses for all pairs of communities, using the convention $H_{0, kl}: b_{kl}^{(d)} = b^{(n)}_{kl}$. The null hypothesis $H_0$ is therefore equivalent to saying that for every pair of communities $k$ and $l$, $H_{0,kl}$ is true. For the alternative hypothesis, $H_A: B^{(d)} \\neq B^{(n)}$, the statement is therefore equivalent to saying that for at least one pair of communities $k$ and $l$, $b^{(d)}_{kl} \\neq b^{(n)}_{kl}$. We will write down each of these statements as well as individual hypotheses for all pairs of communities, using the convention $H_{A, kl} : b_{kl}^{(d)} \\neq b^{(n)}_{kl}$. The alternative hypothesis $H_A$ is therefore equivalent to saying that for at least one pair of communities $k$ and $l$, that $H_{A,kl}$ is true.\n",
    "\n",
    "Now that we have broken a statement about two matrices down into numerous statements about two probabilities, we have almost completed our job. As it turns out, we have already seen the way we will test this, back in [testing for differences](ch7:testing)! Seeking to test whether a pair of block probabilities between communities $k$ and $l$ are the same, $H_{0,kl}: b_{kl}^{(d)} =  b_{kl}^{(n)}$, against whether the pair of block probabilities between communities $k$ and $l$ are different, $H_{A, kl}:  b_{kl}^{(d)} \\neq  b_{kl}^{(n)}$, is the [two-sample testing problem](ch7:testing:twosample)! How did we address this problem before?\n",
    "\n",
    "We used Fisher's exact test! Remember that with Fisher's exact test, for two probabilities that we want to compare, we construct the following contingency table for each pair of communities $k$ and $l$, with the entries:\n",
    "\n",
    "\n",
    "| | day | night |\n",
    "| --- | --- | --- |\n",
    "| Number of edges | $a$ | $b$ |\n",
    "| Number of non-edges | $c$ | $d$ |\n",
    "\n",
    "Where entry $a$ is the total number of edges between nodes of community $k$ with nodes of community $l$ in the daytime network, and $b$ is the total number of edges between nodes of community $k$ with nodes of community $l$ in the daytime network. The entry $c$ the total number of edges between nodes of community $k$ with nodes of community $l$ in the daytime network that do not exist (the number of adjacencies in cluster one with an adjacency of zero), and the entry $d$ is the total number of edges between nodes of community $k$ with nodes of community $l$ in the night time network that do not exist. We implement this using `numpy` and `scipy`, just like we did before, but this time for each pair of communities. To identify which adjacency matrix entries correspond to a given pair of communities, we use `np.outer`. Finally, we visualize the $p$-values of Fisher's exact test for each pair of communities using a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "K = 3\n",
    "Pvals = np.empty((K, K))\n",
    "# fill matrix with NaNs\n",
    "Pvals[:] = np.nan\n",
    "\n",
    "# get the indices of the upper triangle of Aday\n",
    "upper_tri_idx = np.triu_indices(Aday.shape[0], k=1)\n",
    "# create a boolean array that is nxn\n",
    "upper_tri_mask = np.zeros(Aday.shape, dtype=bool)\n",
    "# set indices which correspond to the upper triangle to True\n",
    "upper_tri_mask[upper_tri_idx] = True\n",
    "\n",
    "for k in range(0, K):\n",
    "    for l in range(k, K):\n",
    "        comm_mask = np.outer(z == (k+1), z == (l + 1))\n",
    "        table = [[Aday[comm_mask & upper_tri_mask].sum(), Anight[comm_mask & upper_tri_mask].sum()],\n",
    "                 [(Aday[comm_mask & upper_tri_mask] == 0).sum(), (Anight[comm_mask & upper_tri_mask] == 0).sum()]]\n",
    "        Pvals[k,l] = fisher_exact(table)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-middle",
   "metadata": {},
   "source": [
    "### Adjusting for multiple comparisons\n",
    "\n",
    "When we conducted our statistical tests above, you'll notice that we run into the same problem that we had before: since we ran a bunch of tests, we have increased the familywise error rate. This means that we need to take care to handle the fact that we executed multiple comparisons. Again, we use Bonferroni-Holm adjustment for the $p$-values to visualize which community pairings are significantly different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# use bonferroni correction on p-values calculated from previous step\n",
    "Pvals[~np.isnan(Pvals)] = multipletests(Pvals[~np.isnan(Pvals)], method='holm')[1]\n",
    "\n",
    "# remove nans and replace with zeros temporarily\n",
    "Pvals[np.isnan(Pvals)] = 0\n",
    "# symmetrize since the network is undirected\n",
    "Pvals = Pvals + Pvals.T + np.diag(np.diag(Pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(9, 6))\n",
    "plot_block(Pvals, ax=ax, title=\"$p$-value matrix after Bonferrroni-Holm correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-shipping",
   "metadata": {},
   "source": [
    "The $p$-values are all *still* almost all $\\alpha=0.05$ after multiple hypothesis correction, so therefore, it seems like we have evidence to reject the null hypothesis that the block matrices are the same. \n",
    "\n",
    "We still can't answer our question about the block matrices themselves, because we have performed $K \\times K$ tests for each pair of communities, but need a single answer about $H_0$ and $H_A$. To achieve this, we need to combine the $p$-values, which can be done by using Tippett's method for $p$-value combining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import combine_pvalues\n",
    "pval = combine_pvalues(Pvals.flatten(), method=\"tippett\")[0]\n",
    "print(\"p-value: {:3f}\".format(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-quality",
   "metadata": {},
   "source": [
    "The This can all be implemented using the [bilateral connectome package](#https://github.com/neurodata/bilateral-connectome) `stochastic_block_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.stats import stochastic_block_test\n",
    "\n",
    "stat, pvalue, misc = stochastic_block_test(\n",
    "    Aday, Anight, labels1=z, labels2=z, method=\"fisher\", correct_method=\"holm\", combine_method=\"tippett\"\n",
    ")\n",
    "print(\"p-value: {:.3f}\".format(pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-april",
   "metadata": {},
   "source": [
    "The interpretation of a $p$-value below $\\alpha = 0.05$ is that we have evidence to reject the null hypothesis in favor of the alternative hypothesis, that the block matrices are different for day and night times.\n",
    "\n",
    "## Testing whether the block matrices in an SBM are multiples of one another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-honey",
   "metadata": {},
   "source": [
    "Back in [Chapter 4](ch4:prop-net:density) we learned about a useful summary statistic, known as the *network density*. If you remember, the network density was the quantity:\n",
    "\\begin{align*}\n",
    "density(A) &= \\frac{2\\sum_{j > i}a_{ij}}{n(n - 1)}\n",
    "\\end{align*}\n",
    "Which could be thought of as the \"fraction\" of edges which actually exist in a network, divided by the fraction of edges which could exist in a network. As it turns out, the network density plays an extremely large role ion virtually every property of networks which we estimate in machine learning, including the block matrices. However, when we talk about two block matrices being different, we might be trying to describe something more than just that their network densities are different. \n",
    "\n",
    "For instance, in our example, we know that the probability of an edge existing between two cities is, in general, about $50\\%$ higher during the daytime compared to the night time. We don't want to have our answer just be a product of the fact that there were just more edges in the daytime network. Rather, we want to find the *topological* difference between the two networks; that is, that the day time driving patterns between New Jersey and New York towns went *above and beyond* the $50\\%$ increase we otherwise had. For this reason, we revamp our hypothesis a little bit.\n",
    "\n",
    "If you remember, our hypothesis that we ran above was the null hypothesis $H_0: B^{(day)} = B^{(night)}$ that the two block matrices are the same against the alternative $H_A: B^{(day)} \\neq B^{(night)}$ that the block matrices differ. We will change this up a little bit. Now, our null hypothesis becomes $H_0: B^{(day)} = a\\cdot B^{(night)}$ that the two block matrices are the same *up to a rescaling* against $H_A: B^{(day)} \\neq a\\cdot B^{(night)}$. How do we interpret this?\n",
    "\n",
    "In this case, the value $a$ is chosen to be the difference in the expected network densities between the day time and night time networks, the quantity $a = \\frac{p^{(day)}}{p^{(night)}}$. In practice, what we use is an estimate of this quantity, $\\hat a = \\frac{\\hat p^{(day)}}{\\hat p^{(night)}}$, where $\\hat p^{(day)}$ is the network density of the day time network and $\\hat p^{(night)}$ is the network density of the night time network. Accepting the alternative hypothesis here means that the block matrices of the day and night time network are not simply multiples of each other.\n",
    "\n",
    "We address this problem very similarly to above, instead using [Fisher's exact test for non-unity odds ratios](#https://en.wikipedia.org/wiki/Fisher%27s_noncentral_hypergeometric_distribution). We implement this using the [bilateral connectome](#https://github.com/neurodata/bilateral-connectome) package, again showing the p-value matrix to visualize wiuch combinations of communities are substantially different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.sum(ns)\n",
    "\n",
    "density_day = Aday[upper_tri_mask].sum()*2/(n*(n-1))\n",
    "density_night = Anight[upper_tri_mask].sum()*2/(n*(n-1))\n",
    "\n",
    "null_odds = density_day/density_night\n",
    "stat, pvalue, misc = stochastic_block_test(\n",
    "    Aday, Anight, labels1=z, labels2=z, method=\"fisher\", correct_method=\"holm\", combine_method=\"tippett\",\n",
    "    density_adjustment=null_odds\n",
    ")\n",
    "Pval_mtx = misc[\"uncorrected_pvalues\"]\n",
    "\n",
    "Bnight_adj = Bnight*null_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(27, 6))\n",
    "plot_block(Bnight, title=\"Night Block Matrix\", ax=axs[0])\n",
    "plot_block(Bday/null_odds, title=\"Adjusted Day Block Matrix\", ax=axs[1])\n",
    "plot_block(Pval_mtx, title=\"adjusted P-value matrix, p-value={:.3f}\".format(pvalue), ax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-thailand",
   "metadata": {},
   "source": [
    "This shows that after we adjust the block matrix for changes in network density, the difference in the block probability for traveling New York and New Jersey is still significant. This means that the density-adjusted block matrices are different, since the $p$-value is less than $\\alpha = 0.05$. The interpretation here is that, after adjusting for density, we can still reject the null hypothesis in favor of the alternative that the block matrices for day and night time are still different, and that this difference can be accounted for by different traffic patterns between New York and New Jersey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-corner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-column",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-michigan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-passport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-bullet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
