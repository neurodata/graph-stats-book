{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Significant Edges\n",
    "\n",
    "For the next two sections, we will turn back to an example we came across back when we discussed the [Signal Subnetwork model](#link?). If you recall, we had $200$ networks, which were either earthlings or astronauts. The networks each had $5$ nodes, which represented different lobes of the brain. The occipital lobe is responsible for sight, the temporal lobe with emotions and language, the parietal with hearing, the frontal with thinking and movement, and the insula with basic survivability skills. The astronauts were forced to live for several hundred thousand years on a planet in which their eyesight was challenged, and over time, evolution selected the astronauts who had a higher probability of connections between the occipital lobe and other areas of the brain. Our question of interest was as follows: if we are shown a network, how do we decide whether that network is from an earthling or an astronaut? Can we come up with a signal subnetwork classifier?\n",
    "\n",
    "To begin to address this question, we came up with the signal subnetwork model. What we established with the signal subnetwork model was that all of the edges in each network could be broken up into one of two groups: either the signal edges or the non-signal edges. We collected these signal edges into a set called the \"signal subnetwork\", which was a parameter for the signal subnetwork model. For these signal edges, the probability of an edge existing (or not) was different for two (or more) classes in our problem. What this means is that, in our problem above, the edges which had a node in the occipital lobe had a different connection probability for the astronauts than the humans. On the other hand, the non-signal edges did not have a different connection probability for the astronauts than the humans. \n",
    "\n",
    "To start this section off, let's first sample some example networks from the signal subnetwork model. Let's assume that we have a total of $200$ people, each of whom are either astronauts (with probability $\\pi_{ast} = 0.4$) or humans (with probability $\\pi_{earth} = 0.6$). First, we'll roll our 2-sided die $200$ times, where side 1 (class 1) corresponds to astronauts, and side 2 (class 2) corresponds to humans. We will then create the class assignment vector $\\vec y$ using the number of times the die lands on each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pi_ast = 0.45\n",
    "pi_hum = 0.55\n",
    "M = 200\n",
    "\n",
    "# roll a 2-sided die 200 times, with probability 0.4 of landing on side 1 (astronaut) \n",
    "# and 0.6 of landing on side 2 (earthling)\n",
    "np.random.seed(1234)\n",
    "classnames = [\"Astronaut\", \"Earthling\"]\n",
    "class_counts = np.random.multinomial(M, pvals=[pi_ast, pi_hum])\n",
    "print(\"Number of individuals who are astronauts: {:d}\".format(class_counts[0]))\n",
    "print(\"Number of individuals who are humans: {:d}\".format(class_counts[1]))\n",
    "\n",
    "# create class assignment vector, and randomly reshuffle class labels for each individual\n",
    "yvec = np.array([1 for i in range(0, class_counts[0])] + [2 for i in range(0, class_counts[1])])\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(yvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct the probability matrices for each class. The probabilities for edges in which a node is in the occipital lobe are higher for the astronauts than the humans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of nodes\n",
    "n = 5\n",
    "# edge probabilities for humans are random\n",
    "np.random.seed(12345)\n",
    "P_hum = np.random.beta(size=n*n, a=3, b=8).reshape(n, n)\n",
    "# networks are undirected, so make the probability matrix symmetric\n",
    "P_hum = (P_hum + P_hum.T)/2\n",
    "# networks are loopless, so remove the diagonal\n",
    "P_hum = P_hum - np.diag(np.diag(P_hum))\n",
    "\n",
    "# the names of each of the five nodes\n",
    "nodenames = [\"Occipital\", \"Frontal\", \"Temporal\", \"Frontal\", \"Insula\"]\n",
    "# the signal edges\n",
    "E = np.array([[0,1,1,1,1], [1,0,0,0,0], [1,0,0,0,0], [1,0,0,0,0], [1,0,0,0,0]], dtype=bool)\n",
    "P_ast = np.copy(P_hum)\n",
    "\n",
    "# probabilities for signal edges are higher in astronauts than humans\n",
    "# 2/3 root function biases them towards 1, which is higher than\n",
    "# whatever they are right now since they are between 0 and 1\n",
    "P_ast[E] = np.power(P_ast[E], 2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_prob(X, title=\"\", nodename=\"Brain Area\", nodetix=[0.5, 1.5, 2.5, 3.5, 4.5],\n",
    "             nodelabs=nodenames, ax=None, vmin=0, vmax=1):\n",
    "    if (ax is None):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=vmin, vmax=vmax, annot=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=nodename, xlabel=nodename)\n",
    "        if (nodetix is not None) and (nodelabs is not None):\n",
    "            ax.set_yticks(nodetix)\n",
    "            ax.set_yticklabels(nodelabs)\n",
    "            ax.set_xticks(nodetix)\n",
    "            ax.set_xticklabels(nodelabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(15, 12))\n",
    "plot_prob(P_hum, title=\"Earthling Probability Matrix\", ax=axs[0,0])\n",
    "plot_prob(P_ast, title=\"Astronaut Probability Matrix\", ax=axs[0,1])\n",
    "plot_prob(E, title=\"Signal Edges\", ax=axs[1,0])\n",
    "plot_prob(P_ast - P_hum, title=\"$P_{ast} - P_{earth}$\", ax=axs[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the class assignment vector $\\vec y$ to sample individual networks for each of the individuals. We plot an adjacency matrix for the first individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graphbook_code import ier\n",
    "\n",
    "# arrange the probability matrices in a list\n",
    "prob_mtxs = [P_ast, P_hum]\n",
    "# initialize empty list for adjacency matrices\n",
    "As = []\n",
    "# generate seeds for reproducibility\n",
    "seeds = np.random.randint(1, np.iinfo(np.int32).max, size=M)\n",
    "\n",
    "for i, y in enumerate(yvec):\n",
    "    # sample adjacency matrix for an individual of class y using the probability\n",
    "    # matrix for that class\n",
    "    np.random.seed(seeds[i])\n",
    "    As.append(ier(prob_mtxs[y - 1], directed=False, loops=False))\n",
    "\n",
    "# stack the adjacency matrices for each individual such that node i is first dimension,\n",
    "# node j is second dimension, and the individual index m is the third dimension\n",
    "As = np.stack(As, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graspologic.plot import heatmap\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 6));\n",
    "plot_prob(As[:,:,0], title=\"First individual, an {}\".format(classnames[yvec[0] - 1]), ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The key ideas we want to capture in our classifier are as follows:\n",
    "1. We want to *only* look at the edges which have a differing connection probability amongst our classes. This means that we only want to look at *signal edges* which are in the *signal subnetwork*, and we want to ignore edges which are not in the signal subnetwork. Edges which are not in the signal subnetwork are simply noise, since they have the same connection probability between the classes, and therefore are not useful for differentiating between the different classes.\n",
    "2. We want to incorporate the structure of the network into the classifier. This means we want to use the fact that nodes are nodes, and that edges are connections between nodes, to determine how to classify networks as earthling or astronaut.\n",
    "\n",
    "To begin, we will start by addressing aim 1. We need to find which edges are in the signal subnetework. Stated another way, we need to *estimate* the signal subnetwork. What this requires is a way to identify which edges will best capture the difference between the classes in our network. Do we know anything which can do this? As it turns out, we can use Fisher's exact test, something we learned about in the section on [testing for differences between groups of edges](http://docs.neurodata.io/graph-stats-book/applications/ch8/testing-differences.html#two-sample-hypothesis-testing-with-coins), to identify the signal subnetwork. In the next section on [identifying significant vertices](#link?), we will address the second point on using the structure of the network to further refine these potential signal edges.\n",
    "\n",
    "## Fisher's Exact Test for Edge Importance\n",
    "\n",
    "If you remember back to the coin flip example, our setting was as follows. We had two coins, coin $1$ and coin $2$, and we wanted to test whether their probabilities of landing on heads were different. Our hypotheses were $H_A: p_1 \\neq p_2$, against the null hypothesis that $H_0: p_1 = p_2$. We therefore had two random samples, the outcomes of ten coin flips from coin one and the outcomes of ten coin flips from coin two, meaning that our test of $H_A$ against $H_0$ fell into the *two-sample testing* regime. To address this, what we used was Fisher's exact test, where we counted the number of times each coin landed on heads and tails, which we aggregated in a table:\n",
    "\n",
    "| | First coin | Second coin |\n",
    "|---|---|---|\n",
    "| Landed on heads | 7 | 1 |\n",
    "| Landed on tails | 3 | 9 |\n",
    "\n",
    "As it turns out, we can adapt this test for our situation here too! For a single edge, what we want to do is test whether $H_A: p_{ij}^{(ast)} \\neq p_{ij}^{(earth)}$, against the null hypothesis that $H_0: p_{ij}^{(ast)} = p_{ij}^{(earth)}$. The key property of Fisher's exact test that makes it desirable for us is that, in a sense, when there is more evidence to support $H_A$, the $p$-value will tend to be smaller, whereas when there is more evidence to support $H_0$, the $p$-value will tend to be larger. We will exploit this feature in our design of a classifier for astronauts versus earthlings. For each edge $(i, j)$, we will construct the following table:\n",
    "\n",
    "| | Astronauts | Earthlings |\n",
    "| --- | --- | --- |\n",
    "| Edge $(i,j)$ exists | Number of astronauts with $(i,j)$ | Number of earthlings with edge $(i,j)$ |\n",
    "| Edge $(i,j)$ does not exist | Number of astronauts with edge $(i,j)$ | Number of earthlings without edge $(i,j)$ |\n",
    "\n",
    "Which we can do in python as follows, for the edge from the occipital lobe to the frontal lobe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1; j = 3  # (1,3) corresponds to the edge from occipital to frontal lobe\n",
    "\n",
    "ast_edge = As[i-1,j-1,yvec == 1].sum()  # count the number of astronauts with edge i,j\n",
    "hum_edge = As[i-1,j-1,yvec == 2].sum()  # count the number of earthlings with edge i,j\n",
    "ast_noedge = class_counts[1 - 1] - ast_edge  # count the number of astronauts without edge i,j\n",
    "hum_noedge = class_counts[2 - 1] - hum_edge  # count the number of earthlings without edge i,j\n",
    "\n",
    "edge_tab = np.array([[ast_edge, hum_edge], [ast_noedge, hum_noedge]])  # arrange as in table shown above\n",
    "\n",
    "print(edge_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the Fisher's exact test $p$-value, using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "test_statistic, pval = fisher_exact(edge_tab)\n",
    "print(\"p-value: {:.4f}\".format(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the edge $(i,j)$ that we chose above corresponded to the edge between the occipital and temporal lobe, we expect this edge to indicate a disparity between the astronauts and the earthlings. Why is this? Well, quite simply, by construction, this edge is a *signal* edge, which means that it carries real *signal* in differentiating a network from a human from a network of an alien. Why is this a big deal?\n",
    "\n",
    "Well, let's see what happens if we were to compute this for a non-signal edge. Let's arbitrarily choose the edge between the temporal and frontal lobes, which corresponds to $i=3$ (the temporal lobe) and $j = 2$ (the frontal lobe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3; j = 2  # (3,2) corresponds to the edge from temporal to frontal lobe\n",
    "\n",
    "ast_edge = As[i-1,j-1,yvec == 1].sum()  # count the number of astronauts with edge i,j\n",
    "hum_edge = As[i-1,j-1,yvec == 2].sum()  # count the number of earthlings with edge i,j\n",
    "ast_noedge = class_counts[1 - 1] - ast_edge  # count the number of astronauts without edge i,j\n",
    "hum_noedge = class_counts[2 - 1] - hum_edge  # count the number of earthlings without edge i,j\n",
    "\n",
    "edge_tab = np.array([[ast_edge, hum_edge], [ast_noedge, hum_noedge]])  # arrange as in table shown above\n",
    "\n",
    "test_statistic, pval = fisher_exact(edge_tab)\n",
    "print(\"p-value: {:.4f}\".format(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now this $p$-value happens to be a *lot* larger than the previous one we saw, now doesn't it? Is this just by chance? The answer is: no! For signal edges, the $p$-value will, by construction, *generally* be smaller than the $p$-value in a non-signal edge. By *generally*, we mean that it will *tend* to be smaller (but not always!). We could certainly get samples of data where this is not the case, analogous to the idea that we could flip a fair coin (with equal probability of seeing heads and tails) 10 times and obtain all 10 flips being heads. For this reason, we will use the Fisher's exact test statistic to quantify how \"important\" an edge is for differentiating the two classes, or as an edge importance statistic. We will exploit this edge importance statistic as we build up our classifier further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using edge importances to estimate the signal subnetwork\n",
    "\n",
    "We will use the Fisher's exact $p$-value to develop an approach which allows us to properly estimate a signal subnetwork model. The key idea is this: if we were to use any one edge of the network, we would not really be able to predict whether someone is an earthling or an alien. The reason for this is that edges either exist, or not, so unless one of the class edge probabilities for a particular edge happens to be really low (near zero) and the other happens to be really high (near one), the only informative decision boundary we could construct would be to assign one class to the networks where that particular edge exists, or the other class to the networks where that particular edge does not exist. Even less interestingly, we could abitrarily say that every network is in a particular class, which is also not going to be a particularly interesting classifier.\n",
    "\n",
    "Another thing we could do would be to use *every* edge in the network to develop a classifier, which also isn't the best we could do. This is because if we use every edge in the classifier, we would have a lot of *noise* from the non-signal edges. This means that even though we might learn *something* from the signal edges, anything we learn is going to end up being diluted down by noise because we are including a lot of uninformative information in our classifier (the non-signal edges).\n",
    "\n",
    "Rather, what we want to do is investigate to find the edges which are carrying all of the signal, and then isolate our downstream learning from information captured by those edges. This is because the signal edges carry all of the informative information about differentiating networks from one class to the other. This begs the question, how do we find the signal edges, and isolate them from the non-signal edges?\n",
    "\n",
    "As we mentioned in the preceding subsection, as it happens, the Fisher's exact test statistic is going to tend to be larger for edges in which there is actual signal (the edge probability is different for astronauts and earthlings), and smaller when there is no signal (the edge probability is the same for astronauts and earthlings). That is, if an edge is in the signal subnetwork, the Fisher's exact test $p$-value is small, and if it is not in the signal subnetwork, the Fisher's exact $p$-value is big. For this reason, what we end up doing is *rank transforming* the Fisher's exact test $p$-values, and then pick an arbitrary number of the lowest ranking $p$-values to retain for classification. \n",
    "\n",
    "We start by constructing a **significance matrix**, which is a collection of all of the Fisher's exact test $p$-values for each edge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_mtx = np.zeros((n,n))\n",
    "ast_idxs = yvec == 1  # the individuals who are astronauts\n",
    "hum_idxs = yvec == 2  # the individuals who are earthlings\n",
    "# since the networks are undirected, only need to compute for upper triangle\n",
    "for i in range(0, n):\n",
    "    for j in range(i+1, n):\n",
    "        ast_edgeij = As[i,j,ast_idxs].sum()\n",
    "        hum_edgeij = As[i,j,hum_idxs].sum()\n",
    "        table = np.array([[ast_edgeij, hum_edgeij],\n",
    "                          [class_counts[0] - ast_edgeij, class_counts[1] - hum_edgeij]])\n",
    "        fisher_mtx[i,j] = fisher_exact(table)[1]\n",
    "fisher_mtx = fisher_mtx + fisher_mtx.T  # symmetrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8, 6));\n",
    "plot_prob(fisher_mtx, title=\"Fisher's exact $p$-values\", vmax=fisher_mtx.max(), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the edges with the highest Fisher's exact test $p$-values in the significance matrix tend to be the edges with a node in the occipital lobe, which are our signal edges. This is great news! Next, we rank the $p$-values in the significance matrix, from largest (a rank of 1) to smallest (the number of elements which are possible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "sig_mtx = rankdata(np.max(fisher_mtx) - fisher_mtx).reshape(n,n)\n",
    "sig_mtx = sig_mtx - np.diag(np.diag(sig_mtx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8, 6));\n",
    "plot_prob(sig_mtx, title=\"Ranked significance matrix\", vmax=sig_mtx.max(), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ranks tend to be larger for edges with a node in the occipital lobe. Then, we select a number of edges (the size of the signal subnetwork) $K$, and retain the top $K$ edges, by their significance rankings. The top $K$ edges by significance are an estimate of the signal subnetwork, $\\hat{\\mathcal S}$.\n",
    "\n",
    "We can implement everything we've learned so far in `graspologic` relatively easily, using the `SignalSubgraph` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.subgraph import SignalSubgraph\n",
    "\n",
    "K = 8  # the number of edges in the subgraph\n",
    "sgest = SignalSubgraph()\n",
    "sgest.fit_transform(As, labels=yvec - 1, constraints=K);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we arrange this into a matrix so that we can look at the signal subnetwork we identified, and compare it to the true signal subnetwork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigsub = np.zeros((n,n))  # initialize empty matrix\n",
    "sigsub[sgest.sigsub_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(18, 6));\n",
    "plot_prob(E, title=\"True Signal Subnetwork\", ax=axs[0])\n",
    "plot_prob(sigsub, title=\"Estimated Signal Subnetwork\", ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our estimated signal subnetwork gets all of the edges correct except for the edge in the frontal lobe.\n",
    "\n",
    "A question you might be wondering concerns how exactly we chose the number of signal edges to include in our estimate of the signal subnetwork, $\\hat{\\mathcal S}$. If we knew the number of edges in the signal subnetwork ahead of time, the choice would be easy: just choose $K$ to be the number of edges in the signal subnetwork! In real data, however, this isn't really going to be the case. For this reason, we will have to estimate $K$ using $\\hat K$. To learn how to estimate $K$, we first need to learn how to put what we've covered so far into a classifier, so read on!\n",
    "\n",
    "## Building a classifier using the estimated signal subnetwork\n",
    "\n",
    "Finally, we can use the estimated signal subnetwork that we have constructed to devise a classifier. The objective of a classifier is to take new pieces of data (in this case, new networks), and assign them to the class which they are most likely from. How do we determine which class a network is most likely from?\n",
    "\n",
    "At the moment, we have our estimate of the signal subnetwork, $\\hat{\\mathcal S}$, and have a bunch of networks from one of two classes: astronaut (class 1) or alien (class 2). The edges of the network are binary, and we want to use the edges which are in the signal subnetwork to classify points as either astronaut or alien. A natural classifier choice for this situation is known as the Naive Bayes classifier. We have all the ingredients we need to construct the Naive Bayes classifier, so we'll try to put this together. Feel free to skip past this next section if you want to jump right into the implementation of the classifier. In this next section, we will explain some of the intuition of the Naive Bayes classifer, which might require a bit of a probability and statistics background.\n",
    "\n",
    "### Bayes Plugin Classifier (Statistical Intuition)\n",
    "\n",
    "The core idea of the Naive Bayes classifier is, if our data have features which are zeros and ones, we can use the *class-conditional probabilities* to determine whether a point is from class $1$ or class $2$. The idea is as follows. First, we assume that all of the individual features of our data (the features, in our case, are the edges of the network that are in the signal subnetwork) are independent, then the likelihood of observing a particular sequence of edges in the signal subnetwork of the $m^{th}$ network if that network is in class $y$ is as follows. First, we will just write down some simpler notation for the quantity we want:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb P(\\text{observing }A^{(m)}\\text{ given we assume }\\text{$m$ is in class $y$} \\text{ where the signal subnetwork is $\\mathcal S$}) = \\mathbb P(A^{(m)} | y_m = y; \\mathcal S)\n",
    "\\end{align*}\n",
    "\n",
    "Nothing happened in this step just yet; we just made a smaller notation for the quantity on the left that we will use later on. In this next step, we use the fact that, if edges existing and not existing are independent, then the probability of observing a sequence of edges is the product of the probabilities of observing each individual edge. This is called an *independent edge assumption*. We proceed as follows:\n",
    "\\begin{align*}\n",
    "    \\mathbb P(A^{(m)}| y_m = y; \\mathcal S) &= \\prod_{(i,j) \\in \\mathcal S} \\mathbb P(a_{ij}^{(m)} | y_m = y; \\mathcal S)\n",
    "\\end{align*}\n",
    "Note that we are taking the product of each pair of edges, $(i,j)$, which are in the signal sub-network. Next, if we remember back to the Independent-Edge Random Network (IER), we assumed that if $\\mathbf A^{(m)}$ was an $IER_n(P^y)$ network, that every edge $\\mathbf a_{ij}^{(m)}$ had a probability of $p^y_{ij}$ of taking a value of $1$ (the edge exists), and a probability of $1 - p_{ij}^y$ of taking a value of $0$ (the edge does not exist). What this means is that $\\mathbf a_{ij}^{(m)}$ is something called a **Bernoulli distributed random variable**. The probability of seeing a particular realization $a_{ij}^{(m)}$ of a Bernoulli distributed random variable is relatively straightforward to see. We want the quantity $\\mathbb P(A^{(m)} | y_m = y; \\mathcal S)$ to reflect the following two facts we've already discussed:\n",
    "1. If $a_{ij}^{(m)}$ is $1$, then $\\mathbb P(A^{(m)} | \\mathbf y_m = y; \\mathcal S)$ is $p_{ij}^y$.\n",
    "2. If $a_{ij}^{(m)}$ is $0$, then $\\mathbb P(A^{(m)} | \\mathbf y_m = y; \\mathcal S)$ is $1 - p_{ij}^y$.\n",
    "Is there a succinct expression that we can express this with? Yes! Try the following equation:\n",
    "\\begin{align*}\n",
    "    \\mathbb P(A^{(m)} | \\mathbf y_m = y; \\mathcal S) &= (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "Notice that if $a_{ij}^{(m)}$ is $1$, then $1 - a_{ij}^{(m)}$ is $0$. Therefore, $(1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}$ is just $1$, since any number raised to the $0$ power is $1$. On the other hand, $(p_{ij}^y)^{a_{ij}^{(m)}}$ is $p_{ij}^y$, since any number raised to the $1$ power is itself. Therefore, this expression fits the bill for us. So, we can simplify our expression as:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)} | y_m = y; \\mathcal S) &= \\prod_{(i,j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "And we're almost there! The next thing we're going to do is a little tricky, but fortunately we can turn to Bayes' Theorem for help. What we want to do is compute the probability of observing *both* $A^{(m)}$ *and* $y_m$ having the value $y$, not the probability of observing $A^{(m)}$ given that we assume that $y_m$ is $y$. In statistical notation, what this amounts to is:\n",
    "\\begin{align*}\n",
    "    \\mathbb P(\\text{observing }A^{(m)}\\text{ and }\\text{$m$ is in class $y$} \\text{ where the signal subnetwork is $\\mathcal S$}) = \\mathbb P(A^{(m)}, y_m = y; \\mathcal S)\n",
    "\\end{align*}\n",
    "This expression is a little confusing, but its interpretation is relatively straightforward: it is the probability that we see both things happening at the same time (the random network $\\mathbf A^{(m)}$ takes the value $A^{(m)}$ and the random class $\\mathbf y_m$ has the value $y$) rather than just assume that the random class $\\mathbf y_m$ already is $y$. We can compute this quantity by remembering [Bayes' Theorem](#link?):\n",
    "\\begin{align*}\n",
    "     \\mathbb P(A^{(m)} |  \\mathbb y_m = y; \\mathcal S) &= \\frac{\\mathbb P(A^{(m)}, \\mathbb y_m = y; \\mathcal S)}{\\mathbb P(\\mathbb y_m = y)}\n",
    "\\end{align*}\n",
    "Note that this implies the following:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)}, \\mathbb y_m = y; \\mathcal S) &= \\mathbb P(A^{(m)} | \\mathbb y_m = y; \\mathcal S)\\mathbb P(\\mathbb y_m = y)\n",
    "\\end{align*}\n",
    "Plugging in the value we obtained above:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)}, \\mathbb y_m = y; \\mathcal S) &= \\mathbb P(\\mathbb y_m = y)\\prod_{(i, j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "Remember that the parameter of the signal subnetwork model, $\\pi_y$, represented the probability of our $Y$-sided die landing on class $y$, and therefore was the probability that the random class $\\mathbf y_m$ took the value $y$. Therefore, $\\mathbb P(\\mathbb y_m = y) = \\pi_y$, since these two quantites represent the same thing! So:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)}, \\mathbb y_m = y; \\mathcal S) &= \\pi_y \\prod_{(i, j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we introduce a new quantity. This quantity is very similar to the above quantity we came across, but with some terms reversed:\n",
    "\\begin{align*}\n",
    "    \\mathbb P(\\text{$m$ is in class $y$}\\text{ given we observe }$A^{(m)}$ \\text{ where the signal subnetwork is $\\mathcal S$}) = \\mathbb P(\\mathbb y_m = y | A^{(m)}; \\mathcal S)\n",
    "\\end{align*}\n",
    "Remember that we saw that using Bayes Theorem, we can just rewrite this expression as:\n",
    "\\begin{align*}\n",
    "\\mathbb P(\\mathbb y_m = y | A^{(m)}; \\mathcal S) &= \\frac{\\mathbb P(A^{(m)}, \\mathbb y_m = y; \\mathcal S)}{\\mathbb P(A^{(m)}; \\mathcal S)}\n",
    "\\end{align*}\n",
    "But using the expression we just obtained for $\\mathbb P(A^{(m)}, \\mathbb y_m = y; \\mathcal S)$, we can write this down as:\n",
    "\\begin{align*}\n",
    "\\mathbb P(\\mathbb y_m = y | A^{(m)}; \\mathcal S) &= \\frac{\\pi_y \\prod_{(i, j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}}{\\mathbb P(A^{(m)}; \\mathcal S)}\n",
    "\\end{align*}\n",
    "So, what this quantity tells us is the probability that item $m$ is in class $y$, given that we observe that item $m$ has the network $A^{(m)}$ with signal subnetwork $\\mathcal S$! When we have a new piece of data, this is an excellent quantity to compute! The reason for this is that, if we see a new network and want to assign it to a class, we want to choose the class that we think is most reasonable, or most probable. Therefore, given a new network to classify, it is reasonable to just estimate the class of this new network to be the class which is most probable, which is:\n",
    "\\begin{align*}\n",
    "    \\hat y_m &= \\text{argmax}_{y \\in \\left\\{1, ..., Y\\right\\}}\\mathbb P(\\mathbb y_m = y | A^{(m)}; \\mathcal S) \\\\\n",
    "    &= \\text{argmax}_{y \\in \\left\\{1,..., Y\\right\\}}\\frac{\\pi_y \\prod_{(i, j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}}{\\mathbb P(A^{(m)}; \\mathcal S)}\n",
    "\\end{align*}\n",
    "What this states in words is, we check all possible values that $y$ could take (1, 2, 3, ... all the way to $Y$), and compute the probability that the class is $y$ given the network we observed. Then, we just choose which of the values was most plausible, and return it for our prediction $\\hat y_m$. But wait! We can simplify this expression even further. Note that the quantity $\\mathbb P(A^{(m)}; \\mathcal S)$ has *no* dependence on the particular class we are checking for a given value of $y$. This means that the denominator will be the same for all of the possible values of $y$, and therefore won't impact which of the $y$s is the actual maximum. Therefore, we can just drop this term entirely, giving us the quantity that will become the objective function for our classification task:\n",
    "\\begin{align*}\n",
    "\\hat y_m &= \\text{argmax}_{y \\in \\left\\{1,..., Y\\right\\}}\\pi_y \\prod_{(i, j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "Now what you might be wondering is, we don't know $\\mathcal S$, the vector $\\vec \\pi$, nor the probability matrices $P_1,..., P_Y$. This is why this is called a \"Bayes Plugin\" classifier. What we do is we estimate the signal subnetwork $\\mathcal S$ using the approach we outlined above to produce $\\hat{\\mathcal S}$, and then we use the class vector $\\vec y$ that we observed to produce estimates of $\\vec \\pi$, where:\n",
    "\\begin{align*}\n",
    "    \\hat{\\pi_y} &= \\frac{M_y}{M}\n",
    "\\end{align*}\n",
    "where $M$ is the total number of networks, and $M_y$ is the number of networks where $y_m = y$. Finally, we compute the estimated probability entries, using:\n",
    "\\begin{align*}\n",
    "    \\hat p_{ij}^y &= \\frac{1}{M_y} \\sum_{m : y_m = y} a_{ij}^{(m)}\n",
    "\\end{align*}\n",
    "What this means is that, for each edge $(i,j)$ which is in the estimated signal subnetwork $\\hat{\\mathcal S}$, we look at the $m$s where the class $y_m$ is $y$, and then we just compute the fraction of the edges which exist across all of the $M_y$ networks where $y_m = y$. Finally, we estimate the class for our new network $A^{(m)}$ by just \"plugging in\" these values to the objective function:\n",
    "\\begin{align*}\n",
    "    \\hat y_m &= \\text{argmax}_{y \\in \\left\\{1,..., Y\\right\\}}\\hat \\pi_y \\prod_{(i, j) \\in \\hat{\\mathcal S}} (\\hat p_{ij}^y)^{a_{ij}^{(m)}} (1 - \\hat p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "which is the objective function for the Bayes Plugin classifier.\n",
    "\n",
    "### Classification with Bayes Plugin Classifier\n",
    "\n",
    "So, how do we actually use the estimated signal subnetwork $\\hat{\\mathcal S}$ to classify new networks? Quite simply, we can just use sklearn's `BernoulliNB` from `sklearn`, which is the Naive Bayes classifier for data where the features take values of $0$ and $1$ (such as our network). We need to reorganize our data a little bit to get it into the format we want. Our estimated signal subnetwork $\\hat{\\mathcal S}$ is returned to us by `graspologic` in the format of a `[2 x K]` matrix, where $K$ is the number of edges in the signal subnetwork. The first row is the row index of the entry of the signal subnetwork, and the second row is the column index of the entry in the signal subnetwork. We need to coerce this into an `[M x K]` matrix which we will call $D$, where $M$ is the total number of networks, and $K$ is the number of edges in the signal subnetwork. Each entry of this matrix $d_{mk}$ represents the adjacency value of the $m^{th}$ individual for the $k^{th}$ element of the signal subnetwork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = As[sgest.sigsub_[0], sgest.sigsub_[1],:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a Naive Bayes classifier, and fit the classifier using the class vector $\\vec y$ for all of our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "# fit the classifier using the vector of classes for each sample\n",
    "classifier.fit(data, yvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it does! We create $50$ new networks, which are astronauts or earthlings, and assess the performance using the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mtest = 100\n",
    "\n",
    "# roll a 2-sided die 50 times, with probability 0.4 of landing on side 1 (astronaut) \n",
    "# and 0.6 of landing on side 2 (earthling)\n",
    "np.random.seed(12345678)\n",
    "class_countstest = np.random.multinomial(Mtest, pvals=[pi_ast, pi_hum])\n",
    "print(\"Number of individuals who are astronauts: {:d}\".format(class_countstest[0]))\n",
    "print(\"Number of individuals who are humans: {:d}\".format(class_countstest[1]))\n",
    "\n",
    "# create class assignment vector, and randomly reshuffle class labels for each individual\n",
    "yvectest = np.array([1 for i in range(0, class_countstest[0])] + [2 for i in range(0, class_countstest[1])])\n",
    "np.random.seed(12345678)\n",
    "np.random.shuffle(yvectest)\n",
    "\n",
    "Astest = []\n",
    "np.random.seed(1234)\n",
    "seeds = np.random.randint(1, np.iinfo(np.int32).max, size=Mtest)\n",
    "for y in yvectest:\n",
    "    # sample adjacency matrix for an individual of class y using the probability\n",
    "    # matrix for that class\n",
    "    Astest.append(ier(prob_mtxs[y - 1], directed=False, loops=False))\n",
    "\n",
    "# stack the adjacency matrices for each individual such that node i is first dimension,\n",
    "# node j is second dimension, and the individual index m is the third dimension\n",
    "Astest = np.stack(Astest, axis=2)\n",
    "\n",
    "datatest = Astest[sgest.sigsub_[0], sgest.sigsub_[1],:].T\n",
    "\n",
    "predictionstest = classifier.predict(datatest)\n",
    "\n",
    "# classifier accuracy is the fraction of predictions that are correct\n",
    "acc = np.mean(predictionstest == yvectest)\n",
    "print(\"Classifier Accuracy: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means our classifier is right about $60\\%$ of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of number of edges in signal subnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember above, we had a core question which was; how do we decide which number of edges to include in the estimated signal subnetwork?\n",
    "\n",
    "Similar to other machine learning techniques, we use cross validation. **Cross Validation** is a procedure in which we split the dataset into a number of apprroximately equally sized splits (called *folds*), and then we train a machine learning model using a subset of the folds (the *in-sample* folds) and and test the trained model on the excluded subset of the folds (the *out-of-sample* folds). \n",
    "\n",
    "So, how do we proceed for our dataset above? We begin by splitting the data into $20$ folds, and then use $20$-fold cross validation to determine the accuracy of the resulting classifier when we include between $4$ and $20$ edges in the estimated signal subnetwork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "kf = KFold(n_splits=20, random_state=None)\n",
    "\n",
    "xv_res = []\n",
    "for k, (train_index, test_index) in enumerate(kf.split(range(0, M))):\n",
    "    X_train, X_test = As[:,:,train_index], As[:,:,test_index]\n",
    "    y_train = yvec[train_index]; y_test = yvec[test_index]\n",
    "    \n",
    "    for sgsz in np.arange(4, 21, step=2):\n",
    "        # estimate ssg\n",
    "        sgest = SignalSubgraph()\n",
    "        sgest.fit_transform(X_train, labels=y_train - 1, constraints=int(sgsz));\n",
    "        # train classifier\n",
    "        data = X_train[sgest.sigsub_[0], sgest.sigsub_[1],:].T\n",
    "        classifier = BernoulliNB()\n",
    "        # fit the classifier using the vector of classes for each sample\n",
    "        classifier.fit(data, y_train)\n",
    "        \n",
    "        datatest = X_test[sgest.sigsub_[0], sgest.sigsub_[1],:].T\n",
    "        # evaluate performance on test fold        \n",
    "        predictionstest = classifier.predict(datatest)\n",
    "\n",
    "        # classifier accuracy is the fraction of predictions that are correct\n",
    "        acc = np.mean(predictionstest == y_test)\n",
    "        xv_res.append({\"Fold\": k+1, \"K\": sgsz, \"Accuracy\": acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_res = pd.DataFrame(xv_res)\n",
    "\n",
    "xv_acc = df_res.groupby(['K']).agg({\"Accuracy\": \"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "sns.lineplot(x=\"K\", y=\"Accuracy\", data=xv_acc, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Number of Edges\")\n",
    "ax.set_ylabel(\"Average XV Accuracy\")\n",
    "ax.set_title(\"Determining optimal number of edges in signal subnetwork\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of edges with the highest cross-validated accuracy is to include $10$ edges in the signal subnetwork. When we produce an estimate of the signal subnetwork with $10$ edges, we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kopt = 12\n",
    "\n",
    "sgest = SignalSubgraph()\n",
    "sgest.fit_transform(As, labels=yvec - 1, constraints=Kopt);\n",
    "\n",
    "sigsub = np.zeros((n,n))  # initialize empty matrix\n",
    "sigsub[sgest.sigsub_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(18, 6));\n",
    "plot_prob(E, title=\"True Signal Subnetwork\", ax=axs[0])\n",
    "plot_prob(sigsub, title=\"Estimated Signal Subnetwork, $\\hat K = 10$\", ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we are now adding two additional edges which carry no signal. Can we be a little more intelligent with how we identify signal edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
