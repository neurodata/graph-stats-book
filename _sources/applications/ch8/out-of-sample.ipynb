{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a citation network of scholars publishing papers. The nodes are the scholars, and an edge exists in a given pair of scholars if they're published a paper together.\n",
    "\n",
    "You've already found a representation using ASE or LSE and you have a set of latent positions, which you then clustered to figure out who came from which university. It took a long time for you to get this representation - there are a lot of people doing research out there!\n",
    "\n",
    "Now, suppose a new graduate student publishes a paper. Your network gets bigger by a single node, and you'd like to find this person's latent position (thus adding them to the clustering system). To do that, however, you'd have to get an entirely new representation for the network. Re-embedding the entire network with the new node added seems like it should be unecessary - after all, you already know the latent positions for every other node.\n",
    "\n",
    "This section is all about this problem: how to find the representation for new nodes without the computationally expensive task of re-embedding an entire network. As it turns out, there has been some work done, and there is a solution that can get you pretty close the latent position for the new node that you would have had. For more details and formaility, see the 2013 paper \"Out-of-sample extension for latent position graphs\", by Tang et al (although, as with most science, the theory in this paper was built on top of other work from related fields).\n",
    "\n",
    "Let's make a network from an SBM, and an additional node that should belong to the first community. Then, we'll embed the network and explore how to find the latent position for the additional node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Network and an Out-of-Sample Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graspologic.simulations import sbm\n",
    "from graspologic.utils import remove_vertices\n",
    "\n",
    "# Generate parameters\n",
    "B = np.array([[0.8, 0.2],\n",
    "              [0.2, 0.8]])\n",
    "\n",
    "# Generate a network along with community memberships\n",
    "network, labels = sbm(n=[101, 100], p=B, return_labels=True)\n",
    "labels = list(labels)\n",
    "\n",
    "# Grab out-of-sample vertex\n",
    "oos_idx = 0\n",
    "oos_label = labels.pop(oos_idx)\n",
    "A, a = remove_vertices(network, indices=oos_idx, return_removed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now is a network and an additional node. You can see the adjacency matrix for the network below, along with the adjacency vector for the additional node (Here, an “adjacency vector”  is a vector with a 1 in every position that the out-of-sample node has an edge with an in-sample node). The heatmap on the right is a network with two communities, with 100 nodes in each community. The vector on the right is purple on row $i$ if the $i^{th}$ in-sample node is connected to the out-of-sample node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from graphbook_code import heatmap\n",
    "from graphbook_code.plotting import cmaps\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "# heatmap\n",
    "heatmap(A, ax=ax, cbar=False)\n",
    "\n",
    "# adjacency vector\n",
    "ax = fig.add_axes([.85, 0, .1, 1])\n",
    "cmap = cmaps[\"sequential\"]\n",
    "plot = sns.heatmap(a[:, np.newaxis], ax=ax, cbar=False, cmap=cmap, xticklabels=False, yticklabels=20)\n",
    "plt.tick_params(axis='y', labelsize=10, labelleft=False, labelright=True)\n",
    "plt.ylabel(\"in-sample node index\")\n",
    "plot.yaxis.set_label_position(\"right\")\n",
    "\n",
    "# title\n",
    "fig.suptitle(\"Adjacency matrix (left) and vector for additional \\nnode (right)\", y=1.1, fontsize=16, x=.19, ha=\"left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After embedding with ASE, we have an embedding for the original network. The rows of this embedding contain the latent position for each original node. We'll call the embedding X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.embed import AdjacencySpectralEmbed as ASE\n",
    "\n",
    "ase = ASE(n_components=2)\n",
    "ase.fit(A)\n",
    "X = ase.transform(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import plot_latents\n",
    "plot_latents(X, title=\"Latent positions for our original network (X)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Latent Position Matrix Can be used to Estimate Probability Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything up until now has just been pretty standard stuff. We still haven't done anything with our new node - all we have is a big vector that tells us which other nodes it's connected to, and our standard matrix of latent positions. However, it's time for a bit more exploration into the nature of the latent position matrix $X$, and what happens when you view it as a linear transformation. This will get us closer to understanding the out-of-sample embedding.\n",
    "\n",
    "Remember from the section on latent positions that $X$ can be used to estimate the block probability matrix. When you use ASE on a single network to make $X$, $XX^\\top$ estimates $P$: meaning, $XX^\\top_{ij}$, the element on the $i^{(th)}$ row and $j^{(th)}$ column of $XX^\\top$, will estimate the probability that node $i$ has an edge with node $j$.\n",
    "\n",
    "Let's take a single latent position vector out of $X$ - call it $v_i ^\\top$, since we're taking a row vector $v_i$ and transposing. What'll $X v_i^\\top$ look like? Well, that's essentially the same thing as grabbing the $i_{th}$ column of $XX^\\top$. Meaning, $X v_i^\\top$ will be a single vector whose $j^{(th)}$ element estimates the probability that node $i$ will connect to node $j$.\n",
    "\n",
    "You can see this in action below. We took the latent position corresponding to the first node out of the latent position matrix (and called it $v_0$), and then multiplied it by the latent position matrix itself. What emerged is what you see below: a vector that shows the estimated probability that node 0 has an edge with each other node in the network. The true probabilities for the first half of nodes (the ones in the same community) should be .8, and the true probabilities for the second half of nodes in the other community should be .2. The average values were .775 and .149 - so, pretty close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_0 = X[0, :]\n",
    "v_est_proba = X @ v_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 10))\n",
    "sns.heatmap(v_est_proba[:, np.newaxis], cbar=False, cmap=cmap, xticklabels=False, yticklabels=20, ax=ax)\n",
    "ax.text(1.1, 70, s=f\"average value: {v_est_proba[:100].mean():.3f}\", rotation=90)\n",
    "ax.text(1.1, 170, s=f\"average value: {v_est_proba[100:].mean():.3f}\", rotation=90)\n",
    "ax.set_ylabel(\"Node index\")\n",
    "\n",
    "plt.title(\"Estimated probability\\n vector\" + r\" for first node $X v_0^\\top$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going in the Other Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know now that we can use the latent position matrix $X$ to get a set of estimated edge probabilities for every node in a network. Our goal is to go the other direction - to start with \"estimated edge probabilities\", and end with a latent position.\n",
    "\n",
    "First, let's think about the term \"estimated edge probabilities\" for a second. In an ideal world, this would be a vector associated to node $i$ with $n$ elements, where the $j_{th}$ element of the vector contains the probability that node $i$ will connect to node $j$. Adjacency matrices, however, contain only adjacency vectors of 0's and 1's - 0 if there isn't an edge, 1 if there is an edge.\n",
    "\n",
    "If you think about it, however, you can think of a big adjacency vector full of 0's and 1's as kind of an estimate for edge probabilities. Say you sample a node's adjacency vector from an RDPG, then you sample again, and again, averaging your samples will get you closer and closer to the connection probabilities. So you can think of a single adjacency vector as an estimate for the edge probabilities - it's just that the sample size is 1.\n",
    "\n",
    "The point here is that if you can start with a latent position and then estimate the edge probabilities, it's somewhat equivalent (albeit going in the other direction) to start with the adjacency vector for a node and estimate the latent position.\n",
    "\n",
    "Let's call the estimated probability vector $a_i$. We know that $\\hat{a_i} = \\hat{X} \\hat{v_i}^\\top$: you multiply the latent position matrix by the $i_{th}$ latent position to estimate $a_i$ (remember that the ^ hats above letters means we're getting an estimate for something, rather than getting the thing itself). How do we isolate $v_i$?\n",
    "\n",
    "Well, if $X$ were invertible, we could do $\\hat{X}^{-1} \\hat{a_i} = \\hat{v_i}^\\top$: just invert both sides of the equation to get $v_i$ by itself. Unfortunately, however, $X$ will almost never be invertible. We'll have to do the next-best thing, which is to use the *pseudoinverse*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pseudoinverse Gives Us our Best Guess For Inverting a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Moore-Penrose Pseudoinverse is useful to know about for anybody interested in data science. Say you have a matrix which isn't invertible. Call it $T$.\n",
    "\n",
    "The pseudoinverse $T^+$ is the closest approximation you can get to $T^{-1}$. This is best understood visually. Let's take $T$ to be a matrix which projects points on the x-y coordinate axis down to the x-axis, then flips them to their negative on the number line. The matrix would look like this:\n",
    "\n",
    "\\begin{align*}\n",
    "    T &=\n",
    "    \\begin{bmatrix}\n",
    "    -1 & 0 \\\\\n",
    "    0 & 0  \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Some information is inherently lost here. Because the second column is all zeroes, any information in the y-axis can't be recovered. For instance, say we have some vectors with different x-axis and y-axis coordinates:\n",
    "\\begin{align*}\n",
    "    v_1 &= \\begin{bmatrix} 1 & 1 \\end{bmatrix}^\\top \\\\\n",
    "    v_2 &= \\begin{bmatrix} 2 & 2 \\end{bmatrix}^\\top\n",
    "\\end{align*}\n",
    "\n",
    "When we use $T$ as a linear transformation to act on $v_1$ and $v_2$, the y-axis coordinates both collapse to the same thing (0, in this case). Information in the x-axis, however, is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np                 # v 1.19.2\n",
    "import matplotlib.pyplot as plt    # v 3.3.2\n",
    "from graphbook_code import draw_cartesian\n",
    "\n",
    "\n",
    "# make axis\n",
    "ax = draw_cartesian()\n",
    "\n",
    "# Enter x and y coordinates of points and colors\n",
    "xs = [1, 2]\n",
    "ys = [1, 2]\n",
    "colors = ['g', 'r']\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(xs, ys, c=colors)\n",
    "\n",
    "# Draw lines connecting points to axes\n",
    "for x, y, c in zip(xs, ys, colors):\n",
    "    ax.plot([x, x], [0, y], c=c, ls='--', lw=1.5, alpha=0.5)\n",
    "    ax.plot([x, -x], [0, 0], c=c, ls='--', lw=1.5, alpha=0.5)\n",
    "\n",
    "\n",
    "# Draw arrows\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)\n",
    "\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "ax.plot((-1), (0), marker='<', **arrow_fmt)\n",
    "ax.plot((-2), (0), marker='<', **arrow_fmt)\n",
    "\n",
    "# Draw text\n",
    "ax.text(x=.9, y=1.2, s=\"$v_1$ (1, 1)\", fontdict=dict(c=\"green\"))\n",
    "ax.text(x=2.2, y=1.9, s=\"$v_2$ (2, 2)\", fontdict=dict(c=\"red\"))\n",
    "\n",
    "ax.text(x=-1.2, y=-.3, s=\"$T v_1$ (-1, 0)\", fontdict=dict(c=\"green\"))\n",
    "ax.text(x=-2.2, y=-.6, s=\"$T v_2$ (-2, 0)\", fontdict=dict(c=\"red\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to reverse $T$ and bring $Tv_1$ and $Tv_2$ back to $v_1$ and $v_2$. Unfortunately, since both $v_1$ and $v_2$ get squished onto zero in the y-axis position after getting passed through $T$, we've lost all information about what was happening on the y-axis -- that's a lost cause. So it's impossible to get perfectly back to $v_1$ or $v_2$.\n",
    "\n",
    "If you restrict your attention to the x-axis, however, you'll see that $Tv_1$ and $Tv_2$ landed in different places ($v_1$ went to -1, and $v_2$ went to -2). You can use this information about the x-axis location of $Tv_1$ and $Tv_2$ to re-orient the x-axis values back to where they were prior to the vectors getting passed through X, even if it's impossible to figure out where the y-values were.\n",
    "\n",
    "That's what the pseudoinverse does: it reverses what it can, and accepts lost information as a lost cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np                 # v 1.19.2\n",
    "import matplotlib.pyplot as plt    # v 3.3.2\n",
    "\n",
    "# make axis\n",
    "ax = draw_cartesian()\n",
    "\n",
    "# Enter x and y coordinates of points and colors\n",
    "xs = [1, 2]\n",
    "ys = [1, 2]\n",
    "xs_out = [1, 2]\n",
    "ys_out = [0, 0]\n",
    "colors = ['g', 'r']\n",
    "\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(xs, ys, c=colors)\n",
    "ax.scatter(xs_out, ys_out, c=colors)\n",
    "\n",
    "# Draw lines connecting points to axes\n",
    "for x, y, c in zip(xs, ys, colors):\n",
    "    ax.plot([x, x], [0, y], c=c, ls='--', lw=1.5, alpha=0.5)\n",
    "\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "\n",
    "# Draw text\n",
    "ax.text(x=.9, y=1.2, s=\"$v_1$ (1, 1)\", fontdict=dict(c=\"green\"))\n",
    "ax.text(x=2.2, y=1.9, s=\"$v_2$ (2, 2)\", fontdict=dict(c=\"red\"))\n",
    "\n",
    "ax.text(x=.2, y=-.4, s=\"$T^+ (X v_1$)\", fontdict=dict(c=\"green\"))\n",
    "ax.text(x=1.8, y=-.4, s=\"$T^+ (X v_2$)\", fontdict=dict(c=\"red\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Pseudoinverse to Estimate out-of-sample Latent Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how this stuff applies to latent position matrices.\n",
    "\n",
    "Say you have a nonsquare latent position matrix $X$. Like we learned before, we can estimate the probability vector $a_i$ (the vector with its probability of connecting with node $j$ in the $j_{th}$ position) for a node by passing its latent position ($v_i$) through the latent position matrix.\n",
    "\\begin{align*}\n",
    "\\text{estimated probability vector } a_i = X v_i^\\top\n",
    "\\end{align*}\n",
    "\n",
    "You can think of $X$ as a matrix the same way you thought of $T$: right now, it's a linear transformation that eats a vector and doesn't necessarily preserve all the information about that vector when it outputs something (In this case, since $X$ brings lower-dimensional latent positions to higher-dimensional adjacency vectors, what's happening is more of a restriction on which high-dimensional vectors you can access than a loss of information, but that's not particularly important).\n",
    "\n",
    "The pseudoinverse, $\\hat{X}^+$, is the best we can do to bring a higher-dimensional adjacency vector to a lower-dimensional latent position. In practice, the best we can do generally turns out to be a pretty good guess, and so we can get a decent estimation of the latent position $v_i^\\top$.\n",
    "\n",
    "\\begin{align*}\n",
    "X^+ a_i \\approx X^+ X v_i^\\top \\approx v_i^\\top\n",
    "\\end{align*}\n",
    "\n",
    "Let's see it in action. Remember that we already grabbed our out-of-sample latent position and called it `a`. We use numpy's pseudoinverse function to generate the pseudoinverse of the latent position matrix. Finally, we use it to get `a`'s estimated latent position, and call it `v`. You can see the location of this estimate in Euclidean space below: it falls squarely into the first community, which is where it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "\n",
    "# Make the pseudoinverse of the latent position matrix\n",
    "X_pinverse = pinv(X)\n",
    "\n",
    "# Get its estimated latent position\n",
    "v = X_pinverse @ a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# setup\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "gs = fig.add_gridspec(3, 4)\n",
    "\n",
    "# adjacency vector\n",
    "ax = fig.add_subplot(gs[:, 0])\n",
    "sns.heatmap(a[:, np.newaxis], cbar=False, cmap=cmap, xticklabels=False, yticklabels=20, ax=ax)\n",
    "ax.text(1.1, 70, s=f\"average value: {a[:100].mean():.3f}\", rotation=90, c=\"blue\")\n",
    "ax.text(1.1, 170, s=f\"average value: {a[100:].mean():.3f}\", rotation=90, c=\"orange\")\n",
    "ax.set_ylabel(\"Node index\")\n",
    "ax.set_title(r\"Adjacency vector for\" + \"\\n\" + \" the first node $a$\");\n",
    "\n",
    "# latent position plot\n",
    "ax = fig.add_subplot(gs[:, 1:])\n",
    "plot = plot_latents(X, ax=ax, labels=labels, title=\"Latent positions with out-of-sample estimate\")\n",
    "plot.scatter(x=v[0], y=v[1], marker='*', s=300, edgecolor=\"black\")\n",
    "plot.annotate(r\"Estimated latent position for\" + \"\\n\" + \" the first adjacency vector: $X^+ a$\", xy=(v[0]+.002, v[1]+.008), \n",
    "            xytext=(v[0]-.02, v[1]+.2), arrowprops={\"arrowstyle\": \"->\", \"color\": \"k\"})\n",
    "sns.move_legend(ax, \"center right\")\n",
    "fig.subplots_adjust(wspace=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Graspologic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you don't have to do all of this manually. Below we generate an adjacency matrix $A$ from an SBM, as well as the adjacency vector for an out-of-sample node $a_0$. Once we fit an instance of the ASE class, the latent position for any new nodes can be predicted by simply calling `ase.transform` on the new adjacency vectors. \n",
    "\n",
    "You can do the same thing with multiple adjacency vectors if you want by stacking them on top of each other in a numpy array, then transforming the whole stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.embed import AdjacencySpectralEmbed as ASE\n",
    "\n",
    "# Generate parameters\n",
    "B = np.array([[0.8, 0.2],\n",
    "              [0.2, 0.8]])\n",
    "\n",
    "# Generate a network along with community memberships\n",
    "network, labels = sbm(n=[101, 100], p=B, return_labels=True)\n",
    "labels = list(labels)\n",
    "\n",
    "# Grab out-of-sample vertex\n",
    "oos_idx = 0\n",
    "oos_label = labels.pop(oos_idx)\n",
    "A, a_0 = remove_vertices(network, indices=oos_idx, return_removed=True)\n",
    "\n",
    "# Make an ASE model\n",
    "ase = ASE(n_components=2)\n",
    "X = ase.fit_transform(A)\n",
    "\n",
    "# Predict out-of-sample latent positions by transforming\n",
    "v_0 = ase.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "# latent position plot\n",
    "plot = plot_latents(X, ax=ax, labels=labels, title=\"Latent positions with out-of-sample estimate\")\n",
    "plot.scatter(x=v_0[0], y=v_0[1], marker='*', s=300, edgecolor=\"black\")\n",
    "plot.annotate(r\"Estimated latent position for\" + \"\\n\" + \" the first adjacency vector: $X^+ a_0$\", xy=(v_0[0]+.002, v_0[1]+.008), \n",
    "            xytext=(v_0[0]-.1, v_0[1]-.6), arrowprops={\"arrowstyle\": \"->\", \"color\": \"k\"})\n",
    "sns.move_legend(ax, \"center right\")\n",
    "fig.subplots_adjust(wspace=.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
