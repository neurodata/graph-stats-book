{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "practical-finland",
   "metadata": {},
   "source": [
    "# Differences in Block Matrices\n",
    "\n",
    "In the previous section, we saw a situation where we had two networks which we thought could be effectively characterized with RDPG. Further, we knew that the nodes were the same across both networks: that is, we knew that node $1$ in the first network was the same as node $1$ in the second network, so on and so forth all the way up to node $n$. In this situation, we found that we could test whether the latent positions for the underlying RDPGs are the same; that is, whether $H_0: X_1 = X_2R$ against $H_A: H_1 \\neq X_2R$. We called this the two-sample hypothesis test for RDPGs. \n",
    "\n",
    "What if we can take this a step further, however, and we can say that the networks are realizations of SBMs? How can we check whether the block matrices are the same? If you remember from [Chapter 5.4](#link?), we learned that SBMs are just a \"special case\" of the RDPG. What this means is that, since the SBM is an RDPG, the SBM also has a latent position matrix. Therefore, we could test whether the networks are the same by just using the two-sample hypothesis test for the RDPG. The interpretation of rejecting/accepting the alternative hypothesis here was that the latent positions were the same/different across the two networks, and therefore, they share the same probability matrix. This is excellent news, so are we done?\n",
    "\n",
    "Not quite yet; as it turns out, when we think that the networks are realizations of SBMs, there are a lot more interesting questions we can ask about the probabilities that might arise. Specifically, we can deduce many useful ways in which two probability matrices for SBMs might be different, but *still* share similar characteristics.\n",
    "\n",
    "For this example, we will introduce a new scenario. We have two networks which summarize the traffic patterns between $n=100$ towns (represented by the nodes in our network) across $K=3$ states (represented by the communities in our network). The first 45 towns are in New York (the first community), the second 30 towns are in New Jersey (the second community), and the third 25 towns are in Pennsylvania (the third community). The community assignment vector $\\vec z$ looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ns = [45, 30, 25]  # number of students\n",
    "\n",
    "# z is a column vector indicating which state each\n",
    "# town is in\n",
    "z = np.array([1 for i in range(0, ns[0])] + \n",
    "              [2 for i in range(0, ns[1])] +\n",
    "              [3 for i in range(0, ns[2])] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-village",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "def plot_tau(tau, title=\"\", xlab=\"Node\"):\n",
    "    cmap = matplotlib.colors.ListedColormap([\"blue\", 'red', 'green'])\n",
    "    fig, ax = plt.subplots(figsize=(10,2))\n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap((tau - 1).reshape((1,tau.shape[0])), cmap=cmap,\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_ticks([.35, 1, 1.65])\n",
    "        cbar.set_ticklabels(['1 - New York', '2 - New Jersey', '3 - Pennsylvania'])\n",
    "        ax.set(xlabel=xlab)\n",
    "        ax.set_xticks([.5,44.5, 74.5,99.5])\n",
    "        ax.set_xticklabels([\"1\", \"45\", \"75\", \"100\"])\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "plot_tau(z, title=\"$\\\\vec z$, Community Asssignment Vector\",\n",
    "         xlab=\"Town\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-bargain",
   "metadata": {},
   "source": [
    "For a month, we measure the number of drivers who commute from one town to the other in a specified time window, and if more than $1,000$ drivers regularly make this commute, we add an edge between the pair of towns. In general, we know that people tend to commute more frequently within their state, so the probabilities that an edge exists between a pair of towns in the same state exceeds the probabilities that an edge exists between a pair of towns which are not in the same state. Now, here's the twist: we have measured the first network between 8 AM and 8 PM (covering the bulk of the work day), and the second network between 8 PM and 8 AM (covering the bulk of night time). We know that a lot of people in New Jersey tend to commute to new York for the work day, so we the probability of an edge existing between a New Jersey town and a New York town are higher during the day than the night. We don't think that driving patterns themselves really change, but we do think that the probability of an edge existing is about $50\\%$ higher during the daytime. Further, since New York is a large hub for workers, This applies to towns for all pairs of communities in the network. The block matrices look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bnight = np.array([[.3, .15, .15], [.15, .3, .15], [.15, .15, .3]])\n",
    "Bday = Bnight*1.5  # day time block matarix is 50% more than night\n",
    "\n",
    "# people tend to commute from New Jersey to New York during the day\n",
    "Bday[0, 1] = .4; Bday[1,0] = .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_block(X, title=\"\", blockname=\"State\", blocktix=[0.5, 1.5, 2.5],\n",
    "               blocklabs=[\"1 - New York\", \"2 - New Jersey\", \"3 - Pennsylvania\"],\n",
    "               ax=None):\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=0, vmax=1, annot=True)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=blockname, xlabel=blockname)\n",
    "        ax.set_yticks(blocktix)\n",
    "        ax.set_yticklabels(blocklabs)\n",
    "        ax.set_xticks(blocktix)\n",
    "        ax.set_xticklabels(blocklabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "plot_block(Bnight, title=\"Night Block Matrix\", ax=axs[0])\n",
    "plot_block(Bday, title=\"Day Block Matrix\", ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-resident",
   "metadata": {},
   "source": [
    "We then sample two networks with the above parameters, giving us the following two networks for the day and the night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspologic as gp\n",
    "from graspologic.simulations import sbm\n",
    "\n",
    "Aday = sbm(ns, Bday)\n",
    "Anight = sbm(ns, Bnight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "draw_multiplot(Anight, labels=list(z), title=\"Night Time Adjacency Matrix\");\n",
    "draw_multiplot(Aday, labels=list(z), title=\"Day Time Adjacency Matrix\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-clearance",
   "metadata": {},
   "source": [
    "How can we ask whether the block matrices have similarities?\n",
    "\n",
    "## Testing whether the block matrices in an SBM are different\n",
    "\n",
    "Based on what we learned above, we know ahead of time that the block matrices for the SBMs are different. However, how can we actually test this? Well, let's start by being clear about what we mean by \"different\". To make this a little big more mathemattical, we'll introduce some new variables for the block matrices during the day time ($B^{(d)}$) and at night time ($B^{(n)}$) clearly. The block matrices are:\n",
    "\n",
    "\\begin{align*}\n",
    "    B^{(d)} &= \\begin{bmatrix}\n",
    "    b^{(d)}_{11} & b^{(d)}_{12} & b^{(d)}_{13} \\\\\n",
    "    b^{(d)}_{21} & b^{(d)}_{22} & b^{(d)}_{23} \\\\\n",
    "    b^{(d)}_{31} & b^{(d)}_{32} & b^{(d)}_{33}\n",
    "    \\end{bmatrix}; \\;\\;\\; B^{(n)} = \\begin{bmatrix}\n",
    "    b^{(n)}_{11} & b^{(n)}_{12} & b^{(n)}_{13} \\\\\n",
    "    b^{(n)}_{21} & b^{(n)}_{22} & b^{(n)}_{23} \\\\\n",
    "    b^{(n)}_{31} & b^{(n)}_{32} & b^{(n)}_{33}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "The hypothesis we want to test is the null hypothesis that the block matrices are the same, $H_0: B^{(d)} = B^{(n)}$, against the alternative hypothesis that the block matrices are different, $H_A: B^{(d)} \\neq B^{(n)}$. For a matrix, remember that two matrices are equal if all of the entries are identical, and two matrices are unequal if at least one of the entries are unequal. We can rerformulate the null and alternative hypotheses with this logic.\n",
    "\n",
    "For the null hypothesis, $H_0: B^{(d)} = B^{(n)}$, the statement is therefore equivalent to saying that for all pairs of communities $k$ and $l$, $b^{(d)}_{kl} = b^{(n)}_{kl}$. We will write each of these statements down as individual hypotheses for all pairs of communities, using the convention $H_{0, kl}: b_{kl}^{(d)} = b^{(n)}_{kl}$. The null hypothesis $H_0$ is therefore equivalent to saying that for every pair of communities $k$ and $l$, $H_{0,kl}$ is true. For the alternative hypothesis, $H_A: B^{(d)} \\neq B^{(n)}$, the statement is therefore equivalent to saying that for at least one pair of communities $k$ and $l$, $b^{(d)}_{kl} \\neq b^{(n)}_{kl}$. We will write down each of these statements as well as individual hypotheses for all pairs of communities, using the convention $H_{A, kl} : b_{kl}^{(d)} \\neq b^{(n)}_{kl}$. The alternative hypothesis $H_A$ is therefore equivalent to saying that for at least one pair of communities $k$ and $l$, that $H_{A,kl}$ is true.\n",
    "\n",
    "Now that we have broken a statement about two matrices down into numerous statements about two probabilities, we have almost completed our job. As it turns out, we have already seen the way we will test this, back in [Chapter 8.3](#link?)! Seeking to test whether a pair of block probabilities between communities $k$ and $l$ are the same, $H_{0,kl}: b_{kl}^{(d)} =  b_{kl}^{(n)}$, against whether the pair of block probabilities between communities $k$ and $l$ are different, $H_{A, kl}:  b_{kl}^{(d)} \\neq  b_{kl}^{(n)}$, is the [two-sample testing problem](#link?)! How did we address this problem before?\n",
    "\n",
    "We used Fisher's exact test! Remember that with Fisher's exact test, for two probabilities that we want to compare, we construct the following contingency table for each pair of communities $k$ and $l$, with the entries:\n",
    "\n",
    "\n",
    "| | Cluster 1, day | night |\n",
    "| --- | --- | --- |\n",
    "| Number of edges | $a$ | $b$ |\n",
    "| Number of non-edges | $c$ | $d$ |\n",
    "\n",
    "\n",
    "Where entry $a$ is the total number of edges between nodes of community $k$ with nodes of community $l$ in the daytime network, and $b$ is the total number of edges between nodes of community $k$ with nodes of community $l$ in the daytime network. The entry $c$ the total number of edges between nodes of community $k$ with nodes of community $l$ in the daytime network that do not exist (the number of adjacencies in cluster one with an adjacency of zero), and the entry $d$ is the total number of edges between nodes of community $k$ with nodes of community $l$ in the night time network that do not exist. We implement this using `numpy` and `scipy`, just like we did before, but this time for each pair of communities. To identify which adjacency matrix entries correspond to a given pair of communities, we use `np.outer`. Finally, we visualize the $p$-values of Fisher's exact test for each pair of communities using a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "K = 3\n",
    "Pvals = np.empty((K, K))\n",
    "# fill matrix with NaNs\n",
    "Pvals[:] = np.nan\n",
    "\n",
    "# get the indices of the upper triangle of Asrt\n",
    "upper_tri_idx = np.triu_indices(Aday.shape[0], k=1)\n",
    "# create a boolean array that is nxn\n",
    "upper_tri_mask = np.zeros(Aday.shape, dtype=bool)\n",
    "# set indices which correspond to the upper triangle to True\n",
    "upper_tri_mask[upper_tri_idx] = True\n",
    "\n",
    "for k in range(0, K):\n",
    "    for l in range(k, K):\n",
    "        comm_mask = np.outer(z == (k+1), z == (l + 1))\n",
    "        table = [[Aday[comm_mask & upper_tri_mask].sum(), Anight[comm_mask & upper_tri_mask].sum()],\n",
    "                 [(Aday[comm_mask & upper_tri_mask] == 0).sum(), (Anight[comm_mask & upper_tri_mask] == 0).sum()]]\n",
    "        Pvals[k,l] = fisher_exact(table)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-board",
   "metadata": {},
   "source": [
    "Before we visualize these $p$-values, we need to learn a little bit about a fundamental problem in statistics. Basically it goes something like this. If you remember, when we chose whether to accept or reject the alternative hypothesis for a statistical test, we used a value called the $\\alpha$ of the test. The $\\alpha$ of the test, usually $0.05$, indicated what fraction of the time we thought it was acceptable that we would incorrectly accept the alternative hypothesis. This corresponds to saying that we are okay with being wrong about $5\\%$ of the time. Let's say that we have a set of $100,000$ coins, each of which have a probability of landing on heads of $0.5$, and we flip each 30 times. For each coin, we test whether the coin has a probability of landing on heads of $0.5$ against the null hypothesis that the probability of the coin landing on heads is not $0.5$. For 99,999 of the 100,000 coins, the $p$-values exceed $\\alpha$, and therefore we accept the null hypothesis that the coin lands on heads with probability $0.5$. However, one of the coin flip experiments has a $p$-value of $0.01$, so we think that we should accept the alternative hypothesis that this particular coin does not have a probability of landing on heads of $0.5$. Or should we?\n",
    "\n",
    "As it turns out, when you run more than one hypothesis test, the *familywise error rate* itself changes. The **familywise error rate** is the probability that we made an error and incorrectly accepted the alternative hypothesis for *any* of the hypothesis tests we ran. Without getting into too much detail, the problem is that if you were to flip enough fair coins, there is always a chance that you might see a sequence of heads and tails that would suggest to you that the coin might not be fair and have a probabaility of landing on heads of $0.5$. The more coins you flip, the higher the chance that this occurs. For this reason, we use something called *multiple hypothesis correction*, which basically says that instead of using the $p$-values themselves in our evaluation of a bunch of hypothesis tests, we inflate th $p$-values to reflect the fact that we tested a bunch of things at once. One such approach which is extremely popular is called Bonferroni correction, which says that all $p$-values should be multiplied by the total number of hypothesis tests that are performed. We can easily do this using `statsmodels`'s `multipletests` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# use bonferroni correction on p-values calculated from previous step\n",
    "Pvals[~np.isnan(Pvals)] = multipletests(Pvals[~np.isnan(Pvals)], method='bonferroni')[1]\n",
    "\n",
    "# remove nans and replace with zeros temporarily\n",
    "Pvals[np.isnan(Pvals)] = 0\n",
    "# symmetrize since the network is undirected\n",
    "Pvals = Pvals + Pvals.T + np.diag(np.diag(Pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(9, 6))\n",
    "plot_block(Pvals, ax=ax, title=\"$p$-value matrix after Bonferrroni correction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-horizontal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "still-factor",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "italian-soldier",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adult-significance",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "square-moral",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pressed-player",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-synthetic",
   "metadata": {},
   "source": [
    "## Testing whether the block matrices in an SBM are multiples of one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-southeast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-death",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-stake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-record",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-acting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-treaty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-individual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-dodge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-virginia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-embassy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
