{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Significant Edges\n",
    "\n",
    "For the next two sections, we will turn back to an example we came across back when we discussed the [Signal Subnetwork model](#link?). If you recall, we had $100$ networks, which were either earthlings or astronauts. The networks each had $5$ nodes, which represented different lobes of the brain. The occipital lobe is responsible for sight, the temporal lobe with emotions and language, the parietal with hearing, the frontal with thinking and movement, and the insula with basic survivability skills. The astronauts were forced to live for several hundred thousand years on a planet in which their eyesight was challenged, and over time, evolution selected the astronauts who had a higher probability of connections between the occipital lobe and other areas of the brain. Our question of interest was as follows: if we are shown a network, how do we decide whether that network is from an earthling or an astronaut? Can we come up with a signal subnetwork classifier?\n",
    "\n",
    "To begin to address this question, we came up with the signal subnetwork model. What we established wit hthe signal subnetwork model was that all of the edges in each network could be broken up into one of two groups: either the signal edges or the non-signal edges. We collected these signal edges into a set called the \"signal subnetwork\", which was a parameter for the signal subnetwork model. For these signal edges, the probability of an edge existing (or not) was different for two (or more) classes in our problem. What this means is that, in our problem above, the edges which had a node in the occipital lobe had a different connection probability for the astronauts than the humans. On the other hand, the non-signal edges did not have a different connection probability for the astronauts than the humans. \n",
    "\n",
    "To start this section off, let's first sample some example networks from the signal subnetwork model. Let's assume that we have a total of $100$ people, each of whom are either astronauts (with probability $\\pi_{ast} = 0.4$) or humans (with probability $\\pi_{earth} = 0.6$). First, we'll roll our 2-sided die $100$ times, where side 1 (class 1) corresponds to astronauts, and side 2 (class 2) corresponds to humans. We will then create the class assignment vector $\\vec y$ using the number of times the die lands on each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pi_ast = 0.4\n",
    "pi_hum = 0.6\n",
    "M = 100\n",
    "\n",
    "# roll a 2-sided die 100 times, with probability 0.4 of landing on side 1 (astronaut) \n",
    "# and 0.6 of landing on side 2 (earthling)\n",
    "np.random.seed(1234)\n",
    "classnames = [\"Astronaut\", \"Earthling\"]\n",
    "class_counts = np.random.multinomial(M, pvals=[pi_ast, pi_hum])\n",
    "print(\"Number of individuals who are astronauts: {:d}\".format(class_counts[0]))\n",
    "print(\"Number of individuals who are humans: {:d}\".format(class_counts[1]))\n",
    "\n",
    "# create class assignment vector, and randomly reshuffle class labels for each individual\n",
    "yvec = np.array([1 for i in range(0, class_counts[0])] + [2 for i in range(0, class_counts[1])])\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(yvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct the probability matrices for each class. The probabilities for edges in which a node is in the occipital lobe are higher for the astronauts than the humans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of nodes\n",
    "n = 5\n",
    "# edge probabilities for humans are random\n",
    "P_hum = np.random.beta(size=n*n, a=3, b=8).reshape(n, n)\n",
    "# networks are undirected, so make the probability matrix symmetric\n",
    "P_hum = (P_hum + P_hum.T)/2\n",
    "# networks are loopless, so remove the diagonal\n",
    "P_hum = P_hum - np.diag(np.diag(P_hum))\n",
    "\n",
    "# the names of each of the five nodes\n",
    "nodenames = [\"Occipital\", \"Frontal\", \"Temporal\", \"Frontal\", \"Insula\"]\n",
    "# the signal edges\n",
    "E = np.array([[0,1,1,1,1], [1,0,0,0,0], [1,0,0,0,0], [1,0,0,0,0], [1,0,0,0,0]], dtype=bool)\n",
    "P_ast = np.copy(P_hum)\n",
    "\n",
    "# probabilities for signal edges are higher in astronauts than humans\n",
    "# square root function biases them towards 1, which is higher than\n",
    "# whatever they are right now since they are between 0 and 1\n",
    "P_ast[E] = np.sqrt(P_ast[E])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_prob(X, title=\"\", nodename=\"Brain Area\", nodetix=[0.5, 1.5, 2.5, 3.5, 4.5],\n",
    "             nodelabs=nodenames, ax=None, vmin=0, vmax=1):\n",
    "    if (ax is None):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=vmin, vmax=vmax, annot=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=nodename, xlabel=nodename)\n",
    "        if (nodetix is not None) and (nodelabs is not None):\n",
    "            ax.set_yticks(nodetix)\n",
    "            ax.set_yticklabels(nodelabs)\n",
    "            ax.set_xticks(nodetix)\n",
    "            ax.set_xticklabels(nodelabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(15, 12))\n",
    "plot_prob(P_hum, title=\"Earthling Probability Matrix\", ax=axs[0,0])\n",
    "plot_prob(P_ast, title=\"Astronaut Probability Matrix\", ax=axs[0,1])\n",
    "plot_prob(E, title=\"Signal Edges\", ax=axs[1,0])\n",
    "plot_prob(P_ast - P_hum, title=\"$P_{ast} - P_{earth}$\", ax=axs[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the class assignment vector $\\vec y$ to sample individual networks for each of the individuals. We plot an adjacency matrix for the first individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graphbook_code import ier\n",
    "\n",
    "# arrange the probability matrices in a list\n",
    "prob_mtxs = [P_ast, P_hum]\n",
    "# initialize empty list for adjacency matrices\n",
    "As = []\n",
    "np.random.seed(1234)\n",
    "for y in yvec:\n",
    "    # sample adjacency matrix for an individual of class y using the probability\n",
    "    # matrix for that class\n",
    "    As.append(ier(prob_mtxs[y - 1], directed=False, loops=False))\n",
    "\n",
    "# stack the adjacency matrices for each individual such that node i is first dimension,\n",
    "# node j is second dimension, and the individual index m is the third dimension\n",
    "As = np.stack(As, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graspologic.plot import heatmap\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 6));\n",
    "plot_prob(As[:,:,0], title=\"First individual, an {}\".format(classnames[yvec[0] - 1]), ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The key ideas we want to capture in our classifier are as follows:\n",
    "1. We want to *only* look at the edges which have a differing connection probability amongst our classes. This means that we only want to look at *signal edges* which are in the *signal subnetwork*, and we want to ignore edges which are not in the signal subnetwork. Edges which are not in the signal subnetwork are simply noise, since they have the same connection probability between the classes, and therefore are not useful for differentiating between the different classes.\n",
    "2. We want to incorporate the structure of the network into the classifier. This means we want to use the fact that nodes are nodes, and that edges are connections between nodes, to determine how to classify networks as earthling or astronaut.\n",
    "\n",
    "To begin, we will start by addressing aim 1. We need to find which edges are in the signal subnetework. Stated another way, we need to *estimate* the signal subnetwork. What this requires is a way to identify which edges will best capture the difference between the classes in our network. Do we know anything which can do this? As it turns out, we can use Fisher's exact test, something we learned about in the section on [testing for differences between groups of edges](http://docs.neurodata.io/graph-stats-book/applications/ch8/testing-differences.html#two-sample-hypothesis-testing-with-coins), to identify the signal subnetwork. In the next section on [identifying significant vertices](#link?), we will address the second point on using the structure of the network to further refine these potential signal edges.\n",
    "\n",
    "## Fisher's Exact Test for Edge Importance\n",
    "\n",
    "If you remember back to the coin flip example, our setting was as follows. We had two coins, coin $1$ and coin $2$, and we wanted to test whether their probabilities of landing on heads were different. Our hypotheses were $H_A: p_1 \\neq p_2$, against the null hypothesis that $H_0: p_1 = p_2$. We therefore had two random samples, the outcomes of ten coin flips from coin one and the outcomes of ten coin flips from coin two, meaning that our test of $H_A$ against $H_0$ fell into the *two-sample testing* regime. To address this, what we used was Fisher's exact test, where we counted the number of times each coin landed on heads and tails, which we aggregated in a table:\n",
    "\n",
    "| | First coin | Second coin |\n",
    "|---|---|---|\n",
    "| Landed on heads | 7 | 1 |\n",
    "| Landed on tails | 3 | 9 |\n",
    "\n",
    "As it turns out, we can adapt this test for our situation here too! For a single edge, what we want to do is test whether $H_A: p_{ij}^{(ast)} \\neq p_{ij}^{(earth)}$, against the null hypothesis that $H_0: p_{ij}^{(ast)} = p_{ij}^{(earth)}$. The key property of Fisher's exact test that makes this desirable for us is that, when $H_0$ is true (and the probabilities are equal), the Fisher's exact test statistic is smaller, whereas when $H_A$ is true (and the probabilities are not equal), the Fisher's exact test statistic is bigger. We will exploit this feature in our design of a classifier for astronauts versus earthlings. Note that we are using the Fisher's exact test statistic, and *not* the $p$-value of the test, for this step like we did previously when testing for differences between groups of edges. For each edge $(i, j)$, we will construct the following table:\n",
    "\n",
    "| | Astronauts | Earthlings |\n",
    "| --- | --- | --- |\n",
    "| Edge $(i,j)$ exists | Number of astronauts with $(i,j)$ | Number of earthlings with edge $(i,j)$ |\n",
    "| Edge $(i,j)$ does not exist | Number of astronauts with edge $(i,j)$ | Number of earthlings without edge $(i,j)$ |\n",
    "\n",
    "Which we can do in python as follows, for the edge from the occipital lobe to the temporal lobe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1; j = 2  # (1,2) corresponds to the edge from occipital to temporal lobe\n",
    "\n",
    "ast_edge = As[i-1,j-1,yvec == 1].sum()  # count the number of astronauts with edge i,j\n",
    "hum_edge = As[i-1,j-1,yvec == 2].sum()  # count the number of earthlings with edge i,j\n",
    "ast_noedge = class_counts[1 - 1] - ast_edge  # count the number of astronauts without edge i,j\n",
    "hum_noedge = class_counts[2 - 1] - hum_edge  # count the number of earthlings without edge i,j\n",
    "\n",
    "edge_tab = np.array([[ast_edge, hum_edge], [ast_noedge, hum_noedge]])  # arrange as in table shown above\n",
    "\n",
    "print(edge_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the Fisher's exact test statistic on the data, using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "test_statistic, pval = fisher_exact(edge_tab)\n",
    "print(\"Test Statistic: {:.4f}\".format(test_statistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the edge $(i,j)$ that we chose above corresponded to the edge between the occipital and temporal lobe, we expect this edge to indicate a disparity between the astronauts and the earthlings. Why is this? Well, quite simply, by construction, this edge is a *signal* edge, which means that it carries real *signal* in differentiating a network from a human from a network of an alien. Why is this a big deal?\n",
    "\n",
    "Well, let's see what happens if we were to compute this for a non-signal edge. Let's arbitrarily choose the edge between the temporal and frontal lobes, which corresponds to $i=3$ (the temporal lobe) and $j = 2$ (the frontal lobe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3; j = 2  # (3,2) corresponds to the edge from temporal to frontal lobe\n",
    "\n",
    "ast_edge = As[i-1,j-1,yvec == 1].sum()  # count the number of astronauts with edge i,j\n",
    "hum_edge = As[i-1,j-1,yvec == 2].sum()  # count the number of earthlings with edge i,j\n",
    "ast_noedge = class_counts[1 - 1] - ast_edge  # count the number of astronauts without edge i,j\n",
    "hum_noedge = class_counts[2 - 1] - hum_edge  # count the number of earthlings without edge i,j\n",
    "\n",
    "edge_tab = np.array([[ast_edge, hum_edge], [ast_noedge, hum_noedge]])  # arrange as in table shown above\n",
    "\n",
    "test_statistic, pval = fisher_exact(edge_tab)\n",
    "print(\"Test Statistic: {:.4f}\".format(test_statistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now this test statistic happens to be a *lot* smaller than the previous one we saw, now doesn't it? Is this just by chance? The answer is: no! For signal edges, the test statistic will, by construction, *generally* be smaller than the test statistic in a non-signal edge. By *generally*, we mean that it will *tend* to be smaller (but not always!). We could certainly get samples of data where this is not the case, analogous to the idea that we could flip a fair coin (with equal probability of seeing heads and tails) 10 times and obtain all 10 flips being heads. For this reason, we will use the Fisher's exact test statistic to quantify how \"important\" an edge is for differentiating the two classes, or as an edge importance statistic. We will exploit this edge importance statistic as we build up our classifier further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using edge importances to estimate the signal subnetwork\n",
    "\n",
    "We will use the Fisher's exact test statistic to develop an approach which allows us to properly estimate a signal subnetwork model. The key idea is this: if we were to use any one edge of the network, we would not really be able to predict whether someone is an earthling or an alien. The reason for this is that edges either exist, or not, so unless one of the class edge probabilities for a particular edge happens to be really low (near zero) and the other happens to be really high (near one), the only informative decision boundary we could construct would be to assign one class to the networks where that particular edge exists, or the other class to the networks where that particular edge does not exist. Even less interestingly, we could abitrarily say that every network is in a particular class, which is also not going to be a particularly interesting classifier.\n",
    "\n",
    "Another thing we could do would be to use *every* edge in the network to develop a classifier, which also isn't the best we could do. This is because if we use every edge in the classifier, w would have a lot of *noise* from the non-signal edges. This means that even though we might learn *something* from the signal edges, anything we learn is going to end up being diluted down by noise because we are including a lot of uninformative information in our classifier (the non-signal edges).\n",
    "\n",
    "Rather, what we want to do is investigate to find the edges which are carrying all of the signal, and then isolate our downstream learning from information captured by those edges. This is because the signal edges carry all of the informative information about differentiating networks from one class to the other. This begs the question, how do we find the signal edges, and isolate them from the non-signal edges?\n",
    "\n",
    "As we mentioned in the preceding subsection, as it happens, the Fisher's exact test statistic is going to tend to be larger for edges in which there is actual signal (the edge probability is different for astronauts and humans), and smaller when there is no signal (the edge probability is the same for astronauts and humans). That is, if an edge is in the signal subnetwork, the Fisher's exact test statistic is big, and if it is not in the signal subnetwork, the Fisher's exact test statistic is small. For this reason, what we end up doing is *rank transforming* the Fisher's exact test statistics, and then pick an arbitrary number of the highest ranking test statistics to retain for classification. \n",
    "\n",
    "We start by constructing a **significance matrix**, which is a collection of all of the Fisher's exact test statistics for each edge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_mtx = np.zeros((n,n))\n",
    "ast_idxs = yvec == 1  # the individuals who are astronauts\n",
    "hum_idxs = yvec == 2  # the individuals who are earthlings\n",
    "# since the networks are undirected, only need to compute for upper triangle\n",
    "for i in range(0, n):\n",
    "    for j in range(i+1, n):\n",
    "        ast_edgeij = As[i,j,ast_idxs].sum()\n",
    "        hum_edgeij = As[i,j,hum_idxs].sum()\n",
    "        table = np.array([[ast_edgeij, hum_edgeij],\n",
    "                          [class_counts[0] - ast_edgeij, class_counts[1] - hum_edgeij]])\n",
    "        sig_mtx[i,j] = fisher_exact(table)[0]\n",
    "sig_mtx = sig_mtx + sig_mtx.T  # symmetrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8, 6));\n",
    "plot_prob(sig_mtx, title=\"Significance matrix\", vmax=sig_mtx.max(), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the edges with the highest Fisher's exact test statistics in the significance matrix tend to be the edges with a node in the occipital lobe, which are our signal edges. This is great news! Next, we rank the test statistics in the significance matrix, from smallest to largest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "rank_sig_mtx = rankdata(sig_mtx).reshape(n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8, 6));\n",
    "plot_prob(rank_sig_mtx, title=\"Ranked significance matrix\", vmax=rank_sig_mtx.max(), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ranks are biggest for the Fisher's exact test statistics with a node in the occipital lobe. Then, we select a number of edges (the size of the signal subnetwork) $K$, and retain the top $K$ edges, by their test statistic significances. The top $K$ edges by significance are an estimate of the signal subnetwork, $\\hat{\\mathcal S}$.\n",
    "\n",
    "We can implement everything we've learned so far in graspologic relatively easily, using the `SignalSubgraph` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.subgraph import SignalSubgraph\n",
    "\n",
    "K = 8  # the number of edges in the subgraph\n",
    "sgest = SignalSubgraph()\n",
    "sgest.fit_transform(As, labels=yvec - 1, constraints=K);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we arrange this into a matrix so that we can look at the signal subnetwork we identified, and compare it to the true signal subnetwork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigsub = np.zeros((n,n))  # initialize empty matrix\n",
    "sigsub[sgest.sigsub_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(18, 6));\n",
    "plot_prob(E, title=\"True Signal Subnetwork\", ax=axs[0])\n",
    "plot_prob(sigsub, title=\"Estimated Signal Subnetwork\", ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our estimated signal subnetwork looks pretty close to the true signal subnetwork.\n",
    "\n",
    "A question you might be wondering concerns how exactly we chose the number of signal edges to include in our estimate of the signal subnetwork, $\\hat{\\mathcal S}$. If we knew the number of edges in the signal subnetwork ahead of time, the choice would be easy: just choose $K$ to be the number of edges in the signal subnetwork! In real data, however, this isn't really going to be the case. For this reason, we will have to estimate $K$ using $\\hat K$. To learn how to estimate $K$, we first need to learn how to put what we've covered so far into a classifier, so read on!\n",
    "\n",
    "## Building a classifier using the estimated signal subnetwork\n",
    "\n",
    "Finally, we can use the estimated signal subnetwork that we have constructed to devise a classifier. The objective of a classifier is to take new pieces of data (in this case, new networks), and assign them to the class which they are most likely from. How do we determine which class a network is most likely from?\n",
    "\n",
    "At the moment, we have our estimate of the signal subnetwork, $\\hat{\\mathcal S}$, and have a bunch of networks from one of two classes: astronaut (class 1) or alien (class 2). The edges of the network are binary, and we want to use the edges which are in the signal subnetwork to classify points as either astronaut or alien. A natural classifier choice for this situation is known as the Naive Bayes classifier. We have all the ingredients we need to construct the Naive Bayes classifier, so we'll try to put this together. Feel free to skip past this next section if you want to jump right into the implementation of the classifier. In this next section, we will explain some of the intuition of the Naive Bayes classifer, which might require a bit of a probability and statistics background.\n",
    "\n",
    "### Naive Bayes Classifier (Statistical Intuition)\n",
    "\n",
    "The core idea of the Naive Bayes classifier is, if our data have features which are zeros and ones, we can use the *class-conditional probabilities* to determine whether a point is from class $1$ or class $2$. The idea is as follows. First, we assume that all of the individual features of our data (the features, in our case, are the edges of the network that are in the signal subnetwork) are independent, then the likelihood of observing a particular sequence of edges in the signal subnetwork of the $m^{th}$ network if that network is in class $y$ is as follows. First, we will just write down some simpler notation for the quantity we want:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb P(\\text{observing }A^{(m)}\\text{ given we assume }\\text{$m$ is in class $y$} \\text{ where the signal subnetwork is $\\mathcal S$}) = \\mathbb P(A^{(m)} | y_i = y; \\mathcal S)\n",
    "\\end{align*}\n",
    "\n",
    "Nothing happened in this step just yet; we just made a smaller notation for the quantity on the left that we will use later on. In this next step, we use the fact that, if edges existing and not existing are independent, then the probability of observing a sequence of edges is the product of the probabilities of observing each individual edge. This is called an *independent edge assumption*. We proceed as follows:\n",
    "\\begin{align*}\n",
    "    \\mathbb P(A^{(m)}| y_i = y; \\mathcal S) &= \\prod_{(i,j) \\in \\mathcal S} \\mathbb P(a_{ij}^{(m)} | y_i = y; \\mathcal S)\n",
    "\\end{align*}\n",
    "Note that we are taking the product of each pair of edges, $(i,j)$, which are in the signal sub-network. Next, if we remember back to the Independent-Edge Random Network (IER), we assumed that if $\\mathbf A^{(m)}$ was an $IER_n(P^y)$ network, that every edge $\\mathbf a_{ij}^{(m)}$ had a probability of $p^y_{ij}$ of taking a value of $1$ (the edge exists), and a probability of $1 - p_{ij}^y$ of taking a value of $0$ (the edge does not exist). What this means is that $\\mathbf a_{ij}^{(m)}$ is something called a **Bernoulli distributed random variable**. The probability of seeing a particular realization $a_{ij}^{(m)}$ of a Bernoulli distributed random variable is relatively straightforward to see. We want the quantity $\\mathbb P(A^{(m)} | y_i = y; \\mathcal S)$ to reflect the following two facts we've already discussed:\n",
    "1. If $a_{ij}^{(m)}$ is $1$, then $\\mathbb P(A^{(m)} | \\mathbf y_i = y; \\mathcal S)$ is $p_{ij}^y$.\n",
    "2. If $a_{ij}^{(m)}$ is $0$, then $\\mathbb P(A^{(m)} | \\mathbf y_i = y; \\mathcal S)$ is $1 - p_{ij}^y$.\n",
    "Is there a succinct expression that we can express this with? Yes! Try the following equation:\n",
    "\\begin{align*}\n",
    "    \\mathbb P(A^{(m)} | \\mathbf y_i = y; \\mathcal S) &= (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "Notice that if $a_{ij}^{(m)}$ is $1$, then $1 - a_{ij}^{(m)}$ is $0$. Therefore, $(1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}$ is just $1$, since any number raised to the $0$ power is $1$. On the other hand, $(p_{ij}^y)^{a_{ij}^{(m)}}$ is $p_{ij}^y$, since any number raised to the $1$ power is itself. Therefore, this expression fits the bill for us. So, we can simplify our expression as:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)} | y_i = y; \\mathcal S) &= \\prod_{(i,j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "And we're almost there! The next thing we're going to do is a little tricky, but fortunately we can turn to Bayes' Theorem for help. What we want to do is compute the probability of observing *both* $A^{(m)}$ *and* $y_i$ having the value $y$, not the probability of observing $A^{(m)}$ given that we assume that $y_i$ is $y$. In statistical notation, what this amounts to is:\n",
    "\\begin{align*}\n",
    "    \\mathbb P(\\text{observing }A^{(m)}\\text{ and }\\text{$m$ is in class $y$} \\text{ where the signal subnetwork is $\\mathcal S$}) = \\mathbb P(A^{(m)}, y_i = y; \\mathcal S)\n",
    "\\end{align*}\n",
    "This expression is a little confusing, but its interpretation is relatively straightforward: it is the probability that we see both things happening at the same time (the random network $\\mathbf A^{(m)}$ takes the value $A^{(m)}$ and the random class $\\mathbf y_i$ has the value $y$) rather than just assume that the random class $\\mathbf y_i$ already is $y$. We can compute this quantity by remembering Bayes' Theorem:\n",
    "\\begin{align*}\n",
    "     \\mathbb P(A^{(m)}, \\mathbb y_i = y; \\mathcal S) &= \\frac{\\mathbb P(A^{(m)}, \\mathbb y_i = y; \\mathcal S)}{\\mathbb P(\\mathbb y_i = y}\n",
    "\\end{align*}\n",
    "Note that this implies the following:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)}, \\mathbb y_i = y; \\mathcal S) &= \\mathbb P(A^{(m)} | \\mathbb y_i = y; \\mathcal S)\\mathbb P(\\mathbb y_i = y)\n",
    "\\end{align*}\n",
    "Plugging in the value we obtained above:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)}, \\mathbb y_i = y; \\mathcal S) &= \\mathbb P(\\mathbb y_i = y)\\prod_{(i, j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "Remember that the parameter of the signal subnetwork model, $\\pi_y$, represented the probability of our $Y$-sided die landing on class $y$, and therefore was the probability that the random class $\\mathbf y_i$ took the value $y$. Therefore, $\\mathbb P(\\mathbb y_i = y) = \\pi_y$, since these two quantites represent the same thing! Finally:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)}, \\mathbb y_i = y; P_y, \\mathcal S) &= \\pi_y\\prod_{(i, j) \\in \\mathcal S} (p_{ij}^y)^{a_{ij}^{(m)}} (1 - p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "So, what relevance does this have to the Naive bayes classifier? As it turns out, this is the *objective function* for the Naive Bayes classifier. We will simply \"plug in\" estimates of the quantities that we don't have given to us, which are the signal subnetwork $\\mathcal S$, the class probabilities $\\pi_y$, and the class-conditional edge probabilities $p_{ij}^y$:\n",
    "\\begin{align*}\n",
    "\\mathbb P(A^{(m)}, \\mathbb y_i = y; \\hat P_y, \\hat{\\mathcal S}) &= \\hat \\pi_y\\prod_{(i, j) \\in \\mathcal S} (\\hat p_{ij}^y)^{a_{ij}^{(m)}} (1 - \\hat p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "And we classify a point based on which of the possible values that $y$ could take which maximizes this quantity. That is, we are choosing the class for which $A^{(m)}$ is \"most probable\" to be in! In symbols, we classify our points using:\n",
    "\\begin{align*}\n",
    "    \\hat y_m &= \\text{argmax}_{y \\in \\left\\{1,...,Y\\right\\}} \\hat \\pi_y\\prod_{(i, j) \\in \\mathcal S} (\\hat p_{ij}^y)^{a_{ij}^{(m)}} (1 - \\hat p_{ij}^y)^{1 - a_{ij}^{(m)}}\n",
    "\\end{align*}\n",
    "Or that our estimated class $\\hat y_m$ for individual $m$ is the class $y$ in which $\\mathbb P(A^{(m)}, \\mathbb y_i = y; \\hat P_y, \\hat{\\mathcal S})$ is as large as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
