{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection For Timeseries of Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a particular type of sea slug who have gills on the outside of their body. When you squirt water at these gills, they withdraw into the slug. The interesting thing about this type of slug is that the brain network involved in this gill withdrawal reflex is entirely mapped out, from the neurons which detect and transmit information about the water into the slug's brain, to the neurons that leave the brain and fire at its muscles. (For the interested, this is a real thing - look up Eric Kandel's research on Aplysia!)\n",
    "\n",
    "Say you're a researcher studying these sea slugs, and you have a bunch of brain networks of the same slug. Each node is a single neuron, and edges denote connections between neurons. Each of the brain networks that you have were taken at different time points: some before water started getting squirted at the slug's gills, and some as the water was getting squirted. Your goal is to reconstruct when water started to get squirted, using only the networks themselves. You hypothesize that there should be some signal change in your networks which can tell you the particular time at which water started getting squirted. Given the network data you have, how do you figure out which timepoints these are?\n",
    "\n",
    "The broader class of problems this question addresses is called *anomaly detection*. The idea in general is that you have a bunch of snapshots of the same network over time. Although the nodes are the same, the edges are changing at each time point. Your goal is to figure out which time points correspond to the most change, either in the entire network or in particular groups of nodes. You can think of a network as \"anomalous\" with respect to time if some potentially small group of nodes within the network concurrently changes behavior at some point in time compared to the recent past, while the remaining nodes continue with whatever noisy, normal behavior they had.\n",
    "\n",
    "In particular, what we would really like to do is separate the signal from the noise. All of the nodes in the network are likely changing a bit over time, since there is some variability intrinsic in the system. Random noise might just dictate that some edges get randomly deleted and some get randomly created, but we want to figure out when neurons are changing as the result of the squirting in particular.\n",
    "\n",
    "Let's simulate some network timeseries data so that we can explore anomaly detection more thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Network Timeseries Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data generation, we're going to assemble a set of 12 time-points for a network directly from its latent positions (we'll assume that each time-point for the network is drawn from an RDPG). Ten of these time points will just have natural variability, and two will have a subset of nodes whose latent positions were perturbed a bit. These two will be the anomalies.\n",
    "\n",
    "We'll say that the latent positions for the network are one-dimensional, and that it has 100 nodes. There will be the same number of adjacency matrices as there are time points, since our network will be changing over time.\n",
    "\n",
    "For each of the ten normal time points, we'll:\n",
    "1. Generate 100 random latent positions. Each latent position will be a (uniformly) random number between .2 and .8.\n",
    "2. Use graspologic's rdpg function to sample an adjacency matrix using these latent positions.\n",
    "\n",
    "And for each of the two perturbed time points, we'll:\n",
    "1. Generate 100 random latent positions, in the same way as above.\n",
    "2. Add a small amount of noise to 20 of these latent positions.\n",
    "3. Generate an adjacency matrix as above\n",
    "\n",
    "Once we have this simulated data, we'll move into some discussion about how we'll approach detecting the anomalous time points.\n",
    "\n",
    "Below is code for generating the data. We define a function to generate a particular time-point, with an argument which toggles whether we'll perturb latent positions in the time point. Then, we just loop through our time-points and generate a new adjacency matrix for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2994585177148525"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experimentation code block\n",
    "import numpy as np\n",
    "from graspologic.simulations import rdpg\n",
    "\n",
    "nodes = 100\n",
    "\n",
    "X = np.random.uniform(.2, .8, size=(nodes, 2))\n",
    "n1 = rdpg(X[:, 0, np.newaxis])\n",
    "n2 = rdpg(X[:, 1, np.newaxis])\n",
    "\n",
    "from graspologic.embed import OmnibusEmbed as OMNI\n",
    "omni = OMNI(n_components=1)\n",
    "latents = omni.fit_transform([n1, n2])\n",
    "\n",
    "Xhat_1 = latents[0]\n",
    "Xhat_2 = latents[1]\n",
    "\n",
    "np.linalg.norm(Xhat_1 - Xhat_2, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graspologic.simulations import rdpg\n",
    "\n",
    "\n",
    "def gen_timepoint(perturbed=False, n_perturbed=20, perturbation=.1, nodes=100):\n",
    "    nodes = 100\n",
    "    X = np.random.uniform(.2, .8, size=nodes)\n",
    "    if perturbed:\n",
    "        baseline = np.array([1, -1, 0])\n",
    "        delta = np.repeat(baseline, (n_perturbed//2, \n",
    "                                     n_perturbed//2, \n",
    "                                     nodes-n_perturbed))\n",
    "        X += (delta * perturbation)\n",
    "    X = X[:, np.newaxis]\n",
    "    A = rdpg(X)\n",
    "    return A, X\n",
    "    \n",
    "\n",
    "time_points = 12\n",
    "networks = []\n",
    "latents = []\n",
    "\n",
    "for time in range(time_points-2):\n",
    "    A, X = gen_timepoint()\n",
    "    networks.append(A)\n",
    "    latents.append(X)\n",
    "\n",
    "for perturbed_time in range(5, 7):\n",
    "    A, X = gen_timepoint(perturbed=True)\n",
    "    networks.insert(perturbed_time, A)\n",
    "    latents.insert(perturbed_time, X)\n",
    "    \n",
    "networks = np.array(networks)\n",
    "latents = np.array(latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the adjacency matrices we generated below. Note that you can't really distinguish a difference between the ten normal time points and the two perturbed time points with the naked eye, even though the difference is there, so it would be pretty difficult to manually mark the time points - and if you have many time points, rather than just a few, you'd want to be able to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from graphbook_code import heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "def rm_ticks(ax, x=False, y=False, **kwargs):\n",
    "    if x is not None:\n",
    "        ax.axes.xaxis.set_visible(x)\n",
    "    if y is not None:\n",
    "        ax.axes.yaxis.set_visible(y)\n",
    "    sns.despine(ax=ax, **kwargs)\n",
    "\n",
    "fig = plt.figure();\n",
    "\n",
    "perturbed_points = {5, 6}\n",
    "for i in range(time_points):\n",
    "    if i not in perturbed_points:\n",
    "        ax = fig.add_axes([.02*i, -.02*i, .8, .8])\n",
    "    else:\n",
    "        ax = fig.add_axes([.02*i+.8, -.02*i, .8, .8])\n",
    "    ax = heatmap(networks[i], ax=ax, cbar=False)\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Ten Normal Time Points\", loc=\"left\", fontsize=16)\n",
    "    if i == 5:\n",
    "        ax.set_title(\"Two Perturbed Time Points\", loc=\"left\", fontsize=16)\n",
    "    rm_ticks(ax, top=False, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Figure Out Which Time Points Are Anomalies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to start thinking about how we'd approach figuring out which of the time points are anomalies. \n",
    "\n",
    "One of the simplest approaches to this problem might just be to figure out which node has the highest count of edge changes across your timeseries. For each node across the timeseries, you'd count the number of new edges that appeared (compared to the previous point in time), and the number of existing edges that were deleted. Whichever count is highest could be your anomalous node.\n",
    "\n",
    "This might give you a rough estimate -- and you could even potentially find perturbed time points with this approach -- but it's not necessarily the best solution. Counting edges doesn't account for other important pieces of information: for instance, you might be interested in which other nodes new edges were formed with. It seems like deleting or creating edges with more important nodes, for instance, should be weighted higher than deleting or creating edges with unimportant nodes.\n",
    "\n",
    "So let's try another method. You might actually be able to guess it! The idea will be to simply estimate each network's latent positions, followed by a hypothesis testing approach. Here's the idea.\n",
    "\n",
    "Let's call the latent positions for our network $X^{(t)}$ for the snapshot of the network at time $t$. You're trying to find specific time points -- $X^{(i)}$ -- which are different from their previous time point $X^{(i-1)}$ by a large margin. You can define \"different\" as \"difference in matrix norm\". In other words, We're trying to find a time point where the difference in norm between the latent positions at time $t$ and the latent positions at time $t-1$ is greater than some constant $c$:  $||X^{(t)} - X^{(t-1)}|| > c$. The idea is that non-anomalous time points will probably be a bit different, but that the difference will be within some reasonable range of variability.\n",
    "\n",
    "There's an alternate problem where you restrict your view to *nodes* rather than entire adjacency matrices. The idea is that you'd find time-points which are anomalous for particular nodes or groups of nodes, rather than the entire network. The general idea is the same: you find latent positions, then test for how big the difference is between time point $t$ and time point $t-1$. This time, however, your test is for particular nodes. You want to figure out if $||X_i^{(t)} - X_i^{(t-1)}|| > c$, where you're looking at a particular latent position $X_i$ rather than all of them at once.\n",
    "\n",
    "Let's dive into some specifics. First, we'll look at how to estimate the latent positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting if the First Time Series is an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to just estimate the latent positions for each timepoint separately with ASE or LSE, we'd run into the nonidentifiability problem that we've seen a few times over the course of this book: The latent positions would be rotated versions of each other, and we'd have to use something like Procrustes (which adds variance, since it's just an estimate) to rotate them back into the same space.\n",
    "\n",
    "However, since we have multiple time points, each of which is associated to an adjacency matrix, it's natural to use models from the Multiple-Network Representation Learning section (You can go back and read chapter 6.7 if you're fuzzy on the details here). In that section, we introduced the Omnibus Embedding as a way to estimate latent positions for multiple *networks* simultaneously, but all we really need for it is multiple *adjacency matrices*. These exist in our network in the form of its multiple time points; So, we'll just embed each time point with the Omnibus Embedding.\n",
    "\n",
    "We'll set the embedding dimension to 2. This isn't actually technically correct -- since we generated the data with 1-dimensional latent positions - but the extra dimension makes things easier to visualize and only adds a bit of extra noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.embed import OmnibusEmbed as OMNI\n",
    "\n",
    "omni = OMNI(n_components=2)\n",
    "latents_est = omni.fit_transform(networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding our Test Statistic With Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 12\n",
    "t = 1\n",
    "\n",
    "def get_statistic(adjacencies, return_latents=False):\n",
    "    latents_est = omni.fit_transform(adjacencies)\n",
    "    X, Y = latents_est[0], latents_est[1]\n",
    "    y = np.linalg.norm(Y - X, ord=2)\n",
    "    if return_latents:\n",
    "        return y, X\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "y, X = get_statistic(networks[4:6], return_latents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bootstrapping to Figure out the Distribution of the Test Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null hypothesis that X = Y. Bootstrap X.\n",
    "N = 1000\n",
    "y_vs = []\n",
    "for est in range(N):\n",
    "    A_est = rdpg(X)\n",
    "    B_est = rdpg(X)\n",
    "    y_v = get_statistic([A_est, B_est])\n",
    "    y_vs.append(y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot = sns.histplot(y_vs)\n",
    "plot.set_title(\"Distribution of test statistics under $H_0$\");\n",
    "plot.set_xlabel(\"Test statistic value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our test statistic to find p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vs = np.array(y_vs)\n",
    "\n",
    "(y_vs > y).sum()/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing to Figure out the Test Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at testing for differences for the whole network at particular time points first, and then we'll explore what it looks like to test for differences in particular nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define individual vertex anomaly detection for the i-th vertex at time point $t^*$ as a test of the null hypothesis $H_{0i}^{(t*)}$ that t* is an anomaly time for vertex $i$.\n",
    "\n",
    "1. embed with MASE or OMNI\n",
    "    - MASE is better when graphs are close to equation (1), OMNI is better when they're far from eq (2) where  \n",
    "      eq1: exactly COSIE: $X^{(t)} = VS^{(t)}$  \n",
    "      eq2: allows changes in $V$ at some time points $X^{(t)} = VS^{(t)}$, $t \\in [M] - \\{t_1, ..., t_p\\}$, else $V^{(t)} S^P{(t)}$, $t=t_j, j = 1, ..., p$\n",
    "      so MASE is better when the networks don't fundamentally change across time points -- for example, when nodes never change community -- and OMNI is better when you allow for the network to be evolving at a more fundamental level.\n",
    "\n",
    "2. for each time point, get test statistic: for any $\\hat{X} \\in \\mathcal{R}^{n \\times d}$, generate iid samples of $A \\sim RDPG(\\hat{X})$, then get test statistics $y^{(t)}$ or $y_i^{(t)}$ from it\n",
    "    - using $y^{(t)} = ||\\hat{X}^{(t)} - \\hat{X}^{(t-1)}||$ for whole network\n",
    "    - using $y_i^{(t)} = ||\\hat{X_i}^{(t)} - \\hat{X_i}^{(t-1)}||$ for vertex anomaly detection\n",
    "\n",
    "3. Get distribution of test statistic with parametric bootstrap, using $\\hat{X} = \\hat{X}^{(t)}$ as the null distribution. Generate B samples of $y_b^{(t)}$ \n",
    "\n",
    "4. Calculate p-values as $p^{(t)} = \\frac{\\sum_{b=1, ..., B} I(y_b^{(t)} > y^{(t)})}{B}$\n",
    "\n",
    "5. report p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.embed import OmnibusEmbed as OMNI\n",
    "\n",
    "omni = OMNI(n_components=2)\n",
    "latents = omni.fit_transform(networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting anomaly times for particular nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- j1's paper -- heritability\n",
    "- vivek's paper -- mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "guodong's stuff: uses MASE and OMNI combined with DCORR to do hypothesis testing\n",
    "- vivek did something similar for MCC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
