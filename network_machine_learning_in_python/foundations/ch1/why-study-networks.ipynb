{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we study networks?\n",
    "\n",
    "## Why study networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start thinking, you start seeing networks everywhere. The objects could be people, and the edges could be friendships. Or they could be computers, and the relationship could be the information they send to each other. You'd have airflight networks if you're working in air traffic control, where the edges are flights from city A to B; or if you're an epidemiologist studying disease, you could create an infection network. Neuroscientists can explore brain networks, which tell them about neurons and their relationships with each other, and computer scientists often use neural networks, which have become pillars in machine learning.\n",
    "\n",
    "Networks can even be used to visualize ideas: in a Bayesian network, your nodes represent a set of variables and the relationship between them is their conditional dependencies. In a correlation network, your nodes represent variables, and the edges represent the correlation between those variables. You can have ecological networks, electrical networks, gene networks, and you could visualize your team's workflow with a network.\n",
    "\n",
    "Even the way we think is organized as a network. Visualize a concept or object in your head. Maybe you could think about the food you had for breakfast this morning, or the city you live in. Now, think about the connections between those concepts and others. Maybe you had eggs and toast for breakfast. Eggs and toast are connected with a multitude of other concepts in your head: forks and silverware, kitchens, hunger, protein, carbohydrates, your morning routine, chickens, wheat, other breakfast foods, and an innumerable amount of other things. What you're doing right now is exploring a small part of the massive semantic network that lives in your head.\n",
    "\n",
    "Network machine learning is a relatively new field. The vast majority of it has been invented (or discovered) after the year 2000, and many fundamental proofs have only been published recently. \n",
    "\n",
    "For the business-minded folks out there, this is an incredible time to learn about networks. Graph neural networks (GNNs) are becoming increasingly popular as deep learning and neural networks explode, and although this book doesn't focus on GNNs specifically, it does give you the fundamental ideas that you can build off of to understand them. \n",
    "\n",
    "Real-life applications also follow a general trend. You'll see academia spend a lot of time, usually 10-20 years, publishing proof-of-concept papers, discussing possible approaches to solve problems, and developing fundamental tools (usually informally, with somewhat messy code that exists in jupyter notebooks). Then, as the field of research starts maturing, companies and industry people start noticing these new academic tools. They find ways to apply them to make their product or service better, and easy-to-use packages like scikit-learn are developed to make these academic tools mainstream. Network machine learning is at a tipping point right now: its academic foundations have been built up over the past 10-20 years, and the tools for building and working with networks are now starting to move from academia to industry. Congratulations: you could get in early for a wave of application-focused network machine learning tools!\n",
    "\n",
    "## What does statistics have to do with network machine learning?\n",
    "\n",
    "In this book on network machine learning, we are going to focus some level of attention on a particular sub-branch of machine learning, known as statistical learning. **Statistical learning** is a framework for machine learning in which we *infer* things about our network, we use statitistics to refine and conceptualize our problem and quantify how reasonable our conclusions are. What exactly do we mean by this?\n",
    "\n",
    "When we study a network, it is rare that we observe the entire system perfectly in its entirety. Consider, for instance, the case of a social network, in which the nodes of our network are people within the network, and edges are whether groups of people are friends with one another in the social network. In this instance, we could approach the problem in one of two ways. On one hand, we could collect our data for literally every single person in the network. We could study a network with millions or hundreds of millions of nodes, and with an equally (if not even larger) collection of edges representing whether these people are friends.\n",
    "\n",
    "On a related note, let's imagine that we have a brain network. In this brain network, we have several areas of the brain responsible for different function: we have areas of the brain that are responsible for different aspects of eyesight, different aspects of movement, and many other physical or mental functions. In this example, our edges will be whether two brain areas can communicate with one another. By communicate, we mean that the brain area can inform processes that are performed elsewhere in the brain. For instance, if you decide to move your arm to grab a soda, some of the brain areas responsible for eyesight might need to communicate with some of the brain areas responsible for movement. A psychologist hypothesizes that for an individual with a particular mental illness, some of these brain connections might be disrupted, in that they are more or less likely to occur in an individual with the illness than without the illness. He could collect data in the form of brain networks for every single individual with and without the condition, and directly study whether some of the brain areas have different levels of communication in the group of people who have the mental illness compared to those who do not.\n",
    "\n",
    "Finally, any network you might observe might be somewhat random. Let's say in the case of the social network, that people connect with others that simply are recommended by the social network to them. You could try to exhaustively understand why the network showed them the person, and *maybe* come up with a deterministic strategy that determines perfectly who is connected to who, but this might represent learning extremely personal, and identifying, information about the people that the social network does not make accessible to scientists. Further, the brain networks might not necessarily be deterministic. There might be errors in how the brain network is observed, in that some connections might exist in the data that don't exist in reality, or vice versa.\n",
    "\n",
    "In all of these simple examples, there is a big problem: obtaining the data necessary to answer the question is simply infeasible. There are infinitely many ways in which small deviations in your network, or networks, that you actually get to analyze might have imperfections which can most efficiently be described as random. The collection of all of the possible data that could be obtained for an experiment is what is referred to as the **population**. While one expert might focus all of his attention on obtaining the entire network they hope to study, a more diligent expert might use another strategy. Rather than collecting data from every single person on the social network, or obtaining brain networks from every single person who could *potentially* have the mental illness, or collecting pristine data that always represents the underlying network perfectly, you might instead just look at a reasonably sized subset of data which you can feasibly obtain. Maybe this means collecting your social network with only $1000$ nodes instead of one hundred million, or maybe this means looking at $200$ networks instead of brain networks from every person in the United States. Or, this might mean using the network sample you have, and simply accepting that it doesn't perfectly represent the underlying system.\n",
    "\n",
    "In these cases, what you have done is you have collected a **sample**, which can be loosely defined as a set of objects which is collected from the population in some way. The sample itself is where statistics comes into play. The reason this is critical is that, when you reach a conclusion, you do not want to reach a conclusion that only applies to the specific group of people, or the specific network, that you analyzed in your sample. Statistics allows you to be as *specific as you can* about how, exactly, conclusions that you reach on the sample apply to the general population. We try to summarize the relationship in the [Figure on statistical learning](#stat-learning). Throughout this book, the sample itself, and how it relates to the underlying population, will always be in focus and should be at the forefront of your mind. We'll remind you of this later on, but it's important to note that you *can* learn lots of valuable things about a sample without using statistics *at all*, and we will be sure to note when that is the case. However, if you want your conclusions to apply more broadly to the general population rather than the specific sample you collected, a reasonable way to do that quantitatively is using statistical learning.\n",
    "\n",
    "```{figure} Images/apply.png\n",
    "---\n",
    "align: center\n",
    "name: stat-learning\n",
    "---\n",
    "As you can probably tell by now, we have been building up the same figure repeatedly throughout this section to set the tone for how network machine learning works. We have a network population, which in this case, means a set of a potentially infeasibly large network for which we cannot possibly collect the entire network. Note that the network population may have a different interpretation depending on you sepecific network question, and might be populations of many networks, or populations of networks with some level of randomness or uncertainty as to how the network was obtained. You obtain a sample of this network, and then apply your traditional network learning strategy to the sample of the network. Then, you use statistical learning strategies to *apply* to the population what you learned from the sample.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief History of Network Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks have been thought about in one form or another since the 1700's, when Leonhard Euler kickstarted the field with a famous problem about bridges in a city called called KÃ¶nigsberg. Graph theory, which network machine learning has drawn a great deal from, mainly existed as a niche subfield of mathematics for a great deal of time, slowly growing.\n",
    "\n",
    "That all changed with the development of more powerful computational tools, as well as the emergence of the internet and in particular social media platforms. Suddenly, there was a tsunami of real data flowing in, and not enough mathematical background or computational tools to understand it properly.\n",
    "\n",
    "One crucially influential application for networks was in 1996, when a graduate student at Stanford named Larry Page made the PageRank algorithm. The idea was that websites on the internet (which, in 1996, had barely formed) could be ordered into a hierarchy by \"link popularity\": a web page would rank higher the more links there were to it. Larry Page and his friend Sergey Brin realized that PageRank could be used to create a search engine -- and so they used the PageRank algorithm to found a small web searching company they called Google. \n",
    "\n",
    "As the internet became widespread and coding tools became easier to use -- Python became prevalent in machine learning, for instance, and cloud computing came into its own with Amazon's AWS and Microsoft's Azure -- thousands of papers were published in the field of network machine learning. Many of these were focused on community-finding methods, which we'll see a lot of in this book. More recently, tools for dealing with multiple networks at once started coming into their own, and easy-to-use Python packages like networkx and graspologic, which we'll use regularly, became prominent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
