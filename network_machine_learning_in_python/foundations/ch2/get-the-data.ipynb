{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data\n",
    "\n",
    "For this section, you will start to get your hands dirty with some real network datasets. Don't be afraid to walk through these examples with your laptop in a jupyter notebook.\n",
    "\n",
    "## Create the workspace\n",
    "\n",
    "### Installing the latest version of python\n",
    "\n",
    "To begin, you will need a suitable instance of python installed. For this book, you used python 3.8, but the most advanced version of python 3 at the time you are reading should do the trick. You will assume that you are using a Linux or UNIX-like operating system, such as OSX. If you have a Windows computer, you would recommend that you use the [Linux subsystem module](#https://www.laptopmag.com/articles/use-bash-shell-windows-10) to interact with the codebase. Once you have this set up and up and running, you can follow the instructions below for a Debian distribution of Linux.\n",
    "\n",
    "Once you have your computer handy, you can run the following command:\n",
    "\n",
    "```\n",
    "$ python3 -V\n",
    "Python 3.8.0\n",
    "```\n",
    "\n",
    "and verify that the python package manager, `pip`, is installed:\n",
    "\n",
    "```\n",
    "$ python3 -m pip --version\n",
    "pip 22.0.3 from /home/<user>/.virtualenvs/graph-book/lib/python3.8/site-packages/pip (python 3.8)\n",
    "```\n",
    "\n",
    "This command will print the version of `python3` and `pip` that your computer currently has installed. If it returns any version below `3.8.x` or says \"command not found\", you will need to obtain and install python 3 (along with the developer libraries and the package manager, `pip`) before continuing. The developer libraries are critical to ensuring that code upon which packages in python are based (such as the popular `numpy` numerical package) have the appropriate libraries needed to execute code written in *other* programming languages, which might be faster (for instance, `C`). \n",
    "\n",
    "If you have an apple computer using the OSX operating system, you can download and install an appropriate version of python using [The python 3 guide for OSX](#https://docs.python-guide.org/starting/install3/osx/) by first configuring `homebrew` and then installing python. This will also include `pip`. \n",
    "\n",
    "If you are using a Debian-based Linux distribution (such as Ubuntu), you can install python 3 along with the developer libraries and `pip` by typing:\n",
    "\n",
    "```\n",
    "$ sudo apt-get update\n",
    "$ sudo apt-get install -y python3-dev python3-pip\n",
    "```\n",
    "\n",
    "If you have a CentOS distribution (such as CentOS or Red Hat), you can install python3 along with the developer libraries and `pip` by typing:\n",
    "\n",
    "```\n",
    "$ sudo yum update\n",
    "$ sudo yum install -y python3-devel python3-pip\n",
    "```\n",
    "\n",
    "You should make sure that you have the most recent version of `pip` installed. To upgrade the current `pip` package, type:\n",
    "\n",
    "```\n",
    "$ python3 -m pip install --user -U pip\n",
    "Collecting pip\n",
    "[...]\n",
    "Successfully installed pip-22.0.3\n",
    "```\n",
    "\n",
    "You now have `python3`, the python package manager `pip`, and the `python3` developer libraries installed on your computer. \n",
    "\n",
    "### Installing fortran\n",
    "\n",
    "Some of the packages that you will use in the book require the system to have an appropriate version of a programming language called FORTRAN installed. FORTRAN is a numerical programming language which is very fast for mathematical computations. Fortran is included in the package `gcc`, which is a collection of compilers for many programming languages.\n",
    "\n",
    "If you have a MAC system, you can install `gfortran` with the following command:\n",
    "\n",
    "```\n",
    "$ sudo brew install gcc\n",
    "```\n",
    "\n",
    "If you have a Debian system, you can install `gfortran` with the following command:\n",
    "\n",
    "```\n",
    "$ sudo apt-get update\n",
    "$ sudo apt-get install gcc\n",
    "```\n",
    "\n",
    "If you have a CentOS system, you can install `gfortran` with the following command:\n",
    "\n",
    "```\n",
    "$ sudo yum update\n",
    "$ sudo yum install gcc\n",
    "```\n",
    "\n",
    "### Establishing a virtual environment\n",
    "\n",
    "It is often good practice in python to avoid installing many packages directly to `python3` itself. This is because packages in python do not necessarily have the same dependencies, and particular projects might require package versions that conflict with other projects you are working on. For instance, I might have a homework assignment that works only with numpy version 1.18.1, but meanwhile, a work project needs numpy version 1.22.0. For this reason, you strongly encourage you to use virtual environments.\n",
    "\n",
    "To begin working with virtual environments in python, you will need to first obtain the `virtualenv` python package:\n",
    "\n",
    "```\n",
    "$ pip3 install virtualenv\n",
    "Collecting package virtualenv\n",
    "[...]\n",
    "Successfully installed virtualenv-20.13.1\n",
    "```\n",
    "\n",
    "Once you have `virtualenv` installed, you can create your first virtual environment in python. You will first make a directory in your home directory which will allow us to keep track of your virtual environments, and then you will make a new virtual environment for the book which uses `python3`:\n",
    "\n",
    "```\n",
    "$ # create a new directory for virtual environments\n",
    "$ mkdir ~/.virtualenvs\n",
    "$ # make a new virtual environment using python3\n",
    "$ virtualenv -p python3 ~/.virtualenvs/graph-book\n",
    "```\n",
    "\n",
    "Every time you want to use run code for the book, you should first use the following command to activate the virtual environment:\n",
    "\n",
    "```\n",
    "$ # activate the virtual env\n",
    "$ source ~/.virtualenv/bin/activate\n",
    "(graph-book) $ \n",
    "```\n",
    "\n",
    "You should run this command before continuing to the next section.\n",
    "\n",
    "### Installing the dependencies for the book\n",
    "\n",
    "Next, you need to install the python package dependencies for the book. Once you have your virtual environment activated, you will next want to grab the requirements file for the graph book, which can be obtained by typing:\n",
    "\n",
    "\n",
    "```\n",
    "(graph-book) $ wget https://raw.githubusercontent.com/neurodata/graph-stats-book/master/requirements.txt\n",
    "```\n",
    "\n",
    "Next, you will want to install the appropriate python packages specified in the `requirements.txt` file, by typing:\n",
    "\n",
    "```\n",
    "(graph-book) $ pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Since you are in a virtual environment, you no longer have to worry about making sure you are installing these to `python3` or `pip3`, since the virtual environment streamlines all of these function calls for us directly. Finally, you will need to install `jupyter-lab` and the ipython kernel, using the following commands:\n",
    "\n",
    "\n",
    "```\n",
    "(graph-book) $ pip install jupyterlab ipykernel\n",
    "```\n",
    "\n",
    "We need to add your virtual environment to the ipython kernel so that jupyter lab can find it, which you can do by typing:\n",
    "\n",
    "\n",
    "```\n",
    "(graph-book) $ python -m ipykernel install --user --name=graph-book\n",
    "Installed kernelspec myenv in /home/<user>/.local/share/jupyter/kernels/graph-book\n",
    "```\n",
    "\n",
    "This will ensure that packages you install to the `graph-book` virtual environment will be findable from within jupyter.\n",
    "\n",
    "### Setting up jupyter notebook\n",
    "\n",
    "Now that you have all your packages installed, you can finally move to starting up a notebook. Let's begin by first launching jupyter lab:\n",
    "\n",
    "```\n",
    "(graph-book) $ jupyter-lab\n",
    "\n",
    "```\n",
    "\n",
    "Next, you want to create a new notebook using the `graph-book` module, as shown in Figure {numref}`graphbook-mod`:\n",
    "\n",
    "```{figure} ../../Images/jupyter.png\n",
    "---\n",
    "scale: 80%\n",
    "align: center\n",
    "name: graphbook-mod\n",
    "---\n",
    "Select the graph-book kernel, indicated by the red box.\n",
    "```\n",
    "\n",
    "To verify that your notebook has the proper software installed, you will make a code cell which simply imports the `graspologic` package, by typing the following command in the top cell:\n",
    "\n",
    "```\n",
    "import graspologic\n",
    "graspologic.__version__\n",
    "```\n",
    "\n",
    "and then pressing the `Shift + Enter` keys simultaneously. Note that this cell will take a few seconds to execute successfully. In your notebook, this will look like {numref}`graphbook-grasp`:\n",
    "\n",
    "\n",
    "```{figure} ../../Images/jupyter_grasp.png\n",
    "---\n",
    "scale: 80%\n",
    "align: center\n",
    "name: graphbook-grasp\n",
    "---\n",
    "Make sure that graspologic package imports, by verifying that it can print its version successfully.\n",
    "```\n",
    "\n",
    "From this point forward, for each section of the book, you should be able to copy and paste code snippets section by section, and successfully reproduce the executable code contained within the book. You should take care to make sure that if you take this approach that you make sure to copy and paste all code that appears in the section, since there may be modules which are imported in above cells that are assumed to have been imported in later cells. \n",
    "\n",
    "## Downloading the data\n",
    "\n",
    "When you work with network data, it is rarely the case that the *raw data* that you will use is already a network. The **raw data** is the least processed version of the data for your project, and is the information upon which the rest of your data is *derived*. A **derivative** is a piece of data or information that is *derived* from the raw data. Consider, for instance, that you are investigating emailing trends for a company, and trying to see whether employees tend to email their team members more or less frequently than people outside of their team. In this case, your raw data might be a list of emails, coupled with the sender, and the recipient, of each email. You might be responsible for *preprocessing* this data to acquire a network derivative for your later analyses.\n",
    "\n",
    "In this section, you won't worry just yet about preprocessing a raw dataset, and will instead start with some pre-prepared data. You will be working with the left and right mushroom bodies of the *drosophila* (fruit fly). You could navigate over to the github repository for the `graspologic` package and download the file directly, but it tends to be useful to do this programmatically. This is because if the data changes, you might want your analysis to automatically update and pertain to the latest and best version of the data at the time you execute your function. Further, if you intend your code to be reproducible, having a function which downloads and prepares the data in a way in which the computer can use greatly will simplify the process of disseminating your work. \n",
    "\n",
    "First, you fetch the desired data. The `DROS_NAMES` variable just stores the adjacency matrix `\"A\"` and the corresponding labels for each node `\"labels\"`, which you will use later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "MSR_ROOT = \"https://raw.githubusercontent.com/microsoft/graspologic/dev/\"\n",
    "DROS_PATH = os.path.join(\"datasets\", \"drosophila\")\n",
    "DROS_URL = MSR_ROOT + \"graspologic/datasets/drosophila\"\n",
    "\n",
    "DROS_NAMES = {\"Left\": {\"A\": \"left_adjacency.csv\", \"labels\": \"left_cell_labels.csv\"},\n",
    "              \"Right\": {\"A\": \"right_adjacency.csv\", \"labels\": \"right_cell_labels.csv\"}}\n",
    "\n",
    "def fetch_drosophila_data(dros_url=DROS_URL, dros_path=DROS_PATH,\n",
    "                         dros_names=DROS_NAMES):\n",
    "    if not os.path.isdir(dros_path):\n",
    "        os.makedirs(dros_path)\n",
    "    for (name, dictobj) in dros_names.items():\n",
    "        for (objtype, fname) in dros_names[name].items():\n",
    "            csv_path = os.path.join(dros_path, fname)\n",
    "            csv_url = os.path.join(dros_url, fname)\n",
    "            urllib.request.urlretrieve(csv_url, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you call `fetch_drosophila_data()`, it creates a new directory called `datasets/drosophila` in your workspace, and downloads the left and right adjacency matrices as csvs to this directory.\n",
    "\n",
    "Next, we'll try to load the dataset using the standard `loadtxt()` from `numpy`. You should build a small function to do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_drosophila_data(dros_path=DROS_PATH, dros_names=DROS_NAMES,\n",
    "                        return_labels=True):\n",
    "    adj_dict = {}  # make the return object\n",
    "    for (name, dictobj) in dros_names.items():\n",
    "        adj_dict[name] = {}  # dictionary for each adjacency matrix with labels\n",
    "        \n",
    "        adj_path = os.path.join(dros_path, dictobj[\"A\"])\n",
    "        with open(adj_path) as adjfile:\n",
    "            adj_dict[name][\"A\"] = np.loadtxt(adjfile)\n",
    "        \n",
    "        labels_path = os.path.join(dros_path, dictobj[\"labels\"])\n",
    "        with open(labels_path) as labelfile:\n",
    "            adj_dict[name][\"labels\"] = np.loadtxt(labelfile, dtype=str)\n",
    "    return adj_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fetch the data and load it into your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_drosophila_data()\n",
    "dataset = load_drosophila_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data we've loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are separate keys for the left and the right of the drosophila. Let's see what the left contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Left\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two keys here, one for the adjacency matrix of the network (often abbreviated with `\"A\"`, as you have here), and another called `\"labels\"`, which are attributes for each of the nodes of the adjacency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the size of each of these objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Left\"][\"A\"].shape)\n",
    "print(dataset[\"Left\"][\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `\"A\"` is a square matrix with 209 rows and columns (one for each node of the network), and `\"labels\"` is a vector with 209 elements (one for each node of the network). Let's look at the types of the entries of each element. First, we'll look at the adjacency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Left\"][\"A\"].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the adjacency matrix is a square matrix of floating-precision numerical values, and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Left\"][\"labels\"].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a vector of unicode characters. Next, let's learn some things about this adjacency matrix. You start by first breaking the dictionary into individual data objects so your code is a little more readable. In network machine learning, when dealing with a new dataset, your recommendation is to *always*, *always*, start with visualization. You typically visualize network data using a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspologic as gp\n",
    "\n",
    "Aleft=dataset[\"Left\"][\"A\"]\n",
    "labelsleft=dataset[\"Left\"][\"labels\"]\n",
    "Aright=dataset[\"Right\"][\"A\"]\n",
    "labelsright=dataset[\"Right\"][\"labels\"]\n",
    "\n",
    "ax = gp.plot.heatmap(Aleft, inner_hier_labels=labelsleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this has done is it has plotted the adjacency matrix for the left mushroom body of the drosophila as a heatmap. A heatmap is a network visualization in which the $x$ and $y$ coordinates of a given entry in the matrix indicate the pair of nodes an edge is connected to, and the color for the $(x,y)$ point in the figure indicates the weight of the edge between nodes $x$ and $y$. The nodes are also organized by their label. For instance, in this case, all of the nodes with the label `K` are in the top rows and the left columns, all the nodes with the label `P` are in the second set of rows and the second set of columns, so on and so forth.\n",
    "\n",
    "One thing you might be able to notice from the above plot is that a lot of these edge-weights are just $0$. You can compute the fraction of non-zero edges using the below code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fraction of non-zero edge-weights: {:.3f}\".format((Aleft > 0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful summary of the network is to look at a histogram for the non-zero edgeweights. A histogram shows the number of edges (on the vertical axis) which have a given edge weight range (on the horizontal axis). You can call this directly on the non-zero-edges, and it will plot a histogram of the edge weights. You will do this using seaborn's `distplot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 5))\n",
    "sns.histplot(Aleft[Aleft > 0], ax=ax);\n",
    "ax.set_xlabel(\"Edge Weight\")\n",
    "ax.set_title(\"Histogram of left mushroom body non-zero edge weights\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, most of the edge-weights are very small, and as the edge weight increases, fewer and fewer edges have the indicated edge weight. This is called a right-skewed histogram, which means that most of the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
