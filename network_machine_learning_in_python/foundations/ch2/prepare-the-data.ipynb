{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch2:prepare)=\n",
    "# Prepare the Data for Network Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's time for us to prepare our networks for machine learning algorithms. Like before, you are going to try to capture most of these with functions. This is because:\n",
    "1. Functions will make the useful data preparation code that we write usable on new networks,\n",
    "2. You will gradually build libraries of utility functions that we can prepare together into packages of their own or recycle for future projects,\n",
    "3. You can use modularize these functions into other parts of your data pipeline before it gets to your algorithm, to keep a lean module-oriented design,\n",
    "4. You can easily try different transformations of the data and evaluate which ones tend to work best.\n",
    "\n",
    "First, let's re-load the data that we have read in from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MSR_ROOT = \"https://raw.githubusercontent.com/microsoft/graspologic/dev/\"\n",
    "DROS_PATH = os.path.join(\"datasets\", \"drosophila\")\n",
    "DROS_URL = MSR_ROOT + \"graspologic/datasets/drosophila\"\n",
    "\n",
    "DROS_NAMES = {\"Left\": {\"A\": \"left_adjacency.csv\", \"labels\": \"left_cell_labels.csv\"},\n",
    "              \"Right\": {\"A\": \"right_adjacency.csv\", \"labels\": \"right_cell_labels.csv\"}}\n",
    "\n",
    "def fetch_drosophila_data(dros_url=DROS_URL, dros_path=DROS_PATH,\n",
    "                         dros_names=DROS_NAMES):\n",
    "    if not os.path.isdir(dros_path):\n",
    "        os.makedirs(dros_path)\n",
    "    for (name, dictobj) in dros_names.items():\n",
    "        for (objtype, fname) in dros_names[name].items():\n",
    "            csv_path = os.path.join(dros_path, fname)\n",
    "            csv_url = os.path.join(dros_url, fname)\n",
    "            urllib.request.urlretrieve(csv_url, csv_path)\n",
    "\n",
    "def load_drosophila_data(dros_path=DROS_PATH, dros_names=DROS_NAMES,\n",
    "                        return_labels=True):\n",
    "    adj_dict = {}  # make the return object\n",
    "    for (name, dictobj) in dros_names.items():\n",
    "        adj_dict[name] = {}  # dictionary for each adjacency matrix with labels\n",
    "        \n",
    "        adj_path = os.path.join(dros_path, dictobj[\"A\"])\n",
    "        with open(adj_path) as adjfile:\n",
    "            adj_dict[name][\"A\"] = np.loadtxt(adjfile)\n",
    "        \n",
    "        labels_path = os.path.join(dros_path, dictobj[\"labels\"])\n",
    "        with open(labels_path) as labelfile:\n",
    "            adj_dict[name][\"labels\"] = np.loadtxt(labelfile, dtype=str)\n",
    "    return adj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_drosophila_data()\n",
    "dataset = load_drosophila_data()\n",
    "Aleft, labelsleft = (dataset[\"Left\"][\"A\"], dataset[\"Left\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Most network machine learning algorithms cannot work with a node which is *isolated*, a term we will learn in [Chapter 4](#link?) which means that the node has no edges. Let's start with fixing this. We can remove isolated nodes from the network as follows:\n",
    "1. Compute the number of nodes each node connects to. This consists of summing the matrix along the rows and along the columns.\n",
    "2. Identify any nodes which are connected to zero nodes along either the rows or columns. These are the *isolated* nodes.\n",
    "3. Remove the isolated nodes from both the adjacency matrix and the labels.\n",
    "\n",
    "Let's see how this works in practice. We begin by first taking the row and column sums of each node, and then finding the sum across the rows and the columns. Next, we remove all nodes with are not connected to any other nodes (the row and column sum are both zero) from both the adjacency matrix and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_isolates(A, labels):\n",
    "    \"\"\"\n",
    "    A function which removes isolated nodes from the \n",
    "    adjacency matrix A and the labels.\n",
    "    \"\"\"\n",
    "    in_degree = A.sum(axis=0)  # sum along the rows\n",
    "    out_degree = A.sum(axis=1)  # sum along the columns\n",
    "    cum_degree = in_degree + out_degree\n",
    "    A_purged = A[~(cum_degree == 0),:]\n",
    "    A_purged = A_purged[:,~(cum_degree == 0)]\n",
    "    labels_purged = labels[~(cum_degree == 0)]\n",
    "    print(\"Purging {:d} nodes...\".format((cum_degree == 0).sum()))\n",
    "    return (A_purged, labels_purged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aleft, labelsleft = remove_isolates(Aleft, labelsleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So no isolated nodes were found, and consequently no nodes were purged. Great! What else can we do?\n",
    "\n",
    "### Handling categorical variables\n",
    "\n",
    "If you remember, the `labels` variable is a list of characters which gives a unique cell type of each mushroom body node. Unfortunately, many network machine learning algorithms prefer to work with numbers of characters. For this reason, it is instead advantageous to us if we convert these node labels instead to a numerical vector of integers. We arbitrarily just decide a mapping for the different unique labels, numbering them 1, 2, 3, or 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_labels(labels):\n",
    "    labs = {1: \"K\", 2: \"P\", 3: \"O\", 4: \"I\"}\n",
    "    # initialize empty numerical vector and dictionary\n",
    "    # to keep track of the mapping you produce\n",
    "    mapping = {}\n",
    "    numerical_labels = np.empty(labels.shape[0], dtype=int)\n",
    "    for i, lab in labs.items():\n",
    "        numerical_labels[labels == lab] = int(i)\n",
    "        mapping[lab] = i + 1\n",
    "    return numerical_labels, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labsleft, mapping = remap_labels(labelsleft)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, I labels were replaced with the value 1, K labels were replaced with the value 2, O labels were replaced with the value 3, and P labels were replaced with the value 4.\n",
    "\n",
    "To streamline the process of cleaning up the raw data, you will often need to write custom data cleaners. You will want your cleaners to work seamlessly with `sklearn`'s functions, such as pipelines, and will require you to only implement three class methods: `fit()`, `transform()`, `fit_transform()`. By adding `TransformerMixin` as a base class, we do not even have to implement the third one! If we use `BaseEstimator` as a base class, we will also obtain `get_params()` and `set_params()`, which will be useful for hyperparameter tuning steps later on. For example, here is an example cleaner class which purges the adjacency matrix of isolates and remaps the categorical labels to numbers. Note that a key step to implementing this all as cleanly as possible is that the inputs, an adjacency matrix and a vector of node labels, are passed in as a *single* tuple object. This is because `sklearn` anticipates that the return arguments from calls of `transform()` can be passed sequentially to one another, which we will see later on when we try to string several of these transformers together into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class CleanData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        (A, labels) = X\n",
    "        Acleaned, labels = remove_isolates(A, labels)\n",
    "        labels_cleaned, mapping = remap_labels(labels)\n",
    "        self.A_ = Acleaned\n",
    "        self.labels_ = labels_cleaned\n",
    "        self.mapping_ = mapping\n",
    "        return (self.A_, self.labels_)\n",
    "    \n",
    "data_cleaner = CleanData()\n",
    "(Aleft_clean, labelsleft_clean) = data_cleaner.transform((Aleft, labelsleft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge weight transformations\n",
    "\n",
    "One of the most important transformations that we will come across in network machine learning is called *edge-weight transformation*. Many networks you enounter, such as the drosophila mushroom body, will have edge weights which do not just take values of 1 or 0 (edge or no edge, a *binary* network); rather, many of the networks you come across may have discrete-weighted edges (the edges take non-negative inter values, such as 0, 1, 2, 3, ...), or decimal-weight edges (the edges take values like 0, 0.1234, 0.234, 2.4234, ...). For a number of reasons icussed later in [Chapter 4](#link?), this is often not really a desirable characteristic.  The edges in a network might be error prone, and it might only be desirable to capture one (or a few) properties about the edge weights, rather than just leave them in their raw values. Further, a lot of the techniques we come across throughout this book might not even *work* on networks which are not binary. For this reason, we need to get accustomed to transforming the edge weights to take new sets of values.\n",
    "\n",
    "There are two common approaches to transform edge weights: the first is called binarization (set all of the edges to take a value of 0 or 1), and the second is called an ordinal transformation. \n",
    "\n",
    "### Binarization of edges \n",
    "\n",
    "Binarization is quite simple: the edges in the raw network take non-binary values (0s and 1s), and you need them to for your algorithm. How do you solve this? \n",
    "\n",
    "The simplest thing to do is usually to just look at which edges take a value of zero, and keep them as zero, and then look at all of the edges which take a non-zero value, and set them to one. In effect, what this does is it just takes the original non-binary network, and converts it to a binary one. Let's take a look at how we can implement this using `graspologic`. We first look at the network before binarization, and then after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspologic as gp\n",
    "\n",
    "Aleft_bin = gp.utils.binarize(Aleft_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(18, 6))\n",
    "gp.plot.heatmap(Aleft_clean, ax=ax[0], inner_hier_labels=labelsleft, title=\"Weighted Drosophila Mushroom Body\")\n",
    "gp.plot.heatmap(Aleft_bin, ax=ax[1], inner_hier_labels=labelsleft, title=\"Binary Drosophila Mushroom Body\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That heatmap looks a whole lot different, particularly in the top left corner. What happened was that edges in the weighted drosophila mushroom body have very small edge weights in the upper left corner, which *almost* look like they are zero. But when we binarize the network, we see that this is no longer the case: all of the edge weights which are non-zero took a value of one (and are dark red) and all of the edge weights which are zero stay at zero (and are white). \n",
    "\n",
    "Another way we could have normalized these edge weights is through something called a *pass to ranks*. Through a pass to ranks, the edge weights are discarded entirely, with one exception: the edges which are non-zero are first ranked, from smallest to largest, with the largest item having a rank of one, and the smallest item having a rank of $\\frac{1}{\\text{number of non-zero edges}}$. This is called an *ordinal transformation*, in that it preserves the *orders* of the edge-weights, but discards all other information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aleft_ptr = gp.utils.pass_to_ranks(Aleft_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we plot the resulting connectome, before and after passing to ranks, as heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(18, 6))\n",
    "gp.plot.heatmap(Aleft_clean, ax=ax[0], inner_hier_labels=labelsleft, title=\"Weighted Drosophila Mushroom Body\")\n",
    "gp.plot.heatmap(Aleft_ptr, ax=ax[1], inner_hier_labels=labelsleft, title=\"Drosophila Mushroom Body, Passed to Ranks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shifted the histogram of edge-weights, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(10, 10))\n",
    "sns.histplot(Aleft_clean[Aleft_clean > 0].flatten(), ax=ax[0]);\n",
    "ax[0].set_xlabel(\"Edge weight\")\n",
    "ax[0].set_title(\"Histogram of left mushroom body non-zero edge weights\");\n",
    "\n",
    "sns.histplot(Aleft_ptr[Aleft_ptr > 0].flatten(), ax=ax[1]);\n",
    "ax[1].set_xlabel(\"ptr(Edge weight)\")\n",
    "ax[1].set_title(\"Histogram of left mushroom body, passed-to-ranks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the desirable property that it bounds the network's edge weights to be between $0$ and $1$, as we can see above, which is often crucial if we seek to compare two or more networks and the edge weights are relative in magnitude (an edge's weight might mean something in relation to another edge's weight in that same network, but an edge's weight means nothing in relation to another edge's weight in a separate network). Further, passing to ranks is not very susceptible to outliers, as we will see in later chapters. \n",
    "\n",
    "Again, we will turn the edge-weight transformation step into its own class, much like we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        A, labels = X\n",
    "        A_scaled = gp.utils.binarize(A)\n",
    "        return (A_scaled, labels)\n",
    "    \n",
    "feature_scaler = FeatureScaler()\n",
    "A_cleaned_scaled, _ = feature_scaler.transform((Aleft_clean, labelsleft_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation pipelines\n",
    "\n",
    "As you can see, there are a number of data transformations that need to be executed to prepare network data for machine learning algorithms. One thing that might be desirable is to develop a pipeline which automates the data preparation process for you. We will perform this using the `Pipeline` class from `sklearn`. The `Pipeline` class can help us apply sequences of transformations. Here is a simple pipeline for doing all of the steps we have performed so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('cleaner', CleanData()),\n",
    "    ('scaler', FeatureScaler()),\n",
    "])\n",
    "\n",
    "left_tr = num_pipeline.fit_transform((Aleft, labelsleft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline class takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers, which implement the `fit_transform()` method. In our case, this is handled directly by the `TransformerMixin` base class.\n",
    "\n",
    "When you call the `fit_transform()` method of the numerical pipeline, it calls the `fit_transform()` method on each of the transformers, and passes the output of each call as the parameter to the next call, until it reaches the final estimator, for which it just calls the `fit()` method. \n",
    "\n",
    "Next, we'll see the real handiness of the `Pipeline` module. The reason we went to lengths to define a pipeline was that we wanted to have an easily reproducible procedure that we could efficiently apply to new datasets. We'll see how we can do that using both the left and right mushroom bodies of the drosophila, which we perform below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_tr = num_pipeline.fit_transform((Aleft, labelsleft))\n",
    "right_tr = num_pipeline.fit_transform((dataset[\"Right\"][\"A\"], dataset[\"Right\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the mushroom bodies, after transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "gp.plot.heatmap(left_tr[0], inner_hier_labels=left_tr[1], title=\"Left Mushroom Body, Preprocessed\", ax=axs[0])\n",
    "gp.plot.heatmap(right_tr[0], inner_hier_labels=right_tr[1], title=\"Right Mushroom Body, Preprocessed\", ax=axs[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
