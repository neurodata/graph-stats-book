{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch2:prepare)=\n",
    "# Prepare the Data for Network Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's time for us to prepare our networks for machine learning algorithms. Like before, you are going to try to capture most of these with functions. This is because:\n",
    "1. Functions will make the useful data preparation code that we write usable on new networks,\n",
    "2. You will gradually build libraries of utility functions that we can prepare together into packages of their own or recycle for future projects,\n",
    "3. You can use modularize these functions into other parts of your data pipeline before it gets to your algorithm, to keep a lean module-oriented design,\n",
    "4. You can easily try different transformations of the data and evaluate which ones tend to work best.\n",
    "\n",
    "First, let's re-load the data that we have read in from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from graspologic.utils import import_edgelist\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# the AWS bucket the data is stored in\n",
    "BUCKET_ROOT = \"open-neurodata\"\n",
    "parcellation = \"Schaefer400\"\n",
    "FMRI_PREFIX = \"m2g/Functional/BNU1-11-12-20-m2g-func/Connectomes/\" + parcellation + \"_space-MNI152NLin6_res-2x2x2.nii.gz/\"\n",
    "FMRI_PATH = os.path.join(\"datasets\", \"fmri\")  # the output folder\n",
    "DS_KEY = \"abs_edgelist\"  # correlation matrices for the networks to exclude\n",
    "\n",
    "def fetch_fmri_data(bucket=BUCKET_ROOT, fmri_prefix=FMRI_PREFIX,\n",
    "                    output=FMRI_PATH, name=DS_KEY):\n",
    "    \"\"\"\n",
    "    A function to fetch fMRI connectomes from AWS S3.\n",
    "    \"\"\"\n",
    "    # check that output directory exists\n",
    "    if not os.path.isdir(FMRI_PATH):\n",
    "        os.makedirs(FMRI_PATH)\n",
    "    # start boto3 session anonymously\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    # obtain the filenames\n",
    "    bucket_conts = s3.list_objects(Bucket=bucket, \n",
    "                    Prefix=fmri_prefix)[\"Contents\"]\n",
    "    for s3_key in bucket_conts:\n",
    "        # get the filename\n",
    "        s3_object = s3_key['Key']\n",
    "        # verify that we are grabbing the right file\n",
    "        if name not in s3_object:\n",
    "            op_fname = os.path.join(FMRI_PATH, str(s3_object.split('/')[-1]))\n",
    "            if not os.path.exists(op_fname):\n",
    "                s3.download_file(bucket, s3_object, op_fname)\n",
    "\n",
    "def read_fmri_data(path=FMRI_PATH):\n",
    "    \"\"\"\n",
    "    A function which loads the connectomes as adjacency matrices.\n",
    "    \"\"\"\n",
    "    # import edgelists with graspologic\n",
    "    # edgelists will be all of the files that end in a csv\n",
    "    networks = [import_edgelist(fname) for fname in glob.glob(os.path.join(path, \"*.csv\"))]\n",
    "    return networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_fmri_data()\n",
    "As = read_fmri_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = As[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Most network machine learning algorithms cannot work with a node which is *isolated*, a term we will learn in [Chapter 4](#link?) which means that the node has no edges. Let's start with fixing this. We can remove isolated nodes from the network as follows:\n",
    "1. Compute the number of nodes each node connects to. This consists of summing the matrix along the rows (or columns). The network is *undirected*, a property you will learn in [properties of networks](ch4:prop-net), which means that if a node can communicate with another node, that other node can also communicate with that node\n",
    "2. Identify any nodes which are connected to zero nodes along either the rows or columns. These are the *isolated* nodes.\n",
    "3. Remove the isolated nodes from the adjacency matrix.\n",
    "\n",
    "Let's see how this works in practice. We begin by first taking the row sums of each node, which tells us how many nodes that each node is connected to. Next, we remove all nodes with are not connected to any other nodes (the row and column sum are both zero) from both the adjacency matrix and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_isolates(A):\n",
    "    \"\"\"\n",
    "    A function which removes isolated nodes from the \n",
    "    adjacency matrix A.\n",
    "    \"\"\"\n",
    "    degree = A.sum(axis=0)  # sum along the rows to obtain the node degree\n",
    "    out_degree = A.sum(axis=1)\n",
    "    A_purged = A[~(degree == 0),:]\n",
    "    A_purged = A_purged[:,~(degree == 0)]\n",
    "    print(\"Purging {:d} nodes...\".format((degree == 0).sum()))\n",
    "    return A_purged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = remove_isolates(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So no isolated nodes were found, and consequently no nodes were purged. Great! What else can we do?\n",
    "\n",
    "It turns out that a common thing to do with functional MRI connectomes is to, in fact, not look at the correlations themselves, but the *absolute correlations*. The intuition here is that, if one node is less active when another node is active, that *kind of* indicates that they are still operating together, right? Let's take a look at how we can compute the absolute correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from graphbook_code import heatmap\n",
    "\n",
    "A_abs = np.abs(A)\n",
    "fig, axs = plt.subplots(1,3, figsize=(21, 6))\n",
    "heatmap(A, ax=axs[0], title=\"Human Connectome, Raw\", vmin=np.min(A), vmax=1)\n",
    "heatmap(A_abs, ax=axs[1], title=\"Human Connectome, Absolute\", vmin=np.min(A), vmax=1)\n",
    "heatmap(A_abs - A, ax=axs[2], title=\"Difference(Absolute - Raw)\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the right-most plot, a good chunk of the values changed, which is indicated by larger differences from the raw to the real data.\n",
    "\n",
    "To streamline the process of cleaning up the raw data, you will often need to write custom data cleaners. You will want your cleaners to work seamlessly with `sklearn`'s functions, such as pipelines, and will require you to only implement three class methods: `fit()`, `transform()`, `fit_transform()`. By adding `TransformerMixin` as a base class, we do not even have to implement the third one! If we use `BaseEstimator` as a base class, we will also obtain `get_params()` and `set_params()`, which will be useful for hyperparameter tuning steps you might need in other settings. For example, here is an example cleaner class which purges the adjacency matrix of isolates and remaps the categorical labels to numbers. Note that a key step to implementing this all as cleanly as possible is that the inputs, an adjacency matrix and a vector of node labels, are passed in as a *single* tuple object. This is because `sklearn` anticipates that the return arguments from calls of `transform()` can be passed sequentially to one another, which we will see later on when we try to string several of these transformers together into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class CleanData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(\"Cleaning data...\")\n",
    "        Acleaned = remove_isolates(X)\n",
    "        A_abs_cl = np.abs(Acleaned)\n",
    "        self.A_ = A_abs_cl\n",
    "        return self.A_\n",
    "\n",
    "data_cleaner = CleanData()\n",
    "A_clean = data_cleaner.transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge weight transformations\n",
    "\n",
    "One of the most important transformations that we will come across in network machine learning is called *edge-weight transformation*. Many networks you enounter, such as the human diffusion connectome, will have edge weights which do not just take values of 1 or 0 (edge or no edge, a *binary* network); rather, many of the networks you come across may have discrete-weighted edges (the edges take non-negative inter values, such as 0, 1, 2, 3, ...), or decimal-weight edges (the edges take values like 0, 0.1234, 0.234, 2.4234, ...). For a number of reasons discussed later in {numref}`ch4:regularization`, this is often not really a desirable characteristic.  The edges in a network might be error prone, and it might only be desirable to capture one (or a few) properties about the edge weights, rather than just leave them in their raw values. Further, a lot of the techniques we come across throughout this book might not even *work* on networks which are not binary. For this reason, we need to get accustomed to transforming the edge weights to take new sets of values.\n",
    "\n",
    "There are two common approaches to transform edge weights: the first is called binarization (set all of the edges to take a value of 0 or 1), and the second is called an ordinal transformation. \n",
    "\n",
    "### Binarization of edges \n",
    "\n",
    "Binarization is quite simple: the edges in the raw network take non-binary values (values other than just 0s and 1s), and you need them to be 0s and 1s for your algorithm. How do you solve this? \n",
    "\n",
    "The simplest thing to do is usually to just look at which edges take a value of zero, and keep them as zero, and then look at all of the edges which take a non-zero value, and set them to one. In effect, what this does is it just takes the original non-binary network, and converts it to a binary one. Let's take a look at how we can implement this using `graspologic`. We first look at the network before binarization, and then after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.utils import binarize\n",
    "\n",
    "threshold = 0.4\n",
    "A_bin = binarize(A_clean > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(18, 6))\n",
    "heatmap(A_clean, ax=ax[0], title=\"Weighted Connectome\")\n",
    "heatmap(A_bin, ax=ax[1], title=\"Binarized Connectome\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the heat map looks like it has maintained a lot of the \"general idea\" of the heatmap in the left, but it's a lot simpler. Whereas the edge weights in the left plot were *continuous*, we've now *binarized* the edges of the network to only take two possible values ($0$ or $1$). This has the effect of potentially reducing the *variance* (since we no longer will need as complicated of descriptions to summarize the edge-weights), but potentially increasing the *bias* (since we have simplified our data and have therefore potentially \"thrown away\" information that might be important).\n",
    "\n",
    "Another way we could have normalized these edge weights is through something called a *pass to ranks*. Through a pass to ranks, the edge weights are discarded entirely, with one exception: the edges which are non-zero are first ranked, from smallest to largest, with the largest item having a rank of one, and the smallest item having a rank of $\\frac{1}{\\text{number of non-zero edges}}$. This is called an *ordinal transformation*, in that it preserves the *orders* of the edge-weights, but discards all other information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.utils import pass_to_ranks\n",
    "\n",
    "A_ptr = pass_to_ranks(A_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we plot the resulting connectome, before and after passing to ranks, as heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(18, 6))\n",
    "heatmap(A_clean, ax=ax[0], title=\"Weighted connectome\")\n",
    "heatmap(A_bin, ax=ax[1], title=\"Binary connectome\")\n",
    "heatmap(A_ptr, ax=ax[2], title=\"Ranked connectome\", vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shifted the histogram of edge-weights, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(10, 10))\n",
    "sns.histplot(A_clean[A_clean > 0].flatten(), ax=ax[0]);\n",
    "ax[0].set_xlabel(\"Edge weight\")\n",
    "ax[0].set_title(\"Histogram of human connectome non-zero edge weights\");\n",
    "\n",
    "sns.histplot(A_ptr[A_ptr > 0].flatten(), ax=ax[1]);\n",
    "ax[1].set_xlabel(\"ptr(Edge weight)\")\n",
    "ax[1].set_title(\"Histogram of human connectome, passed-to-ranks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the desirable property that it bounds the network's edge weights to be between $0$ and $1$, as we can see above, which is often crucial if we seek to compare two or more networks and the edge weights are relative in magnitude (an edge's weight might mean something in relation to another edge's weight in that same network, but an edge's weight means nothing in relation to another edge's weight in a separate network). Further, passing to ranks is not very susceptible to outliers, as we will see in later chapters. \n",
    "\n",
    "Again, we will turn the edge-weight transformation step into its own class, much like we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"Scaling edge-weights...\")\n",
    "        A_scaled = gp.utils.pass_to_ranks(X)\n",
    "        return (A_scaled)\n",
    "    \n",
    "feature_scaler = FeatureScaler()\n",
    "A_cleaned_scaled = feature_scaler.transform(A_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation pipelines\n",
    "\n",
    "As you can see, there are a number of data transformations that need to be executed to prepare network data for machine learning algorithms. One thing that might be desirable is to develop a pipeline which automates the data preparation process for you. We will perform this using the `Pipeline` class from `sklearn`. The `Pipeline` class can help us apply sequences of transformations. Here is a simple pipeline for doing all of the steps we have performed so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('cleaner', CleanData()),\n",
    "    ('scaler', FeatureScaler()),\n",
    "])\n",
    "\n",
    "A_xfm = num_pipeline.fit_transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline class takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers, which implement the `fit_transform()` method. In our case, this is handled directly by the `TransformerMixin` base class.\n",
    "\n",
    "When you call the `fit_transform()` method of the numerical pipeline, it calls the `fit_transform()` method on each of the transformers, and passes the output of each call as the parameter to the next call, until it reaches the final estimator, for which it just calls the `fit()` method. \n",
    "\n",
    "Next, we'll see the real handiness of the `Pipeline` module. The reason we went to lengths to define a pipeline was that we wanted to have an easily reproducible procedure that we could efficiently apply to new connectomes. Let's see how easy it is to apply to the second subject in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_xfm2 = num_pipeline.fit_transform(As[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the connectome, after transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "heatmap(As[0], title=\"Connectome 2, Raw\", ax=axs[0], vmin=np.min(As[0]), vmax=1)\n",
    "heatmap(A_xfm2, title=\"Connectome 2, Preprocessed\", ax=axs[1], vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
