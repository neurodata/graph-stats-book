{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualize the Data to Gain Insights\n",
    "\n",
    "So, now we've got a codebase accumulated to read in the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MSR_ROOT = \"https://raw.githubusercontent.com/microsoft/graspologic/dev/\"\n",
    "DROS_PATH = os.path.join(\"datasets\", \"drosophila\")\n",
    "DROS_URL = MSR_ROOT + \"graspologic/datasets/drosophila\"\n",
    "\n",
    "DROS_NAMES = {\"Left\": {\"A\": \"left_adjacency.csv\", \"labels\": \"left_cell_labels.csv\"},\n",
    "              \"Right\": {\"A\": \"right_adjacency.csv\", \"labels\": \"right_cell_labels.csv\"}}\n",
    "\n",
    "def fetch_drosophila_data(dros_url=DROS_URL, dros_path=DROS_PATH,\n",
    "                         dros_names=DROS_NAMES):\n",
    "    if not os.path.isdir(dros_path):\n",
    "        os.makedirs(dros_path)\n",
    "    for (name, dictobj) in dros_names.items():\n",
    "        for (objtype, fname) in dros_names[name].items():\n",
    "            csv_path = os.path.join(dros_path, fname)\n",
    "            csv_url = os.path.join(dros_url, fname)\n",
    "            urllib.request.urlretrieve(csv_url, csv_path)\n",
    "\n",
    "def load_drosophila_data(dros_path=DROS_PATH, dros_names=DROS_NAMES,\n",
    "                        return_labels=True):\n",
    "    adj_dict = {}  # make the return object\n",
    "    for (name, dictobj) in dros_names.items():\n",
    "        adj_dict[name] = {}  # dictionary for each adjacency matrix with labels\n",
    "        \n",
    "        adj_path = os.path.join(dros_path, dictobj[\"A\"])\n",
    "        with open(adj_path) as adjfile:\n",
    "            adj_dict[name][\"A\"] = np.loadtxt(adjfile)\n",
    "        \n",
    "        labels_path = os.path.join(dros_path, dictobj[\"labels\"])\n",
    "        with open(labels_path) as labelfile:\n",
    "            adj_dict[name][\"labels\"] = np.loadtxt(labelfile, dtype=str)\n",
    "    return adj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_drosophila_data()\n",
    "dataset = load_drosophila_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got some code to prepare the data for machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspologic as gp\n",
    "\n",
    "def remove_isolates(A, labels):\n",
    "    \"\"\"\n",
    "    A function which removes isolated nodes from the \n",
    "    adjacency matrix A and the labels.\n",
    "    \"\"\"\n",
    "    in_degree = A.sum(axis=0)  # sum along the rows\n",
    "    out_degree = A.sum(axis=1)  # sum along the columns\n",
    "    cum_degree = in_degree + out_degree\n",
    "    A_purged = A[~(cum_degree == 0),:]\n",
    "    A_purged = A_purged[:,~(cum_degree == 0)]\n",
    "    labels_purged = labels[~(cum_degree == 0)]\n",
    "    print(\"Purging {:d} nodes...\".format((cum_degree == 0).sum()))\n",
    "    return (A_purged, labels_purged)\n",
    "\n",
    "def remap_labels(labels):\n",
    "    labs = {1: \"K\", 2: \"P\", 3: \"O\", 4: \"I\"}\n",
    "    # initialize empty numerical vector and dictionary\n",
    "    # to keep track of the mapping you produce\n",
    "    mapping = {}\n",
    "    numerical_labels = np.empty(labels.shape[0], dtype=int)\n",
    "    for i, lab in labs.items():\n",
    "        numerical_labels[labels == lab] = int(i)\n",
    "        mapping[lab] = i + 1\n",
    "    return numerical_labels, mapping\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class CleanData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        (A, labels) = X\n",
    "        Acleaned, labels = remove_isolates(A, labels)\n",
    "        labels_cleaned, mapping = remap_labels(labels)\n",
    "        self.A_ = Acleaned\n",
    "        self.labels_ = labels_cleaned\n",
    "        self.mapping_ = mapping\n",
    "        return (self.A_, self.labels_)\n",
    "    \n",
    "class FeatureScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        A, labels = X\n",
    "        A_scaled = gp.utils.binarize(A)\n",
    "        return (A_scaled, labels)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('cleaner', CleanData()),\n",
    "    ('scaler', FeatureScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Aleft, labelsleft) = num_pipeline.fit_transform((dataset[\"Left\"][\"A\"], dataset[\"Left\"][\"labels\"]))\n",
    "(Aright, labelsright) = num_pipeline.fit_transform((dataset[\"Right\"][\"A\"], dataset[\"Right\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you are left with two adjacency matrices and node labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "gp.plot.heatmap(Aleft, inner_hier_labels=labelsleft, title=\"Left Mushroom Body, Preprocessed\", ax=axs[0])\n",
    "gp.plot.heatmap(Aright, inner_hier_labels=labelsright, title=\"Right Mushroom Body, Preprocessed\", ax=axs[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's next? Now is where the fun starts: you are ready to tackle some network machine learning algorithms.\n",
    "\n",
    "## Choosing an appropriate network machine learning model\n",
    "\n",
    "The most crucial step in network machine learning is figuring out an appropriate *model* for your data. A *statistical model*, which we will become more accustomed to in [Chapter 5](#link?), are sets of assumptions that describe how we think the data behaves.\n",
    "\n",
    "In the above plot, what we immediately notice is that in general, nodes with a label of 2 tend to show similar behavior to one another: they are well connected with nodes labeled 2, 3, or 1 in the top right of the heatmap, or nodes labelled 4 and 1 in the bottom left of the heatmap. This applies to both the left and right mushroom bodies. As we keep going down the groups of nodes, we see this pattern continue: nodes labelled 4 tend to be completely unconnected to any nodes except for nodes labelled 2 in the bottom left of the heatmap, and are not connected to any other nodes in the entire network. This pattern continues for the other nodes in the network. If we were being naive, we might just say that the left mushroom body and the right mushroom body look the same, end of story. However, do they *actually*? As it turns out, there are some pretty big differences!\n",
    "\n",
    "As it turns out, this pattern of nodes having similar patterns as other nodes in the same group is a statistical model you will learn about later on: the [stochastic block model](#link?) (SBM). You will first fit an SBM to the left and right mushroom bodies, using `graspologic`'s `SBMEstimator` class. The `SBMEstimator` class takes an adjacency matrix and the node labels as leading arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.models import SBMEstimator\n",
    "\n",
    "left_sbm = SBMEstimator()\n",
    "left_sbm.fit(Aleft, labelsleft);\n",
    "right_sbm = SBMEstimator()\n",
    "right_sbm.fit(Aright, labelsright);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the SBM does is basically, for each block of edges in the adjacency matrix, it computes the fraction of the edges which exist (an estimate of the \"probability\" of an edge). This is organized into what is called the block matrix, which is the `block_p_` attribute of the `SBMEstimator`. We visualize these block probability matrices also as heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_block(X, blockname=\"Node Group\", blocktix=[0.5, 1.5, 2.5, 3.5],\n",
    "               blocklabs=[\"1\", \"2\", \"3\", \"4\"], ax=None, title=\"\"):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=0, vmax=1, annot=True)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=blockname, xlabel=blockname)\n",
    "        ax.set_yticks(blocktix)\n",
    "        ax.set_yticklabels(blocklabs)\n",
    "        ax.set_xticks(blocktix)\n",
    "        ax.set_xticklabels(blocklabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(27, 6))\n",
    "plot_block(left_sbm.block_p_, title=\"Left Block Matrix\", ax=axs[0])\n",
    "plot_block(right_sbm.block_p_, title=\"Right Block Matrix\", ax=axs[1])\n",
    "plot_block(left_sbm.block_p_ - right_sbm.block_p_, title=\"Left - Right\", ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block matrices look quite similar, at first glance. However, as you can see, some of the blocks are pretty different in the right-most plot, such as the block (node group 4, node group 3). Network machine learning is all about cleaning up this ambiguous language here. Can we be a bit more precise about what we mean by \"pretty different\"?\n",
    "\n",
    "As we will see in [Chapter 9](#link?), there are many ways in which we can, in fact, be ultra precise! In fact, using some machine learning techniques, we can even put a probability on the chances we are wrong if we say the matrices are \"different\"! Putting some precision on the words \"same\" or \"different\" (how similar? how dissimilar?) is called hypothesis testing, which you will learn all about in the [Applications](#link?) section of the book. In the next code block, we will perform a simple test of how similar or different the left and right mushroom bodies are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pkg.stats import stochastic_block_test\n",
    "\n",
    "nl = Aleft.shape[0]\n",
    "nr = Aright.shape[0]\n",
    "density_left = Aleft.sum()*1/(nl*(nl-1))\n",
    "density_right = Aright.sum()*1/(nr*(nr-1))\n",
    "\n",
    "null_odds = density_left/density_right\n",
    "stat, pvalue, misc = stochastic_block_test(\n",
    "    Aleft, Aright, labels1=labelsleft, labels2=labelsright, method=\"fisher\",\n",
    "    null_odds=null_odds\n",
    ")\n",
    "Pval_mtx = misc[\"uncorrected_pvalues\"]\n",
    "\n",
    "right_sbm.block_p_adj_ = right_sbm.block_p_*null_odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the left and right blocks again, and we also include the probability that we would be *wrong* to say that the two blocks are different, the $p$-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(27, 6))\n",
    "plot_block(left_sbm.block_p_, title=\"Left Block Matrix\", ax=axs[0])\n",
    "plot_block(right_sbm.block_p_adj_, title=\"Right Block Matrix, Adjusted\", ax=axs[1])\n",
    "plot_block(Pval_mtx, title=\"p-values per-block\", ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it turns out that four of the blocks are pretty radically different: we can see that we have $p$-values of $0.0064$, $9.1 \\times 10^{-6}$, $1.6 \\times 10^{-9}$, and $1.4 \\times 10^{-23}$! Remember that these are the probabilities that we would be *wrong* to say that the two blocks are different, so this means that with a *very* high probability, four of the blocks are different!\n",
    "\n",
    "We have now discovered a new neurobiological insight, that the left and right mushroom bodies of the drosophila are not similar! This provides us with evidence that the fly mushroom body, and consequently, its *brain*, are bilaterally *asymmetric*: the left and right sides of the brain have unique functionality. We know that human beings' have asymmetrical brains, but it is incredible to think that with a few snippets of code, we can show that fruit flies share this property too. At the time of the writing of this textbook, this result hasn't even been discovered yet!\n",
    "\n",
    "## Try it out!\n",
    "\n",
    "Hopefully this chapter gave you a small scale peek at what a network machine learning project looks like, and showed you a brief introduction to some tools you can use to gain novel insights from your network data. While what we did in this chapter was relatively straightforward, the process from obtaining your data to choosing appropriate network machine learning problems can be extremely arduous! In fact, as a network machine learning scientist, you might find that just obtaining your data in a useful form (a network) and cleaning the data to be usable might take an *enormous* chunk of your time!\n",
    "\n",
    "If you haven't already done so, now is a fantastic time to grab your laptop, select a network dataset you are interested in, and try to work through the whole process from A to Z. If you need some pointers, the `graspologic` package [makes several datasets available to you](#https://microsoft.github.io/graspologic/latest/reference/reference/datasets.html). We'd recommend working through the contents of this book by first using the example data that is presented in the chapter, and then try to apply the techniques to a network dataset of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-book",
   "language": "python",
   "name": "graph-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
