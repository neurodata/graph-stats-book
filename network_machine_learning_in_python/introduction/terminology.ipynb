{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bright-server",
   "metadata": {},
   "source": [
    "# Terminology and Math Refresher\n",
    "\n",
    "In this section, we outline some background terminology which will come up repeatedly throughout the book. This section attempts to standardize some background material that we think is useful going in. It is important to realize that many of the concepts discussed below are only crucial for understanding the advanced, starred sections. If you aren't familiar with some (or any!) of the below concepts, we don't think this would detract from your understanding of the broader content.\n",
    "\n",
    "## Mathematical Concepts\n",
    "\n",
    "| Symbol | Definition | Description/Example |\n",
    "| --- | --- | --- |\n",
    "| $x$ | A number | $x = 5$ |\n",
    "| $\\vec x$ | A column vector | $x = (5, 2)$ |\n",
    "| $x_{i}$ | The $i^{th}$ element of a vector | $x_2 = 2$ |\n",
    "| $\\sum_{n = i}^j x_n$ | A sum from the $i^{th}$ to $j^{th}$ element of $x$ | $\\sum_{n = 1}^2 x_n = 7$ |\n",
    "| $\\prod_{n = i}^j x_n$ | A product from the $i^{th}$ to the $j^{th}$ element of $x$ | $\\prod_{n = 1}^2 x_n = 10$ |\n",
    "| $X$ | A matrix | $X = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}$ |\n",
    "|$x_{ij}$ | The $(i,j)^{th}$ element of a matrix | $x_{2,2} = 4$ |\n",
    "| $nrow(X)$ | The number of rows of $X$ | $nrow(X) = 2$ |\n",
    "| $ncol(X)$ | The number of columns of $X$ | $ncol(X) = 2$ |\n",
    "\n",
    "## Mathematical Operations\n",
    "| Operation | Name | Definition |\n",
    "| --- | --- | --- |\n",
    "| $\\vec x^\\top \\vec y$ | The euclidean inner product | $\\sum_{i = 1}^n x_i y_i$ |\n",
    "| $C = AB$ | Matrix multiplication | $c_{ij} = \\sum_k a_{ik}b_{kj}$ |\n",
    "| $\\vert\\vert\\vec x\\vert\\vert_k$ | The $k$-norm of $\\vec x$ | $\\left(\\sum_{i = 1}^n |x_i|^k\\right)^{1/k}$ |\n",
    "| $\\vert\\vert\\vec x - \\vec y\\vert\\vert_2$ | The Euclidean distance between $\\vec x$ and $\\vec y$ | $\\sqrt{\\sum_{i = 1}^n (x_i - y_i)^2}$ |\n",
    "| $\\vert\\vert X\\vert\\vert_F$ | The Frobenius norm of $X$ | $\\sqrt{\\sum_{i = 1}^n \\sum_{j = 1}^m x_{i,j}^2}$ |\n",
    "\n",
    "To understand the technical details of numerous sections in this book, it would also be useful to be familiar with the concept of the singular value decomposition (svd), a technique for decomposing matrices. The linear algebra book \"Numerical Linear Algebra\" by Nick Trefethan  provides a succinct background in all of the properties of the svd, and many we will not touch on. We would recommend taking a look at Lectures 1 through 5 if you haven't seen the singular value decomposition before, which will refresh your memory on the basic properties of matrices, such as the concepts of matrix multipliplication and other manipulations, and give you some detailed background in the singular value decomposition.\n",
    "\n",
    "## Probability/Statistics Concepts\n",
    "\n",
    "| Symbol/Concept | Explanation | Example |\n",
    "| --- | --- | --- |\n",
    "| $\\mathbf x$ | A random variable | $\\mathbf x$ takes the value $0$ or $1$ each with probability $0.5$ |\n",
    "| $\\vec{\\mathbf x}$ | A random vector | $\\vec{\\mathbf x} = (\\mathbf x_1, \\mathbf x_2)$ |\n",
    "| $\\mathbf X$ | A random matrix | $\\mathbf X = \\begin{bmatrix} \\mathbf x_{11} & \\mathbf x_{12} \\\\ \\mathbf x_{21} & \\mathbf x_{22} \\end{bmatrix}$ |\n",
    "| $Pr(A)$ | Probability that a statement $A$ happens | $Pr(\\mathbf x = 1) = 0.5$ |\n",
    "| $Bern(p)$ | The Bernoulli distribution with probability $p$ | If $\\mathbf x$ is a $Bern(p)$ random variable, $Pr(\\mathbf x = 1) = p, Pr(\\mathbf x = 0) = 1 - p$ | \n",
    "| Conditional probability | the probability of one event, given that another event occurs | $Pr(A \\vert B) = \\frac{Pr(A, B)}{Pr(B)}$ |\n",
    "\n",
    "For most of this work, we will assume a fairly limited background in probability and statistics, with two notable exceptions. To understand the technical details of [Chapter 7](#link?) on the theoretical results, a working familiarity with the common notations and concepts from undergraduate and graduate statistics will be valuable. You should have some level of familiarity with the concepts of multivariate and matrix-valued random variables, and desirable properties of estimators for random quantities, such as unbiasedness and asymptotic consistency. If you are unfamiliar with these concepts and wish to tackle [Sections 7.1 and 7.2](#link?), it would be valuable to brush up on these ahead of time. For [Chapter 7.3](#link?), we would recommend a foundational background in high-dimensional statistical inference and concentration phenomena. The results of [Section 7.3](#link?) are largely provided for posterity to give you resources to understand *why* the techniques we focus on in this book work, and are neither strictly essential to having an intuitional understanding of the techniques and how to apply them nor essential to deriving value from this work. We believe that for many audience members, the entirety of Chapter 7 can be omitted without missing too much for most applications of network machine learning. We will provide a detailed background sheet of the concepts to brush up on as these chapters arise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-haven",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-webmaster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
