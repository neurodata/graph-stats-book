{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "driving-russell",
   "metadata": {},
   "source": [
    "# Network Models\n",
    "\n",
    "Probably the easiest kinds of statistical models for us to think about are the *network models*. These types of models (like the name imples) describe the random processes which you'd find when you're only looking at one network. We can have models which assume all of the nodes connect to each other essentially randomly, models which assume that the nodes are in distinct *communities*, and many more.\n",
    "\n",
    "The important realization to make about statistical models is that a model is *not* a network: it's the random process that *creates* a network. You can sample from a model a bunch of times, and because it's a random process, you'll end up with networks that look a little bit different each time -- but if you sampled a lot of networks and then averaged them, then you'd likely be able to get a reasonable ballpark estimation of what the model that they come from looks like.  \n",
    "\n",
    "Let's pretend that we have a network, and the network is unweighted (meaning, we only have edges or not-edges) and undirected (meaning, edges connect nodes both ways). It'd have an adjacency matrix which consists of only 1's and 0's, because the only information we care about is whether there's an edge or not. The model that generated this network is pretty straightforward: there's just some universal probability that each node connects to each other node, and there are 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-validation",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from graspologic.simulations import er_np\n",
    "from graphbook_code import binary_heatmap\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "n = 10\n",
    "p = .5\n",
    "A = er_np(n, p)\n",
    "binary_heatmap(A, ax=ax, yticklabels=5, linewidths=1, linecolor=\"black\", title=\"A small, simple network\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-establishment",
   "metadata": {},
   "source": [
    "This small, simple network is one of many possible networks that we can generate with this model. Here are some more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-pizza",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "for ax in axs.flat:\n",
    "    A = er_np(n, p)\n",
    "    hmap = binary_heatmap(A, ax=ax, yticklabels=5, linewidths=1, linecolor=\"black\")\n",
    "plt.suptitle(\"Three small, simple networks\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-mirror",
   "metadata": {},
   "source": [
    "One reasonable question to ask is how *many* possible networks we could make in this simple scenario? We've already made four, and it seems like there are more that this model could potentially generate.\n",
    "\n",
    "As it turns out, \"more\" is a pretty massive understatement. To actually figure out the number, think about the first potential edge: there are two possibilities (edge or no edge), so you can generate two networks from a one-node model. Now, let's add an additional node. For each of the first two possibilities, there are two more -- so there are $2 \\times 2 = 4$ total possible networks. Every node that we add doubles the number of networks - and since a network with $n$ nodes has $n \\times n$ edges, the total number of possible networks ends up being $2^{n \\times n} = 2^{n^2}$! So this ten-node model can generate $2^{10^2} = 2^{100}$ networks, which is, when you think carefully, an absurdly, ridiculously big number.\n",
    "\n",
    "Throughout many of the succeeding sections, we will attempt to make the content accessible to readers with, and without, a more technical background. To this end, we have added sections with trailing asterisks (\\*). While we believe these sections build technical depth, we don't think they are critical to understanding many of the core ideas for network machine learning. In contrast with unstarred sections, these sections will assume familiarity with more advanced mathematical and probability concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-virgin",
   "metadata": {},
   "source": [
    "## Foundation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-terrorism",
   "metadata": {},
   "source": [
    "To understand network models, it is crucial to understand the concept of a network as a random quantity, taking a probability distribution. We have a realization $A$, and we think that this realization is random in some way. Stated another way, we think that there exists a network-valued random variable $\\mathbf A$ that governs the realizations we get to see. Since $\\mathbf A$ is a random variable, we can describe it using a probability distribution. The distribution of the random network $\\mathbf A$ is the function $\\mathbb P$ which assigns probabilities to every possible configuration that $\\mathbf A$ could take. Notationally, we write that $\\mathbf A \\sim \\mathbb P$, which is read in words as \"the random network $\\mathbf A$ is distributed according to $\\mathbb P$.\" \n",
    "\n",
    "In the preceding description, we made a fairly substantial claim: $\\mathbb P$ assigns probabilities to every possible configuration that realizations of $\\mathbf A$, denoted by $A$, could take. How many possibilities are there for a network with $n$ nodes? Let's limit ourselves to simple networks: that is, $A$ takes values that are unweighted ($A$ is *binary*), undirected ($A$ is *symmetric*), and loopless ($A$ is *hollow*). In words, $\\mathcal A_n$ is the set of all possible adjacency matrices $A$ that correspond to simple networks with $n$ nodes. Stated another way: every $A$ that is found in $\\mathcal A$ is a *binary* $n \\times n$ matrix ($A \\in \\{0, 1\\}^{n \\times n}$), $A$ is symmetric ($A = A^\\top$), and $A$ is *hollow* ($diag(A) = 0$, or $A_{ii} = 0$ for all $i = 1,...,n$). We describe $\\mathcal A_n$ as:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal A_n = \\left\\{A : A \\textrm{ is an $n \\times n$ matrix with $0$s and $1$s}, A\\textrm{ is symmetric}, A\\textrm{ is hollow}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "To summarize the statement that $\\mathbb P$ assigns probabilities to every possible configuration that realizations of $\\mathbf A$ can take, we write that $\\mathbb P : \\mathcal A_n \\rightarrow [0, 1]$. This means that for any $A \\in \\mathcal A_n$ which is a possible realization of a random network $\\mathbf A$, that $\\mathbb P(\\mathbf A = A)$ is a probability (it takes a value between $0$ and $1$). If it is completely unambiguous what the random variable $\\mathbf A$ refers to, we might abbreviate $\\mathbb P(\\mathbf A = A)$ with $\\mathbb P(A)$. This statement can alternatively be read that the probability that the random variable $\\mathbf A$ takes the value $A$ is $\\mathbb P(A)$. Finally, let's address that question we had in the previous paragraph. How many possible adjacency matrices are in $\\mathcal A_n$?\n",
    "\n",
    "Let's imagine what just one $A \\in \\mathcal A_n$ can look like. Note that each matrix $A$ has $n \\times n = n^2$ possible entries, in total, since $A$ is an $n \\times n$ matrix. There are $n$ possible self-loops for a network, but since $\\mathbf A$ is simple, it is loopless. This means that we can subtract $n$ possible edges from $n^2$, leaving us with $n^2 - n = n(n-1)$ possible edges that might not be unconnected. If we think in terms of a realization $A$, this means that we are ignoring the diagonal entries $a_{ii}$, for all $i \\in [n]$.  Remember that a simple network is also undirected. In terms of the realization $A$, this means that for every pair $i$ and $j$, that $a_{ij} = a_{ji}$. If we were to learn about an entry in the upper triangle of $A$ where $a_{ij}$ is such that $j > i$, note that we have also learned what $a_{ji}$ is, too. This symmetry of $A$ means that of the $n(n-1)$ entries that are not on the diagonal of $A$, we would, in fact, \"double count\" the possible number of unique values that $A$ could have. This means that $A$ has a total of $\\frac{1}{2}n(n - 1)$ possible entries which are *free*, which is equal to the expression $\\binom{n}{2}$. Finally, note that for each entry of $A$, that the adjacency can take one of two possible values: $0$ or $1$. To write this down formally, for every possible edge which is randomly determined, we have *two* possible values that edge could take. Let's think about building some intuition here:\n",
    "1. If $A$ is $2 \\times 2$, there are $\\binom{2}{2} = 1$ unique entry of $A$, which takes one of $2$ values. There are $2$ possible ways that $A$ could look:\n",
    "\\begin{align*}\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 \\\\\n",
    "        1 & 0\n",
    "    \\end{bmatrix}\\textrm{ or }\n",
    "    \\begin{bmatrix}\n",
    "        0 & 0 \\\\\n",
    "        0 & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "2. If $A$ is $3 \\times 3$, there are $\\binom{3}{2} = \\frac{3 \\times 2}{2} = 3$ unique entries of $A$, each of which takes one of $2$ values. There are $8$ possible ways that $A$ could look:\n",
    "\\begin{align*}\n",
    "&\\begin{bmatrix}\n",
    "    0 & 1 & 1 \\\\\n",
    "    1 & 0 & 1 \\\\\n",
    "    1 & 1 & 0\n",
    "    \\end{bmatrix}\\textrm{ or }\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 & 0 \\\\\n",
    "    1 & 0 & 1 \\\\\n",
    "    0 & 1 & 0\n",
    "    \\end{bmatrix}\\textrm{ or }\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 1 \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "    1 & 1 & 0\n",
    "    \\end{bmatrix}\n",
    "    \\textrm{ or }\\\\\n",
    "&\\begin{bmatrix}\n",
    "    0 & 1 & 1 \\\\\n",
    "    1 & 0 & 0 \\\\\n",
    "    1 & 0 & 0\n",
    "    \\end{bmatrix}\\textrm{ or }\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    1 & 0 & 0\n",
    "    \\end{bmatrix}\\textrm{ or }\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "    0 & 1 & 0\n",
    "    \\end{bmatrix}\\textrm{ or }\\\\\n",
    "&\\begin{bmatrix}\n",
    "    0 & 1 & 0 \\\\\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\\textrm{ or }\n",
    "\\begin{bmatrix}\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "How do we generalize this to an arbitrary choice of $n$? The answer is to use *combinatorics*. Basically, the approach is to look at each entry of $A$ which can take different values, and multiply the total number of possibilities by $2$ for every element which can take different values. Stated another way, if there are $2$ choices for each one of $x$ possible items, we have $2^x$ possible ways in which we could select those $x$ items. But we already know how many different elements there are in $A$, so we are ready to come up with an expression for the number. In total, there are $2^{\\binom n 2}$ unique adjacency matrices in $\\mathcal A_n$. Stated another way, the *cardinality* of $\\mathcal A_n$, described by the expression $|\\mathcal A_n|$, is $2^{\\binom n 2}$. The **cardinality** here just means the number of elements that the set $\\mathcal A_n$ contains. When $n$ is just $15$, note that $\\left|\\mathcal A_{15}\\right| = 2^{\\binom{15}{2}} = 2^{105}$, which when expressed as a power of $10$, is more than $10^{30}$ possible networks that can be realized with just $15$ nodes! As $n$ increases, how many unique possible networks are there? In the below figure, look at the value of $|\\mathcal A_n| = 2^{\\binom n 2}$ as a function of $n$. As we can see, as $n$ gets big, $|\\mathcal A_n|$ grows really really fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-pension",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from math import comb\n",
    "\n",
    "\n",
    "n = np.arange(2, 51)\n",
    "logAn = np.array([comb(ni, 2) for ni in n])*np.log10(2)\n",
    "\n",
    "ax = sns.lineplot(x=n, y=logAn)\n",
    "ax.set_title(\"\")\n",
    "ax.set_xlabel(\"Number of Nodes\")\n",
    "ax.set_ylabel(\"Number of Possible Graphs $|A_n|$ (log scale)\")\n",
    "ax.set_yticks([50, 100, 150, 200, 250, 300, 350])\n",
    "ax.set_yticklabels([\"$10^{{{pow:d}}}$\".format(pow=d) for d in [50, 100, 150, 200, 250, 300, 350]])\n",
    "ax;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-munich",
   "metadata": {},
   "source": [
    "So, now we know that we have probability distributions on networks, and a set $\\mathcal A_n$ which defines all of the adjacency matrices that every probability distribution must assign a probability to. Now, just what is a network model? A **network model** is a set $\\mathcal P$ of probability distributions on $\\mathcal A_n$. Stated another way, we can describe $\\mathcal P$ to be:\n",
    "\\begin{align*}\n",
    "    \\mathcal P &\\subseteq \\{\\mathbb P: \\mathbb P\\textrm{ is a probability distribution on }\\mathcal A_n\\}\n",
    "\\end{align*}\n",
    "\n",
    "In general, we will simplify $\\mathcal P$ through something called *parametrization*. We define $\\Theta$ to be the set of all possible parameters of the random network model, and $\\theta \\in \\Theta$ is a particular parameter choice that governs the parameters of a specific network-valued random variaable $\\mathbf A$. In this case, we will write $\\mathcal P$ as the set:\n",
    "\\begin{align*}\n",
    "    \\mathcal P(\\Theta) &= \\left\\{\\mathbb P_\\theta : \\theta \\in \\Theta\\right\\}\n",
    "\\end{align*}\n",
    "If $\\mathbf A$ is a random network that follows a network model, we will write that $\\mathbf A \\sim \\mathbb P_\\theta$, for some choice $\\theta$. We will often use the shorthand $\\mathbf A \\sim \\mathbb P$.\n",
    "\n",
    "If you are used to traditional univariate or multivariate statistical modelling, an extremely natural choice for when you have a discrete sample space (like $\\mathcal A_n$, which is discrete because we can count it) would be to use a categorical model. In the categorical model, we would have a single parameter for all possible configurations of an $n$-node network; that is, $|\\theta| = \\left|\\mathcal A_n\\right| = 2^{\\binom n 2}$. What is wrong with this model? The limitations are two-fold:\n",
    "1. As we explained previously, when $n$ is just $15$, we would need over $10^{30}$ bits of storage just to define $\\theta$. This amounts to more than $10^{8}$ zetabytes, which exceeds the storage capacity of *the entire world*.\n",
    "2. With a single network observed (or really, any number of networks we could collect in the real world) we would never be able to get a reasonable estimate of $2^{\\binom n 2}$ parameters for any reasonably non-trivial number of nodes $n$. For the case of one observed network $A$, an estimate of $\\theta$ (referred to as $\\hat\\theta$) would simply be for $\\hat\\theta$ to have a $1$ in the entry corresponding to our observed network, and a $0$ everywhere else. Inferentially, this would imply that the network-valued random variable $\\mathbf A$ which governs realizations $A$ is deterministic, even if this is not the case. Even if we collected potentially *many* observed networks, we would still (with very high probability) just get $\\hat \\theta$ as a series of point masses on the observed networks we see, and $0$s everywhere else. This would mean our parameter estimates $\\hat\\theta$ would not generalize to new observations at *all*, with high probability.\n",
    "\n",
    "So, what are some more reasonable descriptions of $\\mathcal P$? We explore some choices below. Particularly, we will be most interested in the *independent-edge* networks. These are the families of networks in which the generative procedure which governs the random networks assume that the edges of the network are generated *independently*. **Statistical Independence** is a property which greatly simplifies many of the modelling assumptions which are crucial for proper estimation and rigorous statistical inference, which we will learn more about in the later chapters.\n",
    "\n",
    "### Equivalence Classes*\n",
    "\n",
    "In all of the below models, we will explore the concept of the **probability equivalence class**, or an *equivalence class*, for short. The probability is a function which in general, describes how effective a particular observation can be described by a random variable $\\mathbf A$ with parameters $\\theta$, written $\\mathbf A \\sim F(\\theta)$. The probability will be used to describe the probability $\\mathbb P_\\theta(\\mathbf A)$ of observing the realization $A$ if the underlying random variable $\\mathbf A$ has parameters $\\theta$.  Why does this matter when it comes to equivalence classes? An equivalence class is a subset of the sample space $E \\subseteq \\mathcal A_n$, which has the following properties. Holding the parameters $\\theta$ fixed:\n",
    "\n",
    "1. If $A$ and $A'$ are members of the same equivalence class $E$ (written $A, A' \\in E$), then $\\mathbb P_\\theta(A) = \\mathbb P_\\theta(A')$. \n",
    "2. If $A$ and $A''$ are members of different equivalence classes; that is, $A \\in E$ and $A'' \\in E'$ where $E, E'$ are equivalence classes, then $\\mathbb P_\\theta(A) \\neq \\mathbb P_\\theta(A'')$.\n",
    "3. Using points 1 and 2, we can establish that if $E$ and $E'$ are two different equivalence classes, then $E \\cap E' = \\varnothing$. That is, the equivalence classes are **mutually disjoint**.\n",
    "4. We can use the preceding properties to deduce that given the sample space $\\mathcal A_n$ and a probability function $\\mathbb P_\\theta$, we can define a partition of the sample space into equivalence classes $E_i$, where $i \\in \\mathcal I$ is an arbitrary indexing set. A **partition** of $\\mathcal A_n$ is a sequence of sets which are mutually disjoint, and whose union is the whole space. That is, $\\bigcup_{i \\in \\mathcal I} E_i = \\mathcal A_n$. \n",
    "\n",
    "We will see more below about how the equivalence classes come into play with network models, and in a later section, we will see their relevance to the estimation of the parameters $\\theta$.\n",
    "\n",
    "(representations:whyuse:networkmodels:iern)=\n",
    "### Independent-Edge Random Networks*\n",
    "\n",
    "The below models are all special families of something called **independent-edge random networks**. An independent-edge random network is a network-valued random variable, in which the collection of edges are all independent. In words, this means that for every adjacency $\\mathbf a_{ij}$ of the network-valued random variable $\\mathbf A$, that $\\mathbf a_{ij}$ is independent of $\\mathbf a_{i'j'}$, any time that $(i,j) \\neq (i',j')$. When the networks are simple, the easiest thing to do is to assume that each edge $(i,j)$ is connected with some probability (which might be different for each edge) $p_{ij}$. We use the $ij$ subscript to denote that this probability is not necessarily the same for each edge. This simple model can be described as $\\mathbf a_{ij}$ has the distribution $Bern(p_{ij})$, for every $j > i$, and is independent of every other edge in $\\mathbf A$. We only look at the entries $j > i$, since our networks are simple. This means that knowing a realization of $\\mathbf a_{ij}$ also gives us the realizaaion of $\\mathbf a_{ji}$ (and thus $\\mathbf a_{ji}$ is a *deterministic* function of $\\mathbf a_{ij}$). Further, we know that the random network is loopless, which means that every $\\mathbf a_{ii} = 0$. We will call the matrix $P = (p_{ij})$ the **probability matrix** of the network-valued random variable $\\mathbf A$. In general, we will see a common theme for the probabilities of a realization $A$ of a network-valued random variable $\\mathbf A$, which is that it will greatly simplify our computation. Remember that if $\\mathbf x$ and $\\mathbf y$ are binary variables which are independent, that $\\mathbb P(\\mathbf x = x, \\mathbf y = y) = \\mathbb P(\\mathbf x = x) \\mathbb P(\\mathbf y = y)$. Using this fact:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb P(\\mathbf A = A) &= \\mathbb P(\\mathbf a_{11} = a_{11}, \\mathbf a_{12} = a_{12}, ..., \\mathbf a_{nn} = a_{nn}) \\\\\n",
    "    &= \\mathbb P(\\mathbf a_{ij} = a_{ij} \\text{ for all }j > i) \\\\\n",
    "    &= \\prod_{j > i}\\mathbb P(\\mathbf a_{ij} = a_{ij}), \\;\\;\\;\\;\\textrm{Independence Assumption}\n",
    "\\end{align*}\n",
    "Next, we will use the fact that if a random variable $\\mathbf a_{ij}$ has the Bernoulli distribution with probability $p_{ij}$, that $\\mathbb P(\\mathbf a_{ij} = a_{ij}) = p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - p_{ij}}$:\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(A) &= \\prod_{j > i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - p_{ij}}\n",
    "\\end{align*}\n",
    "\n",
    "Now that we've specified a probability and a very generalizable model, we've learned the full story behind network models and are ready to skip to estimating parameters, right? *Wrong!* Unfortunately, if we tried too estimate anything about each $p_{ij}$ individually, we would obtain that $p_{ij} = a_{ij}$ if we only have one realization $A$. Even if we had many realizations of $\\mathbf A$, this still would not be very interesting, since we have a *lot* of $p_{ij}$s to estimate, and we've ignored any sort of structural model that might give us deeper insight into $\\mathbf A$. In the below sections, we will learn successively less restrictive (and hence, *more expressive*) assumptions about $p_{ij}$s, which will allow us to convey fairly complex random networks, but *still* enable us with plenty of intteresting things to learn about later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-contribution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
