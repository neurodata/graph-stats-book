{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "labeled-henry",
   "metadata": {},
   "source": [
    "# Random Dot Product Graphs (RDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-shopper",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graspologic.simulations import rdpg\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "n = 100  # the number of nodes in our network\n",
    "\n",
    "# design the latent position matrix X according to \n",
    "# the rules we laid out previously\n",
    "X = np.zeros((n,2))\n",
    "for i in range(0, n):\n",
    "    X[i,:] = [(n - i)/n, i/n]\n",
    "\n",
    "from graspologic.simulations import rdpg\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "A = rdpg(X, directed=False, loops=False)\n",
    "draw_multiplot(A, title=\"RDPG Simulation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-hygiene",
   "metadata": {},
   "source": [
    "## Latent Positions\n",
    "\n",
    "Let's imagine that we have a network which follows the Stochastic Block Model. To make this example a little bit more concrete, let's borrow the code example from the [section on Stochastic Block Models](link?). The nodes of our network represent each of the $100$ students in our network. Remember that $\\tau$ is the community assignment vector, which indicates which community (one of two schools) each node (student) is in. Here, the first $50$ students attend school $1$, and the second $50$ students attend school $2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-recipe",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "def plot_tau(tau, title=\"\", xlab=\"Node\"):\n",
    "    cmap = matplotlib.colors.ListedColormap([\"skyblue\", 'blue'])\n",
    "    fig, ax = plt.subplots(figsize=(10,2))\n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap((tau - 1).reshape((1,tau.shape[0])), cmap=cmap,\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_ticks([0.25, .75])\n",
    "        cbar.set_ticklabels(['School 1', 'School 2'])\n",
    "        ax.set(xlabel=xlab)\n",
    "        ax.set_xticks([.5,49.5,99.5])\n",
    "        ax.set_xticklabels([\"1\", \"50\", \"100\"])\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "n = 100  # number of students\n",
    "\n",
    "# tau is a column vector of 150 1s followed by 50 2s\n",
    "# this vector gives the school each of the 300 students are from\n",
    "tau = np.vstack((np.ones((int(n/2),1)), np.full((int(n/2),1), 2)))\n",
    "\n",
    "plot_tau(tau, title=\"Tau, Node Assignment Vector\",\n",
    "        xlab=\"Student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-madness",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "And the block probability matrix $B$ is a $2 \\times 2$ matrix, where each entry $b_{kl}$ indicates the probability that a node assigned to community $k$ is connected to a node assigned to community $l$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-share",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "K = 2  # 2 communities in total\n",
    "# construct the block matrix B as described above\n",
    "B = [[0.5, 0.2], [0.2, 0.3]]\n",
    "\n",
    "def plot_block(X, title=\"\", blockname=\"School\", blocktix=[0.5, 1.5],\n",
    "               blocklabs=[\"School 1\", \"School 2\"]):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=0, vmax=1)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=blockname, xlabel=blockname)\n",
    "        ax.set_yticks(blocktix)\n",
    "        ax.set_yticklabels(blocklabs)\n",
    "        ax.set_xticks(blocktix)\n",
    "        ax.set_xticklabels(blocklabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "plot_block(B, title=\"Block Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-guidance",
   "metadata": {},
   "source": [
    "Are there any other ways to describe this scenario, other than using both the community assignment vector $\\vec\\tau$ and the block matrix $B$?\n",
    "\n",
    "Remember, for a given $\\vec\\tau$ and $B$, that a network which is SBM can be generated using the approach that, given that $\\tau_i = \\ell$ and $\\tau_j = k$, each edge $(i, j)$ comes down to a coin flip, where the edge exists if the coin lands on heads with probability $b_{\\tau_i \\tau_j}$ or does not exist if the coin lands on tails with probability $1- b_{\\tau_i \\tau_j}$. However, there's another way we could write down this generative model. Suppose we had a $n \\times n$ probability matrix, where for every $j > i$:\n",
    "\\begin{align*}\n",
    "    p_{ji} = p_{ij}, p_{ij} = \\begin{cases}\n",
    "        b_{11} & \\tau_i = 1, \\tau_j = 1 \\\\\n",
    "        b_{12} & \\tau_i = 1, \\tau_j = 2 \\\\\n",
    "        b_{22} & \\tau_i = 2, \\tau_j = 1\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "This matrix $P$ with entries $p_{ij}$ is the probability matrix associated with the SBM. Simply put, this matrix describes the probability of each edge $(i,j)$ existing. What does $P$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-initial",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_prob(X, title=\"\", nodename=\"Student\", nodetix=None,\n",
    "             nodelabs=None, ax=None):\n",
    "    if (ax is None):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=0, vmax=1)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=nodename, xlabel=nodename)\n",
    "        if (nodetix is not None) and (nodelabs is not None):\n",
    "            ax.set_yticks(nodetix)\n",
    "            ax.set_yticklabels(nodelabs)\n",
    "            ax.set_xticks(nodetix)\n",
    "            ax.set_xticklabels(nodelabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "P = np.zeros((n,n))\n",
    "P[0:50,0:50] = .5\n",
    "P[50:100, 50:100] = .3\n",
    "P[0:50,50:100] = .2\n",
    "P[50:100,0:50] = .2\n",
    "\n",
    "ax = plot_prob(P, title=\"Probability Matrix\", nodetix=[0,100],\n",
    "              nodelabs=[\"1\", \"100\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-conspiracy",
   "metadata": {},
   "source": [
    "As we can see, $P$ captures a similar modular structure to the actual adjacency matrix corresponding to the SBM network. When we say this network is *modular*, we mean that it looks block-y, in that there are clusters of edges sharing a similar probability. Also, $P$ captures the probability of connections between each pair of students. Indeed, it is the case that $P$ contains the information of both $\\vec\\tau$ and $B$. This means that we can write down a generative model by specifying *only* $P$, and we no longer need to specify $\\vec\\tau$ nor $B$ at all.\n",
    "\n",
    "Now, your next thought might be that this requires a *lot* more space to represent an $SBM_n(\\vec\\tau, B)$ network, and you'd be right: $\\vec \\tau$ has $n$ entries, and $B$ has $K \\times K$ entries, where $K$ is typically much smaller than $n$. On the other hand, in this formulation, $P$ has $\\binom{n}{2}$ entries, which is much bigger than $n + K \\times K$ (since $K$ is usually much smaller than $n$). The advantage is that under this formulation, $P$ doesn't need to have this rigorous modular structure characteristic of SBM networks, and can look a *lot* more interesting. As we will see in later chapters, this network representation will prove extremely flexible for allowing us to capture networks that are fairly complex. Further, we can also perform analysis on the matrix $X$ itself, which will prove very useful for estimation of SBMs. What is so special about this formulation of the SBM problem?\n",
    "\n",
    "As it turns out, for a *positive semi-definite* probability matrix $P$, $P$ can be decomposed using a matrix $X$, where $P = X X^\\top$. We will call a single row of $X$ the vector $\\vec x_i$. Remember, using this expression, each entry $p_{ij}$ is the product $\\vec x_i^\\top \\vec x_j$, for all $i, j$. Like $P$, $X$ has $n$ rows, each of which corresponds to a single node in our network. However, the special property of $X$ is that it doesn't *necessarily* have $n$ columns: rather, $X$ often will have many fewer columns than rows. For instance, with $P$ as above, there in fact exists an $X$ with just $2$ columns that can be used to describe $P$. Let's take a look at what $X$ looks like. We won't discuss how to compute this $X$ just yet, but we'll try to build some insight into what's going on here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-boards",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "from graphbook_code import cmaps\n",
    "\n",
    "U, S, V = svd(P)\n",
    "X = U[:,0:2] @ np.sqrt(np.diag(S[0:2]))\n",
    "\n",
    "def plot_latent(X, title=\"\", nodename=\"Student\", ylabel=\"Latent Dimension\",\n",
    "                nodetix=None, nodelabs=None, dimtix=None, dimlabs=None):\n",
    "    fig, ax = plt.subplots(figsize=(3, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        lim_max = np.max(np.abs(X))\n",
    "        vmin = -lim_max; vmax = lim_max\n",
    "        ax = sns.heatmap(X, cmap=cmaps[\"divergent\"],\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=nodename, xlabel=ylabel)\n",
    "        if (nodetix is not None) and (nodelabs is not None):\n",
    "            ax.set_yticks(nodetix)\n",
    "            ax.set_yticklabels(nodelabs)\n",
    "        if (dimtix is not None) and (dimlabs is not None):\n",
    "            ax.set_xticks(dimtix)\n",
    "            ax.set_xticklabels(dimlabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "\n",
    "ax = plot_latent(X, title=\"Latent Position Matrix\", nodetix=[0, 49, 99],\n",
    "              nodelabs=[\"1\", \"50\", \"100\"], dimtix=[0.5,1.5], dimlabs=[\"1\", \"2\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-protocol",
   "metadata": {},
   "source": [
    "One thing we will come back to in a second is the fact that $X$ in this case is relatively simple: there are *only* $4$ unique entries, even though there are $200$ total entries in $X$ (2 columns for each of $100$ students). \n",
    "\n",
    "Like we said previously, it turns out that $P$ can be described using *only* this matrix $X$, as $P = XX^\\top$! Let's see this in action, by comparing the $P$ we had above to $XX^\\top$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_XXt = X @ X.T\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "plot_prob(P,\n",
    "        title=\"$P$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "plot_prob(P_XXt,\n",
    "        title=\"$XX^T$\",\n",
    "        ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-champagne",
   "metadata": {},
   "source": [
    "As we can see, $P$ and $XX^\\top$ are identical! \n",
    "\n",
    "This formulation of the SBM lends itself to an even further generalization of our single network models. Like we said a few figures ago, $X$ only has $4$ unique entries, but there is no reason for this restriction really. The matrix $X$ can have *any* number of unique entries, so long as the product of $X$ and its transpose, $XX^\\top$, ends up being a probability matrix (every entry is a number between $0$ and $1$).\n",
    "\n",
    "This matrix $X$ will be called the **latent position matrix**, and each row $\\vec x_i$ will be called the **latent position of the node** $i$. In matrix form, $X$ looks like this:\n",
    "\n",
    "\\begin{align*}\n",
    " X = \\begin{bmatrix}\n",
    "     \\vec x_1 \\\\\n",
    "     \\vec x_2 \\\\\n",
    "     \\vdots \\\\\n",
    "     \\vec x_n\n",
    " \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We will call the columns of $X$ the **latent dimensions**, and the total number of columns that $X$ has will be called the **latent dimensionality**. We will often use the letter $d$ to denote the latent dimensionality of the latent position matrix $X$. For this reason, we say that $X$ is an $n$ row (one for each node) and $d$ column (one for each latent dimension) matrix. Therefore, the latent position of the node $i$, $\\vec x_i$, is a $d$-dimensional vector. In a few words, the latent dimensionality describes the complexity that the resulting probability matrix $P = XX^\\top$ has: if there are more latent dimensions, $P$ can look much more complicated than what we have seen thus far!\n",
    "\n",
    "Now that we have some intuition built up, let's circle back to random networks. We will call this particular network model the RDPG model. The way that we can think of the $RDPG$ random network is that the edges depend on a latent position matrix $X$. For each pair of nodes $i$ and $j$, we have a unique coin (we will call this the $(i,j)$ coin) which has a $\\vec x_i^\\top \\vec x_j$ chance of landing on heads, and a $1 - \\vec x_i^\\top \\vec x_j$ chance of landing on tails. If the $(i,j)$ coin lands on heads, the edge between nodes $i$ and $j$ exists, and if the $(i,j)$ coin lands on tails, the edge between nodes $i$ and $j$ does not exist. As before, this coin flip is performed independent of the coin flips for all of the other edges. The notation we use here is just what we are used to in the preceding sections. If $\\mathbf A$ is a random network which is $RDPG$ with a latent position matrix $X$, we say that $\\mathbf A$ is an $RDPG_n(X)$ random network. \n",
    "\n",
    "The procedure below will produce for us a network $A$, which has nodes and edges, where the underlying random network $\\mathbf A$ is an $RDPG_n(X)$ random network:\n",
    "\n",
    "```{admonition} Simulating a realization from an $RDPG_n(X)$ random network\n",
    "1. Determine a latent position matrix, $X$, whose rows $\\vec x_i$ are the latent positions of the nodes in the network.\n",
    "2. For each edge between nodes $i$ and $j$:\n",
    "3.     Obtain a weighted coin $(i,j)$ which has a probability of $\\vec x_i^\\top \\vec x_j$ of landing on heads, and a $1 - \\vec x_i^\\top \\vec x_j$ probability of landing on tails.\n",
    "4.     Flip the $(i,j)$ coin, and if it lands on heads, the corresponding entry $a_{ij}$ in the adjacency matrix is $1$. If the coin lands on tails, the corresponding entry $a_{ij} = 0$.\n",
    "5. The adjacency matrix we produce, $A$, is a realization of an $RDPG_n(X)$ random network.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-armstrong",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "We will let $X$ be a little more complex than in our preceding example. Our $X$ will produce a $P$ that still *somewhat* has a modular structure, but not quite as much as before. Let's assume that we have $100$ people who live along a very long road that is $100$ miles long, and each person is $1$ mile apart. The nodes of our network represent the people who live along our assumed street. If two people are closer to one another, it might make sense to think that they have a higher probability of being friends. If two people are neighbors, we think that they will have a very high probability of being friends (almost $1$) and when people are very far apart, we think that they will have a very low probability of being friends (almost $0$). What could we use for $X$?\n",
    "\n",
    "Remember that the latent positions for each node $i$ are denoted by the vector $\\vec x_i$. One possible approach would be to let each $\\vec x_i$ be defined as follows:\n",
    "\\begin{align*}\n",
    "    \\vec x_i = \\begin{bmatrix}\n",
    "        \\frac{100 - i}{100} \\\\\n",
    "        \\frac{i}{100}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "For instance, $\\vec x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$, and $\\vec x_{100} = \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix}$. Note that:\n",
    "\\begin{align*}\n",
    "p_{1,100} = \\vec x_1^\\top \\vec x_j = 1 \\cdot 0 + 0 \\cdot 1 = 0\n",
    "\\end{align*}\n",
    "What happens in between?\n",
    "\n",
    "Let's consider another person, person $30$. Note that person $30$ lives closer to person $1$ than to person $100$.  Here, $\\vec x_{30} = \\begin{bmatrix} \\frac{7}{10}\\\\ \\frac{3}{10}\\end{bmatrix}$. This gives us that:\n",
    "\\begin{align*}\n",
    "p_{1,30} &= \\vec x_1^\\top \\vec x_{30} = \\frac{7}{10}\\cdot 1 + 0 \\cdot \\frac{3}{10} = \\frac{7}{10} \\\\\n",
    "p_{30, 100} &= \\vec x_{30}^\\top x_{100} = \\frac{7}{10} \\cdot 0 + \\frac{3}{10} \\cdot 1 = \\frac{3}{10}\n",
    "\\end{align*}\n",
    "So this means that person $1$ and person $30$ have a $70\\%$ probability of being friends, but person $30$ and $100$ have onl6 a $30\\%$ probability of being friends.\n",
    "\n",
    "Intuitively, it seems like our probability matrix $P$ will capture the intuitive idea we described above. First, we'll take a look at $X$, and then we'll look at $P$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # the number of nodes in our network\n",
    "\n",
    "# design the latent position matrix X according to \n",
    "# the rules we laid out previously\n",
    "X = np.zeros((n,2))\n",
    "for i in range(0, n):\n",
    "    X[i,:] = [(n - i)/n, i/n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-death",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_lp(X, title=\"\", ylab=\"Student\"):\n",
    "    fig, ax = plt.subplots(figsize=(4, 10))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=ylab)\n",
    "        ax.set_yticks([0, 29, 69, 99])\n",
    "        ax.set_yticklabels([\"1\", \"30\", \"70\", \"100\"])\n",
    "        ax.set_xticks([.5, 1.5])\n",
    "        ax.set_xticklabels([\"Dimension 1\", \"Dimension 2\"])\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "plot_lp(X, title=\"Latent Position Matrix, X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-northwest",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "The latent position matrix $X$ that we plotted above is $n \\times d$ dimensions. There are a number of approaches, other than looking at a heatmap of $X$, with which we can visualize $X$ to derive insights as to its structure. When $d=2$, another popular visualization is to look at the latent positions, $\\vec x_i$, as individual points in $2$-dimensional space. This will give us a scatter plot of $n$ points, each of which has two coordinates. Each point is the latent position for a single node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-brake",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_latents(latent_positions, title=None, labels=None, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ss = 2*np.arange(0, 50)\n",
    "    plot = sns.scatterplot(x=latent_positions[ss, 0], y=latent_positions[ss, 1], hue=labels, \n",
    "                           s=10, ax=ax, palette=\"Set1\", color='k', **kwargs)\n",
    "    ax.set_title(title)\n",
    "    ax.set(ylabel=\"Dimension 1\", xlabel=\"Dimension 2\")\n",
    "    ax.set_title(title)\n",
    "    return plot\n",
    "\n",
    "# plot\n",
    "plot_latents(X, title=\"Latent Position Matrix, X\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-poison",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "The above scatter plot has been subsampled to show only every $2^{nd}$ latent position, so that the individual $2$-dimensional latent positions are discernable. Due to the way we constructed $X$, the scatter plot would otherwise appear to be a line (due to points overlapping one another). The reason that the points fall along a vertical line when plotted as a vector is due to the method we used to construct entries of $X$, described above. Next, we will look at the probability matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-remainder",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_prob(X.dot(X.transpose()), title=\"Probability Matrix, P=$XX^T$\",\n",
    "         nodelabs=[\"1\", \"30\", \"70\", \"100\"], nodetix=[0,29,69,99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-cologne",
   "metadata": {},
   "source": [
    "Finally, we will sample an RDPG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import rdpg\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "# sample an RDPG with the latent position matrix\n",
    "# created above\n",
    "A = rdpg(X, loops=False, directed=False)\n",
    "\n",
    "# and plot it\n",
    "ax = draw_multiplot(A, title=\"$RDPG_{100}(X)$ Simulation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
