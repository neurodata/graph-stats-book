{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "likely-miller",
   "metadata": {},
   "source": [
    "# Random Dot Product Graphs (RDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-photography",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graspologic.simulations import rdpg\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "n = 100  # the number of nodes in our network\n",
    "\n",
    "# design the latent position matrix X according to \n",
    "# the rules we laid out previously\n",
    "X = np.zeros((n,2))\n",
    "for i in range(0, n):\n",
    "    X[i,:] = [(n - i)/n, i/n]\n",
    "\n",
    "from graspologic.simulations import rdpg\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "A = rdpg(X, directed=False, loops=False)\n",
    "draw_multiplot(A, title=\"RDPG Simulation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-shower",
   "metadata": {},
   "source": [
    "## Latent Positions\n",
    "\n",
    "Let's imagine that we have a network which follows the *a priori* Stochastic Block Model. To make this example a little bit more concrete, let's borrow the code example from above. The nodes of our network represent each of the $100$ students in our network. The node assignment vector represents which of the two schools eaach student attends, where the first $50$ students attend the first school, and the second $50$ students attend school $2$. Remember that $\\tau$ and $B$ look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-architecture",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "def plot_tau(tau, title=\"\", xlab=\"Node\"):\n",
    "    cmap = matplotlib.colors.ListedColormap([\"skyblue\", 'blue'])\n",
    "    fig, ax = plt.subplots(figsize=(10,2))\n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap((tau - 1).reshape((1,tau.shape[0])), cmap=cmap,\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_ticks([0.25, .75])\n",
    "        cbar.set_ticklabels(['School 1', 'School 2'])\n",
    "        ax.set(xlabel=xlab)\n",
    "        ax.set_xticks([.5,49.5,99.5])\n",
    "        ax.set_xticklabels([\"1\", \"50\", \"100\"])\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "n = 100  # number of students\n",
    "\n",
    "# tau is a column vector of 150 1s followed by 50 2s\n",
    "# this vector gives the school each of the 300 students are from\n",
    "tau = np.vstack((np.ones((int(n/2),1)), np.full((int(n/2),1), 2)))\n",
    "\n",
    "plot_tau(tau, title=\"Tau, Node Assignment Vector\",\n",
    "        xlab=\"Student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-formation",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "K = 2  # 2 communities in total\n",
    "# construct the block matrix B as described above\n",
    "B = [[0.5, 0.2], [0.2, 0.3]]\n",
    "\n",
    "def plot_block(X, title=\"\", blockname=\"School\", blocktix=[0.5, 1.5],\n",
    "               blocklabs=[\"School 1\", \"School 2\"]):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=0, vmax=1)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=blockname, xlabel=blockname)\n",
    "        ax.set_yticks(blocktix)\n",
    "        ax.set_yticklabels(blocklabs)\n",
    "        ax.set_xticks(blocktix)\n",
    "        ax.set_xticklabels(blocklabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "plot_block(B, title=\"Block Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-thumbnail",
   "metadata": {},
   "source": [
    "Are there any other ways to describe this scenario, other than using both $\\tau$ and $B$?\n",
    "\n",
    "What if we were to look at the probabilities for *every* pair of edges? Remember, for a given $\\tau$ and $B$, that a network which is SBM can be generated using the approach that, given that $\\tau_i = \\ell$ and $\\tau_j = k$, that $\\mathbf a_{ij} \\sim Bern(b_{\\ell k})$. That is, every entry is Bernoulli, with the probability indicated by appropriate entry of the block matrix corresponding to the pair of communities each node is in. However, there's another way we could write down this generative model. Suppose we had a $n \\times n$ probability matrix, where for every $j > i$:\n",
    "\\begin{align*}\n",
    "    p_{ji} = p_{ij}, p_{ij} = \\begin{cases}\n",
    "        b_{11} & \\tau_i = 1, \\tau_j = 1 \\\\\n",
    "        b_{12} & \\tau_i = 1, \\tau_j = 2 \\\\\n",
    "        b_{22} & \\tau_i = 2, \\tau_j = 1\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "This matrix $P$ with entries $p_{ij}$ is the probability matrix associated with the *a priori* SBM, which we described in the section [](representations:whyuse:networkmodels:iern). \n",
    "If you've been following the advanced sections, you will already be familiar with this term. Simply put, this matrix describes the probability of each edge $(i,j)$ existing. What does $P$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-broad",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_prob(X, title=\"\", nodename=\"Student\", nodetix=None,\n",
    "             nodelabs=None):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False, vmin=0, vmax=1)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=nodename, xlabel=nodename)\n",
    "        if (nodetix is not None) and (nodelabs is not None):\n",
    "            ax.set_yticks(nodetix)\n",
    "            ax.set_yticklabels(nodelabs)\n",
    "            ax.set_xticks(nodetix)\n",
    "            ax.set_xticklabels(nodelabs)\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "P = np.zeros((n,n))\n",
    "P[0:50,0:50] = .5\n",
    "P[50:100, 50:100] = .3\n",
    "P[0:50,50:100] = .2\n",
    "P[50:100,0:50] = .2\n",
    "\n",
    "ax = plot_prob(P, title=\"Probability Matrix\", nodetix=[0,100],\n",
    "              nodelabs=[\"1\", \"100\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-portfolio",
   "metadata": {},
   "source": [
    "As we can see, $P$ captures a similar modular structure to the actual adjacency matrix corresponding to the SBM network. When we say this network is *modular*, we mean that it looks block-y, in that there are clusters of edges sharing a similar probability. Also, $P$ captures the probability of connections between each pair of students. Indeed, it is the case that $P$ contains the information of both $\\vec\\tau$ and $B$. This means that we can write down a generative model by specifying *only* $P$, and we no longer need to specify $\\vec\\tau$ and $B$ at all. To write down the generative model in this way, we say that for all $j > i$, that $\\mathbf a_{ij} \\sim Bern(p_{ij})$ independently, where $\\mathbf a_{ji} = \\mathbf a_{ij}$, and $\\mathbf a_{ii} = 0$.\n",
    "\n",
    "What is so special about this formulation of the SBM problem? As it turns out, for a *positive semi-definite* probability matrix $P$, $P$ can be decomposed using a matrix $X$, where $P = X X^\\top$. We will call a single row of $X$ the vector $\\vec x_i$. Remember, using this expression, each entry $p_{ij}$ is the product $\\vec x_i^\\top \\vec x_j$, for all $i, j$. Like $P$, $X$ has $n$ rows, each of which corresponds to a single node in our network. However, the special property of $X$ is that it doesn't *necessarily* have $n$ columns: rather, $X$ often will have many fewer columns than rows. For instance, with $P$ as above, there in fact exists an $X$ with just $2$ columns that can be used to describe $P$. This matrix $X$ will be called the **latent position matrix**, and each row $\\vec x_i$ will be called the **latent position of a node**.\n",
    "\n",
    "Now, your next thought might be that this requires a *lot* more space to represent an SBM network, and you'd be right: $\\vec \\tau$ has $n$ entries, and $B$ has $K \\times K$ entries, where $K$ is typically much smaller than $n$. On the other hand, in this formulation, $P$ has $\\binom{n}{2}$ entries, which is much bigger than $n + K \\times K$ (since $K$ is usually much smaller than $n$). The advantage is that under this formulation, $P$ doesn't need to have this rigorous modular structure characteristic of SBM networks, and can look a *lot* more interesting. As we will see in later chapters, this network representation will prove extremely flexible for allowing us to capture networks that are fairly complex. Further, we can also perform analysis on the matrix $X$ itself, which will prove very useful for estimation of SBMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-harvey",
   "metadata": {},
   "source": [
    "## *A Priori* RDPG\n",
    "\n",
    "The *a priori* Random Dot Product Graph is an RDPG in which we know *a priori* the latent position matrix $X$. The *a priori* RDPG has the following parameter:\n",
    "\n",
    "| Parameter | Space | Description |\n",
    "| --- | --- | --- |\n",
    "| $X$ | $ \\mathbb R^{n \\times d}$ | The matrix of latent positions for each node $n$. |\n",
    "\n",
    "$X$ is called the **latent position matrix** of the RDPG. We write that $X \\in \\mathbb R^{n \\times d}$, which means that it is a matrix with real values, $n$ rows, and $d$ columns. We will use the notation $\\vec x_i$ to refer to the $i^{th}$ row of $X$. $\\vec x_i$ is referred to as the **latent position** of a node $i$. This looks something like this:\n",
    "\\begin{align*}\n",
    "    X = \\begin{bmatrix}\n",
    "     \\vec x_{1}^\\top \\\\\n",
    "     \\vdots \\\\\n",
    "     \\vec x_n^\\top\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "Noting that $X$ has $d$ columns, this implies that $\\vec x_i \\in  \\mathbb R^d$, or that each node's latent position is a real-valued $d$-dimensional vector.\n",
    "\n",
    "What is the generative model for the *a priori* RDPG? As we discussed above, given $X$, for all $j > i$, $\\mathbf a_{ij} \\sim Bern(\\vec x_i^\\top \\vec x_j)$ independently. If $i < j$, $\\mathbf a_{ji} = \\mathbf a_{ij}$ (the network is *undirected*), and $\\mathbf a_{ii} = 0$ (the network is *loopless*). If $\\mathbf A$ is an *a priori* RDPG with parameter $X$, we write that $\\mathbf A \\sim RDPG_n(X)$. \n",
    "\n",
    "### Code Examples\n",
    "\n",
    "We will let $X$ be a little more complex than in our preceding example. Our $X$ will produce a $P$ that still *somewhat* has a modular structure, but not quite as much as before. Let's assume that we have $100$ people who live along a very long road that is $100$ miles long, and each person is $1$ mile apart. The nodes of our network represent the people who live along our assumed street. If two people are closer to one another, it might make sense to think that they have a higher probability of being friends. If two people are neighbors, we think that they will have a very high probability of being friends (almost $1$) and when people are very far apart, we think that they will have a very low probability of being friends (almost $0$). What could we use for $X$?\n",
    "\n",
    "Remember that the latent positions for each node $i$ are denoted by the vector $\\vec x_i$. One possible approach would be to let each $\\vec x_i$ be defined as follows:\n",
    "\\begin{align*}\n",
    "    \\vec x_i = \\begin{bmatrix}\n",
    "        \\frac{100 - i}{100} \\\\\n",
    "        \\frac{i}{100}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "For instance, $\\vec x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$, and $\\vec x_{100} = \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix}$. Note that:\n",
    "\\begin{align*}\n",
    "p_{1,100} = \\vec x_1^\\top \\vec x_j = 1 \\cdot 0 + 0 \\cdot 1 = 0\n",
    "\\end{align*}\n",
    "What happens in between?\n",
    "\n",
    "Let's consider another person, person $30$. Note that person $30$ lives closer to person $1$ than to person $100$.  Here, $\\vec x_{30} = \\begin{bmatrix} \\frac{7}{10}\\\\ \\frac{3}{10}\\end{bmatrix}$. This gives us that:\n",
    "\\begin{align*}\n",
    "p_{1,30} &= \\vec x_1^\\top \\vec x_{30} = \\frac{7}{10}\\cdot 1 + 0 \\cdot \\frac{3}{10} = \\frac{7}{10} \\\\\n",
    "p_{30, 100} &= \\vec x_{30}^\\top x_{100} = \\frac{7}{10} \\cdot 0 + \\frac{3}{10} \\cdot 1 = \\frac{3}{10}\n",
    "\\end{align*}\n",
    "So this means that person $1$ and person $30$ have a $70\\%$ probability of being friends, but person $30$ and $100$ have onl6 a $30\\%$ probability of being friends.\n",
    "\n",
    "Intuitively, it seems like our probability matrix $P$ will capture the intuitive idea we described above. First, we'll take a look at $X$, and then we'll look at $P$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # the number of nodes in our network\n",
    "\n",
    "# design the latent position matrix X according to \n",
    "# the rules we laid out previously\n",
    "X = np.zeros((n,2))\n",
    "for i in range(0, n):\n",
    "    X[i,:] = [(n - i)/n, i/n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-genesis",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_lp(X, title=\"\", ylab=\"Student\"):\n",
    "    fig, ax = plt.subplots(figsize=(4, 10))\n",
    "    \n",
    "    with sns.plotting_context(\"talk\", font_scale=1):\n",
    "        ax = sns.heatmap(X, cmap=\"Purples\",\n",
    "                        ax=ax, cbar_kws=dict(shrink=1), yticklabels=False,\n",
    "                        xticklabels=False)\n",
    "        ax.set_title(title)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        ax.set(ylabel=ylab)\n",
    "        ax.set_yticks([0, 29, 69, 99])\n",
    "        ax.set_yticklabels([\"1\", \"30\", \"70\", \"100\"])\n",
    "        ax.set_xticks([.5, 1.5])\n",
    "        ax.set_xticklabels([\"Dimension 1\", \"Dimension 2\"])\n",
    "        cbar.ax.set_frame_on(True)\n",
    "    return\n",
    "\n",
    "plot_lp(X, title=\"Latent Position Matrix, X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-concentrate",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "The latent position matrix $X$ that we plotted above is $n \\times d$ dimensions. There are a number of approaches, other than looking at a heatmap of $X$, with which we can visualize $X$ to derive insights as to its structure. When $d=2$, another popular visualization is to look at the latent positions, $\\vec x_i$, as individual points in $2$-dimensional space. This will give us a scatter plot of $n$ points, each of which has two coordinates. Each point is the latent position for a single node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-seattle",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_latents(latent_positions, title=None, labels=None, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ss = 2*np.arange(0, 50)\n",
    "    plot = sns.scatterplot(x=latent_positions[ss, 0], y=latent_positions[ss, 1], hue=labels, \n",
    "                           s=10, ax=ax, palette=\"Set1\", color='k', **kwargs)\n",
    "    ax.set_title(title)\n",
    "    ax.set(ylabel=\"Dimension 1\", xlabel=\"Dimension 2\")\n",
    "    ax.set_title(title)\n",
    "    return plot\n",
    "\n",
    "# plot\n",
    "plot_latents(X, title=\"Latent Position Matrix, X\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-diving",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "The above scatter plot has been subsampled to show only every $2^{nd}$ latent position, so that the individual $2$-dimensional latent positions are discernable. Due to the way we constructed $X$, the scatter plot would otherwise appear to be a line (due to points overlapping one another). The reason that the points fall along a vertical line when plotted as a vector is due to the method we used to construct entries of $X$, described above. Next, we will look at the probability matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-cleaners",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_prob(X.dot(X.transpose()), title=\"Probability Matrix, P=$XX^T$\",\n",
    "         nodelabs=[\"1\", \"30\", \"70\", \"100\"], nodetix=[0,29,69,99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-mention",
   "metadata": {},
   "source": [
    "Finally, we will sample an RDPG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import rdpg\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "# sample an RDPG with the latent position matrix\n",
    "# created above\n",
    "A = rdpg(X, loops=False, directed=False)\n",
    "\n",
    "# and plot it\n",
    "ax = draw_multiplot(A, title=\"$RDPG_{100}(X)$ Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-desire",
   "metadata": {},
   "source": [
    "### Probability*\n",
    "\n",
    "Given $X$, the probability for an RDPG is relatively straightforward, as an RDPG is another Independent-Edge Random Graph. The independence assumption vastly simplifies our resulting expression. We will also use many of the results we've identified above, such as the p.m.f. of a Bernoulli random variable. Finally, we'll note that the probability matrix $P = (\\vec x_i^\\top \\vec x_j)$, so $p_{ij} = \\vec x_i^\\top \\vec x_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(A) &= \\mathbb P_\\theta(A) \\\\\n",
    "    &= \\prod_{j > i}\\mathbb P(\\mathbf a_{ij} = a_{ij}),\\;\\;\\;\\; \\textrm{Independence Assumption} \\\\\n",
    "    &= \\prod_{j > i}(\\vec x_i^\\top \\vec x_j)^{a_{ij}}(1 - \\vec x_i^\\top \\vec x_j)^{1 - a_{ij}},\\;\\;\\;\\; a_{ij} \\sim Bern(\\vec x_i^\\top \\vec x_j)\n",
    "\\end{align*}\n",
    "\n",
    "Unfortunately, the probability equivalence classes are a bit harder to understand intuitionally here compared to the ER and SBM examples so we won't write them down here, but they still exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-turning",
   "metadata": {},
   "source": [
    "## *A Posteriori* RDPG\n",
    "\n",
    "Like for the *a posteriori* SBM, the *a posteriori* RDPG introduces another strange set: the **intersection of the unit ball and the non-negative orthant**. Huh? This sounds like a real mouthful, but it turns out to be rather straightforward. You are probably already very familiar with a particular orthant: in two-dimensions, an orthant is called a quadrant. Basically, an orthant just extends the concept of a quadrant to spaces which might have more than $2$ dimensions. The non-negative orthant happens to be the orthant where all of the entries are non-negative. We call the **$K$-dimensional non-negative orthant** the set of points in $K$-dimensional real space, where:\n",
    "\\begin{align*}\n",
    "    \\left\\{\\vec x \\in \\mathbb R^K : x_k \\geq 0\\text{ for all $k$}\\right\\}\n",
    "\\end{align*}\n",
    "In two dimensions, this is the traditional upper-right portion of the standard coordinate axis. To give you a picture, the $2$-dimensional non-negative orthant is the blue region of the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-coupon",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axisartist import SubplotZero\n",
    "import matplotlib.patches as patch\n",
    "\n",
    "class myAxes():\n",
    "    \n",
    "    def __init__(self, xlim=(-5,5), ylim=(-5,5), figsize=(6,6)):\n",
    "        self.xlim = xlim\n",
    "        self.ylim = ylim\n",
    "        self.figsize  = figsize\n",
    "        self.__scale_arrows()\n",
    "    def __drawArrow(self, x, y, dx, dy, width, length):\n",
    "        plt.arrow(\n",
    "            x, y, dx, dy, \n",
    "            color       = 'k',\n",
    "            clip_on     = False, \n",
    "            head_width  = self.head_width, \n",
    "            head_length = self.head_length\n",
    "        ) \n",
    "        \n",
    "    def __scale_arrows(self):\n",
    "        \"\"\" Make the arrows look good regardless of the axis limits \"\"\"\n",
    "        xrange = self.xlim[1] - self.xlim[0]\n",
    "        yrange = self.ylim[1] - self.ylim[0]\n",
    "        \n",
    "        self.head_width  = min(xrange/30, 0.25)\n",
    "        self.head_length = min(yrange/30, 0.3)\n",
    "        \n",
    "    def __drawAxis(self):\n",
    "        \"\"\"\n",
    "        Draws the 2D cartesian axis\n",
    "        \"\"\"\n",
    "        # A subplot with two additional axis, \"xzero\" and \"yzero\"\n",
    "        # corresponding to the cartesian axis\n",
    "        ax = SubplotZero(self.fig, 1, 1, 1)\n",
    "        self.fig.add_subplot(ax)\n",
    "        \n",
    "        # make xzero axis (horizontal axis line through y=0) visible.\n",
    "        for axis in [\"xzero\",\"yzero\"]:\n",
    "            ax.axis[axis].set_visible(True)\n",
    "        # make the other axis (left, bottom, top, right) invisible\n",
    "        for n in [\"left\", \"right\", \"bottom\", \"top\"]:\n",
    "            ax.axis[n].set_visible(False)\n",
    "            \n",
    "        # Plot limits\n",
    "        plt.xlim(self.xlim)\n",
    "        plt.ylim(self.ylim)\n",
    "        ax.set_yticks([-1, 1, ])\n",
    "        ax.set_xticks([-2, -1, 0, 1, 2])\n",
    "        # Draw the arrows\n",
    "        self.__drawArrow(self.xlim[1], 0, 0.01, 0, 0.3, 0.2) # x-axis arrow\n",
    "        self.__drawArrow(0, self.ylim[1], 0, 0.01, 0.2, 0.3) # y-axis arrow\n",
    "        self.ax=ax\n",
    "        \n",
    "    def draw(self):\n",
    "        # First draw the axis\n",
    "        self.fig = plt.figure(figsize=self.figsize)\n",
    "        self.__drawAxis()\n",
    "\n",
    "axes = myAxes(xlim=(-2.5,2.5), ylim=(-2,2), figsize=(9,7))\n",
    "axes.draw()\n",
    "\n",
    "rectangle =patch.Rectangle((0,0), 3, 3, fc='blue',ec=\"blue\", alpha=.2)\n",
    "axes.ax.add_patch(rectangle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-minneapolis",
   "metadata": {},
   "source": [
    "Now, what is the unit ball? You are probably familiar with the idea of the unit ball, even if you haven't heard it called that specifically. Remember that the Euclidean norm for a point $\\vec x$ which has coordinates $x_i$ for $i=1,...,K$ is given by the expression:\n",
    "\\begin{align*}\n",
    "    \\left|\\left|\\vec x\\right|\\right|_2 = \\sqrt{\\sum_{i = 1}^K x_i^2}\n",
    "\\end{align*}\n",
    "The Euclidean unit ball is just the set of points whose Euclidean norm is at most $1$. To be more specific, the **closed unit ball** with the Euclidean norm is the set of points:\n",
    "\\begin{align*}\n",
    "    \\left\\{\\vec x \\in \\mathbb R^K :\\left|\\left|\\vec x\\right|\\right|_2 \\leq 1\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "We draw the $2$-dimensional unit ball with the Euclidean norm below, where the points that make up the unit ball are shown in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-episode",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "axes = myAxes(xlim=(-2.5,2.5), ylim=(-2,2), figsize=(9,7))\n",
    "axes.draw()\n",
    "\n",
    "circle =patch.Circle((0,0), 1, fc='red',ec=\"red\", alpha=.3)\n",
    "axes.ax.add_patch(circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-average",
   "metadata": {},
   "source": [
    "Now what is their intersection? Remember that the intersection of two sets $A$ and $B$ is the set:\n",
    "\\begin{align*}\n",
    "    A \\cap B &= \\{x : x \\in A, x \\in B\\}\n",
    "\\end{align*}\n",
    "That is, each element must be in *both* sets to be in the intersection. The interesction of the unit ball and the non-negative orthant will be the set:\n",
    "\n",
    "\\begin{align*}\n",
    "   \\mathcal X_K = \\left\\{\\vec x \\in \\mathbb R^K :\\left|\\left|\\vec x\\right|\\right|_2 \\leq 1, x_k \\geq 0 \\textrm{ for all $k$}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "visually, this will be the set of points in the *overlap* of the unit ball and the non-negative orthant, which we show below in purple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-florence",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "axes = myAxes(xlim=(-2.5,2.5), ylim=(-2,2), figsize=(9,7))\n",
    "axes.draw()\n",
    "\n",
    "circle =patch.Circle((0,0), 1, fc='red',ec=\"red\", alpha=.3)\n",
    "axes.ax.add_patch(circle)\n",
    "rectangle =patch.Rectangle((0,0), 3, 3, fc='blue',ec=\"blue\", alpha=.2)\n",
    "axes.ax.add_patch(rectangle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-preview",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "This space has an *incredibly* important corollary. It turns out that if $\\vec x$ and $\\vec y$ are both elements of $\\mathcal X_K$, that $\\left\\langle \\vec x, \\vec y \\right \\rangle = \\vec x^\\top \\vec y$, the **inner product**, is at most $1$, and at least $0$. Without getting too technical, this is because of something called the Cauchy-Schwartz inequality and the properties of $\\mathcal X_K$. If you remember from linear algebra, the Cauchy-Schwartz inequality states that $\\left\\langle \\vec x, \\vec y \\right \\rangle$ can be at most the product of $\\left|\\left|\\vec x\\right|\\right|_2$ and $\\left|\\left|\\vec y\\right|\\right|_2$. Since $\\vec x$ and $\\vec y$ have norms both less than or equal to $1$ (since they are on the *unit ball*), their inner-product is at most $1$. Further, since $\\vec x$ and $\\vec y$ are in the non-negative orthant, their inner product can never be negative. This is because both $\\vec x$ and $\\vec y$ have entries which are not negative, and therefore their element-wise products can never be negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-canyon",
   "metadata": {},
   "source": [
    "The *a posteriori* RDPG is to the *a priori* RDPG what the *a posteriori* SBM was to the *a priori* SBM. We instead suppose that we do *not* know the latent position matrix $X$, but instead know how we can characterize the individual latent positions. We have the following parameter:\n",
    "\n",
    "| Parameter | Space | Description |\n",
    "| --- | --- | --- |\n",
    "| F | inner-product distributions | A distribution which governs each latent position. |\n",
    "\n",
    "The parameter $F$ is what is known as an **inner-product distribution**. In the simplest case, we will assume that $F$ is a distribution on a subset of the possible real vectors that have $d$-dimensions with an important caveat: for any two vectors within this subset, their inner product *must* be a probability. We will refer to the subset of the possible real vectors as $\\mathcal X_K$, which we learned about above. This means that for any $\\vec x_i, \\vec x_j$ that are in $\\mathcal X_K$, it is always the case that $\\vec x_i^\\top \\vec x_j$ is between $0$ and $1$. This is essential because like previously, we will describe the distribution of each edge in the adjacency matrix using $\\vec x_i^\\top \\vec x_j$ to represent a probability. Next, we will treat the latent position matrix as a matrix-valued random variable which is *latent* (remember, *latent* means that we don't get to see it in our real data). Like before, we will call $\\vec{\\mathbf x}_i$ the random latent positions for the nodes of our network. In this case, each $\\vec {\\mathbf x}_i$ is sampled independently and identically from the inner-product distribution $F$ described above. The latent-position matrix is the matrix-valued random variable $\\mathbf X$ whose entries are the latent vectors $\\vec {\\mathbf x}_i$, for each of the $n$ nodes. \n",
    "\n",
    "The model for edges of the *a posteriori* RDPG can be described by conditioning on this unobserved latent-position matrix. We write down that, conditioned on $\\vec {\\mathbf x}_i = \\vec x$ and $\\vec {\\mathbf x}_j = \\vec y$, that if $j > i$, then $\\mathbf a_{ij}$ is sampled independently from a $Bern(\\vec x^\\top \\vec y)$ distribution. As before, if $i < j$, $\\mathbf a_{ji} = \\mathbf a_{ij}$ (the network is *undirected*), and $\\mathbf a_{ii} = 0$ (the network is *loopless*). If $\\mathbf A$ is the adjacency matrix for an *a posteriori* RDPG with parameter $F$, we write that $\\mathbf A \\sim RDPG_n(F)$. \n",
    "\n",
    "### Probability*\n",
    "\n",
    "The probability for the *a posteriori* RDPG is fairly complicated. This is because, like the *a posteriori* SBM, we do not actually get to see the latent position matrix $\\mathbf X$, so we need to use *integration* to obtain an expression for the probability. Here, we are concerned with realizations of $\\mathbf X$. Remember that $\\mathbf X$ is just a matrix whose rows are $\\vec {\\mathbf x}_i$, each of which individually have have the distribution $F$; e.g., $\\vec{\\mathbf x}_i \\sim F$ independently. For simplicity, we will assume that $F$ is a disrete distribution on $\\mathcal X_K$. This makes the logic of what is going on below much simpler since the notation gets less complicated, but does not detract from the generalizability of the result (the only difference is that sums would be replaced by multivariate integrals, and probability mass functions replaced by probability density functions). \n",
    "\n",
    "We will let $p$ denote the probability mass function (p.m.f.) of this discrete distribution function $F$. The strategy will be to use the independence assumption, followed by integration over the relevant rows of $\\mathbf X$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb P_\\theta(A) &= \\mathbb P_\\theta(\\mathbf A = A) \\\\\n",
    "&= \\prod_{j > i} \\mathbb P(\\mathbf a_{ij} = a_{ij}), \\;\\;\\;\\;\\textrm{Independence Assumption} \\\\\n",
    "\\mathbb P(\\mathbf a_{ij} = a_{ij})&= \\sum_{\\vec x \\in \\mathcal X_K}\\sum_{\\vec y \\in \\mathcal X_K}\\mathbb P(\\mathbf a_{ij} = a_{ij}, \\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y),\\;\\;\\;\\;\\textrm{integration over }\\vec {\\mathbf x}_i \\textrm{ and }\\vec {\\mathbf x}_j\n",
    "\\end{align*}\n",
    "Next, we will simplify this expression a little bit more, using the definition of a conditional probability like we did before for the SBM:\n",
    "\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "\\mathbb P(\\mathbf a_{ij} = a_{ij}, \\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y) &= \\mathbb P(\\mathbf a_{ij} = a_{ij}| \\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y) \\mathbb P(\\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y)\n",
    "\\end{align*}\n",
    "\n",
    "Further, remember that if $\\mathbf a$ and $\\mathbf b$ are independent, then $\\mathbb P(\\mathbf a = a, \\mathbf b = b) = \\mathbb P(\\mathbf a = a)\\mathbb P(\\mathbf b = b)$. Using that $\\vec x_i$ and $\\vec x_j$ are independent, by definition:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb P(\\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y) &= \\mathbb P(\\vec{\\mathbf x}_i = \\vec x) \\mathbb P(\\vec{\\mathbf x}_j = \\vec y)\n",
    "\\end{align*}\n",
    "\n",
    "Which means that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb P(\\mathbf a_{ij} = a_{ij}, \\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y) &=  \\mathbb P(\\mathbf a_{ij} = a_{ij} | \\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y)\\mathbb P(\\vec{\\mathbf x}_i = \\vec x) \\mathbb P(\\vec{\\mathbf x}_j = \\vec y)\n",
    "\\end{align*}\n",
    "Finally, we that conditional on $\\vec{\\mathbf x}_i = \\vec x_i$ and $\\vec{\\mathbf x}_j = \\vec x_j$, $\\mathbf a_{ij}$ is $Bern(\\vec x_i^\\top \\vec x_j)$. This means that in terms of our probability matrix, each entry $p_{ij} = \\vec x_i^\\top \\vec x_j$. Therefore:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb P(\\mathbf a_{ij} = a_{ij}| \\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y) &= (\\vec x^\\top \\vec y)^{a_{ij}}(1 - \\vec x^\\top\\vec y)^{1 - a_{ij}}\n",
    "\\end{align*}\n",
    "This implies that:\n",
    "\\begin{align*}\n",
    "\\mathbb P(\\mathbf a_{ij} = a_{ij}, \\vec{\\mathbf x}_i = \\vec x, \\vec{\\mathbf x}_j = \\vec y) &=  (\\vec x^\\top \\vec y)^{a_{ij}}(1 - \\vec x^\\top\\vec y)^{1 - a_{ij}}\\mathbb P(\\vec{\\mathbf x}_i = \\vec x) \\mathbb P(\\vec{\\mathbf x}_j = \\vec y)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "So our complete expression for the probability is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb P_\\theta(A) &= \\prod_{j > i}\\sum_{\\vec x \\in \\mathcal X_K}\\sum_{\\vec y \\in \\mathcal X_K} (\\vec x^\\top \\vec y)^{a_{ij}}(1 - \\vec x^\\top\\vec y)^{1 - a_{ij}}\\mathbb P(\\vec{\\mathbf x}_i = \\vec x) \\mathbb P(\\vec{\\mathbf x}_j = \\vec y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-exchange",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generalized Random Dot Product Graph (GRDPG)\n",
    "\n",
    "The Generalized Random Dot Product Graph, or GRDPG, is the most general random network model we will consider in this book. Note that for the RDPG, the probability matrix $P$ had entries $p_{ij} = \\vec x_i^\\top \\vec x_j$. What about $p_{ji}$? Well, $p_{ji} = \\vec x_j^\\top \\vec x_i$, which is exactly the same as $p_{ij}$! This means that even if we were to consider a directed RDPG, the probabilities that can be captured are *always* going to be symmetric. The generalized random dot product graph, or GRDPG, relaxes this assumption. This is achieved by using *two* latent positin matrices, $X$ and $Y$, and letting $P = X Y^\\top$. Now, the entries $p_{ij} = \\vec x_i^\\top \\vec y_j$, but $p_{ji} = \\vec x_j^\\top \\vec y_i$, which might be different.\n",
    "\n",
    "### *A Priori* GRDPG\n",
    "\n",
    "The *a priori* GRDPG is a GRDPG in which we know *a priori* the latent position matrices $X$ and $Y$. The *a priori* GRDPG has the following parameters:\n",
    "\n",
    "| Parameter | Space | Description |\n",
    "| --- | --- | --- |\n",
    "| $X$ | $ \\mathbb R^{n \\times d}$ | The matrix of left latent positions for each node $n$. |\n",
    "| $Y$ | $ \\mathbb R^{n \\times d}$ | The matrix of right latent positions for each node $n$. |\n",
    "\n",
    "$X$ and $Y$ behave nearly the same as the latent position matrix $X$ for the *a priori* RDPG, with the exception that they will be called the **left latent position matrix** and the **right latent position matrix** respectively. Further, the vectors $\\vec x_i$ will be the left latent positions, and $\\vec y_i$ will be the right latent positions, for a given node $i$, for each node $i=1,...,n$.\n",
    "\n",
    "What is the generative model for the *a priori* GRDPG? As we discussed above, given $X$ and $Y$, for all $j \\neq i$, $\\mathbf a_{ij} \\sim Bern(\\vec x_i^\\top \\vec y_j)$ independently. If we consider only loopless networks, $\\mathbf a_{ij} = 0$. If $\\mathbf A$ is an *a priori* GRDPG with left and right latent position matrices $X$ and $Y$, we write that $\\mathbf A \\sim GRDPG_n(X, Y)$.\n",
    "\n",
    "### *A Posteriori* GRDPG\n",
    "\n",
    "The *A Posteriori* GRDPG is very similar to the *a posteriori* RDPG. We have two parameters:\n",
    "\n",
    "| Parameter | Space | Description |\n",
    "| --- | --- | --- |\n",
    "| F | inner-product distributions | A distribution which governs the left latent positions. |\n",
    "| G | inner-product distributions | A distribution which governs the right latent positions. |\n",
    "\n",
    "Here, we treat the left and right latent position matrices as latent variable matrices, like we did for *a posteriori* RDPG. That is, the left latent positions are sampled independently and identically from $F$, and the right latent positions $\\vec y_i$ are sampled independently and identically from $G$. \n",
    "\n",
    "The model for edges of the *a posteriori* RDPG can be described by conditioning on the unobserved left and right latent-position matrices. We write down that, conditioned on $\\vec {\\mathbf x}_i = \\vec x$ and $\\vec {\\mathbf y}_j = \\vec y$, that if $j \\neq i$, then $\\mathbf a_{ij}$ is sampled independently from a $Bern(\\vec x^\\top \\vec y)$ distribution. As before, assuming the network is loopless, $\\mathbf a_{ii} = 0$. If $\\mathbf A$ is the adjacency matrix for an *a posteriori* RDPG with parameter $F$, we write that $\\mathbf A \\sim GRDPG_n(F, G)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-concrete",
   "metadata": {},
   "source": [
    "## Inhomogeneous Erd&ouml;s-R&eacute;nyi (IER)\n",
    "\n",
    "In the preceding models, we typically made assumptions about how we could characterize the edge-existence probabilities using fewer than $\\binom n 2$ different probabilities (one for each edge). The reason for this is that in general, $n$ is usually relatively large, so attempting to actually learn $\\binom n 2$ different probabilities is not, in general, going to be very feasible (it is *never* feasible when we have a single network, since a single network only one observation for each independent edge). Further, it is relatively difficult to ask questions for which assuming edges share *nothing* in common (even if they don't share the same probabilities, there may be properties underlying the probabilities, such as the *latent positions* that we saw above with the RDPG, that we might still want to characterize) is actually favorable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-radius",
   "metadata": {},
   "source": [
    "Nonetheless, the most general model for an independent-edge random network is known as the Inhomogeneous Erd&ouml;s-R&eacute;nyi (IER) Random Network. An IER Random Network is characterized by the following parameters:\n",
    "\n",
    "| Parameter | Space | Description |\n",
    "| --- | --- | --- |\n",
    "| $P$ | [0,1]$^{n \\times n}$ | The edge probability matrix. |\n",
    "\n",
    "The probability matrix $P$ is an $n \\times n$ matrix, where each entry $p_{ij}$ is a probability (a value between $0$ and $1$). Further, if we restrict ourselves to the case of simple networks like we have done so far, $P$ will also be symmetric ($p_{ij} = p_{ji}$ for all $i$ and $j$). The generative model is similar to the preceding models we have seen: given the $(i, j)$ entry of $P$, denoted $p_{ij}$, the edges $\\mathbf a_{ij}$ are independent $Bern(p_{ij})$, for any $j > i$. Further, $\\mathbf a_{ii} = 0$ for all $i$ (the network is *loopless*), and $\\mathbf a_{ji} = \\mathbf a_{ij}$ (the network is *undirected*). If $\\mathbf A$ is the adjacency maatrix for an IER network with probability matarix $P$, we write that $\\mathbf A \\sim IER_n(P)$.\n",
    "\n",
    "It is worth noting that *all* of the preceding models we have discussed so far are special cases of the IER model. This means that, for instance, if we were to consider only the probability matrices where all of the entries are the same, we could represent the ER models. Similarly, if we were to only to consider the probability matrices $P$ where $P = XX^\\top$, we could represent any RDPG.\n",
    "\n",
    "The IER Random Network can be thought of as the limit of Stochastic Block Models, as the number of communities equals the number of nodes in the network. Stated another way, an SBM Random Network where each node is in its own community is equivalent to an IER Random Network. Under this formulation, note that the block matarix for such an SBM, $B$, would have $n \\times n$ unique entries. Taking $P$ to be this block matrix shows that the IER is a limiting case of SBMs.\n",
    "\n",
    "### Probability*\n",
    "\n",
    "The probability for a network which is IER is very straightforward. We use the independence assumption, and the p.m.f. of a Bernoulli-distributed random-variable $\\mathbf a_{ij}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(A) &= \\mathbb P(\\mathbf A = A) \\\\\n",
    "    &= \\prod_{j > i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - a_{ij}}\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
