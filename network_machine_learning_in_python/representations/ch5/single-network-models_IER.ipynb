{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charming-province",
   "metadata": {},
   "source": [
    "(ch5:ier)=\n",
    "# Inhomogeneous Erdos Renyi (IER) Random Network Model\n",
    "\n",
    "Now that you've learned about the ER, SBM, and RDPG random networks, it's time to figure out why we keep using coin flips! To this end, you will learn about the most general model for an independent edge random network, the Inhomogeneous Erdos Renyi (IER) Random Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-outdoors",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graspologic.simulations import sample_edges\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "def my_unit_circle(r):\n",
    "   d = 2*r + 1\n",
    "   rx, ry = d/2, d/2\n",
    "   x, y = np.indices((d, d))\n",
    "   return (np.abs(np.hypot(rx - x, ry - y)-r) < 0.5).astype(int)\n",
    "\n",
    "def add_smile():\n",
    "    Ptmp = np.zeros((51, 51))\n",
    "    Ptmp[2:45, 2:45] = my_unit_circle(21)\n",
    "    mask = np.zeros((51, 51), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    upper_left = np.rot90(mask)\n",
    "    Ptmp[upper_left] = 0\n",
    "    return Ptmp\n",
    "    \n",
    "def smiley_prob(upper_p, lower_p):\n",
    "    smiley = add_smile()\n",
    "    P = my_unit_circle(25)\n",
    "    P[5:16, 25:36] = my_unit_circle(5)\n",
    "    P[smiley != 0] = smiley[smiley != 0]\n",
    "    \n",
    "    mask = np.zeros((51, 51), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    P[~mask] = 0\n",
    "    P = (P + P.T - np.diag(np.diag(P))).astype(float)\n",
    "    P[P == 1] = lower_p\n",
    "    P[P == 0] = upper_p\n",
    "    return P\n",
    "\n",
    "P = smiley_prob(.95, 0.05)\n",
    "A = sample_edges(P, directed=False, loops=False)\n",
    "\n",
    "draw_multiplot(A, title=\"IER Random Network\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-vertical",
   "metadata": {},
   "source": [
    "## The Inhomogeneous Erdos-Renyi (IER) Random Network Model is parametrized by a matrix of independent-edge probabilities\n",
    "\n",
    "The IER random network is the most general random network model for a binary graph. The way you can think of the IER random network is that a probability matrix $P$ with $n$ rows and $n$ columns defines each of the edge-existence probabilities for pairs of nodes in the network. For each pair of nodes $i$ and $j$, you have a unique coin which has a $p_{ij}$ chance of landing on heads, and a $1 - p_{ij}$ chance of landing on tails. If the coin lands on heads, the edge between nodes $i$ and $j$ exists, and if the coin lands on tails, the edge between nodes $i$ and $j$ does not exist. This coin flip is performed independently of the coin flips for all of the other edges. If $\\mathbf A$ is a random network which is $IER$ with a probability matrix $P$, we say that $\\mathbf A$ is an $IER_n(P)$ random network.\n",
    "\n",
    "\n",
    "### Generating a sample from an $IER_n(P)$ random network\n",
    "\n",
    "As before, you can develop a procedure to produce for you a network $A$, which has nodes and edges, where the underlying random network $\\mathbf A$ is an $IER_n(P)$ random network:\n",
    "\n",
    "```{admonition} Simulating a sample from an $IER_n(P)$ random network\n",
    "1. Choose a probability matrix $P$, whose entries $p_{ij}$ are probabilities.\n",
    "2. For each pair of nodes $i$ an $j$:\n",
    "    * Obtain a weighted coin $(i,j)$ which has a probability $p_{ij}$ of landing on heads, and a $1 - p_{ij}$ probability of landing on tails.\n",
    "    * Flip the $(i,j)$ coin, and if it lands on heads, the corresponding entry $a_{ij}$ in the adjacency matrix is $1$. If the coin lands on tails, the corresponding entry $a_{ij}$ is $0$. \n",
    "3. The adjacency matrix you produce, $A$, is a sample of an $IER_n(P)$ random network. \n",
    "```\n",
    "\n",
    "Let's put this in practice. Since you know about probability matrices from the section on RDPG, you'll first take a look at the probability matrix for your IER network. This example is very unnecessarily sophisticated, but we have a reason for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_unit_circle(r):\n",
    "   d = 2*r + 1\n",
    "   rx, ry = d/2, d/2\n",
    "   x, y = np.indices((d, d))\n",
    "   return (np.abs(np.hypot(rx - x, ry - y)-r) < 0.5).astype(int)\n",
    "\n",
    "def add_smile():\n",
    "    Ptmp = np.zeros((51, 51))\n",
    "    Ptmp[2:45, 2:45] = my_unit_circle(21)\n",
    "    mask = np.zeros((51, 51), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    upper_left = np.rot90(mask)\n",
    "    Ptmp[upper_left] = 0\n",
    "    return Ptmp\n",
    "    \n",
    "def smiley_prob(upper_p, lower_p):\n",
    "    smiley = add_smile()\n",
    "    P = my_unit_circle(25)\n",
    "    P[5:16, 25:36] = my_unit_circle(5)\n",
    "    P[smiley != 0] = smiley[smiley != 0]\n",
    "    \n",
    "    mask = np.zeros((51, 51), dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    P[~mask] = 0\n",
    "    P = (P + P.T - np.diag(np.diag(P))).astype(float)\n",
    "    P[P == 1] = lower_p\n",
    "    P[P == 0] = upper_p\n",
    "    return P\n",
    "\n",
    "P = smiley_prob(.95, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-message",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import heatmap\n",
    "\n",
    "ax = heatmap(P, vmin=0, vmax=1, title=\"Probability matrix for smiley face example\")\n",
    "ax.set_xlabel(\"Node\")\n",
    "ax.set_ylabel(\"Node\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-batch",
   "metadata": {},
   "source": [
    "If the edge is between a pair of nodes that defines the smiley face, the probability is $0.05$, whereas the edge probability for the other edges in the network is $0.95$. A network realization looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sample_edges(P, directed=False, loops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-villa",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "draw_multiplot(A, title=\"Sample from IER Network\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-clone",
   "metadata": {},
   "source": [
    "We used this example to show you the key idea behind the IER Network's probability matrix is simple: the entries can really be anything as long as they are probabilities (between $0$ and $1$), and the resulting matrix is symmetric (in the case of undirected networks). There are no additional requirements for structure to the network.\n",
    "\n",
    "## The IER model unifies independent-edge random network models\n",
    "\n",
    "It is important to realize that all of the networks you have learned so far are also IER random networks. The previous single network models you have covered to date simply place restrictions on the way in which you acquire $P$. For instance, in an $ER_n(p)$ random network, all entries $p_{ij}$ of $P$ can just be set equal to $p$. To see that an $SBM_n(\\vec z, B)$ random network is also $IER_n(P)$, you can construct the probability matrix $P$ such that $p_{ij} = b_{kl}$ of the block matrix $B$ when the community of node $i$ is $z_i = k$ and the community of node $j$ is $z_j = l$. To see that an $RDPG_n(X)$ random network is also $IER_n(P)$, you can construct the probability matrix $P$ such that $P = XX^\\top$. This shows that the IER random network is the most general of the single network models you have studied so far. \n",
    "\n",
    "```{figure} Images/unify_sn.png\n",
    "---\n",
    "align: center\n",
    "name: ch5:unify_sn\n",
    "---\n",
    "The IER network unifies independent-edge random networks, as it is the most general independent-edge random network model. What this means is that all models discussed so far, and many not discussed but found in the appendix, can be conceptualized as special cases of the IER network. Stated in other words, a probability matrix is the fundamental unit necessary to describe an independent-edge random network model. For the models covered so far, all GRDPGs are IER networks (by taking $P$ to be $XY^\\top$), all RDPGs are GRDPGs (by taking the right latent positions $Y$ to be equal to the left latent positions $X$), all SBMs are RDPGs, and all ERs are SBMs, whereas the converses in general are not true. \n",
    "```\n",
    "\n",
    "(ch5:ier:sbmprob)=\n",
    "### How do you obtain a probability matrix for an SBM programmatically?\n",
    "\n",
    "One thing that will come up frequently over the course of this book is that we will need to sometimes programmatically generate the probability matrix directly (rather than just using the community assignment vector $\\vec z$ and the block probability matrix $B$). This isn't too difficult to do. \n",
    "\n",
    "Basically, the idea is as follows. If $\\vec z$ is the community assignment vector and assigns the $n$ nodes of the network to one of $K$ communities (taking values from $1$, $2$, ..., $K$), and $B$ is the $K \\times K$ block probability matrix:\n",
    "1. You begin by constructing a matrix, $Z$, that has $n$ rows and $K$ columns; one row for each node, and one column for each community. \n",
    "2. For each node in the network $i$:\n",
    "    + For each community in the network $k$, if $z_i = k$, then let $z_{ik} = 1$. if $z_i \\neq k$, then let $z_{ik} = 0$. \n",
    "3. Let $P = ZBZ^\\top$.\n",
    "\n",
    "The matrix produced, $P$, is the *full* probability matrix for the SBM, where entry $p_{ij}$ is the probability that node $i$ is connected to node $j$. \n",
    "\n",
    "Let's work through this example programmatically, because it comes up a few times over the course of the book. Let's imagine that we have $50$ nodes, of which the first $25$ are in community one and the second $25$ are in community two. The code to do this conversion looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_community_mtx(z):\n",
    "    \"\"\"\n",
    "    A function to generate the one-hot-encoded community\n",
    "    assignment matrix from a community assignment vector.\n",
    "    \"\"\"\n",
    "    K = len(np.unique(z))\n",
    "    n = len(z)\n",
    "    Z = np.zeros((n, K))\n",
    "    for i, zi in enumerate(z):\n",
    "        Z[i, zi - 1] = 1\n",
    "    return Z\n",
    "\n",
    "# the community assignment vector\n",
    "z = [1 for i in range(0, 25)] + [2 for i in range(0, 25)]\n",
    "Z = make_community_mtx(z)\n",
    "# block matrix\n",
    "B = np.array([[0.6, 0.3], [0.3, 0.6]])\n",
    "# probability matrix\n",
    "P = Z @ B @ Z.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-backing",
   "metadata": {},
   "source": [
    "### Limitations of the IER model when you have one network\n",
    "\n",
    "This construction is not entirely useful when you have one sample, because for each edge, you only get to see one sample (either the edge exists or doesn't exist). This means that you won't really be able to learn about the individual edge probabilities in practice when you only have a single sample of a network in a way which would differentiate them from other single networks. What we mean by this is, if you learn about other single networks, you are still *technically* learning about an IER random network, because like you saw in the last paragraph, all of these other single network models are just *special cases* of the IER random network. However, you can't learn any random network with one sample that could *only* be described by an IER random network, so it almost feels a *little bit useless* from what you've covered so far. As you will learn shortly, the IER random network is, on the other hand, *extremely* useful when you have multiple networks. When you have multiple networks, with some limited assumptions, you *can* learn about networks that you would otherwise not be able to describe with more simple random networks like ER, SBM, or RDPG random netweorks. This will make the IER random network *extremely powerful* when you have multiple samples of network data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abae66e-9f23-4027-b417-ba3b8d4da177",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "The survey paper {cite:p}`Athreya2017Jan` and the appendix section {numref}`app:ch12:foundation` give an overview of the theoretical properties of IER networks. \n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
