{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why embed networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks by themselves can have interesting properties, but a network is not how you traditionally organize data in computer science. In almost any ML algorithm - whether you're using a neural network or a decision tree, whether your goal is to classify observations or to predict values using regression - you'll see data organized into a matrix, where the rows represent observations and the columns represent features, or variables. Each observation, or row of the matrix, is traditionally represented as a single point in $d$-dimensional space (if there are $d$ columns in the matrix). If you have two columns, for instance, you could represent data organized in this way on an x/y coordinate plane. The first column would represent the x-axis, and the second column would represent the y-axis.\n",
    "\n",
    "For example, the data below is organized traditionally. On the left is the data matrix; each observation has its own row, with two features across the columns. The x-column contains the first feature for each observation, and the y-column contains the second feature for each observation. You can see the two clusters of data numerically, through the color mapping.\n",
    "\n",
    "On the right is the same data, but plotted in Euclidean space. Each column of the data matrix gets its own axis in the plot. The x and y axis location of the $i^{th}$ point in the scatterplot is the same as the x and y values of the $i^{th}$ row of the data matrix. You can see the two clusters of data geometrically, through the location of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "# make the data\n",
    "centers = np.array([[-2, -2], \n",
    "                    [2, 2]])\n",
    "X, labels = make_blobs(n_samples=10, cluster_std=0.5,\n",
    "                  centers=centers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from graphbook_code import GraphColormap, draw_cartesian\n",
    "\n",
    "# convert data into a DataFrame\n",
    "data = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "# setup\n",
    "cmap = GraphColormap(\"divergent\", k=2).color\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = GridSpec(1, 3)\n",
    "axm = fig.add_subplot(gs[0])\n",
    "axs = fig.add_subplot(gs[1:])\n",
    "\n",
    "# plot left\n",
    "hm = sns.heatmap(data, ax=axm, yticklabels=False, \n",
    "                 cmap=cmap, annot=True, cbar=True)\n",
    "hm.hlines(range(len(data)), *hm.get_xlim(), colors='k', alpha=.1)\n",
    "\n",
    "# plot right\n",
    "draw_cartesian(ax=axs)\n",
    "plot = sns.scatterplot(data=data, x='x', y='y', legend=False, ax=axs, color=\"k\")\n",
    "\n",
    "# lines\n",
    "max_ = int(data.values.max()) + 1\n",
    "plot.vlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "plot.hlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "\n",
    "# ticks\n",
    "plot.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "plot.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "# set axis bounds\n",
    "lim = (-max_, max_)\n",
    "plot.set(xlim=lim, ylim=lim)\n",
    "\n",
    "# title, etc\n",
    "plt.suptitle(\"Euclidean data represented as a data matrix and represented in Euclidean space\", fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful for our data to be organized like this, since it opens the door to a wide variety of machine learning methods. With the data above, for example, you could use scikit-learn to perform simple K-Means Clustering to find the two clusters of observations. Below, you import scikit-learn's K-Means clustering algorithm. K-Means finds a pre-determined number of clusters in your data by setting randomly determined starting-points, and then iterating to get these points closer to the true cluster means. It outputs the community membership labels for each observation, which you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "predicted_labels = KMeans(n_clusters=2).fit_predict(X)\n",
    "print(\"Predicted labels: \", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "from graphbook_code import cmaps\n",
    "\n",
    "# plot right\n",
    "plot = sns.scatterplot(data=data, x='x', y='y', ax=ax, \n",
    "                       hue=predicted_labels, palette=cmaps[\"qualitative\"])\n",
    "\n",
    "# lines\n",
    "plot.vlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "plot.hlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "\n",
    "# ticks\n",
    "plot.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "plot.yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "\n",
    "# title\n",
    "plot.set_title(\"Clustered data after K-Means\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network-valued data are different. Take the Stochastic Block Model below, shown as both a layout plot and an adjacency matrix. Say your goal is to view the nodes as particular observations, and you'd like to cluster the data in the same way you clustered the Euclidean data above. Intuitively, you'd expect to find two groups: one for the first set of heavily connected nodes, and one for the second set. Unfortunately, traditional machine learning algorithms won't work on data represented as a network: it doesn't live in the traditional rows-as-observations, columns-as-features format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from graspologic.simulations import sbm\n",
    "np.random.seed(1)\n",
    "\n",
    "p = np.array([[.9, .1],\n",
    "              [.1, .9]])\n",
    "A, labels = sbm([25, 25], p, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import draw_multiplot\n",
    "axs = draw_multiplot(A, labels=labels, title=\"A Network With Two Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You, of course, *can* make up methods which work directly on networks - algorithms which run by traversing along edges, for instance, or which use network statistics like node degree to learn, and so on - and data scientists have developed many algorithms like this. But to be able to use the entire toolbox that machine learning offers, you'd like to be able to figure out a way to *represent* networks in Euclidean space as tabular data. This is why having good embedding methods, like Spectral Embedding (which we'll learn about soon), is useful. There's another problem with networks that make embedding into lower-dimensional space useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High dimensionality of network data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other problem with network data is its high dimensionality. You could view each element of an adjacency matrix as its own (binary, for unweighted networks) dimension, for instance -- although you could also make the argument that talking about dimensionality doesn't even make *sense* with network data, since it doesn't live in Euclidean space. Regardless, if you were to view the elements of the adjacency matrix as their own dimensions, you can get to a fairly unmanageable number of dimensions fairly quickly. Many dimensions can generally be unmanageable largely because of a machine learning concept called the *curse of dimensionality*, described below.\n",
    "\n",
    "```{admonition} The Curse of Dimensionality\n",
    "Our intuition often fails when observations have a lot of features -- meaning, observations that, when you think of them geometrically, are points in very high-dimensional space. \n",
    "\n",
    "For example, pick an observation randomly in a 10,000-dimensional unit hypercube (meaning, a $1 \\times 1 \\times \\dots \\times 1$ cube, with ten thousand 1s). You can also just think of this point as a vector with 10,000 elements, each of which has a value between 0 and 1. There's a probability greater than 99.999999% that the point will be located a distance less than .001 from a border of the hypercube. This probability is only 0.4% in a unit square. This actually makes intuitive sense: if you think about measuring a lot of attributes of an object, there's a decent chance it'll be extreme in at least one of those attributes. Take yourself, for example. You're probably normal in a lot of ways, but I'm sure you can think of a part of yourself which is extreme compared to other people.\n",
    "\n",
    "An even bigger shocker: if you pick two random points in a unit square with two dimensions, they'll be on average 0.52 units of distance away from each other. However, if you pick two random points in a unit hypercube with a million dimensions, they'll be around 408 units away from each other. This implies that, on average, any set of points that you generate from some random process when you're in high dimensions will be extremely far away from each other.\n",
    "\n",
    "What this comes down to is that almost every observation in ultra-high dimensions is extremely lonely, hugging the edge of the space it lives in, all by itself. These facts mess with many traditional machine learning methods which use relative distances, or averages (very few points in high-dimensional space will actually be anywhere near their average!) {cite:t}`homl`\n",
    "```\n",
    "\n",
    "### The dimensions are intrinsically linked\n",
    "\n",
    "If you are familiar with linear regression, you can understand another major problem when dealing with network data. Not only are there a *lot* of dimensions to network data, but these dimensions are *intrinsically linked* to one another. \n",
    "\n",
    "Let's start you off by first doing a quick recap of a logistic regression problem. Let's say that you have $100$ lobsters, and you know that these lobsters are a mix of male and female lobsters. Lobsters are *indeterminate growers*, which means that they just keep growing as they get older and older. Of these lobsters, you know whether they are male or female, and you also know their age. As it turns out, male lobsters have *substantially* larger claws than female lobsters. You want to learn whether, given whether a lobster is male or female and its age, can you predict its claw size?\n",
    "\n",
    "Because the lobster keeps growing, its age is obviously going to be extremely impactful on the size of its claw. Also, because male lobsters have bigger claws than female lobsters, the biological sex of the lobster will be a big factor on the size of the claw. You come up with a model that the age and biological sex operate as predictors for the claw size. You do this by just saying that claw size is a function of biological sex and age. Perhaps, on average, being male tends to increase claw size by $2$ inches on average, and every year of life tends to add another few millimeters to its claw size.\n",
    "\n",
    "When you force this through the wheel of linear regression like this, you are shortchanging yourself. You might be able to use your logistic regression to conclude that being male tends to make lobster claws larger, and being older tends to make lobster claws larger, but you will *totally miss* that for male lobsters who are old, the impact on claw size is a *lot* more! This is called an *interaction*, and the basic idea is that having a *set* of characteristics in conjunction (such as being both male *and* old) has a substantial role on your overall regression. If you steer clear of this when you do your linear regression, you have *completely* missed a valuable part of the system, and your algorithm won't work quite as well as it could. If you add this additional term to your model (biological sex, age, *and* the interaction of biological sex with age) you will do a much better job at predicting claw size.\n",
    "\n",
    "All the same, networks have this sort of coupling of the edges. This is called *dependence*, which (without getting too technical), means that edges are related. Let's imagine you have a social network of $100$ students. Here, the nodes are the students, and the edges are whether or not a pair of students are friends. If you knew that the school president was extremely popular, then all of the edges in the network that are associated with the school president are also *interacting*, in that if you knew that an edge had a node which represented the school president, it would be *much* more likely to exist (since the school president has many friends). While this would be an extreme example, as you can see, these sorts of *dependences* permeate through the entire network. If Alice is friends with Bob and Bob is friends with Desean, it might make sense to think that Alice might be more likely to be friends with Desean too. This would be another type of edge interaction. This pattern just builds and builds in network data. Since there are an enormous number of edges to start with, and a similarly enormous number of interactions between edges and all of the other edges they tend to be associated with, successfully capturing the system underlying the network becomes a real headache.\n",
    "\n",
    "## How do you embed network data?\n",
    "\n",
    "\n",
    "This is where network embedding methods come into play. Because networks represented as adjacency matrices are extremely high-dimensional, they run into many of the issues described above. Embedding, much like traditional dimensionality reduction methods in machine learning like Principle Component Analysis (PCA), allows us to move down to a more manageable number of dimensions while still preserving useful information about the network.\n",
    "\n",
    "Embedding is, first and foremost, a tool. As you read previously, you can rarely perform analyses on network data in its rawest form. Embedding is the \"bridge\" which provides functionality you need (namely, making the data *smaller*, and *simpler*) so that you can churn it through the machine learning algorithms you are familiar with from elsewhere on your network data. You can embed your network, and doing *nothing else*, answer questions you might have about it. There is, however, another role that embedding has. If you make some assumptions about the network, you can also use embeddings to *learn* valuable representations of your data that will allow you to answer more formal questions than you could using simple, out-of-the-box machine learning algorithms. Using embeddings coupled with statistical models, you can *estimate* parameters for the statistical models you learned about in [Chapter 5](#link?), and then use statistical properties about these parameters to perform *model-based* inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You often embed to estimate latent positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding methods which we'll explore the most in this book are the spectral methods. These methods pull heavily from linear algebra to keep the information about our network which is useful for learning about statistical network models - and use that information to place nodes in Euclidean space. We'll explore other methods as well. It's worth it to know a bit of linear algebra review here, particularly on concepts like eigenvectors and eigenvalues, as well as the properties of symmetric matrices. We'll guide you as clearly as possible through the math in future sections.\n",
    "\n",
    "Spectral embedding methods in particular, which we'll talk about in the next section, will estimate an embedding called the latent position matrix. This is an $n \\times d$ matrix (where this are $n$ rows, one for each node, and $d$ dimensions for each row). The latent position matrix is thus organized like a traditional data table, with nodes corresponding to observations, and you could plot the rows as points in Euclidean space. You'll have a gentle introduction here before we dive into the weeds in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the heck is the latent position matrix, anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actually is a latent position? How can you interpret a latent position matrix?\n",
    "\n",
    "To recap from [Chapter 5](#link?), assuming you believe your network might be some type of random dot product graph (remember that this can include SBMs, ER networks, and more), you can think of every node as being secretly associated with a position in Euclidean space. This position (relative to the positions associated with other nodes) tells you the probability that one node will have an edge with another node.\n",
    "\n",
    " Remember that $X$ has $n$ rows (the number of nodes) and $d$ columns (the number of dimensions). Although in practice you almost never know what the latent position matrix *actually* is, you can *estimate it* by embedding your network.\n",
    "\n",
    "You're going to cheat a bit and use an embedding method (in this case, adjacency spectral embedding) before we've discussed it, just to show what this looks like. In the next section, you'll learn how this embedding is happening, but for now, just think of it as a way to estimate the latent positions for the nodes of a network and move from network space to Euclidean space.\n",
    "\n",
    "Below you make a network, which in this case is an SBM. From the network, you can estimate a set of latent positions, where $n=20$ rows for each node and $d=2$ dimensions. Usually when something is an estimation for something else in statistics, you put a hat over it: $\\hat{X}$. We'll do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import sbm\n",
    "from graspologic.embed import AdjacencySpectralEmbed as ASE\n",
    "import numpy as np\n",
    "\n",
    "# make a network\n",
    "B = np.array([[0.8, 0.1], \n",
    "              [0.1, 0.8]])\n",
    "n = [10, 10]\n",
    "A, labels = sbm(n=n, p=B, return_labels=True)\n",
    "\n",
    "# embed\n",
    "ase = ASE(n_components=2)\n",
    "X = ase.fit_transform(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from graphbook_code import cmaps, plot_latents\n",
    "\n",
    "fig = plt.figure(figsize=(5, 10))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "cmap = cmaps[\"sequential\"]\n",
    "ax = sns.heatmap(X, cmap=cmap, cbar=False, xticklabels=1, yticklabels=2, ax=ax)\n",
    "ax.set_title(\"A Set Of Estimated Latent Positions $\\hat{X}$ \\n(Matrix Representation)\", \n",
    "             loc='left', fontdict={\"fontsize\": 20});\n",
    "\n",
    "ax_eucl = fig.add_axes([1.2, 0, 2, 1])\n",
    "plot_latents(X, labels=labels, title=\"A Set of Estimated Latent Positions $\\hat{X}$ (Euclidean Representation)\", \n",
    "             fontdict={\"fontsize\": 20}, s=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to emphasize here that you're modeling our networks as *random dot-product graphs* (RDPGs). One implication is that you can think of a network as having some underlying probability distribution, and any specific network is one of many possible realizations of that distribution. It also means that each edge in our network has some *probability* of existing: nodes 0 and 3, for instance, may or may not have an edge. The concept of a latent position only works under the assumption that the network is drawn from an RDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating a probability matrix from the estimated patent positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can find an estimate of the block probability matrix $\\hat P$ for your network using the latent positions. How would you generate $\\hat P$ from $\\hat X$?\n",
    "\n",
    "Well, you'd just multiply it by its transpose: $\\hat P = \\hat X\\hat X^\\top$. This operation will take the dot product between every row of $X$ and put it in the result. $(\\hat X\\hat X^\\top)_{ij}$ will just be the dot product between rows $i$ and $j$ of the estimated latent position matrix (which is the estimated probability that nodes $i$ and $j$ will be connected). So, $\\hat X\\hat X^\\top$ is just the $n \\times n$ estimated probability matrix - if you've estimated your latent positions using samples of networks, you can also estimate the probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphbook_code import text\n",
    "\n",
    "\n",
    "shape = (X.shape[0]//2, X.shape[0]//2)\n",
    "B0 = np.full(shape, .8)\n",
    "B1 = np.full(shape, .1)\n",
    "\n",
    "# block probability matrix\n",
    "B = np.block([[B0, B1],\n",
    "              [B1, B0]])\n",
    "\n",
    "\n",
    "\n",
    "from graphbook_code import heatmap\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "heatmap(X@X.T, title=\"Estimated block \\nprobability matrix\", ax=axs[0], cbar=False);\n",
    "heatmap(B, title=\"Actual block \\nprobability matrix\", ax=axs[1], cbar=False)\n",
    "\n",
    "# text\n",
    "text(\".8\", .25, .75)\n",
    "text(\".8\", .75, .25)\n",
    "text(\".1\", .25, .25)\n",
    "text(\".1\", .75, .75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thinking About latent positions geometrically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also think about this stuff geometrically. The dot product between any two vectors $\\vec x_i$ and $\\vec x_j$, geometrically, is their lengths multiplied together and then weighted by the cosine of the angle between them. Smaller angles have cosines close to 1, and larger angles have cosines close to 0. So, nodes whose latent positions have larger angles between them tend to have lower edge probabilities, and nodes whose latent positions have smaller angles between them tend to have higher edge probabilities. This is the core intuition you need to understand why you can find communities and do downstream inference with latent position matrices: two nodes whose latent positions are further apart will have a smaller probability of having an edge between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "####### First Ax\n",
    "# background plot\n",
    "ax = axs[0]\n",
    "plot = sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=labels,\n",
    "    s=80,\n",
    "    ax=ax,\n",
    "    palette=\"tab10\",\n",
    "    color=\"k\",\n",
    ")\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.set_xlim([-1.2, 1.2])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "\n",
    "# plot vector arrows\n",
    "u_i = X[0]\n",
    "u_j = X[-1]\n",
    "ax.arrow(0, 0, u_i[0], u_i[1], head_width=0.03, head_length=0.03, fc='lightblue', ec='black')\n",
    "ax.arrow(0, 0, u_j[0], u_j[1], head_width=0.03, head_length=0.03, fc='orange', ec='black')\n",
    "\n",
    "# Text and text arrow\n",
    "style = \"Simple, tail_width=0.5, head_width=4, head_length=8\"\n",
    "kw = dict(arrowstyle=style, color=\"k\", alpha=.3)\n",
    "text_arrow = patches.FancyArrowPatch((0.2, 0.15), (.05, .01), connectionstyle=\"arc3, rad=.2\", **kw)\n",
    "txt = r\"\"\"\n",
    "angle close to 90°, cos(angle) close to 0, so \n",
    "dot product = probability of edge smaller\n",
    "\"\"\"\n",
    "ax.text(0.22, 0.07, txt)\n",
    "ax.add_patch(text_arrow)\n",
    "\n",
    "ax.set_title(\"Latent Positions In Different Communities \\nHave A Lower Dot Product\", y=1.05, fontsize=22);\n",
    "\n",
    "####### Second Ax\n",
    "ax = axs[1]\n",
    "plot = sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=labels,\n",
    "    s=80,\n",
    "    ax=ax,\n",
    "    palette=\"tab10\",\n",
    "    color=\"k\",\n",
    ")\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.set_xlim([-1.2, 1.2])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "\n",
    "# plot vector arrows\n",
    "u_i = X[-1]\n",
    "u_j = X[-2]\n",
    "ax.arrow(0, 0, u_j[0], u_i[1], head_width=0.03, head_length=0.03, fc='orange', ec='black')\n",
    "ax.arrow(0, 0, u_j[0], u_j[1], head_width=0.03, head_length=0.03, fc='orange', ec='black')\n",
    "\n",
    "# Text and text arrow\n",
    "style = \"Simple, tail_width=0.5, head_width=4, head_length=8\"\n",
    "kw = dict(arrowstyle=style, color=\"k\", alpha=.3)\n",
    "text_arrow = patches.FancyArrowPatch((0.2, .15), (0, 0), connectionstyle=\"arc3, rad=.7\", **kw)\n",
    "txt = r\"\"\"\n",
    "angle close to 0°, cos(angle) close to 1, so \n",
    "dot product = probability of edge larger\n",
    "\"\"\"\n",
    "ax.text(0.22, 0.07, txt)\n",
    "ax.add_patch(text_arrow)\n",
    "\n",
    "ax.set_title(\"Latent Positions In The Same Community \\nHave A Higher Dot Product\", y=1.05, fontsize=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an *estimate* for the latent positions, there's math that shows that you get a pretty good estimate for the block probability matrix as well. In practice, that's what you're actually doing: getting an estimate of the latent positions with spectral embedding, then using those to do more downstream tasks or estimating block probability matrices."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
