{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why embed networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks by themselves are interesting objects, but a network is not how we traditionally organize data in machine learning. In almost any ML algorithm - whether you're using a neural network, or a decision tree, or whether your goal is to classify observations or to predict a value using regression - you'll see data organized into a matrix, where the rows represent observations and the columns represent features, or variables. Each observation, then, or row of the matrix, is traditionally represented as a single point in d-dimensional space: each column gets its own axis in a plot.\n",
    "\n",
    "For example, the data below is organized traditionally. On the left is the data matrix; you can see that there are two feature columns, one for each axis. The x-column contains the x-coordinates for each datapoint, and the x-column contains the y-coordinates for each data point. We can see two clusters of data numerically.\n",
    "\n",
    "On the right is the same data, but plotted in Euclidean space. Each column of the data matrix gets its own axis of the plot, so that the x and y axis location of the $i^{th}$ observation in the scatterplot is the same as the x and y values of the $i^{th}$ row of the data matrix. We can see the two clusters of data geometrically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# make the data\n",
    "centers = np.array([[-2, -2], \n",
    "                    [2, 2]])\n",
    "X, labels = make_blobs(n_samples=10, cluster_std=0.5,\n",
    "                  centers=centers, shuffle=False)\n",
    "\n",
    "# convert data into a DataFrame\n",
    "data = pd.DataFrame(X, columns=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from graphbook_code import GraphColormap\n",
    "\n",
    "# setup\n",
    "cmap = GraphColormap(\"divergent\", k=2).color\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = GridSpec(1, 3)\n",
    "axm = fig.add_subplot(gs[0])\n",
    "axs = fig.add_subplot(gs[1:])\n",
    "\n",
    "# plot left\n",
    "hm = sns.heatmap(data, ax=axm, yticklabels=False, \n",
    "                 cmap=cmap, annot=True, cbar=True)\n",
    "hm.hlines(range(len(data)), *hm.get_xlim(), colors='k', alpha=.1)\n",
    "\n",
    "# plot right\n",
    "plot = sns.scatterplot(data=data, x='x', y='y', legend=False, ax=axs)\n",
    "\n",
    "# lines\n",
    "max_ = int(data.values.max()) + 1\n",
    "plot.vlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "plot.hlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "\n",
    "# ticks\n",
    "plot.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "plot.yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "\n",
    "# set axis bounds\n",
    "lim = (-max_, max_)\n",
    "plot.set(xlim=lim, ylim=lim)\n",
    "\n",
    "# title, etc\n",
    "plt.suptitle(\"Euclidean data represented as a data matrix and represented in Euclidean space\", fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning methods require our data to be organized like this. With the data above, for example, we could use scikit-learn to perform simple K-Means Clustering to find two groups of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "predicted_labels = KMeans(n_clusters=2).fit_predict(X)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "# plot right\n",
    "plot = sns.scatterplot(data=data, x='x', y='y', ax=ax, \n",
    "                       hue=predicted_labels, palette=cmap)\n",
    "\n",
    "# lines\n",
    "plot.vlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "plot.hlines(0, -max_, max_, colors=\"black\", lw=.9, linestyle=\"dashed\", alpha=.2)\n",
    "\n",
    "# ticks\n",
    "plot.xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "plot.yaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "\n",
    "# title\n",
    "plot.set_title(\"Clustered data after K-Means\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network-valued data is different. Take the Stochastic Block Model below, shown as both a layout plot and an adjacency matrix. Say your goal is to view the nodes as particular datapoints, and you'd like to cluster the data in the same way you clustered the Euclidean data above. Intuitively, you'd expect to find two groups: one for the first set of heavily connected nodes, and one for the second set. Unfortunately, traditional machine learning algorithms won't work on data represented as a network: it doesn't live in the traditional rows-as-observations, columns-as-features format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from graspologic.simulations import sbm\n",
    "np.random.seed(1)\n",
    "\n",
    "p = np.array([[.9, .1],\n",
    "              [.1, .9]])\n",
    "A, labels = sbm([25, 25], p, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%aimport graphbook_code\n",
    "%autoreload 2\n",
    "\n",
    "from graphbook_code import draw_multiplot\n",
    "axs = draw_multiplot(A, labels=labels, title=\"A Network With Two Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You, of course, *can* make up methods which work directly on networks - algorithms which run by traversing along edges, for instance, or which use network statistics like node degree to learn, and so on - and the machine learning field has developed many algorithms like this. But to be able to use the entire toolbox that machine learning offers, without having to design special network-specific methods, you'd like to be able to figure out a way to *represent* networks in Euclidean space, as tabular data. This is why having good embedding methods, like Spectral Embedding, is useful. There's another problem with networks that make embedding into lower-dimensional space useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Dimensionality of Network Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other problem with network data is its high dimensionality. When you view the nodes as dimensions, each node has a number of dimensions equal to its possible edge count - so if you have n nodes, each node has n-1 dimensions. This can get to a fairly unmanageable number of dimensions when you have a lot of nodes fairly quickly, largely because of a machine learning concept called the *curse of dimensionality*. \n",
    "\n",
    "```{admonition} The Curse of Dimensionality\n",
    "Our intuition often fails when observations have a lot of features -- meaning, observations that, when you think of them geometrically, are points in very high-dimensional space. \n",
    "\n",
    "For example, pick a point randomly in a 10,000-dimensional unit hypercube (meaning, a $1 \\times 1 \\times \\dots \\times 1$ cube, with ten thousand 1s). You can also just think of these point as a vector with 10,000 elements, each of which has a value between 0 and 1. There's a probability greater than 99.999999% that the point will be located a distance less than .001 from a border. This probability is only 0.4% in a unit square. This actually makes intuitive sense: if you think about measuring a lot of attributes of an object, there's a decent chance it'll be extreme in at least one of those attributes. Take yourself, for example. You're probably normal in a lot of ways, but I'm sure you can think of a part of yourself which is extreme compared to other people.\n",
    "\n",
    "An even bigger shocker: if you pick two random points in a unit square with two dimensions, they'll be on average 0.52 units of distance away from each other. However, if you pick two random points in a unit hypercube with a million dimensions, they'll be around 408 units away from each other. This implies that, on average, any set of points that you generate from some random process when you're in high dimensions will be extremely far away from each other.\n",
    "\n",
    "What this comes down to is that almost every point in ultra-high dimensions is extremely lonely, hugging the edge of the space it lives in, all by itself. These facts mess with many traditional machine learning methods which use relative distances, or averages (very few points in high-dimensional space will actually be anywhere near their average!) {cite:t}`homl`\n",
    "```\n",
    "\n",
    "This is where network embedding methods come into play. Because networks are extremely high-dimensional, they inherently have many of the issues described above. Embedding, much like traditional dimensionality reduction methods in machine learning like Principle Component Analysis (PCA), allow us to move down to a much more manageable number of dimensions while still preserving information about the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Often Embed To Find Latent Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding methods which we'll explore the most in this book are the spectral methods. These methods pull heavily from linear algebra to keep only the information about our network which is useful - and use that information to place nodes in Euclidean space. We'll explore other methods as well. It's worth it to know a bit of linear algebra review here, particularly on concepts like eigenvectors and eigenvalues, as well as the properties of symmetric matrices. We'll guide you as clearly as possible thorugh the math in future sections.\n",
    "\n",
    "Spectral embedding methods in particular, which we'll talk about in the next section, will give you an embedding called the latent position matrix. This is an $n \\times d$ matrix (where this are $n$ data points, one for each row, and $d$ dimensions for each data point). The latent position matrix is thus organized like a traditional data table, and you could plot the rows as points in space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What The Heck Is The Latent Position Matrix, Anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actually is a latent position? How can we interpret a latent position matrix?\n",
    "\n",
    "Well, assuming you're viewing your network as some type of random dot product graph (or a stochastic block model - remember that this is a particular kind of random dot product graph), you can think of every node as being secretly associated with a position in space. This position (relative to the position for other nodes) tells you the probability that one node will have an edge with another node.\n",
    "\n",
    "Let's call the latent position matrix, $X$. Remember that $X$ has $n$ rows and $d$ columns. We're going to cheat a bit and use an embedding method (in this case, adjacency spectral embedding) before we've discussed it. In the next section, you'll learn how this embedding is happening, but for now, just think of it as a way to estimate the latent positions for the nodes of a network.\n",
    "\n",
    "Below is a set of latent positions, where $n=20$ nodes and $d=2$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import sbm\n",
    "from graspologic.embed import AdjacencySpectralEmbed as ASE\n",
    "import numpy as np\n",
    "\n",
    "# make a network\n",
    "B = np.array([[0.8, 0.1], \n",
    "              [0.1, 0.8]])\n",
    "n = [10, 10]\n",
    "A, labels = sbm(n=n, p=B, return_labels=True)\n",
    "\n",
    "# embed\n",
    "ase = ASE(n_components=2)\n",
    "X = ase.fit_transform(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from graphbook_code import cmaps, plot_latents\n",
    "\n",
    "fig = plt.figure(figsize=(5, 10))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "cmap = cmaps[\"sequential\"]\n",
    "ax = sns.heatmap(X, cmap=cmap, cbar=False, xticklabels=1, yticklabels=2, ax=ax)\n",
    "ax.set_title(\"A Set Of Estimated Latent Positions $\\hat{X}$ \\n(Matrix Representation)\", \n",
    "             loc='left', fontdict={\"fontsize\": 20});\n",
    "\n",
    "ax_eucl = fig.add_axes([1.2, 0, 2, 1])\n",
    "plot_latents(X, labels=labels, title=\"A Set of Estimated Latent Positions $\\hat{X}$ (Euclidean Representation)\", \n",
    "             fontdict={\"fontsize\": 20}, s=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we're modeling our networks as *random dot-product graphs*. This means we can think of a network as having some underlying probability distribution, and any specific network is one of many possible realizations of that distribution. It also means that each edge in our network has some *probability* of existing: nodes 0 and 3, for instance, may or may not have an edge. The chance they do has some probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Latent Position Matrix Tells You About Edge Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent position matrix $X$ that we can find from spectral embedding is related to these edge probabilities: If you take the dot product (or the weighted sum) of row $i$ with row $j$, you'll get an *estimate* of the probability that nodes $i$ and $j$ have an edge between them. Incidentally, this means that the dot product between any two rows of the latent position matrix has to be bound between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking About Latent Positions Geometrically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about this dot product edge probabilities stuff geometrically. The dot product between any two vectors $u_i$ and $u_j$, geometrically, is their lengths multiplied together and then weighted by the cosine of the angle between them. Smaller angles have cosines close to 1, and larger angles have cosines close to 0. So, nodes whose latent positions have larger angles between them tend to have lower edge probabilities, and nodes whose latent positions have smaller angles between them tend to have higher edge probabilities. This is the core intuition you need to understand why you can find communities and do downstream inference with latent position matrices: two nodes whose latent positions are further apart will have a smaller probability of having an edge between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "####### First Ax\n",
    "# background plot\n",
    "ax = axs[0]\n",
    "plot = sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=labels,\n",
    "    s=80,\n",
    "    ax=ax,\n",
    "    palette=\"tab10\",\n",
    "    color=\"k\",\n",
    ")\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.set_xlim([-1.2, 1.2])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "\n",
    "# plot vector arrows\n",
    "u_i = X[0]\n",
    "u_j = X[-1]\n",
    "ax.arrow(0, 0, u_i[0], u_i[1], head_width=0.03, head_length=0.03, fc='lightblue', ec='black')\n",
    "ax.arrow(0, 0, u_j[0], u_j[1], head_width=0.03, head_length=0.03, fc='orange', ec='black')\n",
    "\n",
    "# Text and text arrow\n",
    "style = \"Simple, tail_width=0.5, head_width=4, head_length=8\"\n",
    "kw = dict(arrowstyle=style, color=\"k\", alpha=.3)\n",
    "text_arrow = patches.FancyArrowPatch((0.2, 0.15), (.05, .01), connectionstyle=\"arc3, rad=.2\", **kw)\n",
    "txt = r\"\"\"\n",
    "angle close to 90°, cos(angle) close to 0, so \n",
    "dot product = probability of edge smaller\n",
    "\"\"\"\n",
    "ax.text(0.22, 0.07, txt)\n",
    "ax.add_patch(text_arrow)\n",
    "\n",
    "ax.set_title(\"Latent Positions In Different Communities \\nHave A Lower Dot Product\", y=1.05, fontsize=22);\n",
    "\n",
    "####### Second Ax\n",
    "ax = axs[1]\n",
    "plot = sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=labels,\n",
    "    s=80,\n",
    "    ax=ax,\n",
    "    palette=\"tab10\",\n",
    "    color=\"k\",\n",
    ")\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.set_xlim([-1.2, 1.2])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "\n",
    "# plot vector arrows\n",
    "u_i = X[-1]\n",
    "u_j = X[-2]\n",
    "ax.arrow(0, 0, u_j[0], u_i[1], head_width=0.03, head_length=0.03, fc='orange', ec='black')\n",
    "ax.arrow(0, 0, u_j[0], u_j[1], head_width=0.03, head_length=0.03, fc='orange', ec='black')\n",
    "\n",
    "# Text and text arrow\n",
    "style = \"Simple, tail_width=0.5, head_width=4, head_length=8\"\n",
    "kw = dict(arrowstyle=style, color=\"k\", alpha=.3)\n",
    "text_arrow = patches.FancyArrowPatch((0.2, .15), (0, 0), connectionstyle=\"arc3, rad=.7\", **kw)\n",
    "txt = r\"\"\"\n",
    "angle close to 0°, cos(angle) close to 1, so \n",
    "dot product = probability of edge larger\n",
    "\"\"\"\n",
    "ax.text(0.22, 0.07, txt)\n",
    "ax.add_patch(text_arrow)\n",
    "\n",
    "ax.set_title(\"Latent Positions In The Same Community \\nHave A Higher Dot Product\", y=1.05, fontsize=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making A Block Probability Matrix From The Latent Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the latent position matrix $X$ has $n$ rows, with each row corresponding to a node in your network, and the dot product between rows $i$ and $j$ of $X$ is the probability that node $i$ will connect with node $j$. (Here, remember that $X$ is from the adjacency matrix, not the Laplacian). The block probability matrix $P$ has the edge probability between nodes $i$ and $j$ in the $(i, j)_{th}$ position. How would you generate $P$ from $X$?\n",
    "\n",
    "Well, you'd just multiply it by its transpose: $P = XX^\\top$. This operation will take the dot product between every row of $X$ and put it in the result: $(XX^\\top)_{ij}$ will just be the dot product between rows $i$ and $j$ of the latent position matrix (which is the probability that nodes $i$ and $j$ will be connected). So, $XX^\\top$ is just an $n \\times n$ block probability matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphbook_code import text\n",
    "\n",
    "shape = (X.shape[0]//2, X.shape[0]//2)\n",
    "B0 = np.full(shape, .8)\n",
    "B1 = np.full(shape, .1)\n",
    "\n",
    "# block probability matrix\n",
    "B = np.block([[B0, B1],\n",
    "              [B1, B0]])\n",
    "\n",
    "\n",
    "\n",
    "from graphbook_code import heatmap\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "heatmap(X@X.T, title=\"Estimated block \\nprobability matrix\", ax=axs[0], cbar=False);\n",
    "heatmap(B, title=\"Actual block \\nprobability matrix\", ax=axs[1], cbar=False)\n",
    "\n",
    "# text\n",
    "text(\".8\", .25, .75)\n",
    "text(\".8\", .75, .25)\n",
    "text(\".1\", .25, .25)\n",
    "text(\".1\", .75, .75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an *estimate* for the latent positions, there's math that shows that you get a pretty good estimate for the block probability matrix as well. In practice, that's what you're actually doing: getting an estimate of the latent positions with spectral embedding, then using those to do more downstream tasks or estimating block probability matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
