{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Parameters in Network Models via MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you learned about random networks which can be described using single network models, one of the key things we covered were the *parameters* that define the underlying random networks. If you see a network which is a sample of a random network, you do *not*, in practice, know what those parameters that describe the random network are. However, you have a slight problem, because learning about the underlying random network *requires* us to have some understanding of the parameters that define it. What are you to do?\n",
    "\n",
    "To overcome this hurdle, you must *estimate* the parameters of the underlying random network. At a very high level, **estimation** is a procedure to calculate properties about a random variable (or a set of random variables) using only the data you are given: finitely many (in network statistics, often just one) samples which you assume are samples of the random variable you want to learn about. Here, what you want to obtain are ways in which you can *estimate* the parameters of the underlying random network, when you have a sample of a random network.\n",
    "\n",
    "## Erdös-Rényi (ER)\n",
    "\n",
    "Recall that the Erdös-Rényi (ER) network has a single parameter: the probability of each edge existing, which we termed $p$. Due to the simplicity of an ER random network, we can resort to a technique called Maximum Likelihood Estimation (MLE), through which you can obtain a reasonable *guess* at $p$, which for various reasons, is *desirable*. You can look at [Chapter 7](#link?) for more details as to why this estimate is desirable.\n",
    "\n",
    "We'll develop some intuition for this procedure by going back to our coin flip example. You have a coin, for which you don't know the probability it lands on heads is. However, you are told you can flip that coin, and after $100$ flips, you have to guess what the probability of heads is for the coin. For instance, if you flip the coin $100$ times, and it lands on heads $45$ times, what would you guess that the probability the coin lands on heads would be?\n",
    "\n",
    "If you thought to yourself that you would guess $\\frac{45}{100}$, which is just the ratio of the number of heads that you saw to the total number of coin flips, you would be correct. This is called the *maximum likelihood estimate* of the probability for a binary (heads or tails, $1$ or $0$) random variable. For the ER random network, this works exactly the same way. You find that the best estimate of the probability of an edge existing in an ER random network is just the ratio of the total number of edges in the network, $m$, divided by the total number of edges possible in the network, which is $\\binom n 2$! Our result is:\n",
    "\\begin{align*}\n",
    "    \\hat p &= \\frac{m}{\\binom n 2}.\n",
    "\\end{align*}\n",
    "\n",
    "To bring this back to our coin flip example, this is like you are saying that there is a single coin. You flip the coin once for every possible edge between those pairs of communities, $\\binom n 2$. When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads you obtained, $m$, and divide by the number of coin flips you made, $\\binom n 2$. \n",
    "\n",
    "Let's work on an example. You will use a sample of a random network which is ER, with $40$ nodes and an edge probability of $0.2$. You begin by simulating and visualizing the appropriate network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import er_np\n",
    "\n",
    "p = 0.2\n",
    "A = er_np(n=40, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import draw_multiplot\n",
    "draw_multiplot(A, title=\"Simulated $ER_{40}(0.2)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you fit the appropriate model, from graspologic, and plot the estimated probability matrix $\\hat P$ against the true probability matrix $P$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graspologic.models import EREstimator\n",
    "\n",
    "model = EREstimator(directed=False, loops=False)\n",
    "model.fit(A)\n",
    "Phat = model.p_mat_\n",
    "\n",
    "P = p*np.ones((40, 40))  # default entries to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "heatmap(Phat,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat P_{ER}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "P = P - np.diag(np.diag(P))\n",
    "\n",
    "heatmap(P,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$P_{ER}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "heatmap(np.abs(Phat - P),\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$|\\hat P_{ER} - P_{ER}|$\",\n",
    "        ax=axs[2])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not half bad! The estimated probability matrix $\\hat P$ looks extremely similar to the true probability matrix $P$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Block Model\n",
    "\n",
    "The Stochastic Block Model also has a single parameter: the block matrix, $B$, whose entries $b_{kk'}$ denote the probabilities of edges existing or not existing between pairs of communities in the Stochastic Block Model. When you apply the method of MLE to the SBM, what you find is that, where $m_{kk'}$ is the total number of edges between nodes in communities $k$ and $k'$, and $n_{kk'}$ is the number of edges possible between nodes in communities $k$ and $k'$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat b_{kk'} = \\frac{m_{kk'}}{n_{kk'}}.\n",
    "\\end{align*}\n",
    "\n",
    "Intuitively, the estimate of the block probability $b_{kk'}$ is the ratio of how many edges you see between communities $k$ and $k'$ $m_{kk'}$ and how many edges you could have seen $n_{kk'}$! To bring this back to our coin flip example, this is like you are saying that there is one coin called coin $(k, k')$ for each pair of communities in our network. You flip each coin once for every possible edge between those pairs of communities, $n_{kk'}$. When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads you obtained, $m_{kk'}$, and divide by the number of coin flips you made, $n_{kk'}$. \n",
    "\n",
    "Let's work through an example network, with 20 nodes in each community, and a block matrix of:\n",
    "\\begin{align*}\n",
    "    B &= \\begin{bmatrix}\n",
    "        .8 & .2 \\\\\n",
    "        .2 & .8\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Which corresponds to a probability matrix $P$ where each entry is:\n",
    "\\begin{align*}\n",
    "    p_{ij} &= \\begin{cases}\n",
    "    0.8 & i, j \\leq 20 \\text{ or }i, j \\geq 20 \\\\\n",
    "    0.2 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "You begin by simulating an appropriate SBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import sbm\n",
    "\n",
    "n = [20, 20]\n",
    "B = [[.8, .2],\n",
    "     [.2, .8]]\n",
    "\n",
    "A = sbm(n=n, p=B)\n",
    "\n",
    "z = [0 for i in range(0,n[0])] + [1 for i in range(0, n[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_multiplot(A, labels=z, title=\"Simulated SBM(B)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit an appropriate SBM, and investigate the estimate of $B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.models import SBMEstimator\n",
    "\n",
    "model = SBMEstimator(directed=False, loops=False)\n",
    "model.fit(A, y=z)\n",
    "Bhat = model.block_p_\n",
    "Phat = model.p_mat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "heatmap(Bhat,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat B_{SBM}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "heatmap(np.array(B),\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$B_{SBM}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "heatmap(np.abs(Bhat - np.array(B)),\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$|\\hat B_{SBM} - B_{SBM}|$\",\n",
    "        ax=axs[2])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our estimate $\\hat B$ is very similar to the true block matrix $B$. This is further reflected by looking at the probability matrix, like you did for the ER example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "heatmap(Phat,\n",
    "        inner_hier_labels=z,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat P_{SBM}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "P = 0.2*np.ones((n[0] + n[1], n[0] + n[1]))  # default entries to 0.2\n",
    "P[0:20,0:20] = 0.8  # B11\n",
    "P[20:40,20:40] = 0.8  # B22\n",
    "np.fill_diagonal(P, 0)  # loopless\n",
    "\n",
    "heatmap(P,\n",
    "        inner_hier_labels=z,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$P_{SBM}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "heatmap(np.abs(Phat - P),\n",
    "        inner_hier_labels=z,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$|\\hat P_{SBM} - P_{SBM}|$\",\n",
    "        ax=axs[2])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
