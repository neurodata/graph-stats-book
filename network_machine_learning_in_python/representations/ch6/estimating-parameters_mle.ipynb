{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Parameters in Network Models via MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout Chapter 5, we spent a lot of attention developing intuition for many of the network models that are essential to understanding random networks. Recall that the notation that we use for a random network (more specifically, a network-valued random variable), $\\mathbf A$, does *not* refer to any network we could ever hope to see (or as we introduced in the previous chapter, *realize*) in the real world. This issue is extremely important in network machine learning, so we will try to drive it home one more time: no matter how much data we collected (unless we could get infinite data, which we *can't*), we can never hope to understand the true distribution of $\\mathbf A$. As network scientists, this leaves us with a bit of a problem: what, then, can we do to make useful claims about $\\mathbf A$, if we can't actually see $\\mathbf A$ nor its distribution?\n",
    "\n",
    "This is where statistics, particularly, **estimation**, comes into play. At a very high level, estimation is a procedure to calculate properties about a random variable (or a set of random variables) using *only* the data we are given: finitely many (in network statistics, often just *one*) samples which we assume are *realizations* of the random variable we want to learn about. The properties of the random variable that we seek to learn about are called **estimands**, and  In the case of our network models, in particular, we will attempt to obtain reasonable estimates of the parameters (our *estimands*) associated with random networks.\n",
    "\n",
    "The most useful property that we will leverage which was developed in Chapter $5$ is the independent-edge assumption. As we discussed, when working with independent-edge random network models, we will assume that edges in our random network are *independent*. This means that the probability of observing a particular realization of a random network is, in fact, the product of the probabilities of observing each edge in the random network. Notationally, what this means is that if $\\mathbf A$ is a random network with $n$ nodes and edges $\\mathbf a_{ij}$, and $A$ is a realization of that random network with edges $a_{ij}$, then:\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(\\mathbf A = A) &= \\mathbb P(\\mathbf a_{11} = a_{11}, \\mathbf a_{12} = a_{12}, ..., \\mathbf a_{nn} = a_{nn}) \\\\\n",
    "    &= \\prod_{i, j} \\mathbb P_\\theta(\\mathbf a_{ij} = a_{ij})\n",
    "\\end{align*}\n",
    "In the special case where our networks are simple (undirected and loopless), this simplifies to:\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(\\mathbf A = A) &= \\prod_{i < j} \\mathbb P_\\theta(\\mathbf a_{ij} = a_{ij})\n",
    "\\end{align*}\n",
    "for any network realization $A$ which is simple. This is because if $\\mathbf a_{ij} = a$, then we also know that $\\mathbf a_{ji} = a$, and we only need to worry about one of the edges (we chose the edges in the upper right triangle of the adjacency matrix arbitrarily).  Further, since $A$ is also simple, then we know hat $\\mathbf a_{ii} = 0$; that is, no nodes have loops, so we don't need to worry about the case where $i = j$ either.\n",
    "\n",
    "We will set the scene for the later examples using a common example. Let's say we flip a coin $10$ times, and see $6$ heads. What is the probability that the coin lands on heads? Intuitively, the answer is rather simple! It feels like it should just be $\\frac{6}{10}$. And in one particular way, that really is the *best* guess we could make!\n",
    "\n",
    "Below, we discuss the nitty-gritty technical details of how we learn about random networks using a particular method known as Maximum Likelihood Estimation (MLE). Maximum likelihood estimation is why $\\frac{6}{10}$ is a great guess for our coin flip example. Finding MLEs can be pretty difficult, so we leave the details in starred sections. If you aren't familiar with MLE, you can skip these, and still learn how to use the results!\n",
    "\n",
    "## The Method of Maximum Likelihood Estimation (MLE)*\n",
    "\n",
    "Let's think about what exactly this means using an example that you are likely familiar with. I have a single coin, and I want to know the probability of the outcome of a roll of that coin being a heads. For sake of argument, we will call this coin *fair*, which means that the true probability it lands on heads (or tails) is $0.5$. In this case, I would call the outcome of the $i^{th}$ coin flip the random variable $\\mathbf x_i$, and it can produce realizations which take one of two possible values: a heads (an outcome of a $1$) or a tails (an outcome of a $0$). We will say that we see $10$ total coin flips. We will number these realizations as $x_i$, where $i$ goes from $1$ to $10$. To recap, the boldfaced $\\mathbf x_i$ denotes the random variable, and the unbolded $x_i$ denotes the realization which we actually see. Our question of interest is: how do we estimate the probability of the coin landing on a heads, if we don't know anything about the true probability value $p$, other than the outcomes of the coin flips we got to observe?\n",
    "\n",
    "Here, since $\\mathbf x_i$ takes the value $1$ or $0$ each with probability $0.5$, we would say that $\\mathbf x_i$ is a $Bernoulli(0.5)$ random variable. This means that the random variable $\\mathbf x$ has the Bernoulli distribution, and the probability of a heads, $p$, is $0.5$. All $10$ of our $\\mathbf x_i$ are called *identically distributed*, since they all have the same $Bernoulli(0.5)$ distribution.\n",
    "\n",
    "We will also assume that the outcomes of the coin flips are mutually independent, which is explained in the terminology section.\n",
    "\n",
    "For any one coin flip, the probability of observing the outcome $i$ is, by definition of the Bernoulli distribution:\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(\\mathbf x_i = x_i) = p^{x_i} (1 - p)^{1 - x_i}\n",
    "\\end{align*}\n",
    "Note that we use the notation $\\mathbb P_\\theta$ to indicate that the probability is a function of the parameter set $\\theta$ for the random variable $\\mathbf x_i$. Here, since the only parameter for each $\\mathbf x_i$ is a probability $p$, then $\\theta = p$.\n",
    "\n",
    "If we saw $n$ total outcomes, the probability is, using the definition of mutual independence:\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(\\mathbf x_1 = x_1, ..., \\mathbf x_{n} = x_{n}; p) &= \\prod_{i = 1}^{n}\\mathbb P(\\mathbf x_i = x_i) \\\\\n",
    "    &= \\prod_{i = 1}^n p^{x_i}(1 - p)^{1 - x_i} \\\\\n",
    "    &= p^{\\sum_{i = 1}^n x_i}(1 - p)^{n - \\sum_{i = 1}^n x_i}\n",
    "\\end{align*}\n",
    "What if we saw $10$ coin flips, and $6$ were heads? Can we take a \"guess\" at what $p$ might be? Intuitively your first reaction might be to say a good guess of $p$, which we will abbreviate $\\hat p$, would be $0.6$, which is $6$ heads of $10$ outcomes. In many ways, this intuitive guess is spot on. However, in network machine learning, we like to be really specific about why, exactly, this guess makes sense. \n",
    "\n",
    "Looking at the above equation, one thing we can do is use the technique of **maximum likelihood estimation**. We call the function $\\mathbb P(\\mathbf x_1 = x_1, ..., \\mathbf x_n = x_n; p)$ the *likelihood* of our sequence, for a given value of $p$. Note that we have added the term \"$; p$\" to our notation, which is simply to emphasize the dependence of the likelihood on the probability. So, what we *really* want to do is find the value that $p$ could take, which *maximizes* the likelihood. Let's see what the likelihood function looks like as a function of different values of $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = np.linspace(.02, .98, num=49)\n",
    "nflips = 10; nheads = 6\n",
    "likelihood = p**(nheads)*(1 - p)**(nflips - nheads)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(x=p, y=likelihood, ax=ax)\n",
    "ax.axvline(.6, color=\"red\", linestyle=\"--\")\n",
    "ax.set(xlabel=\"Bernoulli probability parameter, p\", ylabel=\"Likelihood, $P_{\\\\theta}(x_1, ..., x_{10})$\")\n",
    "ax;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it turns out that our intuitive answer, that $p=0.6$, is in fact the Maximum Likelihood Estimate for the Bernoulli probability parameter $p$. Now how do we go about showing this rigorously?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier problem, we often will find, is to instead maximize the *log likelihood* rather than the likelihood itself. This is because the log function is *monotone*, which means that if $\\mathbb P(\\mathbf x_1 = x_1, ..., \\mathbf x_n = x_n; p_1) < \\mathbb P(\\mathbf x_1 = x_1, ..., \\mathbf x_n = x_n; p_2)$, then $\\log\\mathbb P(\\mathbf x_1 = x_1, ..., \\mathbf x_n = x_n; p_1) < \\log \\mathbb P(\\mathbf x_1 = x_1, ..., \\mathbf x_n = x_n; p_2)$ as well for some choices $p_1$ and $p_2$. Without going too down in the weeds, the idea is that the $\\log$ function does not change any critical points of the likelihood. The log likelihood of the above expression is:\n",
    "\\begin{align*}\n",
    "\\log \\mathbb P(\\mathbf x_1 = x_1, ..., \\mathbf x_{n} = x_{n}; p) &= \\log \\left[p^{\\sum_{i = 1}^n x_i}(1 - p)^{n - \\sum_{i = 1}^n x_i}\\right] \\\\\n",
    "&= \\sum_{i = 1}^n x_i \\log(p) + \\left(n - \\sum_{i = 1}^n x_i\\right)\\log(1 - p)\n",
    "\\end{align*}\n",
    "And visually, the log-likelihood now looks instead like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "loglikelihood = nheads*np.log(p) + (nflips - nheads)*np.log(1 - p)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(x=p, y=loglikelihood, ax=ax)\n",
    "ax.axvline(.6, color=\"red\", linestyle=\"--\")\n",
    "ax.set(xlabel=\"Bernoulli probability parameter, p\", ylabel=\"Log Likelihood, $\\\\log P_{\\\\theta}(x_1, ..., x_{10})$\")\n",
    "ax;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can see that the two plots look *almost* nothing alike, the key is the word *almost* here. Notice that the absolute maximum is, in fact, the same regardless of whether we use the likelihood or the log-likelihood. Further, notice that at the maximum, the slope of the tangent line is $0$. You may recall from calculus that this is how we typically go about finding a critical point of a function. Now, let's get make our argument a little more technical. Remembering from calculus $1$ and $2$, to find a maximal point of the log-likelihood function with respect to some variable $p$, our process looks like this:\n",
    "1. Take the derivative of the log-likelihood with respect to $p$,\n",
    "2. Set it equal to $0$ and solve for the critical point $\\tilde p$,\n",
    "3. Verify that the critical point $\\tilde p$ is indeed an estimate of a maximum, $\\hat p$. \n",
    "\n",
    "Proceeding using the result we derived above, and using the fact that $\\frac{d}{du} \\log(u) = \\frac{1}{u}$ and that $\\frac{d}{du} \\log(1 - u) = -\\frac{1}{1 - u}$:\n",
    "\\begin{align*}\n",
    "\\frac{d}{d p}\\log \\mathbb P(\\mathbf x_1 = x_1, ..., \\mathbf x_{n} = x_{n}; p) &= \\frac{\\sum_{i = 1}^n x_i}{p} - \\frac{n - \\sum_{i = 1}^n x_i}{1 - p} = 0 \\\\\n",
    "\\Rightarrow \\frac{\\sum_{i = 1}^n x_i}{p} &= \\frac{n - \\sum_{i = 1}^n x_i}{1 - p} \\\\\n",
    "\\Rightarrow (1 - p)\\sum_{i = 1}^n x_i &= p\\left(n - \\sum_{i = 1}^n x_i\\right) \\\\\n",
    "\\sum_{i = 1}^n x_i - p\\sum_{i = 1}^n x_i &= pn - p\\sum_{i = 1}^n x_i \\\\\n",
    "\\Rightarrow \\tilde p &= \\frac{1}{n}\\sum_{i = 1}^n x_i\n",
    "\\end{align*}\n",
    "We use the notation $\\tilde p$ here to denote that $\\tilde p$ is a critical point of the function.\n",
    "\n",
    "Finally, we must check that this is an estimate of a maximum, which we can do by taking the second derivative and checking that the second derivative is negative. We will omit this since it's a bit intricate and tangential from our argument, but if you work it through, you will find that the second derivative is indeed negative at $\\tilde p$. This means that $\\tilde p$ is indeed an estimate of a maximum, which we would denote by $\\hat p$.\n",
    "\n",
    "Finally, using this result, we find that with $6$ heads in $10$ outcomes, we would obtain an estimate:\n",
    "\\begin{align*}\n",
    "    \\hat p &= \\frac{6}{10} = 0.6\n",
    "\\end{align*}\n",
    "which exactly aligns with our intuition.\n",
    "\n",
    "So, why do we need estimation tools, if in our example, our intuition gave us the answer a whole lot faster? Unfortunately, the particular scenario we described was one of the *simplest possible examples* in which a parameter requires estimation. As the scenario grows more complicated, and *especially* when we extend to network-valued data, figuring out good ways to estimate parameters is extremely difficult. For this reason, we will describe some tools which are very relevant to network machine learning to learn about network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will review estimation techniques for several of the approaches we discussed in Chapter 5, for Single Network Models.\n",
    "\n",
    "## Erdös-Rényi (ER)\n",
    "\n",
    "Recall that the Erdös-Rényi (ER) network has a single parameter: the probability of each edge existing, which we termed $p$. Due to the simplicity of a random network which is ER, we can resort to the Maximum Likelihood technique we described above, and it turns out we obtain virtually the same result. We find that the best estimate of the probability of an edge existing in an ER random network is just the ratio of the total number of edges in the network, $m$, divided by the total number of edges possible in the network, which is $\\binom n 2$! Our result is:\n",
    "\\begin{align*}\n",
    "    \\hat p &= \\frac{m}{\\binom n 2}\n",
    "\\end{align*}\n",
    "\n",
    "Intuitively, the estimate of the probability $p$ is the ratio of how many edges we see in the network, $m$, and how many edges we could have seen $\\binom n 2$! To bring this back to our coin flip example, this is like we are saying that there is a single coin. We flip the coin once for every possible edge between those pairs of communities, $\\binom n 2$. When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads we obtained, $m$, and divide by the number of coin flips we made, $\\binom n 2$. \n",
    "\n",
    "Let's work on an example. We will use a realization of a random network which is ER, with $40$ nodes and an edge probability of $0.2$. We begin by simulating and visualizing the appropriate network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import er_np\n",
    "from graphbook_code import draw_multiplot\n",
    "\n",
    "A = er_np(n=40, p=0.2)\n",
    "\n",
    "draw_multiplot(A, title=\"Simulated ER(0.2)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the appropriate model, from graspologic, and plot the estimated probability matrix $\\hat P$ against the true probability matrix $P$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.plot import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from graspologic.models import EREstimator\n",
    "\n",
    "model = EREstimator(directed=False, loops=False)\n",
    "model.fit(A)\n",
    "Phat = model.p_mat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "heatmap(Phat,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat P_{ER}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "P = 0.2*np.ones((40, 40))  # default entries to 0.2\n",
    "P = P - np.diag(np.diag(P))\n",
    "\n",
    "heatmap(P,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$P_{ER}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not half bad! The estimated probability matrix $\\hat P$ looks extremely similar to the true probability matrix $P$.\n",
    "\n",
    "### MLE for ER*\n",
    "\n",
    "In Chapter 5, we explored the derivation for the probability of observing a realization $A$ of a given random network $\\mathbf A$ which is ER, which is equivalent to the likelihood of $A$. Recall this was:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(A) &= p^{m} \\cdot (1 - p)^{\\binom{n}{2} - m}\n",
    "\\end{align*}\n",
    "\n",
    "where $m = \\sum_{i < j} a_{ij}$ is the total number of edges in the observed network $A$. Our approach here parallels directly the approach for the coin; we begin by taking the log of the probability:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\log \\mathbb P_\\theta(A) &= \\log \\left[p^{m} \\cdot (1 - p)^{\\binom{n}{2} - m}\\right] \\\\\n",
    "    &= m \\log p + \\left(\\binom n 2 - m\\right)\\log (1 - p)\n",
    "\\end{align*}\n",
    "\n",
    "Next, we take the derivative with respect to $p$, set equal to $0$, and we end up with:\n",
    "\\begin{align*}\n",
    "\\frac{d}{d p}\\log \\mathbb P_\\theta(A) &= \\frac{m}{p} - \\frac{\\binom n 2 - m}{1 - p} = 0 \\\\\n",
    "\\Rightarrow \\tilde p &= \\frac{m}{\\binom n 2}\n",
    "\\end{align*}\n",
    "We omitted several detailed steps due to the fact that we show the rigorous derivation above. Checking the second derivative, which we omit since it is rather mathematically tedious, we see that the second derivative at $\\tilde p$ is negative, so we indeed have found an estimate of the maximum, and will be denoted by $\\hat p$. This gives that the Maximum Likelihood Estimate (or, the MLE, for short) of the probability $p$ for a random network $\\mathbf A$ which is ER is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat p &= \\frac{m}{\\binom n 2}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *a priori* Stochastic Block Model\n",
    "\n",
    "The *a priori* Stochastic Block Model also has a single paramter: the block matrix, $B$, whose entries $b_{kk'}$ denote the probabilities of edges existing or not existing between pairs of communities in the Stochastic Block Model. When we apply the method of MLE to the SBM, what we find is that, where $m_{kk'}$ is the total number of edges between nodes in communities $k$ and $k'$, and $n_{kk'}$ is the number of edges possible between nodes in communities $k$ and $k'$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat b_{kk'} = \\frac{m_{kk'}}{n_{kk'}}\n",
    "\\end{align*}\n",
    "\n",
    "Intuitively, the estimate of the block probability $b_{kk'}$ is the ratio of how many edges we see between communities $k$ and $k'$ $m_{kk'}$ and how many edges we could have seen $n_{kk'}$! To bring this back to our coin flip example, this is like we are saying that there is one coin called coin $(k, k')$ for each pair of communities in our network. We flip each coin once for every possible edge between those pairs of communities, $n_{kk'}$. When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads we obtained, $m_{kk'}$, and divide by the number of coin flips we made, $n_{kk'}$. \n",
    "\n",
    "Let's work through an example network, with 20 nodes in each community, and a block matrix of:\n",
    "\\begin{align*}\n",
    "    B &= \\begin{bmatrix}\n",
    "        .8 & .2 \\\\\n",
    "        .2 & .8\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Which corresponds to a probability matrix $P$ where each entry is:\n",
    "\\begin{align*}\n",
    "    p_{ij} &= \\begin{cases}\n",
    "    0.8 & i, j \\leq 20 \\text{ or }i, j \\geq 20 \\\\\n",
    "    0.2 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "We begin by simulating an appropriate SBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.simulations import sbm\n",
    "\n",
    "n = [20, 20]\n",
    "B = [[.8, .2],\n",
    "     [.2, .8]]\n",
    "\n",
    "A = sbm(n=n, p=B)\n",
    "\n",
    "y = [0 for i in range(0,n[0])] + [1 for i in range(0, n[1])]\n",
    "draw_multiplot(A, labels=y, title=\"Simulated SBM(B)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit an appropriate SBM, and investigate the estimate of $B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspologic.models import SBMEstimator\n",
    "\n",
    "model = SBMEstimator(directed=False, loops=False)\n",
    "model.fit(A, y=y)\n",
    "Phat = model.p_mat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "heatmap(Phat,\n",
    "        inner_hier_labels=y,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$\\hat P_{SBM}$\",\n",
    "        ax=axs[0])\n",
    "\n",
    "P = 0.2*np.ones((n[0] + n[1], n[0] + n[1]))  # default entries to 0.2\n",
    "P[0:20,0:20] = 0.8  # B11\n",
    "P[20:40,20:40] = 0.8  # B22\n",
    "np.fill_diagonal(P, 0)  # loopless\n",
    "\n",
    "heatmap(P,\n",
    "        inner_hier_labels=y,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        font_scale=1.5,\n",
    "        title=\"$P_{SBM}$\",\n",
    "        ax=axs[1])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our estimate $\\hat P$ is very similar to the true probability matrix $P$.\n",
    "\n",
    "### MLE for SBM*\n",
    "\n",
    " When we derived the probability for a realization $A$ of a random network $\\mathbf A$ which could be characterized using the *a priori* Stochasic Block Model, we obtained that:\n",
    "\\begin{align*}\n",
    "    \\mathbb P_\\theta(A) &= \\prod_{k, k' \\in [K]}b_{k'k}^{m_{k'k}} \\cdot (1 - b_{k'k})^{n_{k'k - m_{k'k}}}\n",
    "\\end{align*}\n",
    "\n",
    "where $n_{k'k} = \\sum_{i < j}\\mathbb 1_{\\tau_i = k}\\mathbb 1_{\\tau_j = k'}$ was the number of possible edges between nodes in community $k$ and $k'$, and $m_{k'k} = \\sum_{i < j}\\mathbb 1_{\\tau_i = k}\\mathbb 1_{\\tau_j = k'}a_{ij}$ was the number of edges in the realization $A$ between nodes within communities $k$ and $k'$. \n",
    "\n",
    "Noting that the log of the product is the sum of the logs, or that $\\log \\prod_i x_i = \\sum_i \\log x_i$, the log of the probability is:\n",
    "\\begin{align*}\n",
    "    \\log \\mathbb P_\\theta(A) &= \\sum_{k, k' \\in [K]} m_{k'k}\\log b_{k'k} + \\left(n_{k'k} - m_{k'k}\\right)\\log(1 - b_{k'k})\n",
    "\\end{align*}\n",
    "\n",
    "We notice a side-note that we mentioned briefly in the network models section: in a lot of ways, the probability (and consequently, the log probability) of a random network which is an *a priori* SBM behaves very similarly to that of a random network which is ER, with the caveat that the probability term $p$, the total number of possible edges $\\binom n 2$, and the total number of edges $m$ have been replaced with the probability term $b_{k'k}$, the total number of possible edges $n_{k'k}$, and the total number of edges $m_{k'k}$ which *apply only to that particular pair of communities*. In this sense, the *a priori* SBM is kind of like a collection of communities of ER networks. Pretty neat right? Well, it doesn't stop there. When we take the partial derivative of $\\log \\mathbb P_\\theta(A)$ with respect to any of the probability terms $b_{l'l}$, we see an even more direct consequence of this observation:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial }{\\partial b_{l' l}}\\log \\mathbb P_\\theta(A) &= \\frac{\\partial}{\\partial b_{l'l}}\\sum_{k, k' \\in [K]} m_{k'k}\\log b_{k'k} + \\left(n_{k'k} - m_{k'k}\\right)\\log(1 - b_{k'k}) \\\\\n",
    "    &= \\sum_{k, k' \\in [K]} \\frac{\\partial}{\\partial b_{l'l}}\\left[m_{k'k}\\log b_{k'k} + \\left(n_{k'k} - m_{k'k}\\right)\\log(1 - b_{k'k})\\right]\n",
    "\\end{align*}\n",
    "Now what? Notice that any of the summands in which $k \\neq l$ and $k' \\neq l'$, the partial derivative with respect to $b_{l'l}$ is in fact exactly $0$! Why is this? Well, let's consider a $k$ which is different from $l$, and a $k'$ which is different from $l'$. Notice that:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial b_{l'l}}\\left[m_{k'k}\\log b_{k'k} + \\left(n_{k'k} - m_{k'k}\\right)\\log(1 - b_{k'k})\\right] = 0\n",
    "\\end{align*}\n",
    "which simply follows since the quantity to the right of the partial derivative is not a funcion of $b_{l'l}$ at all! Therefore:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial }{\\partial b_{l' l}}\\log \\mathbb P_\\theta(A) &= 0 + \\frac{\\partial}{\\partial b_{l'l}}\\left[m_{l'l}\\log b_{l'l} + \\left(n_{l'l} - m_{l'l}\\right)\\log(1 - b_{l'l})\\right] \\\\\n",
    "    &= \\frac{m_{l'l}}{b_{l'l}} - \\frac{n_{l'l} - m_{l'l}}{1 - b_{l'l}} = 0 \\\\\n",
    "\\Rightarrow b_{l'l}^* &= \\frac{m_{l'l}}{n_{l'l}}\n",
    "\\end{align*}\n",
    "\n",
    "Like above, we omit the second derivative test, and conclude that the MLE of the block matrix $B$ for a random network $\\mathbf A$ which is *a priori* SBM is the matrix $\\hat B$ with entries:\n",
    "\\begin{align*}\n",
    "    \\hat b_{l'l} &= \\frac{m_{l'l}}{n_{l'l}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
