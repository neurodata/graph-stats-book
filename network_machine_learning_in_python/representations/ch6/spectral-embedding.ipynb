{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Embedding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the primary embedding tools we'll use in this book is called *adjacency spectral embedding* {cite:t}`spectraltutorial`. You'll see spectral embedding and variations on it repeatedly, both throughout this section and when we get into applications, so it's worth taking the time to understand spectral embedding deeply. If you're familiar with Principal Component Analysis (PCA), this method has a lot of spiritual similarities. We'll need to get into a bit of linear algebra to understand how it works.\n",
    "\n",
    "The basic idea behind Spectral Embedding is to take a network, optionally take its Laplacian, and then find the eigenvectors corresponding to the $d$ largest eigenvalues, depending on how many dimensions ($d$) you'd like to embed your network down to. The new matrix will be organized traditionally, with rows (observations) corresponding to nodes, and columns (features) corresponding to the top eigenvectors of the network. A few concepts come into play here. If you're using Laplacian Spectral Embedding instead of Adjacency Spectral Embedding, you'll need to generate the Laplacian. Then, in either case, you'll need to figure out how to find our matrix's eigenvectors, and you'll need to understand how you can use those eigenvectors to represent nodes in Euclidean space. \n",
    "\n",
    "Let's start with a simple network. We'll take its Laplacian, just to show what that optional step looks like, and then we'll find its eigenvectors. Along the way, we'll learn about Singular Value Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notes* (TODO: remove)\n",
    "- if L has eigenvalue 0 with k different eigenvectors, such that $o = \\lambda_1 = \\dots = \\lambda_k$, graph G has k connected components\n",
    "- if G is connected, $\\lambda_2$ is the algebraic connectivity of G."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have the simple network below. There are six nodes total, numbered 0 through 5, and there are two distinct connected groups (called \"connected components\" in network theory land). Nodes 0 through 2 are all connected to each other, and nodes 3 through 5 are also all connected to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "def add_edge(A, edge: tuple):\n",
    "    \"\"\"\n",
    "    Add an edge to an undirected graph.\n",
    "    \"\"\"\n",
    "    i, j = edge\n",
    "    A[i, j] = 1\n",
    "    A[j, i] = 1\n",
    "    return A\n",
    "\n",
    "A = np.zeros((6, 6))\n",
    "\n",
    "for edge in combinations([0, 1, 2], 2):\n",
    "    add_edge(A, edge)\n",
    "    \n",
    "for edge in combinations([3, 4, 5], 2):\n",
    "    add_edge(A, edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the adjacency matrix and network below. Notice that there are two distrinct blocks in the adjacency matrix: in its upper-left, you can see the edges between the first three nodes, and in the bottom right, you can see the edges between the second three nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import draw_multiplot\n",
    "import networkx as nx\n",
    "\n",
    "draw_multiplot(A, pos=nx.kamada_kawai_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplacian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spectral embedding, we'll either find the eigenvectors of the Laplacian or the eigenvectors of the Adjacency Matrix itself. Since we already have the adjacency matrix, let's take the Laplacian just to see what that looks like.\n",
    "\n",
    "Remember from chapter four that there are a few different types of Laplacian matrices. By default, Graspologic uses the normalized Laplacian $L = I - D^{-1/2} A D^{-1/2}$, but for simplicity, we'll just use the basic, cookie-cutter version of the Laplacian $L = D - A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the degree matrix D\n",
    "degrees = np.count_nonzero(A, axis=0)\n",
    "D = np.diag(degrees)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Laplacian matrix L\n",
    "L = D-A\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from graphbook_code import heatmap\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "from graphbook_code import GraphColormap\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "# First axis (Degree)\n",
    "heatmap(D, ax=axs[0], cbar=False, title=\"Degree Matrix $D$\")\n",
    "\n",
    "# Second axis (-)\n",
    "axs[1].text(x=.5, y=.5, s=\"-\", fontsize=200, \n",
    "            va='center', ha='center')\n",
    "axs[1].get_xaxis().set_visible(False)\n",
    "axs[1].get_yaxis().set_visible(False)\n",
    "sns.despine(ax=axs[1], left=True, bottom=True)\n",
    "\n",
    "# Third axis (Adjacency matrix)\n",
    "heatmap(A, ax=axs[2], cbar=False, title=\"Adjacency Matrix $A$\")\n",
    "\n",
    "# Third axis (=)\n",
    "axs[3].text(x=.5, y=.5, s=\"=\", fontsize=200,\n",
    "            va='center', ha='center')\n",
    "axs[3].get_xaxis().set_visible(False)\n",
    "axs[3].get_yaxis().set_visible(False)\n",
    "sns.despine(ax=axs[3], left=True, bottom=True)\n",
    "\n",
    "# Fourth axis\n",
    "heatmap(L, ax=axs[4], cbar=False, title=\"Laplacian Matrix $L$\")\n",
    "\n",
    "# Colorbar\n",
    "vmin, vmax = np.array(L).min(), np.array(L).max()\n",
    "norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "im = cm.ScalarMappable(cmap=GraphColormap(\"sequential\").color, norm=norm)\n",
    "fig.colorbar(im, ax=axs, shrink=0.8, aspect=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Eigenvectors: Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "Now that we have a Laplacian matrix, we'll want to find its eigenvectors - or, more generally, its \"singular vectors\". To do this, we'll need to use a technique called *Singular Value Decomposition*, or SVD. \n",
    "\n",
    "SVD is a way to break a single matrix apart into three submatrices -- In our case, the matrix will be the Laplacian we just built. Generally, these three submatrices correspond to something called the singular vectors and singular values. These are general versions of eigenvectors and eigenvalues: all matrices have a full set of singular vectors/values, but not all matrices have a full set of eigenvectors/values. In the case of the Laplacian (as with all symmetric matrices that have positive eigenvalues), it turns out that the singular vectors/values and the eigenvectors/values are the same thing. For more technical details on how SVD works, or for explicit proofs, we would recommend a Linear Algebra textbook [Trefethan, LADR]. Let's look at the SVD with a bit more detail. This description of the SVD only applies to square, symmetric matrices, but you can use the SVD on any matrix with a bit more generality.\n",
    "\n",
    "**Singular Value Decomposition** Suppose you have a square, symmetrix matrix $X$. In our case, $X$ corresponds to the Laplacian $L$ (or the adjacency matrix $A$).\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "    X_{11} & & & \" \\\\\n",
    "    & X_{22} & & \\\\\n",
    "    & & \\ddots & \\\\\n",
    "    \" & & & X_{nn}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Then, you can find three matrices - one which rotates vectors in space, one which scales them along each coordinate axis, and another which rotates them back - which, when you multiply them all together, create $X$. This is the essence of Singular Value Decomposition: you can break down any linear transformation into a rotation, a scaling, and another rotation. Let's call the matrix which rotates $U$, and the matrix that scales $\\Sigma$.\n",
    "\n",
    "\\begin{align*}\n",
    "    X &= U \\Sigma U^T\n",
    "\\end{align*}\n",
    "\n",
    "Since $U$ is a rotation matrix, all of its column-vectors are at ninety-degree angles from each other and they all have the unit length of 1. These columns are called the **singular vectors** of X. Since $\\Sigma$ just scales, it's a diagonal matrix: there are values on the diagonals, but nothing on the off-diagonals. The amount that each coordinate axis is scaled are the values on the diagonal entries of $\\Sigma$, $\\sigma_{ii}$. These are the **singular values** of the matrix $X$, and again, if they're all positive (and if X is symmetric), they'll also be X's eigenvalues. This will be the case with both the Laplacian and the adjacency matrix, assuming our network is undirected.\n",
    "\n",
    "\\begin{align*}\n",
    "    X &= \\begin{bmatrix}\n",
    "    \\uparrow & \\uparrow &  & \\uparrow \\\\\n",
    "    u_1 & \\vec u_2 & ... & \\vec u_n \\\\\n",
    "    \\downarrow & \\downarrow &  & \\downarrow\n",
    "    \\end{bmatrix}\\begin{bmatrix}\n",
    "    \\sigma_1 & &  & \\\\\n",
    "    & \\sigma_2 &  & \\\\\n",
    "    & & \\ddots & \\\\\n",
    "    & & & \\sigma_n\n",
    "    \\end{bmatrix}\\begin{bmatrix}\n",
    "    \\leftarrow & \\vec u_1^T & \\rightarrow \\\\\n",
    "    \\leftarrow & \\vec u_2^T & \\rightarrow \\\\\n",
    "    & \\vdots & \\\\\n",
    "    \\leftarrow & \\vec u_n^T & \\rightarrow \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "Let's illustrate the SVD using an example. We will sample a $50 \\times 50$ realization of an adjacency matrix from a random network which is an *a priori* SBM, with $2$ communities. The first 25 nodes will be in the first coommunity, and the second 25 nodes will be in the second community. The within-community probabilities will both be $0.8$, and the between-community probabilities will be $0.1$:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
