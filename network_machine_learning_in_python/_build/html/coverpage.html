
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-2 bd-sidebar site-navigation show single-page" id="site-navigation">
    
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurodata/graph-stats-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fcoverpage.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-introduction/preface">
   Preface
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-introduction/terminology">
   Terminology and Math Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/ch1">
   1. The Network Machine Learning Landscape
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/what-is-a-network">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/why-study-networks">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/examples-of-applications">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/types-of-networks">
     1.4. Types of Networks
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/types-of-learning-probs">
     1.5. Types of Network Learning Problems
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/main-challenges">
     1.6. Main Challenges of Network Learning
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch1/exercises">
     1.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch2/ch2">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch2/big-picture">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch2/get-the-data">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch2/prepare-the-data">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch2/transformation-techniques">
     2.4. Transformation Techniques
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch2/select-and-train">
     2.5. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch2/fine-tune">
     2.6. Fine-Tune your Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch3/ch3">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch3/big-picture">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch3/get-the-data">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch3/discover-and-visualize">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-foundations/ch3/prepare-the-data">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-representations/ch4/ch4">
   1. Properties of Networks as a Statistical Object
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch4/matrix-representations">
     1.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch4/network-representations">
     1.2. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch4/properties-of-networks">
     1.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch4/regularization">
     1.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/ch5">
   2. Why Use Statistical Models?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/why-use-models">
     2.1. Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/single-network-models_ER">
     2.2. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/single-network-models_SBM">
     2.3. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/single-network-models_RDPG">
     2.4. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/multi-network-models">
     2.5. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/models-with-covariates">
     2.6. Network Models with Covariates
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch5/single-network-models_theory">
     2.7. Single network model theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/ch6">
   3. Learning Network Representations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/estimating-parameters_mle">
     3.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/why-embed-networks">
     3.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/spectral-embedding">
     3.3. Spectral Embedding Methods
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/estimating-parameters_spectral">
     3.4. Estimating Parameters in Network Models via Spectral Methods
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/random-walk-diffusion-methods">
     3.5. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/graph-neural-networks">
     3.6. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/multigraph-representation-learning">
     3.7. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/joint-representation-learning">
     3.8. Joint Representation Learning
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch6/estimating-parameters_theory">
     3.9. Model Estimation Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-representations/ch7/ch7">
   4. Theoretical Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch7/theory-single-network">
     4.1. Theory for Single Network Models
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch7/theory-multigraph">
     4.2. Theory for Multiple-Network Models
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-representations/ch7/theory-matching">
     4.3. Theory for Graph Matching
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-applications/ch8/ch8">
   1. Applications When You Have One Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch8/community-detection">
     1.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch8/testing-differences">
     1.2. Testing for Differences between Communities
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch8/model-selection">
     1.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch8/single-vertex-nomination">
     1.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch8/out-of-sample">
     1.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-applications/ch9/ch9">
   2. Applications for Two Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch9/two-sample-hypothesis">
     2.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch9/graph-matching-vertex">
     2.2. Graph Matching
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch9/multiple-vertex-nomination">
     2.3. Vertex Nomination For Multiple Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="coverpage.html#document-applications/ch10/ch10">
   3. Applications for Many Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch10/anomaly-detection">
     3.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch10/significant-edges">
     3.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch10/significant-vertices">
     3.3. Testing for Significant Vertices
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="coverpage.html#document-applications/ch10/significant-communities">
     3.4. Testing for Significant Communities
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference external nav-link" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="hands-on-network-machine-learning-with-scikit-learn-and-graspologic">
<h1>Hands-on Network Machine Learning with Scikit-Learn and Graspologic<a class="headerlink" href="#hands-on-network-machine-learning-with-scikit-learn-and-graspologic" title="Permalink to this headline">¶</a></h1>
<div class="figure align-default" id="maggot-connectome">
<a class="reference internal image-reference" href="_images/umap_pedigo_small.jpg"><img alt="_images/umap_pedigo_small.jpg" src="_images/umap_pedigo_small.jpg" style="height: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">2D representation of a <em>Drosophila</em> larva brain connectome network. Credit to Ben Pedigo, PhD student at Johns Hopkins University.</span><a class="headerlink" href="#maggot-connectome" title="Permalink to this image">¶</a></p>
</div>
<div class="toctree-wrapper compound">
<span id="document-introduction/preface"></span><div class="section" id="preface">
<h2>Preface<a class="headerlink" href="#preface" title="Permalink to this headline">¶</a></h2>
<div class="section" id="network-machine-learning-and-you">
<h3>Network Machine Learning and You<a class="headerlink" href="#network-machine-learning-and-you" title="Permalink to this headline">¶</a></h3>
<p>This book is about networks, and how you can use tools from machine learning to understand and explain them more deeply. Why is this an interesting thing to learn about, and why should you care?</p>
<p>Well, at some level, every aspect of reality seems to be made of interconnected parts. Atoms and molecules are connected to each other with chemical bonds. Your neurons connect to each other through synapses, and the different parts of your brain connect to each other through groups of neurons interacting with each other. At a larger level, you are interconnected with other humans through social networks, and our economy is a global, interconnected trade network. The Earth’s food chain is an ecological network, and larger still, every object with mass in the universe is connected to every other object through a gravitational network.</p>
<p>So if you can understand networks, you can understand a little something about everything!</p>
</div>
<div class="section" id="network-machine-learning-in-your-projects">
<h3>Network Machine Learning in Your Projects<a class="headerlink" href="#network-machine-learning-in-your-projects" title="Permalink to this headline">¶</a></h3>
<p>So, naturally you are excited about network machine learning and you would love to join the party!</p>
<p>Perhaps you’re a researcher and you want to expose shadowy financial networks and corporate fraud? Or create a network framework for measuring teamwork in healthcare? Maybe you’re interested in evolutionary releationships between different animals, or maybe you want to model communities of neurons in the brain?</p>
<p>Or maybe you’re a data scientist and your company has tons of data (user logs, financial data, production data, machine sensor data, hotline stats, HR reports, etc.), and more than likely you could
view the data as a network and unearth some hidden gems of knowledge if you just knew where to look? For example:</p>
<ul class="simple">
<li><p>Explore purchasing networks and isolate the most active customers</p></li>
<li><p>Explore patterns of collaboration in your company’s network of employees</p></li>
<li><p>Detect which transactions are likely to be fraudulent</p></li>
<li><p>Isolate groups in your company which are overperforming or underperforming</p></li>
<li><p>Model the transportation chain necessary to produce and disseminate your product</p></li>
<li><p>And more</p></li>
</ul>
<p>Whatever the reason, you have decided to learn about networks and implement their analysis in your projects. Great idea!</p>
</div>
<div class="section" id="objective-and-approach">
<h3>Objective and Approach<a class="headerlink" href="#objective-and-approach" title="Permalink to this headline">¶</a></h3>
<p>This book assumes you know next to nothing about how networks can be viewed as a statistical object. Its goal is to give you the concepts, the intuitions, and the tools you need to actually implement programs capable of learning from network data.</p>
<p>The book is intended to give you the best introduction you can possibly get to explore and exploit network data. You might be a graduate student, doing research on biochemical networks or trade networks in ancient Mesopotamia. Or you might be a professional interested in an introduction to the field of network data science, because you think it might be useful for your company. Whoever you are, we think you’ll find a lot of things that are useful and interesting in this book!</p>
<p>We’ll cover the fundamentals of network data science, focusing on developing intuition on networks as statistical objects, doing so while paired with relevant Python tutorials. By the end of this book, you will be able to utilize efficient and easy to use tools available for performing analyses on networks. You will also have a whole new range of statistical techniques in your toolbox, such as representations, theory, and algorithms for networks.</p>
<p>We’ll spend this book learning about network algorithms by showing how they’re implemented in production-ready Python frameworks:</p>
<ul class="simple">
<li><p>Numpy and Scipy are used for scientific programming. They give you access to array objects, which are the main way we’ll represent networks computationally.</p></li>
<li><p>Scikit-Learn is very easy to use, yet it implements many Machine Learning algorithms efficiently, so it makes a great entry point for downstream analysis of networks.</p></li>
<li><p>Graspologic is an open-source Python package developed by Microsoft and the NeuroData lab at Johns Hopkins University which gives you utilities and algorithms for doing statistical analyses on network-valued data.</p></li>
</ul>
<p>The book favors a hands-on approach, growing an intuitive understanding of
networks through concrete working examples and a bit of theory.
While you can read this book without picking up your laptop, we highly recommend
you experiment with the code examples available online as Jupyter notebooks at <a class="reference external" href="http://docs.neurodata.io/graph-stats-book/index.html">http://docs.neurodata.io/graph-stats-book/index.html</a>.</p>
</div>
<div class="section" id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h3>
<p>We assume you have a basic knowledge of mathematics. Because network science uses a lot of linear algebra, requiring a bit of linear algebra knowledge is unfortunately unavoidable. (You should know what an eigenvalue is!)</p>
<p>If you care about what’s under the hood mathematically, we have certain sections marked as “advanced material” - you should have a reasonable understanding of college-level math, such as calculus, linear algebra, probability, and statistics for these sections.</p>
<p>You should also probably have some background in programming - we’ll mainly be using Python to build and explore our networks. If you don’t have too much of a Python or math background, don’t worry - we’ll link some resources to give you a head start.</p>
<p>If you’ve never used Jupyter, don’t worry about it. It is a great tool to have in your toolbox and it’s easy to learn. We’ll also link some resources for you if you are not familiar with Python’s scientific libraries, like numpy, scipy, networkx, and scikit-learn.</p>
</div>
<div class="section" id="roadmap">
<h3>Roadmap<a class="headerlink" href="#roadmap" title="Permalink to this headline">¶</a></h3>
<p>This book is organized into three parts.</p>
<p>Part I, Foundations, gives you a brief overview of the kinds of things you’ll be doing in this book, and shows you how to solve a network data science problem from start to finish. It covers the following topics:</p>
<ul class="simple">
<li><p>What a network is and where you can find networks in the wild</p></li>
<li><p>All the reasons why you should care about studying networks</p></li>
<li><p>Examples of ways you could apply network data science to your own projects</p></li>
<li><p>An overview of the types of problems Network Machine Learning is good at dealing with</p></li>
<li><p>The main challenges you’d encounter if you explored Network Learning more deeply</p></li>
<li><p>Exploring a real network data science dataset, to get a broad understanding of what you might be able to learn.</p></li>
</ul>
<p>Part II, Representations, is all about how we can represent networks statistically, and what we can do with those representations. It covers the following topics:</p>
<ul class="simple">
<li><p>Ways you can represent individual networks</p></li>
<li><p>Ways you can represent groups of networks</p></li>
<li><p>The various useful properties different types of networks have</p></li>
<li><p>Types of network representations and why they’re useful</p></li>
<li><p>How to represent networks as a bunch of points in space</p></li>
<li><p>How to represent multiple networks</p></li>
<li><p>How to represent networks when you have extra information about your nodes</p></li>
</ul>
<p>Part III, Applications, is about using the representations from Part II to explore and exploit your networks. It covers the following topics:</p>
<ul class="simple">
<li><p>Figuring out if communities in your networks are different from each other</p></li>
<li><p>Selecting a reasonable model to represent your data</p></li>
<li><p>Finding nodes, edges, or communities in your networks that are interesting</p></li>
<li><p>Finding time points which are anomalies in a network which is evolving over time</p></li>
<li><p>What to do when you have new data after you’ve already trained a network model</p></li>
<li><p>How hypothesis testing works on networks</p></li>
<li><p>Figuring out which nodes are the most similar in a pair of networks</p></li>
</ul>
</div>
<div class="section" id="conventions-used-in-this-book">
<h3>Conventions Used In This Book<a class="headerlink" href="#conventions-used-in-this-book" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="using-code-examples">
<h3>Using Code Examples<a class="headerlink" href="#using-code-examples" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="about-the-authors">
<h3>About the Authors<a class="headerlink" href="#about-the-authors" title="Permalink to this headline">¶</a></h3>
<p><strong>Dr. Joshua Vogelstein</strong> is an Assistant Professor in the Department of Biomedical Engineering at Johns Hopkins University, with joint appointments in Applied Mathematics and Statistics, Computer Science, Electrical and Computer Engineering, Neuroscience, and Biostatistics. His research focuses on the statistics of networks in brain science (connectomes). His lab and collaborators have developed the leading computational algorithms and libraries to perform statistical analysis on networks.</p>
<p><strong>Alex Loftus</strong> is a master’s student at Johns Hopkins University in the Department of Biomedical Engineering, with an undergraduate degree in neuroscience. He has worked on implementing network spectral embedding and clustering algorithms in Python, and helped develop an MRI pipeline to produce brain networks from diffusion MRI data.</p>
<p><strong>Eric Bridgeford</strong> is a PhD student in the Department of Biostatistics at Johns Hopkins University. Eric’s background includes Computer Science and Biomedical Engineering, and he is an avid contributor of packages to CRAN and PyPi for nonparametric hypothesis testing. Eric studies general approaches for statistical inference in network data, with applications to problems with network estimation in MRI connectomics data, including replicability and batch effects.</p>
<p><strong>Dr. Carey E. Priebe</strong> is Professor of Applied Mathematics and Statistics, and a founding member of the Center for Imaging Science (CIS) and the Mathematical Institute for Data Science (MINDS) at Johns Hopkins University. He is a leading researcher in theoretical, methodological, and applied statistics / data science; much of his recent work focuses on spectral network analysis and subsequent statistical inference. Professor Priebe is Senior Member of the IEEE, Elected Member of the International Statistical Institute, Fellow of the Institute of Mathematical Statistics, and Fellow of the American Statistical Association.</p>
<p><strong>Dr. Christopher M. White</strong> is Managing Director, Microsoft Research Special Projects. He leads mission-oriented research and software development teams focusing on high risk problems. Prior to joining Microsoft, he was a Fellow at Harvard for network statistics and machine learning. Chris’s work has been featured in media outlets including Popular Science, CBS’s 60 Minutes, CNN, the Wall Street Journal, Rolling Stone Magazine, TEDx, and Google’s Solve for X. Chris was profiled in a cover feature for the Sept/Oct 2016 issue of Popular Science.</p>
<p><strong>Weiwei Yang</strong> is a Principal Development Manager at Microsoft Research. Her interests are in resource efficient alt-SGD ML methods inspired by biological learning. The applied research group she leads aims to democratize AI by addressing issues of sustainability, robustness, scalability, and efficiency in ML. Her group has applied ML to address social issues such as countering human trafficking and to energy grid stabilizations.</p>
</div>
<div class="section" id="acknowledgements">
<h3>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permalink to this headline">¶</a></h3>
<p>First of all, big thanks to everybody who has been reading the book as we write and giving feedback. So far, this list includes Dax Pryce, Ross Lawrence, Geoff Loftus, Alexandra McCoy, Olivia Taylor, and Peter Brown.</p>
</div>
<div class="section" id="finished-sections">
<h3>Finished Sections<a class="headerlink" href="#finished-sections" title="Permalink to this headline">¶</a></h3>
<p>(lots more in progress…)</p>
<ol class="simple">
<li><p>Preface: <a class="reference internal" href="coverpage.html#document-introduction/preface"><span class="doc std std-doc">Preface</span></a></p></li>
<li><p>Why Use Statistical Models: <a class="reference internal" href="coverpage.html#document-representations/ch5/why-use-models"><span class="doc std std-doc">Why Use Statistical Models?</span></a></p></li>
<li><p>Single-Network Models: <a class="reference internal" href="coverpage.html#document-representations/ch5/single-network-models"><span class="doc std std-doc">Network Models</span></a></p></li>
<li><p>Multi-Network Representation Learning: <a class="reference internal" href="coverpage.html#document-representations/ch6/multigraph-representation-learning"><span class="doc std std-doc">Multiple-Network Representation Learning</span></a></p></li>
<li><p>Joint Representation Learning: <a class="reference internal" href="coverpage.html#document-representations/ch6/joint-representation-learning"><span class="doc std std-doc">Joint Representation Learning</span></a></p></li>
</ol>
</div>
</div>
<span id="document-introduction/terminology"></span><div class="section" id="terminology-and-math-refresher">
<h2>Terminology and Math Refresher<a class="headerlink" href="#terminology-and-math-refresher" title="Permalink to this headline">¶</a></h2>
<p>In this section, we outline some background terminology which will come up repeatedly throughout the book. This section attempts to standardize some background material that we think is useful going in. It is important to realize that many of the concepts discussed below are only crucial for understanding the advanced, starred sections. If you aren’t familiar with some (or any!) of the below concepts, we don’t think this would detract from your understanding of the broader content.</p>
<div class="section" id="vectors-matrices-and-numerical-spaces">
<h3>Vectors, Matrices, and Numerical Spaces<a class="headerlink" href="#vectors-matrices-and-numerical-spaces" title="Permalink to this headline">¶</a></h3>
<p>Throughout this book, we will need some level of familiarity with numerical spaces, and the grammar that we use to describe them. Taking the time to understand this notation will better help you understand many of the concepts in the rest of the book.</p>
<div class="section" id="numerical-spaces">
<h4>Numerical Spaces<a class="headerlink" href="#numerical-spaces" title="Permalink to this headline">¶</a></h4>
<p>Numerical spaces are everywhere. If you have taken any calculus or algebra courses, you are likely familiar with the natural numbers - these are just your basic one, two, three, and so on. This constitutes the most basic numerical space and is denoted by the symbol <span class="math notranslate nohighlight">\(\mathbb N\)</span>. Formally, the natural numbers describes the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb N &amp;= \{1, 2, 3, ...\}
\end{align*}\]</div>
<p>and continues infinitely (notice that neither negative numbers nor numbers with decimal points appear in <span class="math notranslate nohighlight">\(\mathbb N\)</span>). On a similar note, we will frequently resort to short hand to describe subsets of the natural numbers. We will use the symbol <span class="math notranslate nohighlight">\(\in\)</span> (read, “in”) to denote that one quantity is found within a particular set. For example, since <span class="math notranslate nohighlight">\(5\)</span> is a natural number, we would say that <span class="math notranslate nohighlight">\(5 \in \mathbb N\)</span>, which can be thought of as “<span class="math notranslate nohighlight">\(5\)</span> is in the set of natural numbers”. To describe a subset of the first <span class="math notranslate nohighlight">\(5\)</span> natural numbers, we would use the notation <span class="math notranslate nohighlight">\([5]\)</span>, which denotes the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    [5] &amp;= \{1,2,3,4,5\}
\end{align*}\]</div>
<p>In the more general case where we have some variable <span class="math notranslate nohighlight">\(n\)</span> where <span class="math notranslate nohighlight">\(n \in \mathbb N\)</span> (again, <span class="math notranslate nohighlight">\(n\)</span> is some arbitrary natural number), then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    [n] &amp;= \{1,2,...,n\}
\end{align*}\]</div>
<p>The next most basic numerical space is known as the integers, which is just the natural numbers combined with the negative numbers and zero. Specifically:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb Z &amp;= \{..., -2, -1, 0, 1, 2, ...\}
\end{align*}\]</div>
<p>From <span class="math notranslate nohighlight">\(-\infty\)</span> up to <span class="math notranslate nohighlight">\(+\infty\)</span>.</p>
<p>There are many more numerical spaces, but in this book we’ll focus on one in particular: real numbers, denoted <span class="math notranslate nohighlight">\(\mathbb R\)</span>. The real numbers can be thought of as all the numbers that can be represented by a finite or infinite number of decimal places in between (and including) the integers. We won’t go into too many details; if you want more details on the real numbers, a good place to start would be coursework in <strong>real analysis</strong>. Particularly, the real numbers include any natural number, and integer, any decimal, or any irrational number (such as <span class="math notranslate nohighlight">\(\pi\)</span> or <span class="math notranslate nohighlight">\(\sqrt{2}\)</span>). The main thing that is interesting about the real numbers that we will <em>indirectly</em> use throughout the book is that if we have any two real numbers <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> (remember, this would be written <span class="math notranslate nohighlight">\(x, y \in \mathbb R\)</span>), then the products, ratios, or sums of them are also real numbers:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    x \cdot y, \frac{x}{y}, x + y \in \mathbb R
\end{align*}\]</div>
<p>Throughout the book, we will build upon some of these numerical spaces and introduce several new ones along the way that are interesing for network machine learning. We will do this by attempting to relate them back to the basic numerical spaces we have introduced here.</p>
</div>
<div class="section" id="one-dimensional-quantities">
<h4>One-Dimensional Quantities<a class="headerlink" href="#one-dimensional-quantities" title="Permalink to this headline">¶</a></h4>
<p>We will frequently see the term “dimensional” come up in this book, and we will attempt to give some insight into what this means here. If we were to say that <span class="math notranslate nohighlight">\(x \in \mathbb R\)</span>, we know from the above description that this means that <span class="math notranslate nohighlight">\(x\)</span> is a real number, and is therefore “in” the set of real numbers. A one-dimensional quantity is a quantity which is described by a single element from one numerical space. In this instance, <span class="math notranslate nohighlight">\(x\)</span> is described by one real number, and is therefore one-dimensional. We will use a lowercase letter (for instance, <span class="math notranslate nohighlight">\(x, a, b, \alpha, \beta\)</span>; the letters may be Roman or Greek) to denote that a quantity is one-dimensional.</p>
</div>
<div class="section" id="vectors">
<h4>Vectors<a class="headerlink" href="#vectors" title="Permalink to this headline">¶</a></h4>
<p>Building off the concept of one-dimensional variables, what if we had some variable that existed in two dimensions? For instance, consider the following:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x = \begin{bmatrix}1.5 \\ 2\end{bmatrix}
\end{align*}\]</div>
<p>As we can see here, <span class="math notranslate nohighlight">\(\vec x\)</span> is now described by two real numbers (namely, <span class="math notranslate nohighlight">\(1.5\)</span> and <span class="math notranslate nohighlight">\(2\)</span>). This means that <span class="math notranslate nohighlight">\(\vec x\)</span> is now a two-dimensional quantity, since we have two separate values needed to describe <span class="math notranslate nohighlight">\(\vec x\)</span>. In this case, <span class="math notranslate nohighlight">\(\vec x\)</span> no longer is “in” the real numbers, it is instead in the two-dimensional real vectors, or <span class="math notranslate nohighlight">\(\mathbb R^2\)</span>. Here, <span class="math notranslate nohighlight">\(\vec x\)</span> is called a <strong>vector</strong>, and each of its dimensions are defined using the notation <span class="math notranslate nohighlight">\(x_1 = 1.5\)</span> and <span class="math notranslate nohighlight">\(x_2 = 2\)</span>. The subscript <span class="math notranslate nohighlight">\(x_j\)</span> just means the <span class="math notranslate nohighlight">\(j^{th}\)</span> element of <span class="math notranslate nohighlight">\(\vec x\)</span>, which is numbered by counting downwards from the first row (<span class="math notranslate nohighlight">\(j = 1\)</span>) to however many rows <span class="math notranslate nohighlight">\(\vec x\)</span> has in total. Since <span class="math notranslate nohighlight">\(\vec x\)</span> is two-dimensional, we would say that <span class="math notranslate nohighlight">\(j \in [2]\)</span>, which means <span class="math notranslate nohighlight">\(j\)</span> can be either <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(2\)</span>. In general, we will assume that all vectors are <strong>column vectors</strong> unless otherwise stated, which means that <span class="math notranslate nohighlight">\(\vec x\)</span> will be assumed to be vertically aligned. This will not make much of a conceptual difference, but it will play a role when we define operations between vectors and matrices later on. On the other hand, a <strong>row vector</strong> will typically be denoted by using the <strong>transpose</strong> symbol, which we will learn about later on in the section on operators. Unlike a column vector, a row vector is aligned horizontally. For example, a row vector with entries identical to <span class="math notranslate nohighlight">\(\vec x\)</span> will be denoted:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x^\top = \begin{bmatrix}1.5 &amp; 2\end{bmatrix}
\end{align*}\]</div>
<p>In the general case, for any set <span class="math notranslate nohighlight">\(\mathcal S\)</span>, we would say that <span class="math notranslate nohighlight">\(\vec s \in \mathcal S^d\)</span> if (think through this notation!) for any <span class="math notranslate nohighlight">\(j \in [d]\)</span>, <span class="math notranslate nohighlight">\(s_j \in \mathcal S\)</span>. The key aspects are that the symbol for the vector will be a lower case letter (in this example, <span class="math notranslate nohighlight">\(s\)</span>) like the one-dimensional quantity, but will add the <span class="math notranslate nohighlight">\(\vec{}\)</span> symbol to denote that it is a vector with more than one dimension. The quantity <span class="math notranslate nohighlight">\(d\)</span> that you see in the superscript is referred to as the dimensionality. In this example, we would say that <span class="math notranslate nohighlight">\(\vec s\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional <span class="math notranslate nohighlight">\(\mathcal S\)</span>-vector.</p>
</div>
<div class="section" id="matrices">
<h4>Matrices<a class="headerlink" href="#matrices" title="Permalink to this headline">¶</a></h4>
<p>Matrices come up a lot in network science because we often represent networks as matrices: the adjacency matrix, for instance, is a way to represent a network in terms of its edge connections. Because networks can be represented as matrices, we’ll sometimes just talk about matrices directly.</p>
<p>We will see a variety of different types of matrices throughout this book, so let’s start with a simple example. Consider the following marix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X = \begin{bmatrix}
    1.5 &amp; 1.7 \\
    2 &amp; 1.8
    \end{bmatrix}
\end{align*}\]</div>
<p>Here, we can see that <span class="math notranslate nohighlight">\(X\)</span> is described by four real numbers, with a particular arrangement. This time, we say that <span class="math notranslate nohighlight">\(X\)</span> is an element of the set of all possible <span class="math notranslate nohighlight">\(2 \times 2\)</span> (2 rows, and 2 columns) matrices with real entries. In symbols, we would describe this as <span class="math notranslate nohighlight">\(\mathbb R^{2 \times 2}\)</span>, where <span class="math notranslate nohighlight">\(\mathbb R\)</span> says that the elements of the matrix are real numbers, and <span class="math notranslate nohighlight">\(2 \times 2\)</span> means that the matrix has two rows and two columns. We can describe the entries of a matrix using indexing, very similar to what we did for vectors. In matrices, the rows and columns matter. In this case, the rows go from left to right horizontally, and the columns go from top to bottom vertically. The rows will be numbered from the top of the matrix to the bottom, and the columns will be numbered from the left-most column to the right-most column. For instance, the first row of the matrix <span class="math notranslate nohighlight">\(X\)</span> is the row-vector <span class="math notranslate nohighlight">\(\begin{bmatrix}1.5 &amp; 1.7\end{bmatrix}\)</span>, and the second column of the matrix <span class="math notranslate nohighlight">\(X\)</span> is the column-vector <span class="math notranslate nohighlight">\(\begin{bmatrix}1.7 \\ 1.8\end{bmatrix}\)</span>. This subscripts <span class="math notranslate nohighlight">\(x_{ij}\)</span> means the entry of the matrix <span class="math notranslate nohighlight">\(X\)</span> in the <span class="math notranslate nohighlight">\(i^{th}\)</span> row aand the <span class="math notranslate nohighlight">\(j^{th}\)</span> column. In this instance, we would describe that <span class="math notranslate nohighlight">\(x_{11} = 1.5\)</span>, <span class="math notranslate nohighlight">\(x_{12} = 1.7\)</span>, <span class="math notranslate nohighlight">\(x_{21}=2\)</span>, and <span class="math notranslate nohighlight">\(x_{22} = 1.8\)</span>.</p>
<p>In the general case, for a set <span class="math notranslate nohighlight">\(\mathcal S\)</span>, we would say that <span class="math notranslate nohighlight">\(S \in \mathcal S^{r \times c}\)</span> if (think this through!) for any <span class="math notranslate nohighlight">\(i \in [r]\)</span> and any <span class="math notranslate nohighlight">\(j \in [c]\)</span>, <span class="math notranslate nohighlight">\(s_{ij} \in \mathcal S\)</span>. Like before, the key aspects are that the symbol for a matrix will be a capital letter (in this example, <span class="math notranslate nohighlight">\(S\)</span>) to denote that it is a matrix, and its entries <span class="math notranslate nohighlight">\(s_{ij}\)</span> will be denoted using a lowercase letter. The quantity <span class="math notranslate nohighlight">\(r\)</span> is known as the row count and the quantity <span class="math notranslate nohighlight">\(c\)</span> is known as the column count of the matrix <span class="math notranslate nohighlight">\(S\)</span>. In this example, we would say that <span class="math notranslate nohighlight">\(S\)</span> is a <span class="math notranslate nohighlight">\(\mathcal S\)</span>-matrix with <span class="math notranslate nohighlight">\(r\)</span> rows and <span class="math notranslate nohighlight">\(c\)</span> columns.</p>
<p>Another thing we will see arise periodically is that vectors can be denoted as matrices with a single column. For example, in our example above in the vector section, we might equivalently write that <span class="math notranslate nohighlight">\(\vec s \in \mathcal S^{d \times 1}\)</span>. The “1” for the columns just denotes that <span class="math notranslate nohighlight">\(\vec s\)</span> is a column vector with <span class="math notranslate nohighlight">\(d\)</span> rows in total. This will be useful when we define functions for matrices, and use the same notation for functions on vectors.</p>
</div>
</div>
<div class="section" id="useful-functions">
<h3>Useful Functions<a class="headerlink" href="#useful-functions" title="Permalink to this headline">¶</a></h3>
<p>Throughout the book, we will deal with many types of functions which take mathematical objects (potentially multiple) that exist in one numerical space and produce a mathematical object (potentially in a different) numerical space. You are probably familiar with several of these, such as the addition or multiplication operators on one-dimensional quantities. We will touch on some of the more fancy ones that we will see arise throughout the book.</p>
<p>The <strong>sum</strong>, denoted by a fancy capital epsilon <span class="math notranslate nohighlight">\(\sum\)</span>, denotes that we are summing a bunch of items which can be easily indexed. For instance, consider if we have a vector <span class="math notranslate nohighlight">\(\vec x \in \mathbb R^d\)</span>, so <span class="math notranslate nohighlight">\(\vec x\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector. If we wanted to take the sum of all of the elements of <span class="math notranslate nohighlight">\(\vec x\)</span>, we would write:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{i = 1}^d x_i = x_1 + x_2 + ... + x_d
\end{align*}\]</div>
<p>The <em>summand</em> of the sum, the <span class="math notranslate nohighlight">\(x_i\)</span>s next to the <span class="math notranslate nohighlight">\(\sum\)</span> symbol, are the terms that will be summed up. Further, note that the <span class="math notranslate nohighlight">\(\sum\)</span> symbol also indicates the indices of <span class="math notranslate nohighlight">\(\vec x\)</span> that will be summed. Note that on the bottom, we see that the sum says from <span class="math notranslate nohighlight">\(i = 1\)</span> and above it says <span class="math notranslate nohighlight">\(d\)</span>. This means that we sum all the elements of <span class="math notranslate nohighlight">\(x_i\)</span> starting from below at <span class="math notranslate nohighlight">\(1\)</span> and going up until <span class="math notranslate nohighlight">\(d\)</span>. We could say the exact same thing using our shorthand for this set, which we described in the section on natural numbers, <span class="math notranslate nohighlight">\([d]\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_{i \in [d]} x_i = \sum_{i = 1}^d x_i = x_1 + x_2 + ... + x_d
\end{align*}\]</div>
<p>We could similarly define <strong>any</strong> indexing set, such as <span class="math notranslate nohighlight">\(\mathcal I = \{1,3\}\)</span>, and write:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{i \in \mathcal I} x_i = x_1 + x_3
\end{align*}\]</div>
<p>The key is that the notation above or below the summand just tells us which elements we are applying the sum over. For instance, if <span class="math notranslate nohighlight">\(\vec x\)</span> was a <span class="math notranslate nohighlight">\(3\)</span>-dimensional vector:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
   \vec x = \begin{bmatrix}
      1.7 \\ 1.8 \\ 2
   \end{bmatrix}
\end{align*}\]</div>
<p>We would have that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
   \sum_{i = 1}^3 x_i = 5.5
\end{align*}\]</div>
<p>if we were to use <span class="math notranslate nohighlight">\(\mathcal I = \{1,3\}\)</span>, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{i \in \mathcal I}x_i = 3.7
\end{align*}\]</div>
<p>the <strong>product</strong>, denoted by a capital pi <span class="math notranslate nohighlight">\(\prod\)</span>, behaves extremely similarly to the sum, except insted of applying sums, it applies multiplication. For instance, if we instead wanted to multiply all the elements of <span class="math notranslate nohighlight">\(\vec x\)</span>, we would write:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \prod_{i = 1}^d x_i = x_1 \times x_2 \times ... \times x_d
\end{align*}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\times\)</span> is just multiplication like you are probably used to. Again, we have the exact same indexing conventions, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \prod_{i \in [d]} x_i=
    \prod_{i = 1}^d x_i = x_1 \times x_2 \times ... \times x_d
\end{align*}\]</div>
<p>We can again just use indexing sets, too:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \prod_{i \in \mathcal I}x_i = x_1 \times x_3
\end{align*}\]</div>
<p>With <span class="math notranslate nohighlight">\(\vec x\)</span> defined as above in the sum example, we would have that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
   \prod_{i = 1}^3 x_i = 6.12
\end{align*}\]</div>
<p>if we were to use <span class="math notranslate nohighlight">\(\mathcal I = \{1,3\}\)</span>, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \prod_{i \in \mathcal I}x_i = 3.4
\end{align*}\]</div>
<p>The <strong>Euclidean inner product</strong>, or the <em>inner product</em> we will refer to in our book, is obtained by multiplying two vectors element-wise, and summing the result. Suppose we have two vectors <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span>, which are each <span class="math notranslate nohighlight">\(d\)</span>-dimensional real vectors (both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> must have the same number of elements). The inner product is the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \langle \vec x, \vec y\rangle &amp;= \sum_{i = 1}^d x_i y_i
\end{align*}\]</div>
<p>as we will see in a second, in matrix notation, this is exactly equivalent to writing:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \langle \vec x, \vec y\rangle &amp;= \vec x^T \vec y
\end{align*}\]</div>
<p><strong>Matrix multiplication</strong>, denoted by a circle <span class="math notranslate nohighlight">\(\cdot\)</span> (or in most cases, just two matrices side by side, with no separation), is an operation which takes a matrix which has <span class="math notranslate nohighlight">\(r\)</span> rows and <span class="math notranslate nohighlight">\(c\)</span> columns and another matrix which has <span class="math notranslate nohighlight">\(c\)</span> rows and <span class="math notranslate nohighlight">\(l\)</span> columns, and produces a matrix with <span class="math notranslate nohighlight">\(r\)</span> rows and <span class="math notranslate nohighlight">\(l\)</span> columns. Suppose we have a matrix <span class="math notranslate nohighlight">\(A \in \mathbb R^{r \times c}\)</span>, and <span class="math notranslate nohighlight">\(B \in \mathbb R^{c \times l}\)</span>. Here, <span class="math notranslate nohighlight">\(r\)</span>, <span class="math notranslate nohighlight">\(c\)</span>, and <span class="math notranslate nohighlight">\(l\)</span> could be <em>any</em> natural numbers. A matrix multiplication produces a matrix <span class="math notranslate nohighlight">\(D \in \mathbb R^{r \times l}\)</span>, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    d_{ij} = \sum_{k = 1}^c a_{ik}b_{kj}
\end{align*}\]</div>
<p>What does this mean intuitively? Well, let’s think about it. Let’s imagine that the <em>rows</em> of <span class="math notranslate nohighlight">\(A\)</span> are indexed from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(r\)</span>, like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A &amp;= \begin{bmatrix}
        \vec a_1^T \\
        \vec a_2^T \\
        \vdots \\
        \vec a_r^T
    \end{bmatrix}
\end{align*}\]</div>
<p>Note that the vectors <span class="math notranslate nohighlight">\(\vec a_i\)</span> are transposed when oriented in the matrix <span class="math notranslate nohighlight">\(A\)</span>, because they are each <span class="math notranslate nohighlight">\(c\)</span>-dimensional vectors (and by convention in our book, all vectors will be <em>column</em> vecors. So to comprise the rows of <span class="math notranslate nohighlight">\(A\)</span>, they must be “flipped”). Similarly, let’s imagine that the columns of <span class="math notranslate nohighlight">\(B\)</span> are indexed from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(l\)</span>, like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    B &amp;= \begin{bmatrix}
        \vec b_1 &amp; \vec b_2 &amp; ... &amp; \vec b_l
    \end{bmatrix}
\end{align*}\]</div>
<p>So what is the marix <span class="math notranslate nohighlight">\(D\)</span>? Note that each entry, <span class="math notranslate nohighlight">\(d_{ij} = \langle \vec a_i, \vec b_j\rangle = \vec a_i^T \vec b_j\)</span>. So the matrix <span class="math notranslate nohighlight">\(D\)</span> is the matrix whose entries are the <em>inner products of the rows of <span class="math notranslate nohighlight">\(A\)</span> with the columns of <span class="math notranslate nohighlight">\(B\)</span></em>. In a diagram, <span class="math notranslate nohighlight">\(D\)</span> is like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    D &amp;= \begin{bmatrix}
        \vec a_1^T\vec b_1 &amp; ... &amp; \vec a_1^T \vec b_l \\
        \vdots &amp; \ddots &amp; \vdots \\
        \vec a_r^T \vec b_1 &amp; ... &amp; \vec a_r^T \vec b_l
    \end{bmatrix}
\end{align*}\]</div>
<p>As a matter of notation, we might often have the case where we want to discuss or interpret a single element which is a product of two matrices. For instance, suppose we care about the entry <span class="math notranslate nohighlight">\((i, j)\)</span> of <span class="math notranslate nohighlight">\(AB\)</span>. We might also describe the resulting quantity <span class="math notranslate nohighlight">\(d_{ij}\)</span> using the notation <span class="math notranslate nohighlight">\((AB)_{ij}\)</span>. The reason we adopt this notation is that we want to emphasize that the matrix multiplication operation is performed first (it is in <em>parentheses</em>), and then we look at the <span class="math notranslate nohighlight">\((i,j)\)</span> entry of the resulting matrix.</p>
<p>The <strong>Euclidean distance</strong> is the most common distance between vectors we will see in this book. The Euclidean distance effectively tells us how far apart two points in <span class="math notranslate nohighlight">\(d\)</span>-dimensional space are. Given <span class="math notranslate nohighlight">\(\vec x, \vec y \in \mathbb R^d\)</span> (<span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> are <span class="math notranslate nohighlight">\(d\)</span>-dimensional real vectors), the Euclidean distance is the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \delta(\vec x, \vec y) &amp;= \langle \vec x - \vec y, \vec x - \vec y\rangle = \sum_{i = 1}^d (x_i - y_i)^2
\end{align*}\]</div>
<p>In particular, if we check the distance between a vector and the origin (the <strong>zero-vector</strong>, denoted <span class="math notranslate nohighlight">\(0_d\)</span>, which is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector where all entries are <span class="math notranslate nohighlight">\(0\)</span>), we end up with a very useful quantity, called the squared Euclidean norm. We will use a special notation for the Euclidean norm, which is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    ||\vec x||_2^2 &amp;= \delta(\vec x, 0_d) = \sum_{i = 1}^dx_i^2
\end{align*}\]</div>
<p>The subscript <span class="math notranslate nohighlight">\(_2\)</span> just means that this is the “2”-norm, which is a concept outside of the scope of this book. The superscript <span class="math notranslate nohighlight">\(^2\)</span> means that this is the squared Euclidean norm. Therefore, the Euclidean norm itself is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
||\vec x||_2 &amp;= \sqrt{\delta(\vec x, 0_d)} = \sqrt{\sum_{i = 1}^d x_i^2}
\end{align*}\]</div>
<p>What does this mean interpretation wise? The “square” operation basically means, if there are dimensions of <span class="math notranslate nohighlight">\(\vec x\)</span> that are big, the norm will end up being big. If the dimensions of <span class="math notranslate nohighlight">\(\vec x\)</span> are small, they will not contribute very much to the norm.</p>
<p>Based on the equation we saw above for the Euclidean distance, we could also understand the Euclidean distance to be the squared Euclidean norm of the vector which is the difference between <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span>. Using this convention:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \delta(\vec x, \vec y) &amp;= ||\vec x - \vec y||_2^2
\end{align*}\]</div>
<p>In this sense, we can see that the Euclidean distance and the Euclidean norms are attributing a concept of “length” and “how far” a vector is from another (whether that is the origin or an arbitrary real vector). Next, we will see a related concept for matrices. The <strong>squared Frobenius norm</strong> is the quantity, given a matrix <span class="math notranslate nohighlight">\(A \in \mathbb R^{r \times c}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    ||A||_F^2&amp;= \sum_{i = 1}^r \sum_{i = 1}^c a_{ij}^2
\end{align*}\]</div>
<p>Note that this is very similar to the squared Euclidean norm of a vector, except it is applied to both the rows <em>and</em> the columns of <span class="math notranslate nohighlight">\(A\)</span>. Again, we have a similar interpretation to the Euclidean norm. If an entry of <span class="math notranslate nohighlight">\(A\)</span> is big, it will contribute much to the Frobenius norm due to the squared <span class="math notranslate nohighlight">\(a_{ij}\)</span> term. If an entry is smaller, it will not contribute as much. The Frobenius norm itself is just the square root of this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    ||A||_F &amp;= \sqrt{\sum_{i = 1}^r \sum_{i = 1}^c a_{ij}^2}
\end{align*}\]</div>
</div>
<div class="section" id="probability">
<h3>Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h3>
<p>Throughout this book, we will be very concerned with probabilities and probability distributions. For this reason, we will introduce some basic notation that we will be concerned with. In probability analyses, we are concerned with describing things that occur in the real world with some level of uncertainty. We capture this uncertainty using probability, which in essence, describes how likely (or unlikely) a particular outcome is compared to all of the possible outcomes that could be realized. In general, we will call the most basic objects which occur with some uncertainty <strong>random variables</strong>, which is a variable whose values that we get to see in the real world (the <em>realizations</em> of the random variable) depend on some random phenomenon. We will denote a random variable using a similar notation to a one-dimensional variable, with the exception that we will <em>bold face</em> the variable to make clear that it is random. For instance, for a one-dimensional random variable, we will use notation like <span class="math notranslate nohighlight">\(\mathbf x\)</span>.</p>
<p>Like before, we can also have random vectors and random matrices. Like for the random variable, we will denote these with bold faces too. A random vector will be denoted using a bold faced variable with the vector symbol; for example, <span class="math notranslate nohighlight">\(\vec{\mathbf x}\)</span>. Likewise, a random matrix will be denoted using a bold faced upper case letter; for example, <span class="math notranslate nohighlight">\(\mathbf X\)</span>. Similar to how we indexed vectors and matrices, the index positions of random vectors and random matrices are random variables, too. That is, <span class="math notranslate nohighlight">\(\vec{\mathbf x}\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector whose entries are the random variables <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(d\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec{\mathbf x} &amp;= \begin{bmatrix}
        \mathbf x_1 \\
        \vdots \\
        \mathbf x_d
    \end{bmatrix}
\end{align*}\]</div>
<p>And <span class="math notranslate nohighlight">\(\mathbf X\)</span> is a <span class="math notranslate nohighlight">\((r \times c)\)</span> random matrix whose entries are the random varaiables <span class="math notranslate nohighlight">\(\mathbf x_{ij}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(j\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbf X &amp;= \begin{bmatrix}
        \mathbf x_{11} &amp; ... &amp; \mathbf x_{1c} \\
        \vdots &amp; \ddots &amp; \vdots \\
        \mathbf x_{r1} &amp; ... &amp; \mathbf x_{rc}
    \end{bmatrix}
\end{align*}\]</div>
<p>A probability distribution, denoted by <span class="math notranslate nohighlight">\(\mathbb P\)</span>, is a function which gives the probability of a particular value being attained by a random quantity. To state this another way, the probability distribution is concerned with fixing probabilities to realizations of random quantities. to make this a little more concrete, we will give an example with the simplest possible probability distribution, the Bernoulli distribution, denoted <span class="math notranslate nohighlight">\(Bernoulli(p)\)</span>. For the sake of this example, we will say that <span class="math notranslate nohighlight">\(\mathbf x\)</span> is a random variable which is <span class="math notranslate nohighlight">\(Bernoulli(p)\)</span> distributed, which we denote by <span class="math notranslate nohighlight">\(\mathbf x \sim Bernoulli(p)\)</span>. The Bernoulli distribution describes that the probability of the random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> taking a realization of <span class="math notranslate nohighlight">\(1\)</span> is <span class="math notranslate nohighlight">\(p\)</span>, whereas the probability of the random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> taking a realization of <span class="math notranslate nohighlight">\(0\)</span> is <span class="math notranslate nohighlight">\(1 - p\)</span>. Using the probability distribution, we would say that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P(\mathbf x = 0) &amp;= 1 - p \\
    \mathbb P(\mathbf x = 1) &amp;= p
\end{align*}\]</div>
</div>
<div class="section" id="advanced-probability">
<h3>Advanced Probability*<a class="headerlink" href="#advanced-probability" title="Permalink to this headline">¶</a></h3>
<p>the probability distribution for a random vector or a random matrix is described very similarly. The caveat is that with a random vector/matrix, we affix a probability of <em>every element</em> of the random vector/matrix equaling the realized vector/matrix. For instance, if <span class="math notranslate nohighlight">\(\vec{\mathbf x}\)</span> is a random vector taking realizations which are <span class="math notranslate nohighlight">\(d\)</span>-dimensional vectors, and <span class="math notranslate nohighlight">\(\vec x\)</span> is one such <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P(\vec{\mathbf x} = \vec x) = \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_d = x_d)
\end{align*}\]</div>
<p>and likewise, if <span class="math notranslate nohighlight">\(\mathbf X\)</span> is a random matrix taking realizations which are <span class="math notranslate nohighlight">\(r \times c\)</span> matrices, and <span class="math notranslate nohighlight">\(X\)</span> is one such <span class="math notranslate nohighlight">\(r \times c\)</span> matrix, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P(\mathbf X = X) &amp;= \mathbb P(\mathbf x_{11} = x_{11}, ..., \mathbf x_{rc} = x_{rc}) \\
    &amp;= \mathbb P(\mathbf x_{ij} = x_{ij} \text{ for any }i\text{ and }j)
\end{align*}\]</div>
<p>A probability concept we will see arise frequently in the advanced sections of the book is one called independence. A pair of random variables are independent if for any <span class="math notranslate nohighlight">\(x\)</span> which is a possible realization of <span class="math notranslate nohighlight">\(\mathbf x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is a possible realization of <span class="math notranslate nohighlight">\(\mathbf y\)</span>, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P(\mathbf x = x, \mathbf y = y) = \mathbb P(\mathbf x = x) \mathbb P(\mathbf y = y)
\end{align*}\]</div>
<p>A related concept that will be very important in our study of random matrices is the idea of mutual independence. If we have a set of <span class="math notranslate nohighlight">\(n\)</span> random variables <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> for all <span class="math notranslate nohighlight">\(i = 1,..., n\)</span>, this set of random variables is said to be mutually independent if for any <span class="math notranslate nohighlight">\(x_1\)</span> which is a possible realization of <span class="math notranslate nohighlight">\(\mathbf x_1\)</span>, any <span class="math notranslate nohighlight">\(x_2\)</span> which is a possible realization of <span class="math notranslate nohighlight">\(\mathbf x_2\)</span>, and so on up to <span class="math notranslate nohighlight">\(\mathbf x_n\)</span>, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n) = \prod_{i = 1}^n \mathbb P(\mathbf x_i = x_i)
\end{align*}\]</div>
<p>The ways in which this is useful will become more obvious through some of the advanced material of later chapters.</p>
<p>Another important concept we will see arise in some of the advanced material is the idea of conditional distributions. Given <span class="math notranslate nohighlight">\(x\)</span> which is a possible realization of <span class="math notranslate nohighlight">\(\mathbf x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is a possible realization of <span class="math notranslate nohighlight">\(\mathbf y\)</span>, then the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf x\)</span> on <span class="math notranslate nohighlight">\(\mathbf y\)</span> is the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf x = x | \mathbf y = y) &amp;= \frac{\mathbb P(\mathbf x = x, \mathbf y = y)}{\mathbb P(\mathbf y = y)}
\end{align*}\]</div>
<p>While outside the scope of this book, it can be shown that this is a proper probability distribution function, but we mainly are concerned with the fact that this is simply a useful notation for an intuitive idea. What this allows us to capture is the idea of attributing a probability for a random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> obtaining the value <span class="math notranslate nohighlight">\(x\)</span>, given that we already know that <span class="math notranslate nohighlight">\(\mathbf y\)</span> obtains the value <span class="math notranslate nohighlight">\(y\)</span>. A related concept, Baye’s Rule, uses a simple consequence of this theorem. Note that we could flip the probability statement above, and would obtain that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf y = y | \mathbf x = x) &amp;= \frac{\mathbb P(\mathbf x = x, \mathbf y = y)}{\mathbb P(\mathbf x = x)}
\end{align*}\]</div>
<p>a simple rearrangement of terms by multiplying both sides by <span class="math notranslate nohighlight">\(\mathbb P(\mathbf x = x)\)</span> gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf x, \mathbf y)&amp;= 
\mathbb P(\mathbf y = y | \mathbf x = x)\mathbb P(\mathbf x = x)
\end{align*}\]</div>
<p>Substituting this in to our first definition for a conditional distribution of <span class="math notranslate nohighlight">\(\mathbf x\)</span> on <span class="math notranslate nohighlight">\(\mathbf y\)</span> gives:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf x = x | \mathbf y = y) &amp;= \frac{\mathbb P(\mathbf y = y | \mathbf x = x)\mathbb P(\mathbf x = x)}{\mathbb P(\mathbf y = y)}
\end{align*}\]</div>
<p>which is Baye’s Rule.</p>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-foundations/ch1/ch1"></span><div class="section" id="the-network-machine-learning-landscape">
<h2><span class="section-number">1. </span>The Network Machine Learning Landscape<a class="headerlink" href="#the-network-machine-learning-landscape" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-foundations/ch1/what-is-a-network"></span><div class="section" id="what-is-a-network">
<h3><span class="section-number">1.1. </span>What Is A Network?<a class="headerlink" href="#what-is-a-network" title="Permalink to this headline">¶</a></h3>
<p>I would say start with a task: I have a bunch of edges.
how can I find the most similar nodes to a given node, ranked?
if these aren’t connected, should they be?  did we just link predict? (actual question I feel like link prediction is handwavy af)</p>
<div class="section" id="links-for-inspiration">
<h4><span class="section-number">1.1.1. </span>links for inspiration<a class="headerlink" href="#links-for-inspiration" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>ez intro on graps for ML https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b</p></li>
</ul>
</div>
</div>
<span id="document-foundations/ch1/why-study-networks"></span><div class="section" id="why-study-networks">
<h3><span class="section-number">1.2. </span>Why Study Networks?<a class="headerlink" href="#why-study-networks" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>motivation for network analysis: https://arxiv.org/pdf/0912.5410.pdf</p></li>
</ul>
</div>
<span id="document-foundations/ch1/examples-of-applications"></span><div class="section" id="examples-of-applications">
<h3><span class="section-number">1.3. </span>Examples of applications<a class="headerlink" href="#examples-of-applications" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch1/types-of-networks"></span><div class="section" id="types-of-networks">
<h3><span class="section-number">1.4. </span>Types of Networks<a class="headerlink" href="#types-of-networks" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch1/types-of-learning-probs"></span><div class="section" id="types-of-network-learning-problems">
<h3><span class="section-number">1.5. </span>Types of Network Learning Problems<a class="headerlink" href="#types-of-network-learning-problems" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch1/main-challenges"></span><div class="section" id="main-challenges-of-network-learning">
<h3><span class="section-number">1.6. </span>Main Challenges of Network Learning<a class="headerlink" href="#main-challenges-of-network-learning" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch1/exercises"></span><div class="section" id="exercises">
<h3><span class="section-number">1.7. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
<span id="document-foundations/ch2/ch2"></span><div class="section" id="end-to-end-biology-network-machine-learning-project">
<h2><span class="section-number">2. </span>End-to-end Biology Network Machine Learning Project<a class="headerlink" href="#end-to-end-biology-network-machine-learning-project" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-foundations/ch2/big-picture"></span><div class="section" id="look-at-the-big-picture">
<h3><span class="section-number">2.1. </span>Look at the big picture<a class="headerlink" href="#look-at-the-big-picture" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch2/get-the-data"></span><div class="section" id="get-the-data">
<h3><span class="section-number">2.2. </span>Get the Data<a class="headerlink" href="#get-the-data" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch2/prepare-the-data"></span><div class="section" id="prepare-the-data-for-network-algorithms">
<h3><span class="section-number">2.3. </span>Prepare the Data for Network Algorithms<a class="headerlink" href="#prepare-the-data-for-network-algorithms" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch2/transformation-techniques"></span><div class="section" id="transformation-techniques">
<h3><span class="section-number">2.4. </span>Transformation Techniques<a class="headerlink" href="#transformation-techniques" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch2/select-and-train"></span><div class="section" id="select-and-train-a-model">
<h3><span class="section-number">2.5. </span>Select and Train a Model<a class="headerlink" href="#select-and-train-a-model" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch2/fine-tune"></span><div class="section" id="fine-tune-your-model">
<h3><span class="section-number">2.6. </span>Fine-Tune your Model<a class="headerlink" href="#fine-tune-your-model" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
<span id="document-foundations/ch3/ch3"></span><div class="section" id="end-to-end-business-network-machine-learning-project">
<h2><span class="section-number">3. </span>End-to-end Business Network Machine Learning Project<a class="headerlink" href="#end-to-end-business-network-machine-learning-project" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-foundations/ch3/big-picture"></span><div class="section" id="look-at-the-big-picture">
<h3><span class="section-number">3.1. </span>Look at the Big Picture<a class="headerlink" href="#look-at-the-big-picture" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch3/get-the-data"></span><div class="section" id="get-the-data">
<h3><span class="section-number">3.2. </span>Get the Data<a class="headerlink" href="#get-the-data" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch3/discover-and-visualize"></span><div class="section" id="discover-and-visualize-the-data-to-gain-insights">
<h3><span class="section-number">3.3. </span>Discover and Visualize the Data to Gain Insights<a class="headerlink" href="#discover-and-visualize-the-data-to-gain-insights" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-foundations/ch3/prepare-the-data"></span><div class="section" id="prepare-the-data-for-network-algorithms">
<h3><span class="section-number">3.4. </span>Prepare the Data for Network Algorithms<a class="headerlink" href="#prepare-the-data-for-network-algorithms" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-representations/ch4/ch4"></span><div class="section" id="properties-of-networks-as-a-statistical-object">
<h2><span class="section-number">1. </span>Properties of Networks as a Statistical Object<a class="headerlink" href="#properties-of-networks-as-a-statistical-object" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-representations/ch4/matrix-representations"></span><div class="section" id="matrix-representations-of-networks">
<h3><span class="section-number">1.1. </span>Matrix Representations Of Networks<a class="headerlink" href="#matrix-representations-of-networks" title="Permalink to this headline">¶</a></h3>
<p>When we work with networks, we need a way to represent them mathematically and in our code. A network itself lives in network space, which is just the set of all possible networks. Network space is kind of abstract and inconvenient if we want to use traditional mathematics, so we’d generally like to represent networks with groups of numbers to make everything more concrete.</p>
<p>More specifically, we would often like to represent networks with <em>matrices</em>. In addition to being computationally convenient, using matrices to represent networks lets us bring in a surprising amount of tools from linear algebra and statistics. Programmatically, using matrices also lets us use common Python tools for array manipulation like numpy.</p>
<p>The most common matrix representation of a network is called the Adjacency Matrix, and we’ll learn about that first.</p>
<div class="section" id="the-adjacency-matrix">
<h4><span class="section-number">1.1.1. </span>The Adjacency Matrix<a class="headerlink" href="#the-adjacency-matrix" title="Permalink to this headline">¶</a></h4>
<p>The beating heart of matrix representations for networks throughout this book is the adjacency matrix. The idea is pretty straightforward: Let’s say you have a network with <span class="math notranslate nohighlight">\(n\)</span> nodes. You give each node an index – usually some value between 0 and n – and then you create an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix. If there is an edge between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>, you fill the <span class="math notranslate nohighlight">\((i, j)_{th}\)</span> value of the matrix with an entry, usually <span class="math notranslate nohighlight">\(1\)</span> if your network has unweighted edges. In the case of undirected networks, you end up with a symmetric matrix with full of 1’s and 0’s, which completely represents the topology of your network.</p>
<p>Let’s see this in action. We’ll make a network with only three nodes, since that’s small and easy to understand, and then we’ll show what it looks like as an adjacency matrix.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">)</span>

<span class="n">pos</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;0&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;2&#39;</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">)}</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:purple&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="p">,</span>
                <span class="n">font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
                <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/matrix-representations_4_0.png" src="_images/matrix-representations_4_0.png" />
</div>
</div>
<p>Our network has three nodes, labeled <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(2\)</span>, and <span class="math notranslate nohighlight">\(3\)</span>. Each of these three nodes is either connected or not connected to each of the two other nodes. We’ll make a square matrix <span class="math notranslate nohighlight">\(A\)</span>, with 3 rows and 3 columns, so that each node has its own row and column associated to it.</p>
<p>So, let’s fill out the matrix. We start with the first row, which corresponds to the first node, and move along the columns. If there is an edge between the first node and the node whose index matches the current column, put a 1 in the current location. If the two nodes aren’t connected, add a 0. When you’re done with the first row, move on to the second. Keep going until the whole matrix is filled with 0’s and 1’s.</p>
<p>The end result looks like the matrix below. Since the second and third nodes aren’t connected, there is a <span class="math notranslate nohighlight">\(0\)</span> in locations <span class="math notranslate nohighlight">\(A_{2, 1}\)</span> and <span class="math notranslate nohighlight">\(A_{1, 2}\)</span>. There are also zeroes along the diagonals, since nodes don’t have edges with themselves.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Adjacency matrix&quot;</span><span class="p">,</span> 
               <span class="n">xticklabels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">360</span><span class="p">)</span>

<span class="c1"># ticks</span>
<span class="n">yticks</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">()</span>
<span class="n">yticks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s1">&#39;node 0&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">xticks</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">()</span>
<span class="n">xticks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s1">&#39;node 0&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Nodes 0 and 2 </span><span class="se">\n</span><span class="s2">are connected&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Nodes 2 and 1 </span><span class="se">\n</span><span class="s2">aren&#39;t connected&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">1.03</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:purple&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="p">,</span>
                <span class="n">font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
                <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Nodes 0 and 2 </span><span class="se">\n</span><span class="s2">are connected&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">63</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Nodes 2 and 1 </span><span class="se">\n</span><span class="s2">aren&#39;t connected&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=-</span><span class="mi">63</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Layout plot&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/matrix-representations_6_0.png" src="_images/matrix-representations_6_0.png" />
</div>
</div>
<p>Although the adjacency matrix is straightforward and easy to understand, it isn’t the only way to represent networks.</p>
</div>
<div class="section" id="the-incidence-matrix">
<h4><span class="section-number">1.1.2. </span>The Incidence Matrix<a class="headerlink" href="#the-incidence-matrix" title="Permalink to this headline">¶</a></h4>
<p>Instead of having values in a symmetric matrix represent possible edges, like with the Adjacency Matrix, we could have rows represent nodes and columns represent edges. This is called the <em>Incidence Matrix</em>, and it’s useful to know about – although it won’t appear too much in this book. If there are <span class="math notranslate nohighlight">\(n\)</span> nodes and <span class="math notranslate nohighlight">\(m\)</span> edges, you make an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix. Then, to determine whether a node is a member of a given edge, you’d go to that node’s row and the edge’s column. If the entry is nonzero (<span class="math notranslate nohighlight">\(1\)</span> if the network is unweighted), then the node is a member of that edge, and if there’s a <span class="math notranslate nohighlight">\(0\)</span>, the node is not a member of that edge.</p>
<p>You can see the incidence matrix for our network below. Notice that with incidence plots, edges are (generally arbitrarily) assigned indices as well as nodes.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">networkx.linalg.graphmatrix</span> <span class="kn">import</span> <span class="n">incidence_matrix</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span> <span class="n">n_colors</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">I</span> <span class="o">=</span> <span class="n">incidence_matrix</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="n">A</span><span class="p">))</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                   <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edges&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Nodes&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Incidence matrix&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:purple&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="p">,</span>
                <span class="n">font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
                <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">24</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Edge 1&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">65</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">45</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Edge 0&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Layout plot&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/matrix-representations_10_0.png" src="_images/matrix-representations_10_0.png" />
</div>
</div>
<p>When networks are large, incidence matrices tend to be extremely sparse – meaning, their values are mostly 0’s. This is because each column must have exactly two nonzero values along its rows: one value for the first node its edge is connected to, and another for the second. Because of this, incidence matrices are usually represented in Python computationally as scipy’s <em>sparse matrices</em> rather than as numpy arrays, since this data type is much better-suited for matrices which contain mostly zeroes.</p>
<p>You can also add orientation to incidence matrices, even in undirected networks, which we’ll discuss next.</p>
</div>
<div class="section" id="the-oriented-incidence-matrix">
<h4><span class="section-number">1.1.3. </span>The Oriented Incidence Matrix<a class="headerlink" href="#the-oriented-incidence-matrix" title="Permalink to this headline">¶</a></h4>
<p>The oriented incidence matrix is extremely similar to the normal incidence matrix, except that you assign a direction or orientation to each edge: you define one of its nodes as being the head node, and the other as being the tail. For undirected networks, you can assign directionality arbitrarily. Then, for the column in the incidence matrix corresponding to a given edge, the tail node has a value of <span class="math notranslate nohighlight">\(-1\)</span>, and the head node has a value of <span class="math notranslate nohighlight">\(0\)</span>. Nodes who aren’t a member of a particular edge are still assigned values of <span class="math notranslate nohighlight">\(0\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">networkx.linalg.graphmatrix</span> <span class="kn">import</span> <span class="n">incidence_matrix</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span> <span class="n">n_colors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">OI</span> <span class="o">=</span> <span class="n">incidence_matrix</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">oriented</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">OI</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                   <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edges&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Nodes&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Oriented Incidence matrix&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Tail Node&quot;</span><span class="p">,</span> <span class="p">(</span><span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Head Node&quot;</span><span class="p">,</span> <span class="p">(</span><span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="mf">1.95</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:purple&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos</span><span class="p">,</span>
                <span class="n">font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
                <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">24</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Edge 1&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">65</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">45</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Edge 0&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Layout plot&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-.</span><span class="mi">1</span><span class="p">,</span> <span class="o">-.</span><span class="mi">05</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Tail Node&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">-.</span><span class="mi">05</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Head Node&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.9, -0.05, &#39;Head Node&#39;)
</pre></div>
</div>
<img alt="_images/matrix-representations_14_1.png" src="_images/matrix-representations_14_1.png" />
</div>
</div>
<p>Although we won’t use incidence matrices, oriented or otherwise, in this book too much, we introduced them because there’s a deep connection between incidence matrices, adjacency matrices, and a matrix representation that we haven’t introduced yet called the Laplacian. Before we can explore that connection, we’ll discuss one more representation: the degree matrix.</p>
</div>
<div class="section" id="the-degree-matrix">
<h4><span class="section-number">1.1.4. </span>The Degree Matrix<a class="headerlink" href="#the-degree-matrix" title="Permalink to this headline">¶</a></h4>
<p>The degree matrix isn’t a full representation of our network, because you wouldn’t be able to reconstruct an entire network from a degree matrix.</p>
</div>
<div class="section" id="the-laplacian-matrix">
<h4><span class="section-number">1.1.5. </span>The Laplacian Matrix<a class="headerlink" href="#the-laplacian-matrix" title="Permalink to this headline">¶</a></h4>
<div class="section" id="the-symmetric-laplacian">
<h5><span class="section-number">1.1.5.1. </span>The Symmetric Laplacian<a class="headerlink" href="#the-symmetric-laplacian" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="the-random-walk-laplacian">
<h5><span class="section-number">1.1.5.2. </span>The Random-Walk Laplacian<a class="headerlink" href="#the-random-walk-laplacian" title="Permalink to this headline">¶</a></h5>
</div>
</div>
</div>
<span id="document-representations/ch4/network-representations"></span><div class="section" id="representations-of-networks">
<h3><span class="section-number">1.2. </span>Representations of Networks<a class="headerlink" href="#representations-of-networks" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-representations/ch4/properties-of-networks"></span><div class="section" id="properties-of-networks">
<h3><span class="section-number">1.3. </span>Properties of Networks<a class="headerlink" href="#properties-of-networks" title="Permalink to this headline">¶</a></h3>
<div class="section" id="descriptive-properties-of-networks">
<h4><span class="section-number">1.3.1. </span>Descriptive Properties of Networks<a class="headerlink" href="#descriptive-properties-of-networks" title="Permalink to this headline">¶</a></h4>
<p>Remember that a network topology, a collection of nodes <span class="math notranslate nohighlight">\(\mathcal V\)</span>, edges <span class="math notranslate nohighlight">\(\mathcal E\)</span>, can be represented as an <span class="math notranslate nohighlight">\(n \times n\)</span> adjacency matrix, where <span class="math notranslate nohighlight">\(n\)</span> is the total number of nodes. The adjacency matrix looks like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A &amp;= \begin{bmatrix}
        a_{11} &amp; ... &amp; a_{1n} \\
        \vdots &amp; \ddots &amp; \vdots \\
        a_{n1} &amp; ... &amp; a_{nn}
    \end{bmatrix},
\end{align*}\]</div>
<p>Let’s say we have a network representing the five boroughs of New York (Staten Island SI, Brooklyn BK, Queens Q, the Bronx BX, and Manhattan MH). The nodes in our network are the five boroughs. The edges <span class="math notranslate nohighlight">\((i,j)\)</span> of our network exist if one can travel from borough <span class="math notranslate nohighlight">\(i\)</span> to borough <span class="math notranslate nohighlight">\(j\)</span> along a bridge.</p>
<p>Below, we will look at a map of New York City, with the bridges connecting the different boroughs. In the middle, we look at this map as a network layout plot. The arrows indicate the direction of travel. On the right, we look at this map as an adjacency matrix:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">mpimg</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;./img/newyork.png&#39;</span><span class="p">)</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;SI&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Map of New York City Boroughs and Connections&quot;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Layout Plot of New York City Boroughs and Connections&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Adjacency Matrix of New York City Boroughs and Connections&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">])</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_1_0.png" src="_images/properties-of-networks_1_0.png" />
</div>
</div>
<div class="section" id="the-edges-of-undirected-networks-are-bi-directional">
<h5><span class="section-number">1.3.1.1. </span>The edges of undirected networks are bi-directional<a class="headerlink" href="#the-edges-of-undirected-networks-are-bi-directional" title="Permalink to this headline">¶</a></h5>
<p>When you decide to travel from borough <span class="math notranslate nohighlight">\(i\)</span> to borough <span class="math notranslate nohighlight">\(j\)</span>, you care about whether you can <em>actually drive</em> on that bridge! In a similar way, the concept of directedness describes whether we need to worry about one-way bridges and bridge closures. If there are one-way bridges in our network, then a bridge from borough <span class="math notranslate nohighlight">\(i\)</span> to borough <span class="math notranslate nohighlight">\(j\)</span> doesn’t <em>necessarily</em> imply that a bridge from borough <span class="math notranslate nohighlight">\(j\)</span> to borough <span class="math notranslate nohighlight">\(i\)</span> exists (just ask New York drivers). If, for instance, the Brooklyn bridge was closed from Manhattan to Brooklyn, our network might change like this. Note that the red arrow going from Manhattan (MH) to Brooklyn (BK) is no longer present:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Brooklyn Bridge in Service&quot;</span><span class="p">)</span>

<span class="n">G</span><span class="o">.</span><span class="n">remove_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>

<span class="n">G2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>

<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
                <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>
                <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Brooklyn Bridge out of Service from Manhattan to Brooklyn&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_3_0.png" src="_images/properties-of-networks_3_0.png" />
</div>
</div>
<p>Fortunately, in the context of this book, we will usually only worry about the undirected case, or when the presence of an arrow implies that the other direction exists, too. A network is <strong>undirected</strong> if a connection between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span> implies that node <span class="math notranslate nohighlight">\(j\)</span> is also connected to node <span class="math notranslate nohighlight">\(i\)</span>. For this reason, we will usually omit the arrows entirely, like we show below:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Layout Plot of New York, Undirected&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_5_0.png" src="_images/properties-of-networks_5_0.png" />
</div>
</div>
<p>For the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, remember a connection between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> is represented by the adjacency <span class="math notranslate nohighlight">\(a_{ij}\)</span>. This means that if the network is undirected, <span class="math notranslate nohighlight">\(a_{ij} = a_{ji}\)</span>, for all pairs of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. By definition, this tells us that the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> is symmetric, so <span class="math notranslate nohighlight">\(A = A^\top\)</span>.</p>
</div>
<div class="section" id="loopless-networks-do-not-have-self-loops">
<h5><span class="section-number">1.3.1.2. </span>Loopless networks do not have self-loops<a class="headerlink" href="#loopless-networks-do-not-have-self-loops" title="Permalink to this headline">¶</a></h5>
<p>If we are already in a borough, why would we want to take a bridge to that same borough? This logic relates to the concept of <em>self-loops</em> in a network. A <strong>self-loop</strong> in a network describes whether nodes can connect back to themselves. For instance, consider the following loop from Staten Island back to itself. This would have the interpretation of a bridge which connects Staten Island back to itself:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO: add arrow from SI to SI</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;SI&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;SI&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_7_0.png" src="_images/properties-of-networks_7_0.png" />
</div>
</div>
<p>In this example, the concept of self-loops is a little trite, but it is worth mentioning as you might see it arise elsewhere. A network is <strong>loopless</strong> if self-loops are not possible. For the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, a self-loop would be represented by the adjacencies <span class="math notranslate nohighlight">\(a_{ii}\)</span> for all nodes <span class="math notranslate nohighlight">\(i\)</span>. Note that these entries <span class="math notranslate nohighlight">\(a_{ii}\)</span> are all of the <em>diagonal</em> entries of <span class="math notranslate nohighlight">\(A\)</span>. Therefore, for a network which is loopless, all adjacencies <span class="math notranslate nohighlight">\(a_{ii}\)</span> on the diagonal are <span class="math notranslate nohighlight">\(0\)</span>. You might also see this property abbreviated by stating that the diagonal of the adjacency matrix is <span class="math notranslate nohighlight">\(0\)</span>, or <span class="math notranslate nohighlight">\(diag(A) = 0\)</span>.</p>
</div>
<div class="section" id="unweighted-networks-either-have-an-edge-or-they-don-t">
<h5><span class="section-number">1.3.1.3. </span>Unweighted networks either have an edge, or they don’t<a class="headerlink" href="#unweighted-networks-either-have-an-edge-or-they-don-t" title="Permalink to this headline">¶</a></h5>
<p>Do we need to convey information about how long it takes to get from borough <span class="math notranslate nohighlight">\(i\)</span> to borough <span class="math notranslate nohighlight">\(j\)</span> with our network? This fundamental question underlies the concept of <em>weightedness</em> in networks. We could use things called <em>edge-weights</em> <span class="math notranslate nohighlight">\(w(i, j)\)</span> could be used to describe the amount of time it takes to get from borough <span class="math notranslate nohighlight">\(i\)</span> to borough <span class="math notranslate nohighlight">\(j\)</span>. An <strong>edge-weight</strong> <span class="math notranslate nohighlight">\(w(i,j)\)</span> assigns a weight to an edge between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> if that edge exists. If we care about weightedness in the network, the network is called <em>weighted</em>. The adjacencies <span class="math notranslate nohighlight">\(a_{ij}\)</span> of <span class="math notranslate nohighlight">\(A\)</span> for a weighted network take the value of the edge-weight; that is, <span class="math notranslate nohighlight">\(a_{ij} = w_{ij}\)</span> for any edge which exists between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. In the below plot, edge-weight indicates the approximate time to travel from one borough to the other. The network is undirected, so we don’t have to worry about directionality differences. The edge-weight is indicated by the number along the corresponding edge. We can also visualize edge-weights in terms of the adjacency matrix, which we show on the right:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_edge_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="s1">&#39;weight&#39;</span><span class="p">);</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edge_labels</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">edge_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">);</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Weighted Layout Plot of New York City Boroughs and Connections&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Weighted Adjacency Matrix of New York City Boroughs and Connections&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">]);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_9_0.png" src="_images/properties-of-networks_9_0.png" />
</div>
</div>
<p>For most examples in this book, we will usually discuss <em>unweighted</em> or <em>binary</em> networks. A network is <strong>unweighted</strong> or <strong>binary</strong> if we only care about whether edges are <em>present</em> or <em>absent</em>. In a network which is unweighted, an adjacency <span class="math notranslate nohighlight">\(a_{ij}\)</span> takes the value <span class="math notranslate nohighlight">\(1\)</span> if there is an edge from node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span>, and takes the value <span class="math notranslate nohighlight">\(0\)</span> if there is <em>not</em> an edge from node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span>.</p>
<div class="admonition-this-book-considers-simple-networks admonition">
<p class="admonition-title">This book considers <em>simple networks</em></p>
<p>This point is a <em>really</em> big deal conceptually for our study of network machine learning. A <strong>simple network</strong> is loopless, undirected, and unweighted. Most of the examples and techniques we look at in this book are developed in the context of simple networks. Fortunately, this note is largely conceptual, and doesn’t really impact much from an implementation perspective. All the techniques and packages we use will make sensible choices, or will directly extend, to cases that fall outside of this particular setup. If your networks don’t satisfy one or any of these properties, most of the approaches discussed herein will still work. If the technique will not work for the network you have provided, the software package used, <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>, will either give you a warning or an explicit error if there is a substantial issue with the network you have provided.</p>
</div>
</div>
</div>
<div class="section" id="descriptive-properties-of-nodes">
<h4><span class="section-number">1.3.2. </span>Descriptive Properties of Nodes<a class="headerlink" href="#descriptive-properties-of-nodes" title="Permalink to this headline">¶</a></h4>
<p>Just like we have many words and properties which describe the network itself, we also have special vocabulary in network machine learning to describe properties about the individual nodes in the network. Remember that the nodes of the network are the <span class="math notranslate nohighlight">\(n\)</span>-element set <span class="math notranslate nohighlight">\(\mathcal V\)</span>, which is just the collection <span class="math notranslate nohighlight">\(\left\{v_1, ..., v_n\right\}\)</span>, where <span class="math notranslate nohighlight">\(v_1\)</span> is node <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(v_2\)</span> is node <span class="math notranslate nohighlight">\(2\)</span>, so on and so forth. We will tend to use the short-hand <span class="math notranslate nohighlight">\(v_i\)</span> to describe the node <span class="math notranslate nohighlight">\(i\)</span>, for all nodes from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(n\)</span>.</p>
<div class="section" id="node-adjacencies-and-incidences">
<h5><span class="section-number">1.3.2.1. </span>Node adjacencies and incidences<a class="headerlink" href="#node-adjacencies-and-incidences" title="Permalink to this headline">¶</a></h5>
<p>We begin by descrcibing properties of single nodes in a simple network. The simplest property of a network is <em>adjacency</em>. A pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> in an undirected network are <strong>adjacent</strong> or are <strong>neighbors</strong> if an edge exists between them. In terms of the adjacency matrix, two nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are adjacent/neighbors if the element <span class="math notranslate nohighlight">\(a_{ij}\)</span> has a value of one. For instance, in the New York City example, the nodes SI and BK are adjacent/neighbors due to the presence of the green edge, shown in the figure. A related property is known as <em>incidence</em>. A node <span class="math notranslate nohighlight">\(i\)</span> is <strong>incident</strong> an edge <span class="math notranslate nohighlight">\((i, j)\)</span> or an edge <span class="math notranslate nohighlight">\((j,i)\)</span> if it is one of the two nodes which the edge connects. The adjacencies corresponding to this edge, <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\)</span>, will both take a value of one. For instance, the nodes SI and BK are incident the green edge shown in the figure, as this edge connects SI to BK:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>

<span class="n">G2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;(SI, BK) edge highlighted&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_11_0.png" src="_images/properties-of-networks_11_0.png" />
</div>
</div>
<p>These two nodes are <em>adjacent</em> one another due to the fact that an edge exists between them.</p>
</div>
<div class="section" id="node-degree-quantifies-the-number-of-incidences">
<h5><span class="section-number">1.3.2.2. </span>Node degree quantifies the number of incidences<a class="headerlink" href="#node-degree-quantifies-the-number-of-incidences" title="Permalink to this headline">¶</a></h5>
<p>The simplest summary statistic for a node is known as the <em>node degree</em>. The <strong>node degree</strong> of a node <span class="math notranslate nohighlight">\(i\)</span> in a simple network is the number of edges incident to it. Since every edge incident <span class="math notranslate nohighlight">\((i, j)\)</span> which is incident node <span class="math notranslate nohighlight">\(i\)</span> takes the value of <span class="math notranslate nohighlight">\(1\)</span>, we can count the adjacencies that correspond to the edges incident node <span class="math notranslate nohighlight">\(i\)</span>. If an edge does not exist, the adjacency corresponding to this <em>potential</em> edge takes a value of zero. Therefore, we can just sum along the <span class="math notranslate nohighlight">\(i^{th}\)</span> row or the <span class="math notranslate nohighlight">\(i^{th}\)</span> column of the adjacency matrix, since the row (column) correspond to the edges incident node <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    degree(v_i) &amp;= \sum_{j = 1}^n a_{ij} = \sum_{j = 1}^n a_{ji}
\end{align*}\]</div>
<p>This means we will sum all of the potential edges which do <em>not</em> exist (any of the <span class="math notranslate nohighlight">\(a_{ij}\)</span>s which take a value of zero, and therefore no edge exists between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>) with all of the edges which <em>do</em> exist and are incident node <span class="math notranslate nohighlight">\(i\)</span> (since these <span class="math notranslate nohighlight">\(a_{ij}\)</span>s will take a value of one). For instance, if we consider the node BK in our example, we have two incident edges, indicated in green, so <span class="math notranslate nohighlight">\(degree(v_{BK}) = 2\)</span>. When we look at the corresponding adjacency matrix, if we sum the adjacencies for node <span class="math notranslate nohighlight">\(v_{BK}\)</span>, we also get two. The adjacencies which would be summed <span class="math notranslate nohighlight">\(\sum_{i = 1}^n a_{ji}\)</span> are shown in blue, and the adjacencies which would be summed <span class="math notranslate nohighlight">\(\sum_{j = 1}^n a_{ij}\)</span> are shown in green:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>

<span class="n">G2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Edges incident BK&quot;</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_matrix</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Adjacencies of BK&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">])</span>


<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span>
     <span class="n">patches</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">(</span>
         <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
         <span class="mf">1.0</span><span class="p">,</span>
         <span class="mf">5.0</span><span class="p">,</span>
         <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
         <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">lw</span><span class="o">=</span><span class="mi">4</span>
     <span class="p">)</span> <span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span>
     <span class="n">patches</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">(</span>
         <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
         <span class="mf">5.0</span><span class="p">,</span>
         <span class="mf">1.0</span><span class="p">,</span>
         <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span>
         <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">lw</span><span class="o">=</span><span class="mi">4</span>
     <span class="p">)</span> <span class="p">)</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_13_0.png" src="_images/properties-of-networks_13_0.png" />
</div>
</div>
</div>
<div class="section" id="the-degree-matrix-indicates-the-degrees-of-each-node">
<h5><span class="section-number">1.3.2.3. </span>The degree matrix indicates the degrees of each node<a class="headerlink" href="#the-degree-matrix-indicates-the-degrees-of-each-node" title="Permalink to this headline">¶</a></h5>
<p>A useful quantity which we will come across in many of the later chapters of this book is called the <em>degree matrix</em> of the network. The degree matrix is the <em>diagonal</em> matrix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    D &amp;= \begin{bmatrix}
        d_1 &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \ddots &amp; \ddots&amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; d_n
    \end{bmatrix}, \;\;\; d_i = degree(v_i)
\end{align*}\]</div>
<p>This matrix <span class="math notranslate nohighlight">\(D\)</span> is called <strong>diagonal</strong> because all of the entries <span class="math notranslate nohighlight">\(d_{ij} = 0\)</span> unless <span class="math notranslate nohighlight">\(i = j\)</span>. The diagonal entries <span class="math notranslate nohighlight">\(d_{ii}\)</span> of the degree matrix are simply the node degrees <span class="math notranslate nohighlight">\(degree(v_i)\)</span> for each node <span class="math notranslate nohighlight">\(i\)</span>. Using the counting procedure we described above, we can see that the node SI has degree one, the node BK has degree two, the node MH has degree three, the node Q has degree two, and the node BX has degree two. Therefore, the degree matrix is:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">A</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">A</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Degree Matrix of New York City Boroughs&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">]);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_15_0.png" src="_images/properties-of-networks_15_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="network-summary-statistics-tell-us-useful-attributes-about-networks">
<h4><span class="section-number">1.3.3. </span>Network summary statistics tell us useful attributes about networks<a class="headerlink" href="#network-summary-statistics-tell-us-useful-attributes-about-networks" title="Permalink to this headline">¶</a></h4>
<p>When we learn about networks, it is often valuable to compute properties of the network so that we can get a better understanding of the relationships within it. We will caall these properties <em>network summary statistics</em>. Although this book will focus more on finding and using <em>representations</em> of networks than using summary statistics, they’re useful to know about. We will introduce two network summary statistics, the network density and the clustering coefficient, and then show an example as to why we do not find summary statistics all that useful for network machine learning.</p>
<div class="section" id="the-network-density-indicates-the-fraction-of-possible-edges-which-exist">
<h5><span class="section-number">1.3.3.1. </span>The network density indicates the fraction of possible edges which exist<a class="headerlink" href="#the-network-density-indicates-the-fraction-of-possible-edges-which-exist" title="Permalink to this headline">¶</a></h5>
<p>Given athe adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> of a simple network, what fraction of the possible edges <em>actually</em> exist?</p>
<p>To understand this quantity, first we need to understand how many edges are possible in a network. We have <span class="math notranslate nohighlight">\(n\)</span> total nodes in the network, so <span class="math notranslate nohighlight">\(A\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix. Therefore, <span class="math notranslate nohighlight">\(A\)</span> has <span class="math notranslate nohighlight">\(n^2\)</span> total entries. However, it turns out that over <em>half</em> of these entries are redundant. Since we said that the network was loopless, this means that every entry is <em>by default</em> <span class="math notranslate nohighlight">\(0\)</span> along the diagonal. Since each node <span class="math notranslate nohighlight">\(i\)</span> has a corresponding diagonal entry <span class="math notranslate nohighlight">\(a_{ii}\)</span>, this comes to <span class="math notranslate nohighlight">\(n\)</span> entries in total that we do not need to count. This leaves our total possible number of edges at <span class="math notranslate nohighlight">\(n^2\)</span> (the total number of entries in the matrix <span class="math notranslate nohighlight">\(A\)</span>) minus <span class="math notranslate nohighlight">\(n\)</span> (the total number of entries which are automatically <span class="math notranslate nohighlight">\(0\)</span>), or <span class="math notranslate nohighlight">\(n^2 - n = n(n - 1)\)</span>. This quantity represents the total number of possible edges which are <em>not</em> in the diagonal.</p>
<p>What else are we overcounting? Well, as it turns out, since the network is also <em>undirected</em>, every node that is <em>not</em> in the diagonal is also being double counted. Why is this? Remember that an undirected network has an adjacency matrix where for every pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(a_{ij} = a_{ji}\)</span>. This means that we overcount the number of possible edges not in the diagonal by a factor of <em>two</em>, since each off-diagonal entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> has a corresponding entry <span class="math notranslate nohighlight">\(a_{ji}\)</span>. This leaves the total number of possible edges in the network as <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span>, or the total number of possible edges not in the diagonal reduced by a factor of two. This quantity is equivalent to the notation <span class="math notranslate nohighlight">\(\binom n 2\)</span>, which is read as “<span class="math notranslate nohighlight">\(n\)</span> <em>choose</em> <span class="math notranslate nohighlight">\(2\)</span>”. You might see this notation arise in the study of <em>combinatorics</em>, where it is used to answer the question of, “In how many ways can we <em>choose</em> two items from <span class="math notranslate nohighlight">\(n\)</span> items?” In the network below, we see all of the <em>possible</em> edges indicated in red. If you count them up, there are <span class="math notranslate nohighlight">\(\frac{1}{2}\cdot 5 \cdot (5 - 1) = 10\)</span> red edges, in total:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G_poss</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G_node</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">nodes</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;SI&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;MH&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="s2">&quot;BK&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;Q&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="s2">&quot;BX&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)}</span>

<span class="k">for</span> <span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">posn</span><span class="p">)</span> <span class="ow">in</span> <span class="n">nodes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">G_poss</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">posn</span><span class="p">)</span>
    <span class="n">G_node</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">posn</span><span class="p">)</span>
<span class="n">node_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nodes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="c1"># add all possible combinations of nodes as red edges</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">nodei</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">node_keys</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">nodej</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">node_keys</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">G_poss</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">nodei</span><span class="p">,</span> <span class="n">nodej</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_poss</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">1200</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_node</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">1200</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_17_0.png" src="_images/properties-of-networks_17_0.png" />
</div>
</div>
<p>Now, how many edges <em>actually</em> exist in our network? The sum of all of the entries of <span class="math notranslate nohighlight">\(A\)</span> can be represented by the quantity <span class="math notranslate nohighlight">\(\sum_{i = 1}^n \sum_{i = 1}^n a_{ij}\)</span>, however, there are some redundancies. Remember that <span class="math notranslate nohighlight">\(A\)</span> is loopless, so we don’t need to count the diagonal entries at all. This brings our quantity to <span class="math notranslate nohighlight">\(\sum_{i = 1}^n \sum_{i \neq j}a_{ij}\)</span>, since we don’t need to count any edges along the diagonal of <span class="math notranslate nohighlight">\(A\)</span>. Next, remember that if an edge in <span class="math notranslate nohighlight">\(A\)</span> exists between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, that <em>both</em> <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\)</span> take the value of <span class="math notranslate nohighlight">\(1\)</span>, due to the undirected property. This means that to obtain the edge count of <span class="math notranslate nohighlight">\(A\)</span>, that we only need to count <em>either</em> <span class="math notranslate nohighlight">\(a_{ij}\)</span> <em>or</em> <span class="math notranslate nohighlight">\(a_{ji}\)</span>. Somewhat arbitrarily in this book, we will always count the adjacencies <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the upper triangle of <span class="math notranslate nohighlight">\(A\)</span>, which are the entries where <span class="math notranslate nohighlight">\(j &gt; i\)</span>. This brings our quantity to <span class="math notranslate nohighlight">\(\sum_{i = 1}^n \sum_{j &gt; i} a_{ij}\)</span>, which we can write <span class="math notranslate nohighlight">\(\sum_{j &gt; i}a_{ij}\)</span> for short. The edges which exist in our network will be indicated with green, in the following figure, of which there are <span class="math notranslate nohighlight">\(6\)</span> total. Remember that the red edges were the <em>possible</em> edges:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_poss</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">1200</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">1200</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_19_0.png" src="_images/properties-of-networks_19_0.png" />
</div>
</div>
<p>To put it lal together, the <strong>network density</strong> is a summary statistic which indicates the <em>density of edges</em> which are present in the network. For a simple network, the network density can be defined as the ratio between the total number of edges in <span class="math notranslate nohighlight">\(A\)</span> and the total number of edges possible in <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    density(A) &amp;= \frac{\sum_{j &gt; i}a_{ij}}{\frac{n(n - 1)}{2}} = \frac{2\sum_{j &gt; i}a_{ij}}{n(n - 1)}
\end{align*}\]</div>
<p>In our example, this is simply the ratio of green edges which <em>actually</em> exist to red edges which could <em>possibly</em> exist, which is <span class="math notranslate nohighlight">\(\frac{5}{10} = 0.5\)</span>.</p>
</div>
<div class="section" id="the-clustering-coefficient-indicates-how-much-nodes-tend-to-cluster-together">
<h5><span class="section-number">1.3.3.2. </span>The clustering coefficient indicates how much nodes tend to cluster together<a class="headerlink" href="#the-clustering-coefficient-indicates-how-much-nodes-tend-to-cluster-together" title="Permalink to this headline">¶</a></h5>
<p>The clustering coefficient inddicates the fraction of triplets of nodes which are closed. What the heck is that? Let’s look at only Brooklyn, Manhattan, Queens, and the Bronx:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G_clus</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G_clus</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">G2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_clus</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_21_0.png" src="_images/properties-of-networks_21_0.png" />
</div>
</div>
<p>To begin to define the clustering coefficient, we first must understand what a <em>triplet</em> is. A <strong>triplet</strong> is an ordered tuple of three nodes which are connected by two or three edges. For instance, in the above network, we have the following triplets of nodes:</p>
<ol class="simple">
<li><p>(BX, MH, BK), (BX, BK, MH), (MH, BX, BK), (MH, BK, BX), (BK, BX, MH), (BK, MH, BX): two edges,</p></li>
<li><p>(MH, BK, Q), (MH, Q, BK), (BK, MH, Q), (BK, Q, MH), (Q, MH, BK), (Q, BK, MH): two edges,</p></li>
<li><p>(BX, MH, Q), (BX, Q, MH), (MH, BX, Q), (MH, Q, BX), (Q, BX, MH), (Q, MH, BX): three edges,</p></li>
</ol>
<p>and one three-node sets which has no triplets between {BK, BX, Q}, which has no triplets because there is only a single edge between BX andd Q amongst the three nodes. A triplet is <em>closed</em> if there are three edges, and is open if there are only two edges. In our example, there are six closed triplets amongst the nodes {BX, MH, Q}, and there are twele open triplets across {BK, MH, Q} and {BK, MH, BX}. The global clustering coefficient is defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    C &amp;= \frac{\text{number of closed triplets}}{\text{number of closed triplets} + \text{number of open triplets}}
\end{align*}\]</div>
<p>In our example, this comes to <span class="math notranslate nohighlight">\(C = \frac{6}{6 + 12} = \frac{1}{3}\)</span>. This equation can also be understood in terms of the adjacency matrix. Note that if a triplet between nodes <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(j\)</span>, and <span class="math notranslate nohighlight">\(k\)</span> is closed, then all three of the adjacenies <span class="math notranslate nohighlight">\(a_{ij}\)</span>, <span class="math notranslate nohighlight">\(a_{jk}\)</span>, and <span class="math notranslate nohighlight">\(a_{ki}\)</span> have a value of <span class="math notranslate nohighlight">\(1\)</span>. Therefore, if we could the number of times that <span class="math notranslate nohighlight">\(a_{ij}a_{jk}a_{ki} = 1\)</span>, we also count the number of closed triplets! This means that the number of closed triplets can be expressed as <span class="math notranslate nohighlight">\(\sum_{i,j,k}a_{ij}a_{jk}a_{ki}\)</span>.</p>
<p>Further, note that for a given node <span class="math notranslate nohighlight">\(i\)</span>, that we can find an arbitrary triplet (either open or closed) through the following procedure.</p>
<ol class="simple">
<li><p>Pick a single neighbor <span class="math notranslate nohighlight">\(j\)</span> for node <span class="math notranslate nohighlight">\(i\)</span>. Note that the node <span class="math notranslate nohighlight">\(i\)</span> has a number of neighbors equal to <span class="math notranslate nohighlight">\(degree(v_i) = d_i\)</span>, so there are <span class="math notranslate nohighlight">\(d_i\)</span> possible neighbors to choose from.</p></li>
<li><p>Pick a different neighbor <span class="math notranslate nohighlight">\(k\)</span> for node <span class="math notranslate nohighlight">\(i\)</span>. Note that since node <span class="math notranslate nohighlight">\(i\)</span> had <span class="math notranslate nohighlight">\(d_i\)</span> neighbors, it has <span class="math notranslate nohighlight">\(d_i - 1\)</span> neighbors that are not node <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>Since we know that nodes <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(k\)</span> are both neighbors of node <span class="math notranslate nohighlight">\(i\)</span>, we know that <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ik}\)</span> both have values of one, and therefore the edges <span class="math notranslate nohighlight">\((i, j)\)</span> and <span class="math notranslate nohighlight">\((i, k)\)</span> exist. Therefore, the tuple of nodes <span class="math notranslate nohighlight">\((i, j, k)\)</span> is a triplet, because <em>at least</em> two edges exist amongst the three nodes. This tuple is closed if the edge <span class="math notranslate nohighlight">\((j, k)\)</span> exists, and open if the edge <span class="math notranslate nohighlight">\((j, k)\)</span> does not exist.</p></li>
<li><p>Therefore, there are <span class="math notranslate nohighlight">\(d_i (d_i - 1)\)</span> triplets in which node <span class="math notranslate nohighlight">\(i\)</span> is the leading node of the triplet.</p></li>
</ol>
<p>As it turns out, since triplets are <em>ordered tuples</em>, we can repeat this procedure for all nodes, and if we count how many triplets we get in total, we get the <em>total number of triplets</em> for the entire network. Therefore, the number of open and closed triplets in the network is the quantity <span class="math notranslate nohighlight">\(\sum_i d_i (d_i - 1)\)</span>.  Then we could express the clustering coefficient in terms of the adjacency matrix as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    C &amp;= \frac{\sum_{i,j,k}a_{ij}a_{jk}a_{ki}}{\sum_i d_i (d_i - 1)}, \;\;\; d_i = degree(v_i)
\end{align*}\]</div>
<p>which is a bit easier to implement programmatically.</p>
</div>
<div class="section" id="the-path-length-describes-how-far-two-nodes-are">
<h5><span class="section-number">1.3.3.3. </span>The path length describes how far two nodes are<a class="headerlink" href="#the-path-length-describes-how-far-two-nodes-are" title="Permalink to this headline">¶</a></h5>
<p>How many bridges would we need to cross to get from Staten Island to Bronx? This concept relates directly to the concept of the <em>path length</em> in a network. A <strong>path</strong> between two nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> is a sequence of edges which starts at node <span class="math notranslate nohighlight">\(i\)</span>, and traverses through other nodes in the network until reaching node <span class="math notranslate nohighlight">\(j\)</span>. Two nodes are described as <strong>connected</strong> if a path exists between them. The <strong>path length</strong> is the number of edges in the path. For instance, if we remember our network from the New York example, we could get from Staten Island to Bronx in two possible ways, indicated in green and blue in the following example:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;New York Example&quot;</span><span class="p">)</span>

<span class="n">G_short</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G_short</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G_short</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G_short</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G_short</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">G_short</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">G_short</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G_short</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G_short</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_short</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;length 3 path from SI to BX&quot;</span><span class="p">)</span>

<span class="n">G_long</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">G_long</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G_long</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_long</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;length 4 path from SI to BX&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_23_0.png" src="_images/properties-of-networks_23_0.png" />
</div>
</div>
<p>In this case, there are only two paths from SI to BX which do not visit the same node more than once, but in a larger network, there may be <em>many</em> possible paths from one node to another. For this reason, we will usually be interested in one particular path, the <em>shortest path</em>. The <strong>shortest path</strong> or <strong>distance</strong> between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> is the path with the smallest path length that connects nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. In our example, the shortest path is indicated by the green edges, and the shortest path length is therefore three.  If it is not possible to get from node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span> using edges of the network, the shortest path length is defined to be infinite. The shortest path between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> will often be abbreviated using the notation <span class="math notranslate nohighlight">\(l_{ij}\)</span>.</p>
<p>A common summary statistic is to view the <em>distance matrix</em> <span class="math notranslate nohighlight">\(L\)</span>, which is the <span class="math notranslate nohighlight">\(n \times n\)</span> matrix whose entries <span class="math notranslate nohighlight">\(l_{ij}\)</span> are the shortest path lengths between all pairs of nodes in the network. For our New York example, the distance matrix is:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">floyd_warshall_numpy</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">,</span><span class="mf">4.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Distance Matrix L, New York Example&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_25_0.png" src="_images/properties-of-networks_25_0.png" />
</div>
</div>
<p>A common network statistic we can compute using the distance matrix is the <em>average shortest path length</em>. The average shortest path length <span class="math notranslate nohighlight">\(l\)</span> of a simple network is simply the average of all of the shortest paths between two distinct nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of the distance matrix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    l &amp;= \frac{1}{n(n - 1)}\sum_{i \neq j} l_{ij}
\end{align*}\]</div>
</div>
<div class="section" id="network-summary-statistics-can-be-misleading-when-comparing-networks">
<h5><span class="section-number">1.3.3.4. </span>Network summary statistics can be misleading when comparing networks<a class="headerlink" href="#network-summary-statistics-can-be-misleading-when-comparing-networks" title="Permalink to this headline">¶</a></h5>
<p>When we perform network machine learning, we want the data we are analyzing to be <em>sensitive</em> in the sense that, if two networks are <em>different</em> (we use the term <em>different</em> a little loosely here, but we will be more specific in a second!) we want the data to reflect that. Let’s say we had the following four networks:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">G2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">G3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">G4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">pos1</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.47</span><span class="p">),</span>
        <span class="mi">2</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.18</span><span class="p">),</span>
        <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.18</span><span class="p">),</span>
        <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">),</span>
        <span class="mi">5</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">),</span>
        <span class="mi">6</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.31</span><span class="p">),</span>
        <span class="mi">7</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">),</span>
        <span class="mi">8</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">59</span><span class="p">),</span>
        <span class="mi">9</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">59</span><span class="p">),</span>
        <span class="mi">10</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.19</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="mi">11</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.81</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
       <span class="p">}</span>
<span class="n">pos2</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span>
        <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span>
        <span class="mi">2</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">),</span>
        <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">),</span>
        <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="mf">1.35</span><span class="p">),</span>
        <span class="mi">5</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span>
        <span class="mi">6</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span>
        <span class="mi">7</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">.</span><span class="mi">85</span><span class="p">),</span>
        <span class="mi">8</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.6</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">),</span>
        <span class="mi">9</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">),</span>
        <span class="mi">10</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span>
        <span class="mi">11</span><span class="p">:</span> <span class="p">(</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
       <span class="p">}</span>
<span class="n">pos3</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">),</span>
        <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">),</span>
        <span class="mi">2</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">),</span>
        <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">),</span>
        <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">),</span>
        <span class="mi">5</span><span class="p">:</span> <span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">),</span>
        <span class="mi">6</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">),</span>
        <span class="mi">7</span><span class="p">:</span> <span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">),</span>
        <span class="mi">8</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">),</span>
        <span class="mi">9</span><span class="p">:</span> <span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">),</span>
        <span class="mi">10</span><span class="p">:</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">),</span>
        <span class="mi">11</span><span class="p">:</span> <span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">)</span>
       <span class="p">}</span>
<span class="n">pos4</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">73</span><span class="p">),</span>
        <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">29</span><span class="p">,</span> <span class="mf">1.23</span><span class="p">),</span>
        <span class="mi">2</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">58</span><span class="p">,</span> <span class="o">.</span><span class="mi">73</span><span class="p">),</span>
        <span class="mi">3</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">29</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">),</span>
        <span class="mi">4</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">),</span>
        <span class="mi">5</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">),</span>
        <span class="mi">6</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">),</span>
        <span class="mi">7</span><span class="p">:</span> <span class="p">(</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">),</span>
        <span class="mi">8</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.13</span><span class="p">,</span> <span class="mf">1.07</span><span class="p">),</span>
        <span class="mi">9</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.47</span><span class="p">,</span> <span class="mf">1.07</span><span class="p">),</span>
        <span class="mi">10</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.47</span><span class="p">,</span> <span class="o">.</span><span class="mi">73</span><span class="p">),</span>
        <span class="mi">11</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.13</span><span class="p">,</span> <span class="o">.</span><span class="mi">73</span><span class="p">)</span>
       <span class="p">}</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="c1">#%%</span>
<span class="n">G1</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">G1</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
       <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)}</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G1</span><span class="p">,</span> <span class="n">pos1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edges</span><span class="p">(</span><span class="n">G1</span><span class="p">,</span> <span class="n">pos1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network 1&quot;</span><span class="p">)</span>

<span class="c1">#%%</span>
<span class="n">G2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
       <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">)}</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edges</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network 2&quot;</span><span class="p">)</span>

<span class="c1">#%%</span>
<span class="n">G3</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">G3</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
       <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">)}</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G3</span><span class="p">,</span> <span class="n">pos3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edges</span><span class="p">(</span><span class="n">G3</span><span class="p">,</span> <span class="n">pos3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network 3&quot;</span><span class="p">)</span>

<span class="c1">#%%</span>
<span class="n">G4</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">G4</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
       <span class="mi">1</span><span class="p">:</span> <span class="p">(</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">)}</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G4</span><span class="p">,</span> <span class="n">pos4</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_edges</span><span class="p">(</span><span class="n">G4</span><span class="p">,</span> <span class="n">pos4</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network 4&quot;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">node_color</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">);</span>

<span class="n">nx</span><span class="o">.</span><span class="n">average_clustering</span><span class="p">(</span><span class="n">G2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5222222222222223
</pre></div>
</div>
<img alt="_images/properties-of-networks_27_1.png" src="_images/properties-of-networks_27_1.png" />
</div>
</div>
<p>As it turns out, all of these networks share the same number of nodes, the same network density, and the same clustering coefficient:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Network</p></th>
<th class="head"><p>Network Density</p></th>
<th class="head"><p>Clustering Coefficient</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Network 1</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{3}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.6\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Network 2</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{3}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.6\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Network 3</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{3}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.6\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Network 4</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{3}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.6\)</span></p></td>
</tr>
</tbody>
</table>
<p>To conclude our discussion on network properties, we will turn to a final property of a network, known as a subnetwork, which will be useful as a pre-processing step for networks. The concept of a subnetwork will introduce the idea of a <em>connected network</em>, which is a network in which a path exists between all pairs of nodes in the network. Network machine learning methods may exhibit unexpected behavior when the network is not connected, so reducing the network to a connected component is often aa useful pre-processing step to prepare data.</p>
</div>
</div>
<div class="section" id="subnetworks-are-subsets-of-larger-networks">
<h4><span class="section-number">1.3.4. </span>Subnetworks are subsets of larger networks<a class="headerlink" href="#subnetworks-are-subsets-of-larger-networks" title="Permalink to this headline">¶</a></h4>
<p>When we think of an entire network, it is often useful to consider it in smaller bits. For instance, when we were looking at the clustering coefficient, we found it useful to break out the nodes {BK, Q, BX, MH} so we could count triplets:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">G2</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_clus</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">G2</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_29_0.png" src="_images/properties-of-networks_29_0.png" />
</div>
</div>
<p>This portion of the network is called a <em>subnetwork</em>. A <strong>subnetwork</strong> is a network topology whose nodes and edges are <em>subsets</em> of the nodes and edges for another network topology. In this case, the  network toplogy of the New York example is <span class="math notranslate nohighlight">\((\mathcal V, \mathcal E)\)</span> defined by the sets:</p>
<ol class="simple">
<li><p>The nodes <span class="math notranslate nohighlight">\(\mathcal V\)</span>: <span class="math notranslate nohighlight">\(\{SI, BK, Q, MH, BX\}\)</span>,</p></li>
<li><p>The edges <span class="math notranslate nohighlight">\(\mathcal E\)</span>: <span class="math notranslate nohighlight">\(\left\{(SI, BK), (BK, MH), (MH, Q), (MH, BX), (Q, BX)\right\}\)</span>.</p></li>
</ol>
<p>and the subnetwork is the network:</p>
<ol class="simple">
<li><p>The nodes <span class="math notranslate nohighlight">\(\mathcal V_s\)</span>: <span class="math notranslate nohighlight">\(\{BK, Q, MH, BX\}\)</span>,</p></li>
<li><p>The edges <span class="math notranslate nohighlight">\(\mathcal E_s\)</span>: <span class="math notranslate nohighlight">\(\left\{(BK, MH), (MH, Q), (MH, BX), (Q, BX)\right\}\)</span>.</p></li>
</ol>
<p>As we can see, the subnetwork topology <span class="math notranslate nohighlight">\((\mathcal V_s, \mathcal E_s)\)</span> is such that every element in <span class="math notranslate nohighlight">\(\mathcal V_s\)</span> is an element of <span class="math notranslate nohighlight">\(\mathcal V\)</span>, and therefore the nodes of the subnetwork are a subset of the nodes of the complete network. Further, every element in <span class="math notranslate nohighlight">\(\mathcal E_s\)</span> is an element of <span class="math notranslate nohighlight">\(\mathcal E\)</span>, and therefore the edges of the subnetwork are a subset of the edges of the complete network. So the subnetwork toplogy <span class="math notranslate nohighlight">\((\mathcal V_s, \mathcal E_s)\)</span> is a subnetwork of the network topology <span class="math notranslate nohighlight">\((\mathcal V, \mathcal E)\)</span>. This particular subnetwork can be described further as an <strong>induced</strong> subnetwork. A subnetwork of a network is <strong>induced</strong> by a set of vertices as follows:</p>
<ol class="simple">
<li><p>The nodes <span class="math notranslate nohighlight">\(\mathcal V_s\)</span> are a subset of the nodes of the network <span class="math notranslate nohighlight">\(\mathcal V\)</span>,</p></li>
<li><p>The edges <span class="math notranslate nohighlight">\(\mathcal E_s\)</span> consist of <em>all</em> of the edges from the original network which are incident pairs of node in <span class="math notranslate nohighlight">\(\mathcal V_s\)</span>.</p></li>
</ol>
<p>To see an example of a subnetwork which is <em>not</em> an induced subnetwork, we can consider a subnetwork which removes one of the edges that exist in the original network:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BK&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BX&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;SI&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BK&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;Q&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network Topology, New York Example&quot;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_clus</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Induced Subnetwork&quot;</span><span class="p">)</span>
<span class="n">G_clus</span><span class="o">.</span><span class="n">remove_edge</span><span class="p">(</span><span class="s2">&quot;MH&quot;</span><span class="p">,</span> <span class="s2">&quot;BX&quot;</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_clus</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G2</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Subnetwork which is not an induced subnetwork&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_31_0.png" src="_images/properties-of-networks_31_0.png" />
</div>
</div>
<p>A particular induced subnetwork that we will often be concerned with is known as the largest connected component (LCC).</p>
<div class="section" id="the-largest-connected-component-lcc-is-the-largest-subnetwork-of-connected-nodes">
<h5><span class="section-number">1.3.4.1. </span>The largest connected component (LCC) is the largest subnetwork of connected nodes<a class="headerlink" href="#the-largest-connected-component-lcc-is-the-largest-subnetwork-of-connected-nodes" title="Permalink to this headline">¶</a></h5>
<p>To define the largest connected component, we’ll modify our example slightly. Let’s say our network also includes the Boston area, and we have two new nodes, Boston (BO) and Cambridge (CA). Boston and Cambridge are incident several bridges between one another, so an edge exists between them. However, there are no bridges between boroughs of New York and the Boston area, so there are no edges from nodes in the Boston area to nodes in the New York area:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">G_bos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">G_bos</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;BO&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">G_bos</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;CA&quot;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">G_bos</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;BO&quot;</span><span class="p">,</span> <span class="s2">&quot;CA&quot;</span><span class="p">)</span>

<span class="n">pos_bos</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">get_node_attributes</span><span class="p">(</span><span class="n">G_bos</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_bos</span><span class="p">,</span> <span class="n">pos_bos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Bridge Network Topology, Boston + New York&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_33_0.png" src="_images/properties-of-networks_33_0.png" />
</div>
</div>
<p>The entire network topology can be described by the sets:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal V = \{SI, MH, BK, BX, Q, CA, BO\}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal E = \{(SI, BK), (MH, BK), (MH, Q), (MH, BX), (MX, Q), (CA, BO)\}\)</span>.</p></li>
</ol>
<p>Notice that we have two distinct sets of nodes, those of New York and those of Boston, which are <em>only</em> connected amongst one another. Formally, these two sets of nodes can be described as inducing <em>connected components</em> of the network topology <span class="math notranslate nohighlight">\((\mathcal V, \mathcal E)\)</span>. A <strong>connected component</strong> is an induced subnetwork in which any two nodes are connected to each other by a path through the network. The two connected components are the New York induced subnetwork:</p>
<ol class="simple">
<li><p>The nodes <span class="math notranslate nohighlight">\(\mathcal V_N\)</span>: <span class="math notranslate nohighlight">\(\{SI, BK, Q, MH, BX\}\)</span>,</p></li>
<li><p>The edges <span class="math notranslate nohighlight">\(\mathcal E_N\)</span>: <span class="math notranslate nohighlight">\(\left\{(SI, BK), (BK, MH), (MH, Q), (MH, BX), (Q, BX)\right\}\)</span>.</p></li>
</ol>
<p>and the Boston induced subnetwork:</p>
<ol class="simple">
<li><p>The nodes <span class="math notranslate nohighlight">\(\mathcal V_B\)</span>: <span class="math notranslate nohighlight">\(\{CA, BO\}\)</span>,</p></li>
<li><p>The edges <span class="math notranslate nohighlight">\(\mathcal E_B\)</span>: <span class="math notranslate nohighlight">\(\left\{(CA, BO)\right\}\)</span>.</p></li>
</ol>
<p>which we can represent visually here:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_bos</span><span class="p">,</span> <span class="n">pos_bos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;New York + Boston Network Topology&quot;</span><span class="p">)</span>

<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_bos</span><span class="p">,</span> <span class="n">pos_bos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;New York Induced Subnetwork Topology&quot;</span><span class="p">)</span>


<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_networkx</span><span class="p">(</span><span class="n">G_bos</span><span class="p">,</span> <span class="n">pos_bos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">with_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">node_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
        <span class="n">font_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">font_color</span><span class="o">=</span><span class="s2">&quot;whitesmoke&quot;</span><span class="p">,</span> <span class="n">arrows</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Boston Induced Subnetwork Topology&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/properties-of-networks_35_0.png" src="_images/properties-of-networks_35_0.png" />
</div>
</div>
<p>The <strong>largest connected component</strong> (LCC) of a network is the connected component with the most nodes. In our example, the New York connected component has five nodes, whereas the Bosston connected component has two nodes. Therefore, the New York connected component is the LCC.</p>
</div>
</div>
</div>
<span id="document-representations/ch4/regularization"></span><div class="section" id="regularization">
<h3><span class="section-number">1.4. </span>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">¶</a></h3>
<p>In practice, many networks we will encounter in network machine learning will <em>not</em> be simple networks. As we discussed in the preceding discussion, many of the techniques we discuss will be just fine to use with weighted networks. Unfortunately, real world networks are often extremely noisy, and so the analysis of one real world network might not generalize very well to a similar real world network. For this reason, we turn to <em>regularization</em>. <strong>Regularization</strong> is defined as, “the process of adding information in order to solve an ill-posed problem or to prevent overfitting.” In network machine learning, what this usually will entail is modifying the network (or networks) themselves to allow better generalization of our statistical inference to new datasets. For each section, we’ll pose an example, a simulation, and code for how to implement the desired regularization approach. It is important to realize that you might use several of these techniques simultaneously in practice, or you might have a reason to use these techniques that go outside of our working examples.</p>
<p>To start this section off, we’re going to introduce an example that’s going to be fundamental in many future sections we see in this book. We have a group of <span class="math notranslate nohighlight">\(50\)</span> local students who attend a school in our area. The first <span class="math notranslate nohighlight">\(25\)</span> of the students polled are athletes, and thhhe second <span class="math notranslate nohighlight">\(25\)</span> of the students polled are in marching band. We want to analyze how good of friends the students are, and to do so, we will use network machine learning. The nodes of the network will be the students. Next, we will describe how the two networks are collected:</p>
<ol class="simple">
<li><p>Activity/Hobby Network: To collect the first network, we ask each student to select from a list of <span class="math notranslate nohighlight">\(50\)</span> school activities and outside hobbies that they enjoy. For a pair of students <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, the weight of their interest alignment will be a score between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(50\)</span> indicating how many activities or hobbies that they have in common. We will refer to this network as the common interests network. This network is obviously undirected, since if student <span class="math notranslate nohighlight">\(i\)</span> shares <span class="math notranslate nohighlight">\(x\)</span> activities or hobbies with student <span class="math notranslate nohighlight">\(j\)</span>, then student <span class="math notranslate nohighlight">\(j\)</span> also shares <span class="math notranslate nohighlight">\(x\)</span> activities or hobbies with student <span class="math notranslate nohighlight">\(i\)</span>. This network is weighted, since the score is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(50\)</span>. Finally, this network is loopless, because it would not make sense to look at the activity/hobby alignment of a student with themself, since this number would be largely uninformative as every student would have perfect alignment of activities and hobbies with him or herself.</p></li>
<li><p>Friendship Network: To collect the second network, we ask each student to rate how good of friends they are with other students, on a scale from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>. A score of <span class="math notranslate nohighlight">\(0\)</span> means they are not friends with the student or do not know the student, and a score of <span class="math notranslate nohighlight">\(1\)</span> means the student is their best friend. We will refer to this network as the friendship network. This nework is clearly directed, since two students may differ on their understanding of how good of friends they are. This network is weighted, since the score is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Finally, this network is also loopless, because it would not make sense to ask somebody how good of friends they are with themself.</p></li>
</ol>
<p>Our scientific question of interest is how well activities and hobbies align with perceived notions of friendship. We want to use the preceding networks to learn about a hypothetical third network, a network whose nodes are identical to the two networks above, but whose edges are whether the two individuals are friends (or not) on facebook. To answer this question, we have quite the job to do to make our networks better suited to the task! We begin by simulating some example data, shown below as adjacency matrix heatmaps:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">wtargsa</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">p</span><span class="o">=.</span><span class="mi">09</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">p</span><span class="o">=.</span><span class="mi">02</span><span class="p">)],</span>
          <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">p</span><span class="o">=.</span><span class="mi">02</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">p</span><span class="o">=.</span><span class="mi">06</span><span class="p">)]]</span>

<span class="n">wtargsf</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">5</span><span class="p">)],</span>
          <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="p">)]]</span>

<span class="c1"># human brain network</span>
<span class="n">A_activity</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">wt</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">,</span> <span class="n">wtargs</span><span class="o">=</span><span class="n">wtargsa</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># alien brain network</span>
<span class="n">A_friend</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="p">[[</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">wt</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">wtargs</span><span class="o">=</span><span class="n">wtargsf</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_activity</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Activities/Hobbies Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/regularization_2_0.png" src="_images/regularization_2_0.png" />
</div>
</div>
<div class="section" id="regularization-of-the-nodes">
<h4><span class="section-number">1.4.1. </span>Regularization of the Nodes<a class="headerlink" href="#regularization-of-the-nodes" title="Permalink to this headline">¶</a></h4>
<div class="section" id="the-largest-connected-component-is-the-largest-subnetwork-of-connected-nodes">
<h5><span class="section-number">1.4.1.1. </span>The Largest Connected Component is the largest subnetwork of connected nodes<a class="headerlink" href="#the-largest-connected-component-is-the-largest-subnetwork-of-connected-nodes" title="Permalink to this headline">¶</a></h5>
<p>We have already learned about the LCC in the preceding section, so we won’t cover the in-depth, but it is important to realize that this is a node regularization technique.</p>
</div>
<div class="section" id="degree-trimming-removes-nodes-with-low-degree">
<h5><span class="section-number">1.4.1.2. </span>Degree trimming removes nodes with low degree<a class="headerlink" href="#degree-trimming-removes-nodes-with-low-degree" title="Permalink to this headline">¶</a></h5>
<p>Let’s imagine that in our friendship network, there were an additional three athlete students from a nearby school. Perhaps one of these students had a friend in the first school he met at a sports camp, so these students are not a separate component of the network entirely. Even though these students are not <em>totally</em> disconnected from the rest of the network entirely, and therefore would not be removed by computing the LCC, their presence in our analysis might still lead to stability issues in future network machine learning tasks. For this reason, it may be advantageous to remove nodes whose degrees are much different from the other nodes in the network.</p>
</div>
</div>
<div class="section" id="regularizing-the-edges">
<h4><span class="section-number">1.4.2. </span>Regularizing the Edges<a class="headerlink" href="#regularizing-the-edges" title="Permalink to this headline">¶</a></h4>
<div class="section" id="symmetrizing-the-network-gives-us-undirectedness">
<h5><span class="section-number">1.4.2.1. </span>Symmetrizing the network gives us undirectedness<a class="headerlink" href="#symmetrizing-the-network-gives-us-undirectedness" title="Permalink to this headline">¶</a></h5>
<p>If we wanted to learn from the friendship network about whether two people were friends on facebook, a reasonable first place to start might be to <em>symmetrize</em> the friendship network. The facebook network is <em>undirected</em>, which means that if a student <span class="math notranslate nohighlight">\(i\)</span> is friends on facebook with student <span class="math notranslate nohighlight">\(j\)</span>, then student <span class="math notranslate nohighlight">\(j\)</span> is also friends with student <span class="math notranslate nohighlight">\(i\)</span>. On the other hand, as we learned above, the friendship network was directed. Since our question of interest is about an undirected network but the network we have is directed, it might be useful if we could take the directed friendship network and learn an undirected network from it. This relates directly to the concept of <em>interpretability</em>, in that we need to represent our friendship network in a form that will produce an answer or us about our facebook network which we can understand.</p>
<p>Another reason we might seek to symmetrize the friendship network is that we might think that asymmetries that exist in the network are just <em>noise</em>. We might assume that the adjacency entries <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\)</span> relate to one another, so together they might be able to produce a single summary number that better summarizes their relationship all together.</p>
<p>Remember that in a symmetric network, <span class="math notranslate nohighlight">\(a_{ij} = a_{ji}\)</span>, so in an <em>asymmetric</em> network, <span class="math notranslate nohighlight">\(a_{ij} \neq a_{ji}\)</span>. To symmetrize the friendship network, what we want is a <em>new</em> adjacency value, which we will call <span class="math notranslate nohighlight">\(w_{ij}\)</span>, which will be a function of <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\)</span>. Then, we will construct a new adjacency matrix <span class="math notranslate nohighlight">\(A'\)</span>, where each entry <span class="math notranslate nohighlight">\(a_{ij}'\)</span> <em>and</em> <span class="math notranslate nohighlight">\(a_{ji}'\)</span> are set equal to <span class="math notranslate nohighlight">\(w_{ij}\)</span>.  The little apostrophe just signifies that this is a potentially different value than either <span class="math notranslate nohighlight">\(a_{ij}\)</span> or <span class="math notranslate nohighlight">\(a_{ji}\)</span>. Note that by construction, <span class="math notranslate nohighlight">\(A'\)</span> is in fact symmetric, because <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span> due to how we built <span class="math notranslate nohighlight">\(A'\)</span>.</p>
<div class="section" id="ignoring-a-triangle-of-the-adjacency-matrix">
<h6><span class="section-number">1.4.2.1.1. </span>Ignoring a “triangle” of the adjacency matrix<a class="headerlink" href="#ignoring-a-triangle-of-the-adjacency-matrix" title="Permalink to this headline">¶</a></h6>
<p>The easiest way to symmetrize a network <span class="math notranslate nohighlight">\(A\)</span> is to just ignore part of it entirely. In the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, you will remember that we have an upper and a lower triangular part of the matrix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A &amp;= \begin{bmatrix}
        a_{11} &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{blue}{a_{21}} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{blue}{\vdots} &amp;\color{blue}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{blue}{a_{n1}} &amp; \color{blue}{...} &amp; \color{blue}{a_{n,n-1}} &amp; a_{nn}
    \end{bmatrix},
\end{align*}\]</div>
<p>The entries which are listed in <font color="red">red</font> are called the <strong>upper right triangle of the adjacency matrix above the diagonal</strong>. You will notice that for the entries in the upper right triangle of the adjacency matrix, <span class="math notranslate nohighlight">\(a_{ij}\)</span> is such that <span class="math notranslate nohighlight">\(j\)</span> is <em>always</em> greater than <span class="math notranslate nohighlight">\(i\)</span>. Similarly, the entries which are listed in <font color="blue">blue</font> are called the <strong>lower left triangle of the adjacency matrix below the diagonal</strong>. In the lower left triangle, <span class="math notranslate nohighlight">\(i\)</span> is <em>always</em> greater than <span class="math notranslate nohighlight">\(j\)</span>. These are called <em>triangles</em> because of the shape they make when you look at them in matrix form: notice, for instance, that in the upper right triangle, we have a triangle with three corners of values: <span class="math notranslate nohighlight">\(a_{12}\)</span>, <span class="math notranslate nohighlight">\(a_{1n}\)</span>, and <span class="math notranslate nohighlight">\(a_{n-1, n}\)</span>.</p>
<p>So, how do we ignore a triangle all-together? Well, it’s really quite simple! We will visually show how to ignore the lower left triangle of the adjacency matrix. We start by forming a triangle matrix, <span class="math notranslate nohighlight">\(\Delta\)</span>, as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Delta &amp;= \begin{bmatrix}
        0 &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{blue}{0} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{blue}{\vdots} &amp;\color{blue}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{blue}{0} &amp; \color{blue}{...} &amp; \color{blue}{0} &amp; 0
    \end{bmatrix},
\end{align*}\]</div>
<p>Notice that this matrix <em>keeps</em> all of the upper right triangle of the adjacency matrix above the diagonal the same as in the matrix <span class="math notranslate nohighlight">\(A\)</span>, but replaces the lower left triangle of the adjacency matrix below the diagonal and the diagonal with <span class="math notranslate nohighlight">\(0\)</span>s. Notice that the transpose of <span class="math notranslate nohighlight">\(\Delta\)</span> is the matrix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Delta^\top &amp;= \begin{bmatrix}
        0 &amp; \color{blue}{0} &amp; \color{blue}{...} &amp;\color{blue}{0}\\
        \color{red}{a_{12}}&amp; \ddots &amp; \color{blue}{\ddots} &amp; \color{blue}{\vdots} \\
        \color{red}{\vdots}&amp;\color{red}{\ddots} &amp; \ddots &amp; \color{blue}{0} \\
        \color{red}{a_{1n}}&amp;\color{red}{...} &amp;\color{red}{a_{n-1,n}} &amp; 0
    \end{bmatrix}
\end{align*}\]</div>
<p>So when we add the two together, we get this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Delta + \Delta^\top &amp;= \begin{bmatrix}
        0 &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{red}{a_{12}} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{red}{\vdots}&amp;\color{red}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{red}{a_{1n}}&amp;\color{red}{...} &amp;\color{red}{a_{n-1,n}} &amp; 0
    \end{bmatrix},
\end{align*}\]</div>
<p>We’re almost there! We just need to add back the diagonal of <span class="math notranslate nohighlight">\(A\)</span>, which we will do using the matrix <span class="math notranslate nohighlight">\(diag(A)\)</span> which has values <span class="math notranslate nohighlight">\(diag(A)_{ii} = a_{ii}\)</span>, and <span class="math notranslate nohighlight">\(diag(A)_{ij} = 0\)</span> for any <span class="math notranslate nohighlight">\(i \neq j\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A' &amp;= \Delta + \Delta^\top + diag(A) = \begin{bmatrix}
        a_{11} &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{red}{a_{12}} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{red}{\vdots}&amp;\color{red}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{red}{a_{1n}}&amp;\color{red}{...} &amp;\color{red}{a_{n-1,n}} &amp; a_{nn}
    \end{bmatrix},
\end{align*}\]</div>
<p>Which leaves <span class="math notranslate nohighlight">\(A'\)</span> to be a matrix consisting <em>only</em> of entries which were in the upper right triangle of <span class="math notranslate nohighlight">\(A\)</span>. <span class="math notranslate nohighlight">\(A'\)</span> is obviously symmetric, because <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Since the adjacency matrix is symmetric, the network <span class="math notranslate nohighlight">\(A'\)</span> represents is undirected.</p>
<p>So what does this mean in terms of the network itself? What this means is that the network originally had edge weights <span class="math notranslate nohighlight">\(a_{ij}\)</span>, where <span class="math notranslate nohighlight">\(a_{ij}\)</span> might not be equal to <span class="math notranslate nohighlight">\(a_{ji}\)</span>. This means student <span class="math notranslate nohighlight">\(i\)</span> might perceive their friendship with student <span class="math notranslate nohighlight">\(j\)</span> as being stronger or weaker than student <span class="math notranslate nohighlight">\(j\)</span> perceived about student <span class="math notranslate nohighlight">\(i\)</span>. What we did here was we basically just ignored any perceived friendships <span class="math notranslate nohighlight">\(a_{ji}\)</span> when <span class="math notranslate nohighlight">\(j\)</span> exceeded <span class="math notranslate nohighlight">\(i\)</span> (the lower left triangle), and simply “replaced” that perceived friendship with the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the upper right triangle of the adjacency matrix. This produced for us a single friendship strength <span class="math notranslate nohighlight">\(a_{ij}'\)</span> where <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span>.</p>
<p>In graspologic, we can implement this as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">symmetrize</span>

<span class="c1"># symmetrize with upper right triangle</span>
<span class="n">A_friend_upright_sym</span> <span class="o">=</span> <span class="n">symmetrize</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;triu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_upright_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Friendship Network, Upper-Right Symmetrized&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/regularization_8_0.png" src="_images/regularization_8_0.png" />
</div>
</div>
<p>Likewise, we can lower-left symmetrize as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># symmetrize with lower left triangle</span>
<span class="n">A_friend_lowleft_sym</span> <span class="o">=</span> <span class="n">symmetrize</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;tril&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_lowleft_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Friendship Network, Lower-Left Symmetrized&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/regularization_11_0.png" src="_images/regularization_11_0.png" />
</div>
</div>
</div>
<div class="section" id="taking-a-function-of-the-two-values">
<h6><span class="section-number">1.4.2.1.2. </span>Taking a function of the two values<a class="headerlink" href="#taking-a-function-of-the-two-values" title="Permalink to this headline">¶</a></h6>
<p>There are many other ways we can also take a function of <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\)</span> to get a symmetric matrix. One is to just average the two. That is, we can let the matrix <span class="math notranslate nohighlight">\(A'\)</span> be the matrix with entries <span class="math notranslate nohighlight">\(a'_{ij} = \frac{a_{ij} + a_{ji}}{2}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. In matrix form, this operation looks like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A' &amp;= \frac{1}{2} (A + A^\top) \\
    &amp;= \frac{1}{2}\left(\begin{bmatrix}
        a_{11} &amp; ... &amp; a_{1n} \\
        \vdots &amp; \ddots &amp; \vdots \\
        a_{n1} &amp; ... &amp; a_{nn}
    \end{bmatrix} + \begin{bmatrix}
        a_{11} &amp; ... &amp; a_{n1} \\
        \vdots &amp; \ddots &amp; \vdots \\
        a_{1n} &amp; ... &amp; a_{nn}
    \end{bmatrix}\right)\\
    &amp;= \begin{bmatrix}
        \frac{1}{2}(a_{11} + a_{11}) &amp; ... &amp; \frac{1}{2}(a_{1n} + a_{n1}) \\
        \vdots &amp; \ddots &amp; \vdots \\
        \frac{1}{2} (a_{n1} + a_{1n}) &amp; ... &amp; \frac{1}{2}(a_{nn} + a_{nn})
    \end{bmatrix} \\
    &amp;= \begin{bmatrix}
        a_{11} &amp; ... &amp; \frac{1}{2}(a_{1n} + a_{n1}) \\
        \vdots &amp; \ddots &amp; \vdots \\
        \frac{1}{2} (a_{n1} + a_{1n}) &amp; ... &amp; a_{nn}
    \end{bmatrix}
\end{align*}\]</div>
<p>As we can see, for all of the entries, <span class="math notranslate nohighlight">\(a'_{ij} = \frac{1}{2} (a_{ij} + a_{ji})\)</span>, and also <span class="math notranslate nohighlight">\(a_{ji}' = \frac{1}{2}(a_{ji} + a_{ij})\)</span>. These quantities are the same, so <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span>, and <span class="math notranslate nohighlight">\(A'\)</span> is symmetric. As the adjacency matrix is symmetric, the network that <span class="math notranslate nohighlight">\(A'\)</span> represents is undirected.</p>
<p>Remember that the asymmetry in the friendship network means student <span class="math notranslate nohighlight">\(i\)</span> might perceive their friendship with student <span class="math notranslate nohighlight">\(j\)</span> as being stronger or weaker than student <span class="math notranslate nohighlight">\(j\)</span> perceived about student <span class="math notranslate nohighlight">\(i\)</span>. What we did here was instead of just arbitrarily throwing one of those values away, we said that their friendship might be better indicated by averaging the two values. This produced for us a single friendship strength <span class="math notranslate nohighlight">\(a_{ij}'\)</span> where <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span>.</p>
<p>We can implement this in graspologic as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># symmetrize with averaging</span>
<span class="n">A_friend_avg_sym</span> <span class="o">=</span> <span class="n">symmetrize</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;avg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Friendship Network, Symmetrized by Averaging&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/regularization_14_0.png" src="_images/regularization_14_0.png" />
</div>
</div>
<p>We will use the friendship network symmetrized by averaging in several of the below examples, which we will call the “undirected friendship network”.</p>
</div>
</div>
<div class="section" id="diagonal-augmentation">
<h5><span class="section-number">1.4.2.2. </span>Diagonal augmentation<a class="headerlink" href="#diagonal-augmentation" title="Permalink to this headline">¶</a></h5>
<p>In our future works with network machine learning, we will come across numerous techniques which operate on adjacency matrices which are <em>positive semi-definite</em>. This word doesn’t mean a whole lot to us for network machine learning, but it has a big implication when we try to use algorithms on many of our networks. Remember that when we have a loopless network, a common practice in network science is to set the diagonal to zero. What this does is it leads to our adjacency matrices being <em>indefinite</em> (which means, <em>not</em> positive semi-definite). For us, this means that many network machine learning techniques simply cannot operate on these adjacency matrices. However, as we mentioned before, these entries are not actually zero, but simply <em>do not exist</em> and we just didn’t have a better way to represent them. Or do we?</p>
<p><em>Diagonal augmentation</em> is a procedure for imputing the diagonals of adjacency matrices for loopless networks. This gives us “placeholder” values that do not cause this issue of indefiniteness, and allow our network machine learning techniques to still work. Remember that for a simple network, the adjacency matrix will look like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A &amp;= \begin{bmatrix}
        0 &amp; a_{12} &amp; ... &amp; a_{1n} \\
        a_{21}&amp; \ddots &amp; &amp; \vdots \\
        \vdots &amp; &amp; \ddots &amp; a_{n-1, n} \\
        a_{n1} &amp;...&amp; a_{n, n-1} &amp; 0
    \end{bmatrix}
\end{align*}\]</div>
<p>What we do is impute the diagonal entries using the <em>fraction of possible edges which exist</em> for each node. This quantity is simply the node degree <span class="math notranslate nohighlight">\(d_i\)</span> (the number of edges which exist for node <span class="math notranslate nohighlight">\(i\)</span>) divided by the number of possible edges node <span class="math notranslate nohighlight">\(i\)</span> could have (which would be node <span class="math notranslate nohighlight">\(i\)</span> connected to each of the other <span class="math notranslate nohighlight">\(n-1\)</span> nodes). Remembering that the degree matrix <span class="math notranslate nohighlight">\(D\)</span> is the matrix whose diagonal entries are the degrees of each node, the diagonal-augmented adjacency matrix is given by:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A' &amp;= A + \frac{1}{n-1}D = \begin{bmatrix}
        \frac{d_1}{n-1} &amp; a_{12} &amp; ... &amp; a_{1n} \\
        a_{21}&amp; \ddots &amp; &amp; \vdots \\
        \vdots &amp; &amp; \ddots &amp; a_{n-1, n} \\
        a_{n1} &amp;...&amp; a_{n, n-1} &amp; \frac{d_n}{n-1}
    \end{bmatrix}
\end{align*}\]</div>
<p>When the matrices are directed or weighted, the computation is a little different, but fortunately <code class="docutils literal notranslate"><span class="pre">graspologic</span></code> will handle this for us. Let’s see how we would apply this to the directed friendship network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">augment_diagonal</span>

<span class="n">A_friend_aug</span> <span class="o">=</span> <span class="n">augment_diagonal</span><span class="p">(</span><span class="n">A_friend</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network, A&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_aug</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;With Diagonal Augmentation, A&#39;&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_aug</span><span class="o">-</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;A&#39; - A&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/regularization_18_0.png" src="_images/regularization_18_0.png" />
</div>
</div>
<p>As we can see, the diagonal-augmented friendship network and the original directed friendship network differ only in that the diagonals of the diagonal-augmented friendship network are non-zero.</p>
</div>
<div class="section" id="lowering-edge-bias">
<h5><span class="section-number">1.4.2.3. </span>Lowering edge bias<a class="headerlink" href="#lowering-edge-bias" title="Permalink to this headline">¶</a></h5>
<p>As you are probably aware, in all of machine learning, we are always concerned with the <em>bias/variance tradeoff</em>. The <a class="reference external" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%20is%20the%20simplifying%20assumptions,change%20given%20different%20training%20data."><strong>bias/variance tradeoff</strong></a> is an unfortunate side-effect that concerns how well a learning technique will generalize to new datasets.</p>
<ol class="simple">
<li><p><strong>Bias</strong> is a simplifying assumption of a model that makes the task easier to estimate. For instance, if we have a friendship network, we might make simplifying assumptions, such as an assumption that two athletes frorm different sports have an equally likely chance of being friends with a member of the band.</p></li>
<li><p>On the other hand, the <strong>variance</strong> is the degree to which the an estimate of a task will change when given new data. An assumption that if a player is a football player he has a higher chance of being friends with a band member might make sense given that the band performs at football games.</p></li>
</ol>
<p>The “trade-off” is that these two factors tend to be somewhat at odds, in that raising the bias tends to lower the variance, and vice-versa:</p>
<ol class="simple">
<li><p><strong>High bias, but low variance</strong>: Whereas a lower variance model might be better suited to the situation when the data we expect to see is noisy, it might not as faithfully represent the underlying dynamics we think the network possesses. A low variance model might ignore that athletes might have a different chance of being friends with a band member based on their sport all together. This means that while we won’t get the student relationships <em>correct</em>, we might still be able to get a reasonable estimate that we think is not due to overfitting.</p></li>
<li><p><strong>Low bias, but high variance</strong>: Whereas a low bias model might more faithfully model true relationships in our training data, it might fit our training data a little <em>too</em> well. Fitting the training data too well is a problem known as <strong>overfitting</strong>. If we only had three football team members and tried to assume that football players were better friends with band members, we might not be able to well approximate this relationship because of how few individuals we have who reflect this situation.</p></li>
</ol>
<p>Here, we show several strategies to reduce the bias due to edge weight noise in network machine learning.</p>
<div class="section" id="thresholding-converts-weighted-networks-to-binary-networks">
<h6><span class="section-number">1.4.2.3.1. </span>Thresholding converts weighted networks to binary networks<a class="headerlink" href="#thresholding-converts-weighted-networks-to-binary-networks" title="Permalink to this headline">¶</a></h6>
<p>The simplest way to reduce edge bias is the process of <em>thresholding</em>. Through thresholding, we choose a threshold value, <span class="math notranslate nohighlight">\(t\)</span>. Next, we simply set all of the entries of the adjacency matrix less than or equal to <span class="math notranslate nohighlight">\(t\)</span> to zero, and the entries of the adjacency matrix above <span class="math notranslate nohighlight">\(t\)</span> to one.</p>
<p>Some of the most common approaches to choosing this threshold are:</p>
<ol class="simple">
<li><p>Set the threshhold to zero: set all non-zero weighted entries to one, and all zero-weight entries to zero. This is most commonly used when we see zero-inflated networks, or networks where the adjacency matrix takes values that are either zero or some quantity different from one,</p></li>
<li><p>Set te threshold to be the mean: set all values below the mean edge-weight to zero, and all values above the mean edge-weight to one,</p></li>
<li><p>Use a quantile: A quantile is a percentile divided by <span class="math notranslate nohighlight">\(100\)</span>. In this strategy, we identify a target quantile of the edge-weight distribution. What this means is that we are selecting the lowest <em>fraction</em> of the edge-weights (where that fraction is the quantile that we choose) and setting these edges to <span class="math notranslate nohighlight">\(0\)</span>, and selecting the remaining edges to <span class="math notranslate nohighlight">\(1\)</span>. If we select a quantile of <span class="math notranslate nohighlight">\(0.5\)</span>, this means that we take the smallest <span class="math notranslate nohighlight">\(50\%\)</span> of edges and set them to zero, and the largest <span class="math notranslate nohighlight">\(50\%\)</span> of edges and set them to <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
</ol>
<p>We will show how to use the percentile approach to binarization, with both our activity/hobby and friendship networks. We will threshold using the edge-weight in the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile. Our example networks of activity/hobby and friendship were loopless, as you could see above. Remember as we learned in the preceding section, that if the network itself is loopless, the diagonal entries simply <em>do not exist</em>; <span class="math notranslate nohighlight">\(0\)</span> is simply a commonly used placeholder. For this reason, when we compute percentiles of edge-weights, we need to <em>exclude the diagonal</em>. Further, since this network is undirected, we also need to restrict our attention to one triangle of the corresponding adjacency matrix. We choose the upper-right triangle arbitrarily, as the adjacency matrix’s symmetry means the upper-right triangle and lower-right triangle have identical edge-weight distributions. We begin by using this procedure on the friendship network. To complete this processs, we first look at the edge-weight distribution for the friendship network, which is shown below, and identified the edge-weight at the <span class="math notranslate nohighlight">\(0.5\)</span> quartile:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># find the indices which are in the upper triangle and not in the diagonal</span>
<span class="n">upper_tri_non_diag_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">))</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">[</span><span class="n">upper_tri_non_diag_idx</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">seaborn</span> <span class="kn">import</span> <span class="n">histplot</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">[</span><span class="n">upper_tri_non_diag_idx</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Undirected Friendship Network Off-Diagonal Edge-Weight Distribution&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="o">.</span><span class="mi">03</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="s2">&quot;0.5 quantile, t = </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/regularization_23_0.png" src="_images/regularization_23_0.png" />
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(0.5\)</span> quantile, it turns out, is about <span class="math notranslate nohighlight">\(0.3\)</span>. This is because about <span class="math notranslate nohighlight">\(50\%\)</span> of the edges are less than this threshold, and about <span class="math notranslate nohighlight">\(50\%\)</span> of the edges are greater than this threshold. There is exactly one more edge in less than or equal to <span class="math notranslate nohighlight">\(t\)</span>, because this edge is exactly the median (an alternative name for the <span class="math notranslate nohighlight">\(0.5\)</span> quartile) value:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of edges less than or equal to t: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">[</span><span class="n">upper_tri_non_diag_idx</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of edges greater than or equal to t: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">[</span><span class="n">upper_tri_non_diag_idx</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of edges less than or equal to t: 613
Number of edges greater than or equal to t: 612
</pre></div>
</div>
</div>
</div>
<p>Next, we will assign the edges less than or equal to <span class="math notranslate nohighlight">\(t\)</span> to zero, and the edges greater than or equal to <span class="math notranslate nohighlight">\(t\)</span> to one:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A_friend_thresh</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">)</span>  <span class="c1"># copy the network over</span>

<span class="c1"># threshold using t</span>
<span class="n">A_friend_thresh</span><span class="p">[</span><span class="n">A_friend_thresh</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">A_friend_thresh</span><span class="p">[</span><span class="n">A_friend_thresh</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">14</span><span class="o">-</span><span class="n">fb4cba753867</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">A_friend_thresh</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">)</span>  <span class="c1"># copy the network over</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> 
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># threshold using t</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">A_friend_thresh</span><span class="p">[</span><span class="n">A_friend_thresh</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">A_friend_thresh</span><span class="p">[</span><span class="n">A_friend_thresh</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="ne">NameError</span>: name &#39;copy&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Undirected Friendship Network, Weighted&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_thresh</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Undirected Friendship Network, Thresholded at 50-ptile&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<p>Since the friendship network is now undirected (we made the adjacency matrix symmetric through averaging), loopless (because we defined it that way), and binary (because we thresholded the edges), we have now turned it into a <em>simple</em> network! Great job. Next, we will discuss an important property as to <em>why</em> thresholding using a quantile tends to be a very common tactic to obtaining simple networks from networks which are undirected and loopless. Remember that in the last section, we defined the network density for a simple network as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    density(A) &amp;= \frac{2\sum_{j &gt; i}a_{ij}}{n(n - 1)}.
\end{align*}\]</div>
<p>Since we have thresholded at the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile for the symmetric friendship network, this means that about <span class="math notranslate nohighlight">\(50\)</span> percent of the possible edges will exist (the <em>largest</em> <span class="math notranslate nohighlight">\(50\)</span> percent of edges), and <span class="math notranslate nohighlight">\(50\)</span> percent of the possible edges will not exist (the <em>smallest</em> <span class="math notranslate nohighlight">\(50\)</span> percent of edges). Remembering that the number of possible edges was <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span> for an undirected network, this means that <span class="math notranslate nohighlight">\(\sum_{j &gt; i}a_{ij}\)</span> must be half of <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span>, or <span class="math notranslate nohighlight">\(\frac{1}{4}n(n - 1)\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    density(A) &amp;= \frac{2\sum_{j &gt; i}a_{ij}}{n(n - 1)}, \\
    &amp;= \frac{2\cdot \frac{1}{4}n(n - 1)}{n(n - 1)},\;\;\;\sum_{j &gt; i}a_{ij} = \frac{1}{4}n(n - 1) \\
    &amp;= 0.5.
\end{align*}\]</div>
<p>So when we threshold the network at a quantile <span class="math notranslate nohighlight">\(t\)</span>, we end up with a network of density also equal to <span class="math notranslate nohighlight">\(t\)</span>! Let’s confirm that this is the case for our symmetric friendship network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">is_unweighted</span><span class="p">,</span> <span class="n">is_loopless</span><span class="p">,</span> <span class="n">is_symmetric</span>

<span class="k">def</span> <span class="nf">simple_network_dens</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># make sure the network is simple</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">is_unweighted</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">is_loopless</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">is_symmetric</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Network is not simple!&quot;</span><span class="p">)</span>
    <span class="c1"># count the non-zero entries in the upper-right triangle</span>
    <span class="c1"># for a simple network X</span>
    <span class="n">nnz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="c1"># number of nodes</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># number of possible edges is 1/2 * n * (n-1)</span>
    <span class="n">poss_edges</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nnz</span><span class="o">/</span><span class="n">poss_edges</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Network Density: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">simple_network_dens</span><span class="p">(</span><span class="n">A_friend_thresh</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>This is desirable for network machine learning because many network properties (such as the summary statistics we have discussed so far, and numerous other properties we will discuss in later chapters) can vary when the network density changes. This means that a network of a different density might have a higher clustering coefficient than a network of a lower density simply due to the fact that its density is higher (and therefore, there are more opportunities for closed triangles because each node has more connections). This means that when we threshold groups of networks and compare them, thresholding using a quantile will be very valuable.</p>
<p>Note that a common pitfall you might run into with thresholding (and the broader class of techniques known as <em>sparsification</em> approaches) that rely on quantiles occurs when a weighted network can only take non-negative edge-weights. This corresponds to a network with an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> where every <span class="math notranslate nohighlight">\(a_{ij}\)</span> is greater than or equal to <span class="math notranslate nohighlight">\(0\)</span>. In this case, one must be careful to choose a threshold which is not zero. Let’s consider a network were <span class="math notranslate nohighlight">\(60\%\)</span> of the entries are zeros, and <span class="math notranslate nohighlight">\(40\%\)</span> of the entries take a random value between <span class="math notranslate nohighlight">\(5\)</span> and <span class="math notranslate nohighlight">\(10\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_nm</span>

<span class="c1"># 10 nodes</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># total number of edges is 40% of the number of possible edges</span>
<span class="c1"># 1/2 * n * (n-1)</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.4</span><span class="o">*</span><span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">wt</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">,</span> <span class="n">wtargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>If we threshold at the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile, since <span class="math notranslate nohighlight">\(60\)</span> percent of the edges do not exist, then the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile is still just zero:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use the quantile function to obtain the threshold</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">quantile</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">))],</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># quantile = percentile / 100</span>
<span class="n">A_thresh</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>  <span class="c1"># copy the network over</span>

<span class="c1"># threshold using t</span>
<span class="n">A_thresh</span><span class="p">[</span><span class="n">A_thresh</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">A_thresh</span><span class="p">[</span><span class="n">A_thresh</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Threshold for 50th percentile: </span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>And we don’t actually end up with a network having a density of <span class="math notranslate nohighlight">\(0.5\)</span>, but rather, the same as the fraction of non-zero edges in the original network (which was <span class="math notranslate nohighlight">\(40\%\)</span>, or <span class="math notranslate nohighlight">\(0.4\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dens</span> <span class="o">=</span> <span class="n">simple_network_dens</span><span class="p">(</span><span class="n">A_thresh</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_thresh</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;50 ptile thresholded network; dens = </span><span class="si">{:.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dens</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
<p>So the take-home message is that we need to be careful that if we want to conclude that two percentile-thresholded networks have the same network density (equal to the percentile we thresholded at), that we have enough non-zero entries to threshold with across both (or all) of the networks.</p>
</div>
<div class="section" id="sparsification-removes-potentially-spurious-low-weight-edges">
<h6><span class="section-number">1.4.2.3.2. </span>Sparsification removes potentially spurious low-weight edges<a class="headerlink" href="#sparsification-removes-potentially-spurious-low-weight-edges" title="Permalink to this headline">¶</a></h6>
<p>The next simplest edge-weight regularization technique is called <em>sparsificiation</em>. Remember that our undirected friendship network looked like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Undirected Friendship Network&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that for a <em>lot</em> of the off-diagonal entries, many of the values are really tiny compared to the maximum value in the network which is almost <span class="math notranslate nohighlight">\(1\)</span>. What if the way we measured these edges was very sensitive to high values, but had trouble discerning whether a value was actually zero, or was just really small?</p>
<p>For this particular situation, we turn to <em>sparsification</em>. Through sparsification, we proceed very similar to thresholding like we did above. Remember that we chose a threshold, <span class="math notranslate nohighlight">\(t\)</span>, and first set all adjacency values less than or equal to <span class="math notranslate nohighlight">\(t\)</span> to zero. Now, we’re done! We simply skip the step of setting values greater than <span class="math notranslate nohighlight">\(t\)</span> to one. Let’s try an example where we take the friendship network, and sparsify the network using the <span class="math notranslate nohighlight">\(0.7\)</span> quantile. Note that this will lead to the smallest <span class="math notranslate nohighlight">\(70\)</span> percent of edges to take the value of zero, and the largest <span class="math notranslate nohighlight">\(30\)</span> percent of edges will keep their original edge-weights:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span><span class="o">=</span><span class="mf">0.7</span>  <span class="c1"># the quantile to sparsify with</span>

<span class="c1"># use the quantile function to obtain the threshold</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">quantile</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">[</span><span class="n">upper_tri_non_diag_idx</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># quantile = percentile / 100</span>
<span class="n">A_friend_sparse</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">)</span>  <span class="c1"># copy the network over</span>

<span class="c1"># sparsify using t</span>
<span class="n">A_friend_sparse</span><span class="p">[</span><span class="n">A_friend_sparse</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_avg_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Undirected Friendship Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend_sparse</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Undirected Friendship Network, Sparsified at 70th ptile&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that many of the small entries in the off-diagonal areas now have a value of zero. Again, we have the same pitfalls for sparsification as we did with thresholding, where if the network takes only non-negative edge weights and the percentile we choose corresponds to a threshold of zero, we might not actually end up changing anything.</p>
</div>
<div class="section" id="edge-weight-normalization">
<h6><span class="section-number">1.4.2.3.3. </span>Edge-weight normalization<a class="headerlink" href="#edge-weight-normalization" title="Permalink to this headline">¶</a></h6>
<p>With weighted networks, it is often the case that we might want to reshape the distributions of edge-weights in our networks to highlight particular properties. Notice that the edge-weights for our human networks take values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, but for our alien network take values between <span class="math notranslate nohighlight">\(0\)</span> and almost <span class="math notranslate nohighlight">\(40\)</span>. How can we possibly compare between these two networks when the edge-weights take such different ranges of values? We turn to standardization, which allows us to place values from different networks on the same scale.</p>
<div class="section" id="z-scoring-standardizes-edge-weights-using-the-normal-distribution">
<h7><span class="section-number">1.4.2.3.3.1. </span><span class="math notranslate nohighlight">\(z\)</span>-scoring standardizes edge weights using the normal distribution<a class="headerlink" href="#z-scoring-standardizes-edge-weights-using-the-normal-distribution" title="Permalink to this headline">¶</a></h7>
<p>The first approach to edge-weight standardization is known commonly as <span class="math notranslate nohighlight">\(z\)</span>-scoring. Suppose that <span class="math notranslate nohighlight">\(A\)</span> is the adjacency matrix, with entries <span class="math notranslate nohighlight">\(a_{ij}\)</span>. With a <span class="math notranslate nohighlight">\(z\)</span>-score, we will rescale the weights of the adjacency matrix, such that the new edge-weights (called <span class="math notranslate nohighlight">\(z\)</span>-scores) are approximately normally distributed. The reason this can be useful is that the normal distribution is pretty ubiquitous across many branches of science, and therefore, a <span class="math notranslate nohighlight">\(z\)</span>-score is relatively easy to communicate with other scientists. Further, many things that exist in nature can be well-approximated by a normal distribution, so it seems like a reasonable place to start to use a <span class="math notranslate nohighlight">\(z\)</span>-score for edge-weights, too! The <span class="math notranslate nohighlight">\(z\)</span>-score is defined as follows. We will construct the <span class="math notranslate nohighlight">\(z\)</span>-scored adjacency matrix <span class="math notranslate nohighlight">\(Z\)</span>, whose entries <span class="math notranslate nohighlight">\(z_{ij}\)</span> are the corresponding <span class="math notranslate nohighlight">\(z\)</span>-scores of the adjacency matrix’s entries <span class="math notranslate nohighlight">\(a_{ij}\)</span>. For a weighted, loopless network, we use an estimate of the <em>mean</em>, <span class="math notranslate nohighlight">\(\hat \mu\)</span>, and the <em>unbiased</em> estimate of the <em>variance</em>, <span class="math notranslate nohighlight">\(\hat \sigma^2\)</span>), which can be computed as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat\mu &amp;= \frac{1}{n}\sum_{i \neq j}a_{ij},\\
    \hat\sigma^2 &amp;= \frac{1}{n - 1}\sum_{i \neq j} (a_{ij} - \hat\mu)^2.
\end{align*}\]</div>
<p>The <span class="math notranslate nohighlight">\(z\)</span>-score for the <span class="math notranslate nohighlight">\((i,j)\)</span> entry is simply the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    z_{ij} &amp;= \frac{a_{ij} - \hat\mu}{\hat\sigma}
\end{align*}\]</div>
<p>Since our network is loopless, notice that these sums are for all <em>non-diagonal</em> entries where <span class="math notranslate nohighlight">\(i \neq j\)</span>. If the network were not loopless, we would include diagonal entries in the calculation, and instead would sum over all possible combinations of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. the interpretation of the <span class="math notranslate nohighlight">\(z\)</span>-score <span class="math notranslate nohighlight">\(z_{ij}\)</span> is the <em>number of stadard deviations</em> that the entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> is from the mean, <span class="math notranslate nohighlight">\(\hat \mu\)</span>.</p>
<p>We will demonstrate on the directed friendship network. We can implement <span class="math notranslate nohighlight">\(z\)</span>-scoring as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">zscore</span>

<span class="k">def</span> <span class="nf">z_score_loopless</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_loopless</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The network has loops!&quot;</span><span class="p">)</span>
    <span class="c1"># the entries of the adjacency matrix that are not on the diagonal</span>
    <span class="n">non_diag_idx</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">zscore</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">Z</span>

<span class="n">ZA_friend</span> <span class="o">=</span> <span class="n">z_score_loopless</span><span class="p">(</span><span class="n">A_friend</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">ZA_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network, After Z-score&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we will look at the edge-weight histogram for the directed friendship network before and after <span class="math notranslate nohighlight">\(z\)</span>-scoring. Remember that the network is loopless, so again we exclude the diagonal entries:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">seaborn</span> <span class="kn">import</span> <span class="n">histplot</span>

<span class="n">non_diag_idx</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">eye</span><span class="p">(</span><span class="n">A_friend</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A_friend</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Directed Friendship Network, Before Z-score&quot;</span><span class="p">);</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">ZA_friend</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Z-score&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Directed Friendship Network, After Z-score&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>The theory for when, and why, to use <span class="math notranslate nohighlight">\(z\)</span>-scoring for network machine learning tends to go something like this: many things tend to be normally distributed with the same mean and variance, so perhaps that is a reasonable expectation for our network, too. Unfortunately, we find this often to <em>not</em> be the case. In fact, we often find that the specific distribution of edge weights itself often might be lamost infeasible to identify in a population of networks, and therefore <em>almost</em> irrelevant all-together. To this end, we turn to instead <em>ranking</em> the edges.</p>
</div>
<div class="section" id="ranking-edges-preserves-ordinal-relationships">
<h7><span class="section-number">1.4.2.3.3.2. </span>Ranking edges preserves ordinal relationships<a class="headerlink" href="#ranking-edges-preserves-ordinal-relationships" title="Permalink to this headline">¶</a></h7>
<p>The idea behind ranking is as follows. We don’t really know much useful information as to how the distribution of edge weights varies between a given pair of networks. For this reason, we want to virtually eliminate the impact of that distribution <em>almost</em> entirely. However, we know that if one edge-weight is larger than another edge-weight, that we do in fact trust that relationship. What this means is that we want something which preserves <em>ordinal</em> relationships in our edge-weights, but ignores other properties of the edge-weights. An ordinal relationship just means that we have a natural ordering to the edge-weights. This means that we can identify a largest edge-weight, a smallest edge-weight, and every position in between. When we want to preserve ordinal relationships in our network, we do something called <em>passing the non-zero edge-weights to ranks</em>. We will often use the abbreviation <code class="docutils literal notranslate"><span class="pre">ptr</span></code> to define this function because it is so useful for weighted networks. We pass non-zero edge-weights to ranks as follows:</p>
<ol class="simple">
<li><p>Identify all of the non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>Count the number of non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(n_{nz}\)</span>.</p></li>
<li><p>Rank all of the non-zero edges in the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, where for a non-zero entry <span class="math notranslate nohighlight">\(a_{ij}\)</span>, <span class="math notranslate nohighlight">\(rank(a_{ij}) = 1\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the smallest non-zero edge-weight, and <span class="math notranslate nohighlight">\(rank(a_{ij}) = n_{nz}\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the largest edge-weight. Ties are settled by using the average rank of the two entries.</p></li>
<li><p>Report the weight of each non-zero entry <span class="math notranslate nohighlight">\((i,j)\)</span> as <span class="math notranslate nohighlight">\(r_{ij} = \frac{rank(a_{ij})}{n_{nz} + 1}\)</span>, and for eachh zero entry as <span class="math notranslate nohighlight">\(r_{ij} = 0\)</span>.</p></li>
</ol>
<p>Below, we pass-to-ranks the directed friendship network using <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>, showing both the adjacency matrix and the edge-weight distribution before and after <code class="docutils literal notranslate"><span class="pre">ptr</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">pass_to_ranks</span>

<span class="n">RA_friend</span> <span class="o">=</span> <span class="n">pass_to_ranks</span><span class="p">(</span><span class="n">A_friend</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">RA_friend</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Directed Friendship Network, After PTR&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A_friend</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Directed Friendship Network, Before PTR&quot;</span><span class="p">);</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">RA_friend</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">binrange</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;normalized rank&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Directed Friendship Network, After PTR&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>The edge-weights for the adjacency matrix <span class="math notranslate nohighlight">\(R\)</span> after <code class="docutils literal notranslate"><span class="pre">ptr</span></code> has the interpretation that each entry <span class="math notranslate nohighlight">\(r_{ij}\)</span> which is non-zero is the <em>quantile</em> of that entry amongst <em>the other non-zero entries</em>. This is unique in that it is completely <em>distribution-free</em>, which means that we don’t need to assume anything about the distribution of the edge-weights to have a reasonably interpretable quantity. On the other hand, the <span class="math notranslate nohighlight">\(z\)</span>-score had the interpretation of the number of standard deviations from the mean, which is only a sensible quantity to compare if we assume the population of edge-weights are normally distributed.</p>
<p>Another useful quantity related to pass-to-ranks is known as the zero-boosted pass-to-ranks. Zero-boosted pass-to-ranks is conducted as follows:</p>
<ol class="simple">
<li><p>Identify all of the non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>Count the number of non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(n_{nz}\)</span>, <em>and</em> the number of zero-entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(n_z\)</span>. Note that since the values of the adjacency matrix are either zero or non-zero, that <span class="math notranslate nohighlight">\(n_{nz} + n_z = n^2\)</span>, as <span class="math notranslate nohighlight">\(A\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix and therefore has <span class="math notranslate nohighlight">\(n^2\)</span> total entries.</p></li>
<li><p>Rank all of the non-zero edges in the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, where for a non-zero entry <span class="math notranslate nohighlight">\(a_{ij}\)</span>, <span class="math notranslate nohighlight">\(rank(a_{ij}) = 1\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the smallest non-zero edge-weight, and <span class="math notranslate nohighlight">\(rank(a_{ij}) = n_{nz}\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the largest edge-weight. Ties are settled by using the average rank of the two entries.</p></li>
<li><p>Report the weight of each non-zero entry <span class="math notranslate nohighlight">\((i,j)\)</span> as <span class="math notranslate nohighlight">\(r_{ij}' = \frac{n_z + rank(a_{ij})}{n^2 + 1}\)</span>, and for each zero entry as <span class="math notranslate nohighlight">\(r_{ij}' = 0\)</span>.</p></li>
</ol>
<p>The edge-weights for the adjacency matrix <span class="math notranslate nohighlight">\(R'\)</span> after zero-boosted <code class="docutils literal notranslate"><span class="pre">ptr</span></code> have the interpretation that each entry <span class="math notranslate nohighlight">\(r_{ij}'\)</span> is the quantile of that entry amongst <em>all</em> of the entries. Let’s instead use zero-boosted <code class="docutils literal notranslate"><span class="pre">ptr</span></code> on our network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">RA_friend_zb</span> <span class="o">=</span> <span class="n">pass_to_ranks</span><span class="p">(</span><span class="n">A_friend</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;zero-boost&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A_friend</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Directed Friendship Network, Before zb-PTR&quot;</span><span class="p">);</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">RA_friend_zb</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">binrange</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge quantile&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Directed Friendship Network, After zb-PTR&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="logging-reduces-magnitudinal-differences-between-edges">
<h7><span class="section-number">1.4.2.3.3.3. </span>Logging reduces magnitudinal differences between edges<a class="headerlink" href="#logging-reduces-magnitudinal-differences-between-edges" title="Permalink to this headline">¶</a></h7>
<p>When we look at the distribution of non-zero edge-weights for the activity/hobby network or the friendship network, we notice a strange pattern, known as a <em>right-skew</em>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A_activity</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Activity/Hobby Network Edge Distribution&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that <em>most</em> of the edges have weights which are comparatively small, between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(34\)</span>, but some of the edges have weights which are much (much) larger. A <strong>right-skew</strong> exists when the majority of edge-weights are small, but some of the edge-weights take values which are much larger.</p>
<p>What if we want to make these large values more similar in relation to the smaller values, but we simultaneously want to preserve properties of the underlying distribution of the edge-weights? Well, we can’t use <code class="docutils literal notranslate"><span class="pre">ptr</span></code>, because <code class="docutils literal notranslate"><span class="pre">ptr</span></code> will throw away all of the information about the edge-weight distribution other than the ordinal relationship between pairs of edges. To interpret what this means, we might think that there is a big difference between sharing no interests compared to three interests in common, but there is not as much of a difference in sharing ten interests compared to thirteen interests in common.</p>
<p>To do this, we instead turn to the logarithm function. The logarithm function <span class="math notranslate nohighlight">\(log_{10}(x)\)</span> is defined for positive values <span class="math notranslate nohighlight">\(x\)</span> as the value <span class="math notranslate nohighlight">\(c_x\)</span> where <span class="math notranslate nohighlight">\(x = 10^{c_x}\)</span>. In this sense, it is the “number of powers of ten” to obtain the value <span class="math notranslate nohighlight">\(x\)</span>. You will notice that the logarithm function looks like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">logxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">logxs</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">(x)$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>What is key to noice about this function is that, as <span class="math notranslate nohighlight">\(x\)</span> increases, the log of <span class="math notranslate nohighlight">\(x\)</span> increases by a <em>decreasing</em> amount. Let’s imagine we have three values, <span class="math notranslate nohighlight">\(x = .001\)</span>, <span class="math notranslate nohighlight">\(y = .1\)</span>, and <span class="math notranslate nohighlight">\(z = 10\)</span>. A calculator will give you that <span class="math notranslate nohighlight">\(log_{10}(x) = -3, log_{10}(y) = -1\)</span>, and <span class="math notranslate nohighlight">\(log_{10}(z) = 1\)</span>. Even though <span class="math notranslate nohighlight">\(y\)</span> is only <span class="math notranslate nohighlight">\(.099\)</span> units bigger than <span class="math notranslate nohighlight">\(x\)</span>, its logarithm <span class="math notranslate nohighlight">\(log_{10}(y)\)</span> exceeds <span class="math notranslate nohighlight">\(log_{10}(x)\)</span> by two units. on the other hand, <span class="math notranslate nohighlight">\(z\)</span> is <span class="math notranslate nohighlight">\(9.9\)</span> units bigger than <span class="math notranslate nohighlight">\(y\)</span>, but yet its logarithm <span class="math notranslate nohighlight">\(log_{10}(z)\)</span> is still the same two units bigger than <span class="math notranslate nohighlight">\(log_{10}(y)\)</span>. This is because thhe logarithm is instead looking at the fact that <span class="math notranslate nohighlight">\(z\)</span> is <em>one</em> power of ten, <span class="math notranslate nohighlight">\(y\)</span> is <span class="math notranslate nohighlight">\(-1\)</span> powers of ten, and <span class="math notranslate nohighlight">\(z\)</span> is <span class="math notranslate nohighlight">\(-3\)</span> powers of ten. The logarithm has <em>collapsed</em> the huge size difference between <span class="math notranslate nohighlight">\(z\)</span> and the other two values <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> by using exponentiation with <em>base</em> ten.</p>
<p>In this sense, we can also use the logarithm function for our network to reduce the huge size difference between the values in our activity/hobby network. However, we must first add a slight twist: to do this properly and yield an interpretable adjacency matrix, we need to <em>augment</em> the entries of the adjacency matrix <em>if</em> it contains zeros. This is because the <span class="math notranslate nohighlight">\(log_{10}(0)\)</span> is <em>not defined</em>. To augment the adjacency matrix, we will use the following strategy:</p>
<ol class="simple">
<li><p>Identify the entries of <span class="math notranslate nohighlight">\(A\)</span> which take a value of zero.</p></li>
<li><p>Identify the smallest entry of <span class="math notranslate nohighlight">\(A\)</span> which is not-zero, and call it <span class="math notranslate nohighlight">\(a_m\)</span>.</p></li>
<li><p>Compute a value <span class="math notranslate nohighlight">\(\epsilon\)</span> which is an <em>order of magnitude</em> smaller than <span class="math notranslate nohighlight">\(a_m\)</span>. Since we are taking powers of ten, a single order of magnitude would give us that <span class="math notranslate nohighlight">\(\epsilon = \frac{a_m}{10}\)</span>.</p></li>
<li><p>Take the augmented adjacency matrix <span class="math notranslate nohighlight">\(A'\)</span> to be defined with entries <span class="math notranslate nohighlight">\(a_{ij}' = a_{ij} + \epsilon\)</span>.</p></li>
</ol>
<p>Next, since our matrix has values which are now all greater than zero, we can just take the logarithm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">augment_zeros</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">X</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The logarithm is not defined for negative values!&quot;</span><span class="p">)</span>
    <span class="n">am</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)])</span>  <span class="c1"># the smallest non-zero entry of X</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">am</span><span class="o">/</span><span class="mi">10</span>  <span class="c1"># epsilon is one order of magnitude smaller than the smallest non-zero entry</span>
    <span class="k">return</span> <span class="n">X</span> <span class="o">+</span> <span class="n">eps</span>  <span class="c1"># augment all entries of X by epsilon</span>

<span class="n">A_activity_aug</span> <span class="o">=</span> <span class="n">augment_zeros</span><span class="p">(</span><span class="n">A_activity</span><span class="p">)</span>
<span class="c1"># log-transform using base 10</span>
<span class="n">A_activity_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">A_activity_aug</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_activity</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Activity/Hobby Network Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A_activity_log</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Activity/Hobby Network, After Augmentation + log&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>When we plot the augmented and log-transformed data, what we see is that many of the edge-weights we originally might have thought were zero if we only looked at a plot were, in actuality, <em>not</em> zero. In this sense, for non-negative weighted networks, log transforming after zero-augmentation is often very useful for visualization to get a sense of the magnitudinal differences that might be present between edges.</p>
<p>Our edge-weight histogram becomes:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A_activity_log</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">($Edge Weight$ + \epsilon)$&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Activity/Hobby Network Edge Distribution after log&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-representations/ch5/ch5"></span><div class="section" id="why-use-statistical-models">
<h2><span class="section-number">2. </span>Why Use Statistical Models?<a class="headerlink" href="#why-use-statistical-models" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-representations/ch5/why-use-models"></span><div class="section" id="why-use-statistical-models">
<h3><span class="section-number">2.1. </span>Why Use Statistical Models?<a class="headerlink" href="#why-use-statistical-models" title="Permalink to this headline">¶</a></h3>
<p>In network data science, we typically begin with a question of interest, and a network we use to answer that question. To answer this question, we will turn to statistical models. Consider the simplest possible statistical model which has been used for years: the coin flip model. Let’s say we are playing a game with a gambler, and we bet one dollar. If the coin lands on heads, we get our dollar back, and an additional dollar. If the coin lands on tails, we lose our dollar. We get to watch ten coin flips before deciding whether or not to join the game. So, should we play?</p>
<p>A coin has a probability, which we will denote <span class="math notranslate nohighlight">\(p\)</span>, of landing on heads. Since the coin either lands on heads or tails, this means that the probability that the coin lands on tails is <span class="math notranslate nohighlight">\(1-p\)</span>. Reasonably, we might guess that the probability that this coin lands on heads is just <span class="math notranslate nohighlight">\(0.5\)</span> (the coin has an equal chance of landing on heads and tails). Unfortunatetly, the universe is a nefarious place! We could easily construct coins which slightly favor heads (perhaps a coin with a chance of landing on heads of <span class="math notranslate nohighlight">\(0.51\)</span>) <em>or</em> a coin which slightly favors tails (perhaps a coin with a chance of landing on tails of <span class="math notranslate nohighlight">\(0.51\)</span>).</p>
<p>To understand whether or not we should play this game, we turn to <em>statistical modelling</em>. A <strong>statistical model</strong> is a set of assumptions as to how a random system operates. The statistical model delineates what we think comes down to random chance in our system. In our coin flip example, this means that we describe the coin flip using the <em>Bernoulli model</em>, which means that the coin lands on heads with probability <span class="math notranslate nohighlight">\(p\)</span> and tails with probability <span class="math notranslate nohighlight">\(1-p\)</span>. The statistical model is the set of all possible coins which could be used for the game.</p>
<p>A <strong>random variable</strong> is an object whose outcome comes down to random chance. In our coin flip example, the coin flip itself <em>is</em> the random variable. The coin possesses a probability <span class="math notranslate nohighlight">\(p\)</span> of landing on heads and a probability <span class="math notranslate nohighlight">\(1-p\)</span> of landing on tails. To learn about the coin, we conduct an <em>experiment</em>. The experiment here is watching the coin flip ten times, and observing the outcome of the flips. The outcome of the coin flip is called a <em>realization</em> of the random variable. We will never truly understand the coin flip perfectly (we can never say for sure whether the coin will definitely land on heads or tails unless it has two heads or two tails). If we observe enough realizations of coin flips, however, we might be able to describe the coin flip in a way which could work this game in our favor.</p>
<p>The coin flip example is obviously very trivial, but it extends directly to statistical network models which we will use for simple networks. Consider a network of <span class="math notranslate nohighlight">\(100\)</span> students, in which each of the students attends one of two schools. We are interested in understanding whether students are more likely to be friends on a social networking site with students in their school than with students from the other school. Unlike traditional machine learning, in network machine learning we do not observe <span class="math notranslate nohighlight">\(n\)</span> outcomes (or realizations) with <span class="math notranslate nohighlight">\(d\)</span> dimensions. Rather, we see an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, whose nodes are the <span class="math notranslate nohighlight">\(100\)</span> students, and whose edges are the entries <span class="math notranslate nohighlight">\(a_{ij}\)</span> which take a value of one if the two students <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are friends on the social networking site and a value of zero if the two students are not friends on the social networking site.</p>
<p>Like the coin flip example, there is randomness and uncertainty to our social network. Perhaps a pair of students might be friends in real life, but they never got around to adding each other on the social media site. Maybe our students had a fight and are no longer friends, but never bothered to delete one another as friends on the site. Other factors might exist that we don’t know about (sports, hobbies, special interests) that influence whether two people are friends. Our social network might not capture all of the students, and we might be missing a large portion of the community all together. In many additional ways, our social network is noisy, and in order to address our question of interest, we need procedures which account for this uncertainty.</p>
<p>In machine learning, we typically encounter situations in which we have <span class="math notranslate nohighlight">\(n\)</span> observations in <span class="math notranslate nohighlight">\(d\)</span> dimensions. Traditional statistical models include univariate statistical models (models for data with <span class="math notranslate nohighlight">\(1\)</span> dimension) and multivariate statistical models (models for data with <span class="math notranslate nohighlight">\(d &gt; 1\)</span> dimensions), which can capture this traditional data representation. These models are well suited for discovering new insights about individual observations or collections of individual observations. Why do we need special statistical models for networks? Our realizations are not <span class="math notranslate nohighlight">\(n\)</span> disparate observations in <span class="math notranslate nohighlight">\(d\)</span>-dimensions; a realization in network machine learning <strong>is the full network itself</strong>, consisting of nodes, edges, and potential network attributes. We seek to model a representation of the <em>entire</em> network so that we can convey insights about properties of the network. To address our question of interest above, we need to characterize how students relate to other students in the network, not describe individual stdents. To this end, we describe our random network using sets of statistical assumptions, referred to as the <strong>statistical network model</strong>. The coin flip model might haave felt really simple, but we will see how we can use collections of coins to describe pretty complicated random networks throughout this chapter. We break down the key aspects of the coin flip experiment because it is so crucial:</p>
<div class="admonition-the-coin-flip-experiment admonition">
<p class="admonition-title">The Coin Flip Experiment</p>
<p>We had the following items we were concerned with in the coin flip example:</p>
<ol class="simple">
<li><p>The outcomes: The outcomes are either heads or tails. These outcomes will be denoted by the letter <span class="math notranslate nohighlight">\(x\)</span>, which takes values which are H (Heads) or T (Tails). The value <span class="math notranslate nohighlight">\(x\)</span> is called a <strong>realization</strong>.</p></li>
<li><p>The coin which was used: The specific coin being used in the coin flip experiment has a probability <span class="math notranslate nohighlight">\(p\)</span> of landing on heads and a probability <span class="math notranslate nohighlight">\(1 - p\)</span> of landing on tails. We will denote the specific coin being used by the letter <span class="math notranslate nohighlight">\(\mathbf x\)</span>. The bold-faced font means that the coin being used has a random outcome (it might be heads, it might be tails) to differentiate it from the coin flips which we saw and have known outcomes indicated by <span class="math notranslate nohighlight">\(x\)</span> (which has a fixed value, since we flipped the coin and <em>realized</em> the outcome). We don’t know anything about <span class="math notranslate nohighlight">\(p\)</span> just yet, so we can’t describe the coin’s specific random behavior just yet. The value <span class="math notranslate nohighlight">\(\mathbf x\)</span> is called the <strong>random variable</strong>.</p></li>
<li><p>Feasible coins: A possible coin that could have been used in the coin flipping experiment is one which has a probability <span class="math notranslate nohighlight">\(q\)</span> (which might be different from <span class="math notranslate nohighlight">\(p\)</span>) of landing on heads, and <span class="math notranslate nohighlight">\(1 - q\)</span> of landing on tails. A feasible coin will be denoted by <span class="math notranslate nohighlight">\(Bern(q)\)</span>, which just means that the coin lands on heads with probability <span class="math notranslate nohighlight">\(q\)</span> and tails with probability <span class="math notranslate nohighlight">\(1 - q\)</span>.</p></li>
<li><p>The Bernoulli model: The collection of all feasible coins which could have been used. This is described by the set <span class="math notranslate nohighlight">\(\left\{Bern(q) : q \text{ is a probability between \)</span>0<span class="math notranslate nohighlight">\( and \)</span>1<span class="math notranslate nohighlight">\(}\right\}\)</span>. This set is infinitely large, since it contains a feasible coin for <em>any</em> specified probability. The commonality between the feasible coins is that they each have a unique probability of landing on heads and a unique probability of landing on tails. The statistical model is simply the collection of all possible feasible coin which feature this commonality. For instance, there is a coin <span class="math notranslate nohighlight">\(Bern(0.1)\)</span> which has a probability <span class="math notranslate nohighlight">\(0.1\)</span> of landing on heads in this set, and another coin <span class="math notranslate nohighlight">\(Bern(0.9)\)</span> which has a probabiliy <span class="math notranslate nohighlight">\(0.9\)</span> of landing on heads in this set.
The specific coin being used, the random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> which lands on heads with probability <span class="math notranslate nohighlight">\(p\)</span>, behaves <em>exactly</em> like the coin <span class="math notranslate nohighlight">\(Bern(p)\)</span> from the statistical model. For this reason, we will say that the coin <span class="math notranslate nohighlight">\(\mathbf x\)</span> is a <span class="math notranslate nohighlight">\(Bern(p)\)</span> coin.</p></li>
</ol>
</div>
<div class="section" id="models-aren-t-right-why-do-we-care">
<h4><span class="section-number">2.1.1. </span>Models aren’t Right. Why do we Care?<a class="headerlink" href="#models-aren-t-right-why-do-we-care" title="Permalink to this headline">¶</a></h4>
<p>It is important to clarify that we must pay careful attention to the age old aphorism attributed to George Box, a pioneering British statistician of the 20<span class="math notranslate nohighlight">\(^{th}\)</span> century. George Box stated, “all models are wrong, but some are useful.” In this sense, it is important to remember that the statistical model we select is, in practice, <em>never</em> the correct model (this holds for any aspect of machine learning, not just network machine learning). In the context of network science, this means that even if we have a model we think describes our network very well, it is <em>not</em> the case that the model we select actually describes the network precisely and correctly. Despite this, it is often valuable to use statistical models for the simple reason that assuming that a stochastic process (that is, some <em>random</em> process) which governs our data is what allows us to convey <em>uncertainty</em>. To understand the importance of leveraging uncertainty, consider the following scenarios:</p>
<ol class="simple">
<li><p>Lack of information: In practice, we would never have all of the information about the system that produced the network we observe, and uncertainty can be used in place of that information. For instance, in our social network example, we might only know which school that people are from, but there are many other attributes that would impact the friend circle of a given student. We might not know things like which classes people have taken nor which grade they’re in, but we would expect these facts to impact whether a given pair of people might have a higher chance of being friends. We can use uncertainty in our model to capture the fact that we don’t know the classes nor grades of the students.</p></li>
<li><p>We might think the network is deterministic, rather than stochastic: In the extreme case, we might think that if we had <em>all</em> of the information which governs the network, then we could determine exactly what realizations would look like with perfect accuracy. Even if we knew exactly what realizations of the network might look like, this description, too, isn’t likely to be very valuable. If we were to develop a model on the basis of everything, our model would be extremely complex and require a large amount of data. For instance, in our social network example, to know whether two people were friends with perfect accuracy, we might need to have intimate knowledge of every single person’s life (Did they just have a fight with somebody and de-connect with that person? Did they just go to a school dance and meet someone new?).</p></li>
<li><p>We learn from uncertainty and simplicity: When we do statistical inference, it is rarely the case that we prioritize a complex, convoluted model that mirrors our data suspiciously closely. Instead, we are usually interested in knowing how faithfully a simpler, more generally applicable model might describe the network. This relates directly to the concept of the bias-variance tradeoff from machine learning, in which we prefer a model which isn’t too specific (lower bias) but still describes the system effectively (lower variance).</p></li>
</ol>
<p>Therefore, it is crucial to incorporate randomness and uncertainty to understand network data. In practice, we select a model which is appropriate from a family of candidate models on the basis of three primary factors:</p>
<ol class="simple">
<li><p>Utility: The model of interest possesses the level of refinement or complexity that we need to answer our scientific question of interest. What this means is that the coin flip model will allow us to determine whether or not we should gamble.</p></li>
<li><p>Estimation: The data has the level of breadth to facilitate estimation of the parameters of the model of interest. This means that we can use the outcomes of coin flips to guess what the probability that the coin will land on heads is.</p></li>
<li><p>Appropriateness: The model is appropriate for the data we are given. This means that there are not major factors which are unaccounted for by the statistical model, such as if the coin thrower holds a magnet which will alter the outcome of the coin flip.</p></li>
</ol>
<p>For the rest of this section, we will develop intuition for the first point. Later sections will cover estimation of parameters and model selection.</p>
</div>
</div>
<span id="document-representations/ch5/single-network-models_ER"></span><div class="section" id="erdos-renyi-er-random-networks">
<h3><span class="section-number">2.2. </span>Erdös-Rényi (ER) Random Networks<a class="headerlink" href="#erdos-renyi-er-random-networks" title="Permalink to this headline">¶</a></h3>
<p>We will start our description with the simplest random network model. Consider a social network, with 50 students. Our network will have 50 nodes, where each node represents a single student in the network. Edges in the social network represent whether or not a pair of students are friends. What is the simplest way we could describe whether two people are friends?</p>
<p>In this case, the simplest possible thing to do would be to say, for any two students in our network, there is some probability (which we will call <span class="math notranslate nohighlight">\(p\)</span>) that describes how likely they are to be friends. In the below example, for the sake of argument, we will let <span class="math notranslate nohighlight">\(p=0.3\)</span>. What does a realization from this network look like?</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># network with 50 nodes</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># probability of an edge existing is .3</span>

<span class="c1"># sample a single simple adjacency matrix from ER(50, .3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;ER(0.3) Simulation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_ER_1_0.png" src="_images/single-network-models_ER_1_0.png" />
</div>
</div>
<p>As we mentioned in the preface for this chapter, every statisical model in this book will come down to the coin flip model. In network machine learning, we get to see an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> whose entries <span class="math notranslate nohighlight">\(a_{ij}\)</span> are one if the students <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are friends on the social networking site, and <span class="math notranslate nohighlight">\(0\)</span> if the students <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are not friends on the social networking site. Just like we had a random coin <span class="math notranslate nohighlight">\(\mathbf x\)</span> which behaved like a <span class="math notranslate nohighlight">\(Bern(p)\)</span> coin from the collection <span class="math notranslate nohighlight">\(\left\{Bern(q) : q \text{ is a probability between \)</span>0<span class="math notranslate nohighlight">\( and \)</span>1<span class="math notranslate nohighlight">\(}\right\}\)</span>, we will assume that the network <span class="math notranslate nohighlight">\(A\)</span> is a realization of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which behaves like an element of a collection of random neworks. This might seem like a big stretch, so let’s try to put it into perspective.</p>
<p>Since <span class="math notranslate nohighlight">\(A\)</span> was an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix, <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> random matrix. The elements of <span class="math notranslate nohighlight">\(\mathbf A\)</span> will be given by the symbols <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>, which means that each edge <span class="math notranslate nohighlight">\(a_{ij}\)</span> of <span class="math notranslate nohighlight">\(A\)</span> is a realization of the random edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>. Just how do we describe this <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>? Remember that our realizations <span class="math notranslate nohighlight">\(a_{ij}\)</span> are just <span class="math notranslate nohighlight">\(0\)</span>s and <span class="math notranslate nohighlight">\(1\)</span>s, which <em>feels</em> a lot like flipping a coin, doesn’t it? Did the coin land on heads, or did it land on tails? Are the two people <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> friends, or are they not friends? If we had a coin with some probability of landing on heads, we could describe <span class="math notranslate nohighlight">\(a_{ij}\)</span> as a realization of this coin flip. We could assume that a value of one is analogous to the coin landing on heads, and value of zero is analogous to the coin landing on tails. Perhaps we could even model the network using the same approach we took before with the coin flip. This is starting to go somewhere, so let’s continue with the analogies.</p>
<div class="section" id="the-erdos-renyi-random-network-is-parametrized-by-the-independent-edge-probability">
<h4><span class="section-number">2.2.1. </span>The Erdös Rényi random network is parametrized by the independent-edge probability<a class="headerlink" href="#the-erdos-renyi-random-network-is-parametrized-by-the-independent-edge-probability" title="Permalink to this headline">¶</a></h4>
<p>This simple random network model is called the Erdös Rényi (ER) model<sup>1</sup>. The way we can think of an ER random network is that the edges depend <em>only</em> on a probability, <span class="math notranslate nohighlight">\(p\)</span>, and each edge is totally independent of all other edges. We can think of this example as though a coin flip is performed, where the coin has a probability <span class="math notranslate nohighlight">\(p\)</span> of landing on heads, and <span class="math notranslate nohighlight">\(1-p\)</span> of landing on tails. For each edge in the network, we conceptually flip the coin, and if it lands on heads (with probability <span class="math notranslate nohighlight">\(p\)</span>), the edge exists, and if it lands on tails (with probability <span class="math notranslate nohighlight">\(1-p\)</span>) the edge does not exist. The meaning of <em>independence</em> is a little technical and goes a bit outside of the scope of this book, so we will leave it at a very high level as meaning that the outcome of particular coin flips do not impact the outcomes of other coin flips. This is not a very precise definition, but it will be plenty for our purposes. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network which is <span class="math notranslate nohighlight">\(ER_n(p)\)</span> with <span class="math notranslate nohighlight">\(n\)</span> nodes and probability <span class="math notranslate nohighlight">\(p\)</span>, we will often say that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random network.</p>
</div>
<div class="section" id="how-do-we-simulate-realizations-of-er-n-p-random-networks">
<h4><span class="section-number">2.2.2. </span>How do we simulate realizations of <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random networks?<a class="headerlink" href="#how-do-we-simulate-realizations-of-er-n-p-random-networks" title="Permalink to this headline">¶</a></h4>
<p>This approach which we will use to describe random networks is called a <em>generative model</em>, which means that we have described an observable network realization <span class="math notranslate nohighlight">\(A\)</span> of the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> in terms of the parameters of <span class="math notranslate nohighlight">\(\mathbf A\)</span>. In the case of the <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random networks, we have described <span class="math notranslate nohighlight">\(\mathbf A\)</span> in terms of the probability parameter, <span class="math notranslate nohighlight">\(p\)</span>. Generative models are convenient in that we can easily adapt them to tell us exactly how to simulate realizations of the underlying random network. The procedure below will produce for us a network <span class="math notranslate nohighlight">\(A\)</span>, which has nodes and edges, where the underlying random network  <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random network:</p>
<div class="admonition-simulating-a-realization-from-an-er-n-p-random-network admonition">
<p class="admonition-title">Simulating a realization from an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random network</p>
<ol class="simple">
<li><p>Determine a probability, <span class="math notranslate nohighlight">\(p\)</span>, of an edge existing.</p></li>
<li><p>Obtain a weighted coin which has a probability <span class="math notranslate nohighlight">\(p\)</span> of landing on heads, and a probability <span class="math notranslate nohighlight">\(1 - p\)</span> of landing on tails. Note that this probability <span class="math notranslate nohighlight">\(p\)</span> might differ from the “traditional” coin with a probability of landing on heads of approximately <span class="math notranslate nohighlight">\(0.5\)</span>.</p></li>
<li><p>Flip the once for each <em>possible</em> edge <span class="math notranslate nohighlight">\((i, j)\)</span> between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> in the network. For a simple network, we will repeat the coin flip <span class="math notranslate nohighlight">\(\binom n 2\)</span> times.</p></li>
<li><p>For each coin flip which landed on heads, define that the corresponding edge exists, and define that the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the adjacency matrix is <span class="math notranslate nohighlight">\(1\)</span>. For each coin flip which lands on tails, define that the corresponding edge does not exist, and define that <span class="math notranslate nohighlight">\(a_{ij} = 0\)</span>.</p></li>
<li><p>The adjacency matrix we produce, <span class="math notranslate nohighlight">\(A\)</span>, is a realization of an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random network.</p></li>
</ol>
</div>
</div>
<div class="section" id="when-do-we-use-an-er-n-p-network">
<h4><span class="section-number">2.2.3. </span>When do we use an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> Network?<a class="headerlink" href="#when-do-we-use-an-er-n-p-network" title="Permalink to this headline">¶</a></h4>
<p>In practice, the <span class="math notranslate nohighlight">\(ER_n(p)\)</span> model seems like it might be a little too simple to be useful. Why would it ever be useful to think that the best we can do to describe our network is to say that connections exist with some probability? Does this miss a <em>lot</em> of useful questions we might want to answer? Fortunately, there are a number of ways in which the simplicity of the <span class="math notranslate nohighlight">\(ER_n(p)\)</span> model is useful. Given a probability and a number of nodes, we can easily describe the properties we would expect to see in a network if that network were ER. For instance, we know how many edges on average the nodes of an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random nework should have. We can reverse this idea, too: given a network we think might <em>not</em> be ER, we could check whether it’s different in some way from an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random network. For instance, if we see that half the nodes have a ton of edges (meaning, they have a high degree), and half don’t, we might be able to determine that the network is poorly described by an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random network. If this is the case, we might look for other models that could describe our network which are more complex.</p>
<!-- Another utility of the $ER_n(p)$ model is that we might often want to benchmark network algorithms on simulated networks with a given *sparsity*. **Network sparsity** is a feature of a network which describes the degree to which the network possesses fewer edges than the maximum number of possible edges. As an example, when we know ahead of time that the network is going to be sparse (the network has a *small* number of edges which exist relative the number of possible edges), we can use network machine learning techniques which anticipate this sparsity to make the algorithm faster. In a simple network, for instance, the maximum number of possible edges is $\binom n 2$. In an $ER_n(p)$ network with probability $p$, we would expect the network to have on average about $p \binom n 2$ edges; that is, $p$ describes the fraction of total possible edges that we would expect to exist. $ER_n(p)$ networks are extremely cheap to simulate computationally, because "flipping weighted coins" (if you are curious, this is called a *Bernoulli sample* with probability $p$) is usually able to be performed with extremely optimized code in most standard programming languages such as python. Being able to generate networks very easily with a given number of nodes $n$ and a given sparsity allows us to test just how efficient our network machine learning technique is.
-->
<p>In the next code block, we are going to sample a single <span class="math notranslate nohighlight">\(ER_n(p)\)</span> network with <span class="math notranslate nohighlight">\(50\)</span> nodes and an edge probability <span class="math notranslate nohighlight">\(p\)</span> of <span class="math notranslate nohighlight">\(0.3\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># network with 50 nodes</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># probability of an edge existing is .3</span>

<span class="c1"># sample a single simple adjacency matrix from ER(50, .3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$ER_</span><span class="si">{10}</span><span class="s2">(0.3)$ Simulation&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_ER_4_0.png" src="_images/single-network-models_ER_4_0.png" />
</div>
</div>
<p>Above, we visualize the network using a heatmap. The dark squares indicate that an edge exists between a pair of nodes, and white squares indicate that an edge does not exist between a pair of nodes.</p>
<p>Next, let’s see what happens when we use a higher edge probability, like <span class="math notranslate nohighlight">\(p=0.7\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c1"># network has an edge probability of 0.7</span>

<span class="c1"># sample a single adjacency matrix from ER(50, 0.7)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$ER_</span><span class="si">{10}</span><span class="s2">(0.7)$ Simulation&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_ER_7_0.png" src="_images/single-network-models_ER_7_0.png" />
</div>
</div>
<p>As the edge probability increases, the sampled adjacency matrix tends to indicate that there are more connections in the network. This is because there is a higher chance of an edge existing when <span class="math notranslate nohighlight">\(p\)</span> is larger.</p>
</div>
<div class="section" id="just-how-many-networks-are-possible-for-a-network-with-n-nodes">
<h4><span class="section-number">2.2.4. </span>Just how many networks are possible for a network with <span class="math notranslate nohighlight">\(n\)</span> nodes?<a class="headerlink" href="#just-how-many-networks-are-possible-for-a-network-with-n-nodes" title="Permalink to this headline">¶</a></h4>
<p>As you’re going to become accustomed to, we’re going to boil this down again to coin flips. If we had one coin, there are two possible outcomes: either heads or tails. If we had two coins, the first coin could be heads or tails, and the second coin could be heads or tails. Let’s break this down by fixing the outcome of the first coin. If the first coin were heads, there are two possible outcomes for the second coin. If the first coin were tails, there are two possible outcomes for the second coin. This means that the total number of possible outcomes is the sum of the number of possible outcomes if the first coin is heads with the number of possible outcomes if the first coin were tails. This gives us that with two coins, we have four possible outcomes. When we add a third coin, we repeat this process again. If the first coin were heads, the second two coins could take any of four possible outcomes as we just learned. if the first coin were tails, the second two coins could also taake any of four possible outcomes. Therefore, with three coins, we have eight possible outcomes. As we continue this procedure, we quickly will realize that with <span class="math notranslate nohighlight">\(x\)</span> coin flips, we have <span class="math notranslate nohighlight">\(2^x\)</span> possible outcomes.</p>
<p>Remember in Chapter 4 when discussing the <a class="reference external" href="#link?">properties of networks</a>, we determined that there are <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span> possible edges in a simple network, which we could represent using the notation <span class="math notranslate nohighlight">\(\binom n 2\)</span>. In a realized network, each of these edges could exist or not exist, so there are again two possibilities just like the coin flips. Since edges existing or not existing boils down to a coin flip, the number of possible networks with <span class="math notranslate nohighlight">\(n\)</span> nodes is just <span class="math notranslate nohighlight">\(2\)</span> to the power of the number of coin flips that are performed in the network. Here, this is <span class="math notranslate nohighlight">\(2^{\binom n 2}\)</span>. This quantity gets <em>really</em> big <em>really</em> fast! Let’s see just how fast below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">comb</span>

<span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span>
<span class="n">nposs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ns</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">nposs</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of Nodes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Possible Networks (log scale)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">350</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;$10^{{</span><span class="si">{pow:d}</span><span class="s2">}}$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">pow</span><span class="o">=</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">350</span><span class="p">]])</span>
<span class="n">ax</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_ER_10_0.png" src="_images/single-network-models_ER_10_0.png" />
</div>
</div>
<p>This is an enormous quantity! When <span class="math notranslate nohighlight">\(n\)</span> is just <span class="math notranslate nohighlight">\(6\)</span>, the number of possible networks is <span class="math notranslate nohighlight">\(2^{\binom 6 2} = 2^{6}\)</span> which is over <span class="math notranslate nohighlight">\(32,000\)</span>. When <span class="math notranslate nohighlight">\(n\)</span> is <span class="math notranslate nohighlight">\(15\)</span>, the number of posssible networks balloons up to <span class="math notranslate nohighlight">\(2^{\binom{15}{2}} = 2^{105}\)</span> which is over <span class="math notranslate nohighlight">\(10^{30}\)</span>. As the number of nodes increases, the number of possible network balloons really, really fast!</p>
</div>
<div class="section" id="references">
<h4><span class="section-number">2.2.5. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h4>
<p>[1] Erdös P, Rényi A. 1959. “On random graphs, I.” Publ. Math. Debrecen 6:290–297.</p>
</div>
</div>
<span id="document-representations/ch5/single-network-models_SBM"></span><div class="section" id="stochastic-block-models-sbm">
<h3><span class="section-number">2.3. </span>Stochastic Block Models (SBM)<a class="headerlink" href="#stochastic-block-models-sbm" title="Permalink to this headline">¶</a></h3>
<p>Let’s imagine that we have <span class="math notranslate nohighlight">\(100\)</span> students, each of whom can go to one of two possible schools: school one or school two. Our network has <span class="math notranslate nohighlight">\(100\)</span> nodes, and each node represents a single student. The edges of this network represent whether a pair of students are friends. Intuitively, if two students go to the same school, the probably have a higher chance of being friends than if they do not go to the same school. If we were to try to characterize this using an ER random network, we would run into a problem: we have no way to capture the impact that school has on friendships. Intuitively, there must be a better way!</p>
<p>The Stochastic Block Model, or SBM, captures this idea by assigning each of the <span class="math notranslate nohighlight">\(n\)</span> nodes in the network to one of <span class="math notranslate nohighlight">\(K\)</span> communities. A <strong>community</strong> is a group of nodes within the network. In our example case, the communities would represent the schools that students are able to attend. We use <span class="math notranslate nohighlight">\(K\)</span> here to just denote an integer greater than <span class="math notranslate nohighlight">\(1\)</span> (for example, in the school example we gave above, <span class="math notranslate nohighlight">\(K\)</span> is <span class="math notranslate nohighlight">\(2\)</span>) for the number of <em>possible</em> communities that nodes could be members of. In an SBM, instead of describing all pairs of nodes with a fixed probability like with the ER model, we instead describe properties that hold for edges between <em>pairs of communities</em>. In our example, what this means is that if two students go to school one, the probability that they are friends might be different than if the two students went to school two, or if one student went to school one and the other to school two. Let’s take a look at what a realization of this setup we have described might look like:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>  <span class="c1"># network with 50 nodes</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]</span>  <span class="c1"># probability of an edge existing is .3</span>

<span class="c1"># sample a single simple adjacency matrix from ER(50, .3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">ys</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$SBM_n(z, B)$ Simulation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_SBM_2_0.png" src="_images/single-network-models_SBM_2_0.png" />
</div>
</div>
<div class="section" id="defining-the-paramaters-of-an-sbm-random-network">
<h4><span class="section-number">2.3.1. </span>Defining the paramaters of an SBM random network<a class="headerlink" href="#defining-the-paramaters-of-an-sbm-random-network" title="Permalink to this headline">¶</a></h4>
<div class="section" id="the-community-assignment-vector-assigns-nodes-in-the-random-network-to-communities">
<h5><span class="section-number">2.3.1.1. </span>The community assignment vector assigns nodes in the random network to communities<a class="headerlink" href="#the-community-assignment-vector-assigns-nodes-in-the-random-network-to-communities" title="Permalink to this headline">¶</a></h5>
<p>To describe an SBM random network, we proceed very similarly to an ER random network, with a twist. An SBM random network has a parameter, <span class="math notranslate nohighlight">\(\vec z\)</span>, which has a single element for each of the node. We call <span class="math notranslate nohighlight">\(\vec z\)</span> the <strong>community assignment vector</strong>, which means that for each node of our random network, <span class="math notranslate nohighlight">\(z_i\)</span> tells us which community the node is in. To state this another way, <span class="math notranslate nohighlight">\(\vec z\)</span> is a vector where each element <span class="math notranslate nohighlight">\(z_i\)</span> can take one of <span class="math notranslate nohighlight">\(K\)</span> possible values, where <span class="math notranslate nohighlight">\(K\)</span> is the total number of communities in the network. For example, if we had an SBM random network with four nodes in total, and two total communities, each element <span class="math notranslate nohighlight">\(z_i\)</span> can be either <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(2\)</span>. If the first two nodes were in community <span class="math notranslate nohighlight">\(1\)</span>, and the second two in community <span class="math notranslate nohighlight">\(2\)</span>, we would say that <span class="math notranslate nohighlight">\(z_1 = 1\)</span>, <span class="math notranslate nohighlight">\(z_2 = 1\)</span>, <span class="math notranslate nohighlight">\(z_3 = 2\)</span>, and <span class="math notranslate nohighlight">\(z_4 = 2\)</span>, which means that <span class="math notranslate nohighlight">\(\vec z\)</span> looks like:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec z^\top &amp;= \begin{bmatrix}1 &amp; 1 &amp; 2 &amp; 2\end{bmatrix}
\end{align*}\]</div>
</div>
<div class="section" id="the-block-matrix-defines-the-edge-existence-probabilities-between-communities-in-the-random-network">
<h5><span class="section-number">2.3.1.2. </span>The block matrix defines the edge existence probabilities between communities in the random network<a class="headerlink" href="#the-block-matrix-defines-the-edge-existence-probabilities-between-communities-in-the-random-network" title="Permalink to this headline">¶</a></h5>
<p>The other parameter for an SBM random network is called the block matrix, for which we will use the capital letter <span class="math notranslate nohighlight">\(B\)</span>. If there are <span class="math notranslate nohighlight">\(K\)</span> communities in the SBM random network, then <span class="math notranslate nohighlight">\(B\)</span> is a <span class="math notranslate nohighlight">\(K \times K\)</span> matrix, with one entry for each pair of communities. For instance, if <span class="math notranslate nohighlight">\(K\)</span> were two like above, <span class="math notranslate nohighlight">\(B\)</span> would be a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix, and would look like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    B &amp;= \begin{bmatrix}
        b_{11} &amp; b_{12} \\ b_{21} &amp; b_{22}
    \end{bmatrix}
\end{align*}\]</div>
<p>Each of the entries of <span class="math notranslate nohighlight">\(B\)</span>, which we denote as <span class="math notranslate nohighlight">\(b_{kl}\)</span> in the above matrix, is a probability of an edge existing between a node in community <span class="math notranslate nohighlight">\(k\)</span> and a node in community <span class="math notranslate nohighlight">\(l\)</span>.</p>
<p>Fortunately, we can also think of this formulation of a random network using coin flips. In our mini example above, if node <span class="math notranslate nohighlight">\(1\)</span> is in community <span class="math notranslate nohighlight">\(1\)</span> (since <span class="math notranslate nohighlight">\(z_1 = 1\)</span>) and node <span class="math notranslate nohighlight">\(2\)</span> is in community <span class="math notranslate nohighlight">\(1\)</span> (since <span class="math notranslate nohighlight">\(z_2 = 1\)</span>), we have a weighted coin which has a probability <span class="math notranslate nohighlight">\(b_{11}\)</span> (the first row, first column of the block matrix above) of landing on heads, and a <span class="math notranslate nohighlight">\(1 - b_{11}\)</span> chance of landing on tails. An edge between nodes one and two exists if the weighted coin lands on heads, and does not exist if that weighted coin lands on tails. If we wanted to describe an edge between nodes one and three instead, note that <span class="math notranslate nohighlight">\(z_3 = 2\)</span>. Therefore, we use the entry <span class="math notranslate nohighlight">\(b_{12}\)</span> as the probability of obtaining a heads for the weighted coin we flip this time. In the general case, to use the block matrix to obtain the probability of an edge <span class="math notranslate nohighlight">\((i, j)\)</span> existing between any pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> in our network, we will flip a coin with probability <span class="math notranslate nohighlight">\(b_{z_i z_j}\)</span>, where <span class="math notranslate nohighlight">\(z_i\)</span> is the community assignment for the <span class="math notranslate nohighlight">\(i^{th}\)</span> node and <span class="math notranslate nohighlight">\(z_j\)</span> is the community assignment for the <span class="math notranslate nohighlight">\(j^{th}\)</span> node.</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network which is an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> with <span class="math notranslate nohighlight">\(n\)</span> nodes, the community vector <span class="math notranslate nohighlight">\(\vec z\)</span>, and the block matrix <span class="math notranslate nohighlight">\(B\)</span>, we say that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network.</p>
</div>
<div class="section" id="how-do-we-simulate-realizations-of-sbm-n-vec-z-b-random-networks">
<h5><span class="section-number">2.3.1.3. </span>How do we simulate realizations of <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random networks?<a class="headerlink" href="#how-do-we-simulate-realizations-of-sbm-n-vec-z-b-random-networks" title="Permalink to this headline">¶</a></h5>
<p>The procedure below will produce for us a network <span class="math notranslate nohighlight">\(A\)</span>, which has nodes and edges, where the underlying random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network:</p>
<div class="admonition-simulating-a-realization-from-an-sbm-n-vec-z-b-random-network admonition">
<p class="admonition-title">Simulating a realization from an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network</p>
<ol class="simple">
<li><p>Determine a community assignment vector, <span class="math notranslate nohighlight">\(\vec z\)</span>, for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes. Each node should be assigned to one of <span class="math notranslate nohighlight">\(K\)</span> communities.</p></li>
<li><p>Determine a block matrix, <span class="math notranslate nohighlight">\(B\)</span>, for each pair of the <span class="math notranslate nohighlight">\(K\)</span> communities.</p></li>
<li><p>For each pair of communities <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(l\)</span>, obtain a weighted coin (which we will call the <span class="math notranslate nohighlight">\((k,l)\)</span> coin) which as a <span class="math notranslate nohighlight">\(b_{kl}\)</span> chance of landing on heads, and a <span class="math notranslate nohighlight">\(1 - b_{kl}\)</span> chance of landing on tails.</p></li>
<li><p>For each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>:</p>
<ul class="simple">
<li><p>Denote <span class="math notranslate nohighlight">\(z_i\)</span> to be the community assignment of node <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(z_j\)</span> to be the community assignment of node <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>Flip the <span class="math notranslate nohighlight">\((z_i, z_j)\)</span> coin, and if it lands on heads, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the adjacency matrix is <span class="math notranslate nohighlight">\(1\)</span>. If it lands on tails, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the adjacency matrix is <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
</ul>
</li>
<li><p>The adjacency matrix we produce, <span class="math notranslate nohighlight">\(A\)</span>, is a realization of an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network.</p></li>
</ol>
</div>
<p>We just covered a lot of intuition! This intuition will come in handy later, but let’s take a break from the theory by working through an example. Let’s use the school example we started above. Say we have <span class="math notranslate nohighlight">\(100\)</span> students, and we know that each student goes to one of two possible schools. Remember that we already know the community assignment vector <span class="math notranslate nohighlight">\(\vec{z}\)</span> ahead of time. We don’t really care too much about the ordering of the students for now, so let’s just assume that the first <span class="math notranslate nohighlight">\(50\)</span> students all go to the first school, and the second <span class="math notranslate nohighlight">\(50\)</span> students all go to the second school.</p>
<div class="admonition-thought-exercise admonition">
<p class="admonition-title">Thought Exercise</p>
<p>Before you read on, try to think to yourself about what the node-assignment vector <span class="math notranslate nohighlight">\(\vec z\)</span> looks like.</p>
</div>
<p>Next, let’s plot what <span class="math notranslate nohighlight">\(\vec z\)</span> look like:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="k">def</span> <span class="nf">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Node&quot;</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;skyblue&quot;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">((</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">tau</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([</span><span class="s1">&#39;School 1&#39;</span><span class="p">,</span> <span class="s1">&#39;School 2&#39;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="n">xlab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mf">49.5</span><span class="p">,</span><span class="mf">99.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># number of students</span>

<span class="c1"># tau is a column vector of 150 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 300 students are from</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Tau, Node Assignment Vector&quot;</span><span class="p">,</span>
        <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_SBM_4_0.png" src="_images/single-network-models_SBM_4_0.png" />
</div>
</div>
<p>So as we can see, the first <span class="math notranslate nohighlight">\(50\)</span> students are from the first school, and the second <span class="math notranslate nohighlight">\(50\)</span> students are from second school.</p>
<p>Let’s assume that the students from the first school are better friends in general than the students from the second school, so we’ll say that the probability of two students who both go to the first school being friends is <span class="math notranslate nohighlight">\(0.5\)</span>, and the probability of two students who both go to school <span class="math notranslate nohighlight">\(2\)</span> being friends is <span class="math notranslate nohighlight">\(0.3\)</span>. Finally, let’s assume that if one student goes to the first school and the other student goes to school <span class="math notranslate nohighlight">\(2\)</span>, that the probability that they are friends is <span class="math notranslate nohighlight">\(0.2\)</span>.</p>
<div class="admonition-thought-exercise admonition">
<p class="admonition-title">Thought Exercise</p>
<p>Before you read on, try to think to yourself about what the block matrix <span class="math notranslate nohighlight">\(B\)</span> looks like.</p>
</div>
<p>Next, let’s look at the block matrix <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 2 communities in total</span>
<span class="c1"># construct the block matrix B as described above</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> 
     <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_block</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">blockname</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">blocktix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span>
               <span class="n">blocklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;School 1&quot;</span><span class="p">,</span> <span class="s2">&quot;School 2&quot;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_block</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_SBM_7_0.png" src="_images/single-network-models_SBM_7_0.png" />
</div>
</div>
<p>As we can see, the matrix <span class="math notranslate nohighlight">\(B\)</span> is a symmetric block matrix, since our network is undirected. Finally, let’s sample a single network from the <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> with parameters <span class="math notranslate nohighlight">\(\vec z\)</span> and <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># sample a graph from SBM_{300}(tau, B)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">)],</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">ys</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$SBM_n(z, B)$ Simulation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_SBM_9_0.png" src="_images/single-network-models_SBM_9_0.png" />
</div>
</div>
<p>The above network shows students, ordered by the school they are in (first school and the second school, respectively). As we can see in the above network, people from the first school are more connected than people from school <span class="math notranslate nohighlight">\(2\)</span>. Also, the connections between people from different schools (the <em>off-diagonal</em> blocks of the adjacency matrix, the lower left and upper right blocks) appear to be a bit <em>more sparse</em> (fewer edges) than connections betwen schools (the <em>on-diagonal</em> blocks of the adjacency matrix, the upper left and lower right blocks). The above heatmap can be described as <strong>modular</strong>: it has clear communities. Remember that the connections for each node are indicated by a single row, or a single column, of the adjacency matrix. The first half of the rows have strong connections with the first half of the columns, which indicates that the first half of students tend to be better friends with other students in the first half. We can duplicate this argument for the second half of students ot see that it seems reasonable to conclude that there are two communities of students here.</p>
<p>Something easy to mistake about a realization of an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> is that the realizations will <em>not always</em> have the obvious modular structure we can see above when we look at a heatmap. Rather, this modular structure is <em>only</em> made obvious because the students are ordered according to the school in which they are in. What do you think will happen if we look at the students in a random order? Do you think that he structure that exists in this network will be obvious?</p>
<p>The answer is: <em>No!</em> Let’s see what happens when we reorder the nodes from the network into a random order, and pretend we don’t know the true community labels ahead of time:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># generate a reordering of the n nodes</span>
<span class="n">vtx_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">Aperm</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">vtx_perm</span><span class="p">])]</span> <span class="p">[:,</span><span class="n">vtx_perm</span><span class="p">]</span>
<span class="n">yperm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ys</span><span class="p">)[</span><span class="n">vtx_perm</span><span class="p">]</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">Aperm</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$SBM_n(z, B)$ Simulation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_SBM_11_0.png" src="_images/single-network-models_SBM_11_0.png" />
</div>
</div>
<p>Now, the students are <em>not</em> organized according to school, because they have been randomly reordered. It becomes pretty tough to figure out whether there are communities just by looking at an adjacency matrix, unless you are looking at a network in which the nodes are <em>already arranged</em> in an order which respects the community structure. By an <em>order that respects the community structure</em>, we mean that the community assignment vector <span class="math notranslate nohighlight">\(\vec z\)</span> is arranged so that all of the nodes in the first community come first, followed by all of the nodes in the second community, followed by all of the nodes in the third community, so on and so forth up to the nodes of the community <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p>In practice, this means that if you know ahead of time what natural groupings of the nodes might be (such as knowing which school each student goes to) by way of your node attributes, you can visualize your data according to that grouping. If you don’t know anything about natural groupings of nodes, however, we are left with the problem of <em>estimating community structure</em>. A later method, called the <em>spectral embedding</em>, will be paired with clustering techniques to allow us to estimate node assignment vectors.</p>
</div>
</div>
</div>
<span id="document-representations/ch5/single-network-models_RDPG"></span><div class="section" id="random-dot-product-graphs-rdpg">
<h3><span class="section-number">2.4. </span>Random Dot Product Graphs (RDPG)<a class="headerlink" href="#random-dot-product-graphs-rdpg" title="Permalink to this headline">¶</a></h3>
<p>In this section, we are going to discuss the Random Dot Product Graph (RDPG), a further generalization of the random network models we have studied. With the RDPG, we can have random networks which are much more complex than those we saw with the <span class="math notranslate nohighlight">\(ER_n(p)\)</span> and the <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random neworks, but <em>still</em> have a discernable structure to them. For example, here’s a realization from an RDPG random network:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># the number of nodes in our network</span>

<span class="c1"># design the latent position matrix X according to </span>
<span class="c1"># the rules we laid out previously</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">n</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">n</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;RDPG Simulation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_2_0.png" src="_images/single-network-models_RDPG_2_0.png" />
</div>
</div>
<div class="section" id="rethinking-the-stocahstic-block-model-with-a-probability-matrix">
<h4><span class="section-number">2.4.1. </span>Rethinking the Stocahstic Block Model with a Probability Matrix<a class="headerlink" href="#rethinking-the-stocahstic-block-model-with-a-probability-matrix" title="Permalink to this headline">¶</a></h4>
<p>Let’s imagine that we have a network which follows the Stochastic Block Model. To make this example a little bit more concrete, let’s borrow the code example from the <span class="xref myst">section on Stochastic Block Models</span>. The nodes of our network represent each of the <span class="math notranslate nohighlight">\(100\)</span> students in our network. Remember that <span class="math notranslate nohighlight">\(z\)</span> is the community assignment vector, which indicates which community (one of two schools) each node (student) is in. Here, the first <span class="math notranslate nohighlight">\(50\)</span> students attend school <span class="math notranslate nohighlight">\(1\)</span>, and the second <span class="math notranslate nohighlight">\(50\)</span> students attend school <span class="math notranslate nohighlight">\(2\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="k">def</span> <span class="nf">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Node&quot;</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;skyblue&quot;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">((</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">tau</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([</span><span class="s1">&#39;School 1&#39;</span><span class="p">,</span> <span class="s1">&#39;School 2&#39;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="n">xlab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mf">49.5</span><span class="p">,</span><span class="mf">99.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># number of students</span>

<span class="c1"># tau is a column vector of 150 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 300 students are from</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Tau, Node Assignment Vector&quot;</span><span class="p">,</span>
        <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_4_0.png" src="_images/single-network-models_RDPG_4_0.png" />
</div>
</div>
<p>And the block probability matrix <span class="math notranslate nohighlight">\(B\)</span> is a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix, where each entry <span class="math notranslate nohighlight">\(b_{kl}\)</span> indicates the probability that a node assigned to community <span class="math notranslate nohighlight">\(k\)</span> is connected to a node assigned to community <span class="math notranslate nohighlight">\(l\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 2 communities in total</span>
<span class="c1"># construct the block matrix B as described above</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]</span>

<span class="k">def</span> <span class="nf">plot_block</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">blockname</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">blocktix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span>
               <span class="n">blocklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;School 1&quot;</span><span class="p">,</span> <span class="s2">&quot;School 2&quot;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_block</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_6_0.png" src="_images/single-network-models_RDPG_6_0.png" />
</div>
</div>
<p>Are there any other ways to describe this scenario, other than using both the community assignment vector <span class="math notranslate nohighlight">\(\vec z\)</span> and the block matrix <span class="math notranslate nohighlight">\(B\)</span>?</p>
<div class="section" id="probability-matrices-explicitly-state-the-probability-for-each-edge">
<h5><span class="section-number">2.4.1.1. </span>Probability Matrices Explicitly State the Probability for each Edge<a class="headerlink" href="#probability-matrices-explicitly-state-the-probability-for-each-edge" title="Permalink to this headline">¶</a></h5>
<p>Remember, for a given <span class="math notranslate nohighlight">\(\vec z\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, that a network which is SBM can be generated using the approach that, given that <span class="math notranslate nohighlight">\(z_i = \ell\)</span> and <span class="math notranslate nohighlight">\(z_j = k\)</span>, each edge <span class="math notranslate nohighlight">\((i, j)\)</span> comes down to a coin flip, where the edge exists if the coin lands on heads with probability <span class="math notranslate nohighlight">\(b_{z_i z_j}\)</span> or does not exist if the coin lands on tails with probability <span class="math notranslate nohighlight">\(1- b_{z_i z_j}\)</span>. However, there’s another way we could write down this generative model. Suppose we had a <span class="math notranslate nohighlight">\(n \times n\)</span> probability matrix, where for every <span class="math notranslate nohighlight">\(j &gt; i\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p_{ji} = p_{ij}, p_{ij} = \begin{cases}
        b_{11} &amp; z_i = 1, z_j = 1 \\
        b_{12} &amp; z_i = 1, z_j = 2 \\
        b_{22} &amp; z_i = 2, z_j = 2
    \end{cases}
\end{align*}\]</div>
<p>This matrix <span class="math notranslate nohighlight">\(P\)</span> with entries <span class="math notranslate nohighlight">\(p_{ij}\)</span> is the probability matrix associated with the SBM. Simply put, this matrix describes the probability <span class="math notranslate nohighlight">\(p_{ij}\)</span> of each edge <span class="math notranslate nohighlight">\((i,j)\)</span> between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> existing. What does <span class="math notranslate nohighlight">\(P\)</span> look like?</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">plot_prob</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">plot_prob_block_annot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">X_annot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;U3&#39;</span><span class="p">)</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0.5&#39;</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mi">75</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0.3&#39;</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">75</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0.2&#39;</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0.2&#39;</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_annot</span><span class="p">),</span>
                        <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">P</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">3</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
<span class="n">P</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span>
              <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_8_0.png" src="_images/single-network-models_RDPG_8_0.png" />
</div>
</div>
<p>As we can see, <span class="math notranslate nohighlight">\(P\)</span> captures a similar modular structure to the actual adjacency matrix corresponding to the SBM network. When we say this network is <em>modular</em>, we mean that it looks block-y, in that there are clusters of edges sharing a similar probability. Also, <span class="math notranslate nohighlight">\(P\)</span> captures the probability of connections between each pair of students. It is the case that <span class="math notranslate nohighlight">\(P\)</span> contains the information of both <span class="math notranslate nohighlight">\(\vec z\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. This means that we can write down a generative model by specifying <em>only</em> <span class="math notranslate nohighlight">\(P\)</span>, and we no longer need to specify <span class="math notranslate nohighlight">\(\vec z\)</span> nor <span class="math notranslate nohighlight">\(B\)</span> at all.</p>
<p>To represent an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network using <span class="math notranslate nohighlight">\(P\)</span> uses a lot more space than using the community assignment vector <span class="math notranslate nohighlight">\(\vec z\)</span> and the block matrix <span class="math notranslate nohighlight">\(B\)</span> alone: <span class="math notranslate nohighlight">\(\vec z\)</span> has <span class="math notranslate nohighlight">\(n\)</span> entries, and <span class="math notranslate nohighlight">\(B\)</span> has <span class="math notranslate nohighlight">\(K \times K\)</span> entries, where <span class="math notranslate nohighlight">\(K\)</span> is typically much smaller than <span class="math notranslate nohighlight">\(n\)</span>. On the other hand, in this formulation, <span class="math notranslate nohighlight">\(P\)</span> has <span class="math notranslate nohighlight">\(\binom{n}{2}\)</span> entries, which is much bigger than <span class="math notranslate nohighlight">\(n + K \times K\)</span> (since <span class="math notranslate nohighlight">\(K\)</span> is usually much smaller than <span class="math notranslate nohighlight">\(n\)</span>). One thing you might be wondering, however, is can we make things even <em>more</em> general by using <span class="math notranslate nohighlight">\(P\)</span> instead of <span class="math notranslate nohighlight">\(\vec z\)</span> and <span class="math notranslate nohighlight">\(B\)</span>? As we can see in our probability matrix above, there are only <span class="math notranslate nohighlight">\(4\)</span> unique entries in total! Can we make <span class="math notranslate nohighlight">\(P\)</span> any more general, and still have a statistical model that simplifies the network with fewer than <span class="math notranslate nohighlight">\(\binom{n}{2}\)</span> entries?</p>
</div>
<div class="section" id="decomposing-the-probability-matrix-into-simpler-components">
<h5><span class="section-number">2.4.1.2. </span>Decomposing the Probability Matrix into Simpler Components<a class="headerlink" href="#decomposing-the-probability-matrix-into-simpler-components" title="Permalink to this headline">¶</a></h5>
<p>As it turns out, for a Stochastic Block Model, the probability matrix <span class="math notranslate nohighlight">\(P\)</span> can be decomposed using a matrix <span class="math notranslate nohighlight">\(X\)</span>, where <span class="math notranslate nohighlight">\(P = X X^\top\)</span>. This matrix <span class="math notranslate nohighlight">\(X\)</span> is a special matrix called the latent position matrix, which we will discuss further in this section and in the <span class="xref myst">section on estimation</span>. We will call a single row of <span class="math notranslate nohighlight">\(X\)</span> the vector <span class="math notranslate nohighlight">\(\vec x_i\)</span>. Using this expression, each entry <span class="math notranslate nohighlight">\(p_{ij}\)</span> is going to end up being the product <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span>, for all <span class="math notranslate nohighlight">\(i, j\)</span>. Like <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(X\)</span> has <span class="math notranslate nohighlight">\(n\)</span> rows, each of which corresponds to a single node in our network. However, the special property of <span class="math notranslate nohighlight">\(X\)</span> is that it doesn’t <em>necessarily</em> have <span class="math notranslate nohighlight">\(n\)</span> columns: rather, <span class="math notranslate nohighlight">\(X\)</span> often will have many fewer columns than rows. For instance, with <span class="math notranslate nohighlight">\(P\)</span> from the example above on Stochastic Block Models, there in fact exists an <span class="math notranslate nohighlight">\(X\)</span> with just <span class="math notranslate nohighlight">\(2\)</span> columns that can be used to describe <span class="math notranslate nohighlight">\(P\)</span>. Let’s take a look at what <span class="math notranslate nohighlight">\(X\)</span> looks like. We won’t discuss how to compute this <span class="math notranslate nohighlight">\(X\)</span> just yet, but we’ll try to build some insight into what’s going on here:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>

<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">plot_latent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">,</span>
                <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimtix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimlabs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">lim_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="n">lim_max</span><span class="p">;</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">lim_max</span>
        <span class="n">X_annot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;U6&#39;</span><span class="p">)</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;divergent&quot;</span><span class="p">],</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span>
                        <span class="n">annot</span><span class="o">=</span><span class="n">X_annot</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">ylabel</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">dimtix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dimlabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">dimtix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">dimlabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_latent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span>
              <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_10_0.png" src="_images/single-network-models_RDPG_10_0.png" />
</div>
</div>
<p>One thing we will come back to in a second is the fact that <span class="math notranslate nohighlight">\(X\)</span> in this case is relatively simple: there are <em>only</em> <span class="math notranslate nohighlight">\(4\)</span> unique entries, even though there are <span class="math notranslate nohighlight">\(200\)</span> total entries in <span class="math notranslate nohighlight">\(X\)</span> (2 columns for each of <span class="math notranslate nohighlight">\(100\)</span> students).</p>
<p>Like we said previously, it turns out that <span class="math notranslate nohighlight">\(P\)</span> can be described using <em>only</em> this matrix <span class="math notranslate nohighlight">\(X\)</span>, as <span class="math notranslate nohighlight">\(P = XX^\top\)</span>! Let’s see this in action, by comparing the <span class="math notranslate nohighlight">\(P\)</span> we had above to <span class="math notranslate nohighlight">\(XX^\top\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P_XXt</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">P</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">P_XXt</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$XX^T$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_12_0.png" src="_images/single-network-models_RDPG_12_0.png" />
</div>
</div>
<p>As we can see, <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(XX^\top\)</span> are identical!</p>
<p>Describing the probability matrix with this matrix <span class="math notranslate nohighlight">\(X\)</span> lends itself to an even further generalization of our single network models. Like we said a few figures ago, <span class="math notranslate nohighlight">\(X\)</span> only has <span class="math notranslate nohighlight">\(4\)</span> unique entries, but there is no reason for this restriction really. The matrix <span class="math notranslate nohighlight">\(X\)</span> can have <em>any</em> number of unique entries, so long as the product of <span class="math notranslate nohighlight">\(X\)</span> and its transpose, <span class="math notranslate nohighlight">\(XX^\top\)</span>, ends up being a probability matrix (every entry is a number between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>).</p>
</div>
<div class="section" id="the-latent-position-matrix">
<h5><span class="section-number">2.4.1.3. </span>The Latent Position Matrix<a class="headerlink" href="#the-latent-position-matrix" title="Permalink to this headline">¶</a></h5>
<p>This matrix <span class="math notranslate nohighlight">\(X\)</span> is called the <strong>latent position matrix</strong>, and each row <span class="math notranslate nohighlight">\(\vec x_i\)</span> will be called the <strong>latent position of the node</strong> <span class="math notranslate nohighlight">\(i\)</span>. In matrix form, <span class="math notranslate nohighlight">\(X\)</span> looks like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
 X = \begin{bmatrix}
     \vdash &amp; \vec x_1 &amp; \dashv \\
     \vdash &amp; \vec x_2 &amp; \dashv \\
     &amp; \vdots &amp; \\
     \vdash &amp; \vec x_n &amp; \dashv
 \end{bmatrix}
\end{align*}\]</div>
<p>We will call the columns of <span class="math notranslate nohighlight">\(X\)</span> the <strong>latent dimensions</strong>, and the total number of columns that <span class="math notranslate nohighlight">\(X\)</span> has will be called the <strong>latent dimensionality</strong>. We will often use the letter <span class="math notranslate nohighlight">\(d\)</span> to denote the latent dimensionality of the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. For this reason, we say that <span class="math notranslate nohighlight">\(X\)</span> is an <span class="math notranslate nohighlight">\(n\)</span> row (one for each node) and <span class="math notranslate nohighlight">\(d\)</span> column (one for each latent dimension) matrix. Therefore, the latent position of the node <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(\vec x_i\)</span>, is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector. In a few words, the latent dimensionality describes the complexity that the resulting probability matrix <span class="math notranslate nohighlight">\(P = XX^\top\)</span> has: if there are more latent dimensions, <span class="math notranslate nohighlight">\(P\)</span> can look much more complicated than what we have seen thus far!</p>
<p>Let’s think about the latent position matrix in the context of our previous example with the SBM. A common way to explore the latent position matrix is to look at a heatmap (like we did above) or a scatter plot of the latent position matrix. Let’s think about what the scatter plot might look like. The latent dimensionality of the latent position matrix for the SBM example, it turns out, is <span class="math notranslate nohighlight">\(2\)</span>, because we have two total latent dimensions for our latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. We will take the latent dimensions, and make the first latent dimension the <span class="math notranslate nohighlight">\(x\)</span>-axis, and the second latent dimension the <span class="math notranslate nohighlight">\(y\)</span>-axis. We next plot, for each node <span class="math notranslate nohighlight">\(i\)</span>, a single point, whose <span class="math notranslate nohighlight">\(x\)</span>-coordinate will be the first latent dimension for the latent position of node <span class="math notranslate nohighlight">\(i\)</span>, and the <span class="math notranslate nohighlight">\(y\)</span>-coordinate will be the second latent dimension for the latent position of node <span class="math notranslate nohighlight">\(i\)</span>. In symbols, we will plot <span class="math notranslate nohighlight">\((x_{i1}, x_{i2})\)</span>, for each node <span class="math notranslate nohighlight">\(i\)</span>. This means that there will be <span class="math notranslate nohighlight">\(n\)</span>-total points shown in the plot below:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_latent_hm_sc</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">,</span>
                <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimtix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimlabs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;width_ratios&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">]})</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">lim_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="n">lim_max</span><span class="p">;</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">lim_max</span>
        <span class="n">X_annot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;U6&#39;</span><span class="p">)</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;divergent&quot;</span><span class="p">],</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span>
                        <span class="n">annot</span><span class="o">=</span><span class="n">X_annot</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Positions as Heatmap&quot;</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">ylabel</span><span class="p">)</span>

        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">99</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">X_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X_df</span> <span class="o">=</span> <span class="n">X_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;Latent Dimension 1&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;Latent Dimension 2&quot;</span><span class="p">})</span>
    <span class="n">comm_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1&quot;</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">50</span> <span class="k">else</span> <span class="s2">&quot;2&quot;</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]</span>
    <span class="n">X_df</span><span class="p">[</span><span class="s2">&quot;School&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">comm_list</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Latent Dimension 1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Latent Dimension 2&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Positions as Scatterplot&quot;</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_latent_hm_sc</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_14_0.png" src="_images/single-network-models_RDPG_14_0.png" />
</div>
</div>
<p>Why does our scatter plot look like it only has <span class="math notranslate nohighlight">\(2\)</span> points on it? Quite simply, for an SBM, <em>every</em> node within the same community has an <em>identical</em> latent position! This means that all of the nodes representing students who are in school <span class="math notranslate nohighlight">\(1\)</span> have the same latent positition vector as the other students in school <span class="math notranslate nohighlight">\(1\)</span>. Similarly, all of the nodes representing students who are in school <span class="math notranslate nohighlight">\(2\)</span> have the same latent position vector as the other students in school <span class="math notranslate nohighlight">\(2\)</span>. Even though it looks like there is only <span class="math notranslate nohighlight">\(1\)</span> point for each school community, there are really <span class="math notranslate nohighlight">\(50\)</span> total latent position vectors for each student within that community and they just happen to overlap. This does not need to be the case for the latent positions, as we will see in a more complicated example later on.</p>
</div>
</div>
<div class="section" id="the-random-dot-product-graph-rdpg-is-parametrized-by-a-latent-position-matrix">
<h4><span class="section-number">2.4.2. </span>The Random Dot Product Graph (RDPG) is parametrized by a latent position matrix<a class="headerlink" href="#the-random-dot-product-graph-rdpg-is-parametrized-by-a-latent-position-matrix" title="Permalink to this headline">¶</a></h4>
<p>Now that we have some intuition built up, let’s circle back to random networks. We will call this particular network model the Random Dot Product Graph (RDPG) model. The way that we can think of the <span class="math notranslate nohighlight">\(RDPG\)</span> random network is that the edges depend on a latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. For each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, we have a unique coin (we will call this the <span class="math notranslate nohighlight">\((i,j)\)</span> coin) which has a <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span> chance of landing on heads, and a <span class="math notranslate nohighlight">\(1 - \vec x_i^\top \vec x_j\)</span> chance of landing on tails. If the <span class="math notranslate nohighlight">\((i,j)\)</span> coin lands on heads, the edge between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> exists, and if the <span class="math notranslate nohighlight">\((i,j)\)</span> coin lands on tails, the edge between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> does not exist. As before, this coin flip is performed independent of the coin flips for all of the other edges. The notation we use here is just what we are used to in the preceding sections. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network which is <span class="math notranslate nohighlight">\(RDPG\)</span> with a latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, we say that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random network.</p>
</div>
<div class="section" id="how-do-we-simulate-realizations-of-rdpg-n-x-random-networks">
<h4><span class="section-number">2.4.3. </span>How do we simulate realizations of <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random networks?<a class="headerlink" href="#how-do-we-simulate-realizations-of-rdpg-n-x-random-networks" title="Permalink to this headline">¶</a></h4>
<p>The procedure below will produce for us a network <span class="math notranslate nohighlight">\(A\)</span>, which has nodes and edges, where the underlying random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random network:</p>
<div class="admonition-simulating-a-realization-from-an-rdpg-n-x-random-network admonition">
<p class="admonition-title">Simulating a realization from an <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random network</p>
<ol class="simple">
<li><p>Determine a latent position matrix, <span class="math notranslate nohighlight">\(X\)</span>, whose rows <span class="math notranslate nohighlight">\(\vec x_i\)</span> are the latent positions of the nodes in the network.</p></li>
<li><p>For each pair nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>:</p>
<ul class="simple">
<li><p>Obtain a weighted coin <span class="math notranslate nohighlight">\((i,j)\)</span> which has a probability of <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span> of landing on heads, and a <span class="math notranslate nohighlight">\(1 - \vec x_i^\top \vec x_j\)</span> probability of landing on tails.</p></li>
<li><p>Flip the <span class="math notranslate nohighlight">\((i,j)\)</span> coin, and if it lands on heads, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the adjacency matrix is <span class="math notranslate nohighlight">\(1\)</span>. If the coin lands on tails, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij} = 0\)</span>.</p></li>
</ul>
</li>
<li><p>The adjacency matrix we produce, <span class="math notranslate nohighlight">\(A\)</span>, is a realization of an <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random network.</p></li>
</ol>
</div>
<div class="section" id="working-out-edge-probabilities-using-a-latent-position-matrix">
<h5><span class="section-number">2.4.3.1. </span>Working out edge probabilities using a latent position matrix<a class="headerlink" href="#working-out-edge-probabilities-using-a-latent-position-matrix" title="Permalink to this headline">¶</a></h5>
<p>As we can see above, for the RDPG, we think of the edge probabilities <span class="math notranslate nohighlight">\(p_{ij}\)</span> as a weighted coin, which lands on heads according to the <em>dot product</em> <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span>. Let’s explore this in the context of our SBM example from above. Remember that the latent position matrix for our example looked like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">plot_latent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span>
              <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_16_0.png" src="_images/single-network-models_RDPG_16_0.png" />
</div>
</div>
<p>Remember that the first <span class="math notranslate nohighlight">\(50\)</span> students were in school <span class="math notranslate nohighlight">\(1\)</span>, and the second <span class="math notranslate nohighlight">\(50\)</span> students were in school <span class="math notranslate nohighlight">\(2\)</span>. Looking at the above matrix, we can see that for a student in school <span class="math notranslate nohighlight">\(1\)</span>, the latent position vector is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x_i &amp;= \begin{bmatrix}
        -0.672 \\
        -0.221
    \end{bmatrix}
\end{align*}\]</div>
<p>Then for a pair of students <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> who are both in school <span class="math notranslate nohighlight">\(1\)</span> (the community assignments <span class="math notranslate nohighlight">\(z_i\)</span> and <span class="math notranslate nohighlight">\(z_j\)</span> are both <span class="math notranslate nohighlight">\(1\)</span>), the probability they are friends is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    b_{11} = p_{ij} &amp;= \vec x_i^\top \vec x_j = \begin{bmatrix}
        -0.672 &amp;
        -0.221
    \end{bmatrix}\begin{bmatrix}
        -0.672 \\
        -0.221
    \end{bmatrix} \\
    &amp;= (-.672)^2 + (-.221)^2 = .5
\end{align*}\]</div>
<p>For a student in school <span class="math notranslate nohighlight">\(2\)</span>, the latent position vector is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x_i &amp;= \begin{bmatrix}
        -0.415 \\
        0.357
    \end{bmatrix}
\end{align*}\]</div>
<p>So for a pair of students <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> who are both in school <span class="math notranslate nohighlight">\(2\)</span> (the community assignments <span class="math notranslate nohighlight">\(z_i\)</span> and <span class="math notranslate nohighlight">\(z_j\)</span> are both <span class="math notranslate nohighlight">\(2\)</span>) the probability that they are friends is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    b_{11} = p_{ij} &amp;= \vec x_i^\top \vec x_j = \begin{bmatrix}
        -0.415 &amp;
        0.357
    \end{bmatrix}\begin{bmatrix}
        -0.415 \\
        0.357
    \end{bmatrix} \\
    &amp;= (-.415)^2 + (.357)^2 = .3
\end{align*}\]</div>
<p>For a pair of students <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> whho are in different schools:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
b_{12} =  b_{21} = p_{ij} &amp;= \vec x_i^\top \vec x_j = \vec x_j \vec x_i \\
&amp;= \begin{bmatrix}
        -0.672 &amp;
        -0.221
    \end{bmatrix}\begin{bmatrix}
        -0.415 \\
        0.357
    \end{bmatrix} \\
    &amp;= (-.672)\cdot(-.415) + (-.221)\cdot 0.357 = 0.2
\end{align*}\]</div>
<p>This shows that using only the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, we have been able to deduce the corresponding block matrix <span class="math notranslate nohighlight">\(B\)</span>, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    B &amp;= \begin{bmatrix}
        b_{11} &amp; b_{12} \\
        b_{21} &amp; b_{22}
    \end{bmatrix}
\end{align*}\]</div>
</div>
</div>
<div class="section" id="we-can-do-more-complicated-things-with-rdpgs-than-just-sbms">
<h4><span class="section-number">2.4.4. </span>We can do more complicated things with RDPGs than just SBMs<a class="headerlink" href="#we-can-do-more-complicated-things-with-rdpgs-than-just-sbms" title="Permalink to this headline">¶</a></h4>
<p>We will let <span class="math notranslate nohighlight">\(X\)</span> be a little more complex than in our preceding example. Our <span class="math notranslate nohighlight">\(X\)</span> will produce a <span class="math notranslate nohighlight">\(P\)</span> that still <em>somewhat</em> has a modular structure, but not quite as much as before. Let’s assume that we have <span class="math notranslate nohighlight">\(100\)</span> people who live along a very long road that is <span class="math notranslate nohighlight">\(100\)</span> miles long, and each person is <span class="math notranslate nohighlight">\(1\)</span> mile apart. The nodes of our network represent the people who live along our assumed street. The people at the ends of the street host large parties each week, and invite everyone else on the street to their parties. However, if someone lives closer to one party host, they are going to tend to more frequently go to that host’s parties than the other party host. Consequently, when someone lives near a party host, they are going to tend to be better friends with other people who go to that host’s parties more frequently. What could we use for <span class="math notranslate nohighlight">\(X\)</span>?</p>
<p>Remember that the latent positions for each node <span class="math notranslate nohighlight">\(i\)</span> are denoted by the vector <span class="math notranslate nohighlight">\(\vec x_i\)</span>. One possible approach would be to let each <span class="math notranslate nohighlight">\(\vec x_i\)</span> be defined as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x_i = \begin{bmatrix}
        \frac{100 - i}{100} \\
        \frac{i}{100}
    \end{bmatrix}
\end{align*}\]</div>
<p>For instance, <span class="math notranslate nohighlight">\(\vec x_1 = \begin{bmatrix}1 \\ 0\end{bmatrix}\)</span>, and <span class="math notranslate nohighlight">\(\vec x_{100} = \begin{bmatrix} 0 \\ 1\end{bmatrix}\)</span>. Note that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{1,100} = \vec x_1^\top \vec x_j = 1 \cdot 0 + 0 \cdot 1 = 0
\end{align*}\]</div>
<p>What happens in between?</p>
<p>Let’s consider another person, person <span class="math notranslate nohighlight">\(30\)</span>. Note that person <span class="math notranslate nohighlight">\(30\)</span> lives closer to person <span class="math notranslate nohighlight">\(1\)</span> than to person <span class="math notranslate nohighlight">\(100\)</span>.  Here, <span class="math notranslate nohighlight">\(\vec x_{30} = \begin{bmatrix} \frac{7}{10}\\ \frac{3}{10}\end{bmatrix}\)</span>. This gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{1,30} &amp;= \vec x_1^\top \vec x_{30} = \frac{7}{10}\cdot 1 + 0 \cdot \frac{3}{10} = \frac{7}{10} \\
p_{30, 100} &amp;= \vec x_{30}^\top x_{100} = \frac{7}{10} \cdot 0 + \frac{3}{10} \cdot 1 = \frac{3}{10}
\end{align*}\]</div>
<p>So this means that person <span class="math notranslate nohighlight">\(1\)</span> and person <span class="math notranslate nohighlight">\(30\)</span> have a <span class="math notranslate nohighlight">\(70\%\)</span> probability of being friends, but person <span class="math notranslate nohighlight">\(30\)</span> and <span class="math notranslate nohighlight">\(100\)</span> have onl6 a <span class="math notranslate nohighlight">\(30\%\)</span> probability of being friends.</p>
<p>Intuitively, it seems like our probability matrix <span class="math notranslate nohighlight">\(P\)</span> will capture the intuitive idea we described above. First, we’ll take a look at <span class="math notranslate nohighlight">\(X\)</span>, and then we’ll look at <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># the number of nodes in our network</span>

<span class="c1"># design the latent position matrix X according to </span>
<span class="c1"># the rules we laid out previously</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">n</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">n</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_lp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">ylab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">99</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;30&quot;</span><span class="p">,</span> <span class="s2">&quot;70&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_lp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix, X&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_19_0.png" src="_images/single-network-models_RDPG_19_0.png" />
</div>
</div>
<p>The latent position matrix <span class="math notranslate nohighlight">\(X\)</span> that we plotted above is <span class="math notranslate nohighlight">\(n \times d\)</span> dimensions. There are a number of approaches, other than looking at a heatmap of <span class="math notranslate nohighlight">\(X\)</span>, with which we can visualize <span class="math notranslate nohighlight">\(X\)</span> to derive insights as to its structure. When <span class="math notranslate nohighlight">\(d=2\)</span>, another popular visualization is to look at the latent positions, <span class="math notranslate nohighlight">\(\vec x_i\)</span>, as individual points in <span class="math notranslate nohighlight">\(2\)</span>-dimensional space. This will give us a scatter plot of <span class="math notranslate nohighlight">\(n\)</span> points, each of which has two coordinates. Each point is the latent position for a single node:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_latents</span><span class="p">(</span><span class="n">latent_positions</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ss</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">latent_positions</span><span class="p">[</span><span class="n">ss</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">latent_positions</span><span class="p">[</span><span class="n">ss</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                           <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Set1&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plot</span>

<span class="c1"># plot</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix, X&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_21_0.png" src="_images/single-network-models_RDPG_21_0.png" />
</div>
</div>
<p>The above scatter plot has been subsampled to show only every <span class="math notranslate nohighlight">\(2^{nd}\)</span> latent position, so that the individual <span class="math notranslate nohighlight">\(2\)</span>-dimensional latent positions are discernable. Due to the way we constructed <span class="math notranslate nohighlight">\(X\)</span>, the scatter plot would otherwise appear to be a line (due to points overlapping one another). The reason that the points fall along a vertical line when plotted as a vector is due to the method we used to construct entries of <span class="math notranslate nohighlight">\(X\)</span>, described above. Next, we will look at the probability matrix:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_prob</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix, P=$XX^T$&quot;</span><span class="p">,</span>
         <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;30&quot;</span><span class="p">,</span> <span class="s2">&quot;70&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">69</span><span class="p">,</span><span class="mi">99</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_23_0.png" src="_images/single-network-models_RDPG_23_0.png" />
</div>
</div>
<p>Finally, we will sample an RDPG:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="c1"># sample an RDPG with the latent position matrix</span>
<span class="c1"># created above</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$RDPG_</span><span class="si">{100}</span><span class="s2">(X)$ Simulation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_RDPG_25_0.png" src="_images/single-network-models_RDPG_25_0.png" />
</div>
</div>
</div>
</div>
<span id="document-representations/ch5/multi-network-models"></span><div class="section" id="multiple-network-models">
<h3><span class="section-number">2.5. </span>Multiple Network Models<a class="headerlink" href="#multiple-network-models" title="Permalink to this headline">¶</a></h3>
<p>Up to this point, we have studied network models which are useful for a single network. What do we do if we have multiple networks?</p>
<p>Let’s imagine that we have a company with <span class="math notranslate nohighlight">\(45\)</span> total employees. This company is a <em>real</em> innovator, and is focused on developing software for network machine learning for business use. <span class="math notranslate nohighlight">\(10\)</span> of these employees are company administrative executives, <span class="math notranslate nohighlight">\(25\)</span> of these employees are network machine learning experts, and <span class="math notranslate nohighlight">\(10\)</span> of these employees are marketing experts. For each day over the course of a full <span class="math notranslate nohighlight">\(30\)</span>-day month, we study the emails that go back and forth between the employees in the company. We summarize the emailing habits within the company using a nework, where the nodes of the network are employees, and the edges indicate the emailing behaviors between each pair of employees. An edge is said to exist if the two employees have exchanged an email on that day, and an edge does not exist if the two employees have not exchanged an email on that day. In most companies, it is common for employees in a similar role to tend to work more closely together, so we might expect that there is some level of a community structure to the emailing habits. For instance, if two employees are both network machine learning experts, they might exchange more emails between one another than a machine learning expert and a marketing expert. For the sake of this example, we will assume that the networks are organized such that the first day is a Monday, the second day is a Tuesday, so on and so forth. Let’s take a look at an example of some possible realizations of the first <span class="math notranslate nohighlight">\(3\)</span> days worth of emails. What we will see below is that all of the networks appear to have the same community organization, though on Wednesday, we will assume there was an executive board meeting, and in the morning leading up to the board meeting, the administrative executives of the company exchanged more emails than on other days. This is reflected in the fact that there are more emails going back and forth between administrative members in the network for “Wednesday”:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">B1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>

<span class="n">B1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">05</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">05</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">05</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">05</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">05</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">05</span>
<span class="n">B1</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.3</span>

<span class="n">B2</span> <span class="o">=</span> <span class="n">B1</span>

<span class="n">B3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">B1</span><span class="p">)</span>
<span class="n">B3</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B1</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ML&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Adm.&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Mar.&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">ns</span><span class="p">[</span><span class="mi">2</span><span class="p">])]</span>

<span class="n">A2</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B2</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">A3</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B3</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$A^{(1)}$ Monday Emails&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$A^{(2)}$ Tuesday Emails&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$A^{(3)}$ Wednesday Emails&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_1_0.png" src="_images/multi-network-models_1_0.png" />
</div>
</div>
<p>Remember that a random network has an adjacency matrix denoted by a boldfaced uppercase <span class="math notranslate nohighlight">\(\mathbf A\)</span>, and has network realizations <span class="math notranslate nohighlight">\(A\)</span> (which we just call “networks”). When we have multiple networks, we will need to be able to index them individually. For this reason, in this section, we will use the convention that a random network’s adjacency matrix is denoted by a boldfaced uppercase <span class="math notranslate nohighlight">\(\mathbf A^{(m)}\)</span>, where <span class="math notranslate nohighlight">\(m\)</span> tells us which network in our collection we are talking about. The capital letter <span class="math notranslate nohighlight">\(N\)</span> defines the <em>total</em> number of random networks in our collection. In our email example, since we have email networks for <span class="math notranslate nohighlight">\(30\)</span> days, <span class="math notranslate nohighlight">\(N\)</span> is <span class="math notranslate nohighlight">\(30\)</span>. When we use the letter <span class="math notranslate nohighlight">\(m\)</span> itself, we will typically be referring to an arbitrary random network amongst the collection of random networks, where <span class="math notranslate nohighlight">\(m\)</span> beween <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(N\)</span>. When we have <span class="math notranslate nohighlight">\(N\)</span> total networks, we will write down the entire <strong>collection of random networks</strong> using the notation <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\)</span>. With what we already know, for a random network <span class="math notranslate nohighlight">\(\mathbf A^{(m)}\)</span>, we would be able to use a single nework model to describe <span class="math notranslate nohighlight">\(\mathbf A^{(m)}\)</span>. This means, for instance, if we thought that each email network could be represented by an RDPG, that we would have a <em>different</em> latent position matrix <span class="math notranslate nohighlight">\(X^{(m)}\)</span> to define each of the <span class="math notranslate nohighlight">\(30\)</span> networks. In symbols, we would write that each <span class="math notranslate nohighlight">\(\mathbf A^{(m)}\)</span> is an RDPG random nework with latent position matrix <span class="math notranslate nohighlight">\(X^{(m)}\)</span>. What is the problem with this description?</p>
<p>What this description lacks is that, over the course of a given <span class="math notranslate nohighlight">\(30\)</span> days, a <em>lot</em> of the networks are going to show similar emailing patterns. We might expect that this implies that, on some level, the latent position matrices should also show some sort of common structure. However, since we used a <em>unique</em> latent position matrix <span class="math notranslate nohighlight">\(X^{(m)}\)</span> for each random network <span class="math notranslate nohighlight">\(\mathbf A^{(m)}\)</span>, we have inherently stated that we think that the networks have completely distinct latent position matrices. If we were to perform a task downstream, such as whether we could identify which employees are in which community, we would have to analyze each latent position matrix individually, and we would not be able to learn a latent position matrix with shared structure across all <span class="math notranslate nohighlight">\(30\)</span> days. Before we jump into multiple network models, let’s provide some context as tp how we will build these up.</p>
<p>In the below descriptions, we will tend to build off the Random Dot Product Graph (RDPG), and closely related variations of it. Why? Well, as it turns out, the RDPG is extremely flexible, in that we can represent both ER and SBMs as RDPGs, too. This means that building off the RDPG gives us multiple random network models that will be inherently flexible. Further, as we will see in the later section on <a class="reference external" href="#link?">Estimation</a>, the RDPG is extremely well-suited for the situation in which we want to analyze SBMs, but do not know which communities the nodes are in ahead of time. This situation is extremely common across numerous disciplines of network machine learning, such as social networking, neuroscience, and many other fields.</p>
<p>So, how can we model our collection of random networks with shared structure?</p>
<div class="section" id="joint-random-dot-product-graphs-jrdpg-model">
<h4><span class="section-number">2.5.1. </span>Joint Random Dot Product Graphs (JRDPG) Model<a class="headerlink" href="#joint-random-dot-product-graphs-jrdpg-model" title="Permalink to this headline">¶</a></h4>
<p>In our example on email networks, notice that the Monday and Tuesday emails do not look <em>too</em> qualitatively different. It looks like they have relatively similar connectivity patterns between the different employee working groups, and we might even think that the underlying random process that governs these two email networks are <em>identical</em>. In statisical science, when we have a collection of <span class="math notranslate nohighlight">\(N\)</span> random networks that have the same underlying random process, we describe this with the term <strong>homogeneity</strong>. Let’s put what this means into context using our coin flip example. If a pair of coins are <em>homogeneous</em>, this means that the probability that they land on heads is identical. Likewise, this intuition extends directly to random networks. A <strong>homogeneous</strong> collection of random networks <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\)</span> is one in which <em>all</em> of the <span class="math notranslate nohighlight">\(N\)</span> random networks have the <em>same probability matrix</em>. Remember that the probability matrix <span class="math notranslate nohighlight">\(P^{(m)}\)</span> is the matrix whose entries <span class="math notranslate nohighlight">\(p^{(m)}_{ij}\)</span> indicate the probability that an edge exists between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. On the other hand, a <strong>heterogeneous</strong> collection of random networks is a collection of networks <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\)</span> is one in which the probability matrices are <em>not</em> the same for all of the <span class="math notranslate nohighlight">\(N\)</span> networks.</p>
<p>The probability matrices <span class="math notranslate nohighlight">\(P^{(1)}\)</span> and <span class="math notranslate nohighlight">\(P^{(2)}\)</span> for the random networks <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(2)}\)</span> for Monday and Tuesday are shown in the following figure. We also show the difference between the two probability matrices, to make really clear that they are the same:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">p_from_block</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">ns</span><span class="p">):</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ns</span><span class="p">)))</span>
    <span class="n">ns_cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ns</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">n1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="mi">0</span><span class="p">:(</span><span class="nb">len</span><span class="p">(</span><span class="n">ns_cumsum</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]):</span>
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">n2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">k</span><span class="p">:(</span><span class="nb">len</span><span class="p">(</span><span class="n">ns_cumsum</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]):</span>
            <span class="n">P</span><span class="p">[</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">ns_cumsum</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">k</span><span class="p">]:</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">l</span><span class="o">+</span><span class="n">k</span><span class="p">]</span>
            <span class="n">P</span><span class="p">[</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">k</span><span class="p">]:</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span><span class="n">ns_cumsum</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">k</span><span class="p">,</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">P</span>

<span class="n">P1</span> <span class="o">=</span> <span class="n">p_from_block</span><span class="p">(</span><span class="n">B1</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">U1</span><span class="p">,</span> <span class="n">S1</span><span class="p">,</span> <span class="n">V1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">U1</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S1</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>

<span class="n">P2</span> <span class="o">=</span> <span class="n">p_from_block</span><span class="p">(</span><span class="n">B2</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">U2</span><span class="p">,</span> <span class="n">S2</span><span class="p">,</span> <span class="n">V2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">P2</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">U2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S2</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>

<span class="n">P3</span> <span class="o">=</span> <span class="n">p_from_block</span><span class="p">(</span><span class="n">B3</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">U3</span><span class="p">,</span> <span class="n">S3</span><span class="p">,</span> <span class="n">V3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">P3</span><span class="p">)</span>
<span class="n">X3</span> <span class="o">=</span> <span class="n">U3</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S3</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">plot_prob_block_annot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Employee&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">annot</span><span class="p">:</span>
            <span class="n">X_annot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;U4&#39;</span><span class="p">)</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">13</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">13</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">30</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">40</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">40</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">30</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">13</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">13</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">40</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">23</span><span class="p">,</span><span class="mi">40</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">13</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">13</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">]))</span>
            <span class="n">X_annot</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">30</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">30</span><span class="p">]))</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_annot</span><span class="p">),</span>
                            <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">P1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P^{(1)}$ Monday Prob. Matrix&quot;</span><span class="p">)</span>
<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">P2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P^{(2)}$ Tuesday Prob. Matrix&quot;</span><span class="p">)</span>
<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">P1</span> <span class="o">-</span> <span class="n">P2</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">annot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|P^{(1)} - P^{(2)}|$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_3_0.png" src="_images/multi-network-models_3_0.png" />
</div>
</div>
<p>The Joint Random Dot Product Graphs (JRDPG) is the simplest way we can extend the RDPG random network model to multiple random networks. The way we can think of the JRDPG model is that for each of our <span class="math notranslate nohighlight">\(N\)</span> total random neworks, the edges depend on a latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. We say that a collection of random networks <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\)</span> with <span class="math notranslate nohighlight">\(n\)</span> nodes is <span class="math notranslate nohighlight">\(JRDPG_n(X)\)</span> if each random network <span class="math notranslate nohighlight">\(\mathbf A^{(m)}\)</span> is <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> and if the <span class="math notranslate nohighlight">\(N\)</span> networks are independent.</p>
<div class="section" id="the-jrdpg-model-does-not-allow-us-to-convey-unique-aspects-about-the-networks">
<h5><span class="section-number">2.5.1.1. </span>The JRDPG model does not allow us to convey unique aspects about the networks<a class="headerlink" href="#the-jrdpg-model-does-not-allow-us-to-convey-unique-aspects-about-the-networks" title="Permalink to this headline">¶</a></h5>
<p>Under the JRDPG model, each of the <span class="math notranslate nohighlight">\(N\)</span> random networks share the same latent position matrix. Remember that for an RDPG, the probability matrix <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. This means that for all of the <span class="math notranslate nohighlight">\(N\)</span> networks, <span class="math notranslate nohighlight">\(P^{(m)} = XX^\top\)</span> under the JRDPG model. hence, <span class="math notranslate nohighlight">\(P^{(1)} = P^{(2)} = ... = P^{(N)}\)</span>, and <em>all</em> of the probability matrices are <em>identical</em>! This means that the <span class="math notranslate nohighlight">\(N\)</span> random networks are <strong>homogeneous</strong>.</p>
<p>For an RDPG, since <span class="math notranslate nohighlight">\(P = XX^\top\)</span>, the probability matrix depends <em>only</em> on the latent positions <span class="math notranslate nohighlight">\(X\)</span>. This means that we can tell whether a collection of networks are homogeneous just by looking at the latent position matrices! It turns out that the random networks underlying the realizations for the email networks in our given example were just SBMs. From the section on <a class="reference external" href="#link?">Random Dot Product Graphs</a>, we learned that SBMs are just RDPGs with a special latent position matrix. Let’s try this first by looking at the latent position matrices for <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(2)}\)</span> from the random networks for Monday and Tuesday first:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>

<span class="k">def</span> <span class="nf">plot_latent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Employee&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">,</span>
                <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimtix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimlabs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">lim_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">vmin</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="n">lim_max</span>
        <span class="k">if</span> <span class="n">vmax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">vmax</span> <span class="o">=</span> <span class="n">lim_max</span>
        <span class="n">X_annot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">45</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;U6&#39;</span><span class="p">)</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;divergent&quot;</span><span class="p">],</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span>
                        <span class="n">annot</span><span class="o">=</span><span class="n">X_annot</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">ylabel</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">dimtix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dimlabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">dimtix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">dimlabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">vmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span><span class="n">X2</span><span class="p">,</span><span class="n">X3</span><span class="p">]);</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="n">plot_latent</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">44</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;35&quot;</span><span class="p">,</span> <span class="s2">&quot;45&quot;</span><span class="p">],</span>
            <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$X^{(1)}$ Monday LPM&quot;</span><span class="p">)</span>
<span class="n">plot_latent</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">44</span><span class="p">],</span> <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;35&quot;</span><span class="p">,</span> <span class="s2">&quot;45&quot;</span><span class="p">],</span>
            <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$X^{(2)}$ Tuesday LPM&quot;</span><span class="p">)</span>
<span class="n">plot_latent</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X1</span> <span class="o">-</span> <span class="n">X2</span><span class="p">),</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">44</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;35&quot;</span><span class="p">,</span> <span class="s2">&quot;45&quot;</span><span class="p">],</span>
            <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|X^{(1)} - X^{(2)}|$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_5_0.png" src="_images/multi-network-models_5_0.png" />
</div>
</div>
<p>So the latent position matrices for Monday and Tuesday are exactly identical. Therefore, the collection of random networks <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, \mathbf A^{(2)}\right\}\)</span> are homogeneous, and we could model this pair of networks using the JRDPG.</p>
<p>What about Wednesday? Well, as it turns out, Wednesday had a <em>different</em> latent position matrix from both Monday and Tuesday:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">vmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span><span class="n">X2</span><span class="p">,</span><span class="n">X3</span><span class="p">]);</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="n">plot_latent</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">44</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;35&quot;</span><span class="p">,</span> <span class="s2">&quot;45&quot;</span><span class="p">],</span>
            <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$X^{(1)}$ Monday LPM&quot;</span><span class="p">)</span>
<span class="n">plot_latent</span><span class="p">(</span><span class="n">X3</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">44</span><span class="p">],</span> <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;35&quot;</span><span class="p">,</span> <span class="s2">&quot;45&quot;</span><span class="p">],</span>
            <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$X^{(3)}$ Wednesday LPM&quot;</span><span class="p">)</span>
<span class="n">plot_latent</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X1</span> <span class="o">-</span> <span class="n">X3</span><span class="p">),</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">44</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;35&quot;</span><span class="p">,</span> <span class="s2">&quot;45&quot;</span><span class="p">],</span>
            <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|X^{(1)} - X^{(3)}|$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_7_0.png" src="_images/multi-network-models_7_0.png" />
</div>
</div>
<p>So, <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(3)}\)</span> do not have the same latent position matrices. This means that the collections of random networks <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, \mathbf A^{(3)}\right\}\)</span>, <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(2)}, \mathbf A^{(3)}\right\}\)</span>, and <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, \mathbf A^{(2)}, \mathbf A^{(3)}\right\}\)</span> are <em>heterogeneous</em>, because their probability matrices will be different. So, unfortunately, the JRDPG cannot handle the hetereogeneity between the random networks of Monday and Tuesday with the random network for Wednesday. To remove this restrictive homogeneity property of the JRDPG, we will need a new single network model, called the Inhomogeneous Erdos-Renyi (IER) random network model.</p>
</div>
<div class="section" id="the-inhomogeous-erdos-renyi-ier-random-network">
<h5><span class="section-number">2.5.1.2. </span>The Inhomogeous Erdos-Renyi (IER) Random Network<a class="headerlink" href="#the-inhomogeous-erdos-renyi-ier-random-network" title="Permalink to this headline">¶</a></h5>
<p>The IER random network is the most general random network model for a binary graph. The way we can think of the <span class="math notranslate nohighlight">\(IER\)</span> random network is that a probability matrix <span class="math notranslate nohighlight">\(P\)</span> with <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns defines each of the edge-existence probabilities for pairs of nodes in the network. For each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, we have a unique coin which has a <span class="math notranslate nohighlight">\(p_{ij}\)</span> chance of landing on heads, and a <span class="math notranslate nohighlight">\(1 - p_{ij}\)</span> chance of landing on tails. If the coin lands on heads, the edge between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> exists, and if the coin lands on tails, the edge between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> does not exist. This coin flip is performed independently of the coin flips for all of the other edges. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network which is <span class="math notranslate nohighlight">\(IER\)</span> with a probability matrix <span class="math notranslate nohighlight">\(P\)</span>, we say that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(IER_n(P)\)</span> random network.</p>
<p>As before, we can develop a procedure to produce for us a network <span class="math notranslate nohighlight">\(A\)</span>, which has nodes and edges, where the underlying random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(IER_n(P)\)</span> random network:</p>
<div class="admonition-simulating-a-realization-from-an-ier-n-p-random-network admonition">
<p class="admonition-title">Simulating a realization from an <span class="math notranslate nohighlight">\(IER_n(P)\)</span> random network</p>
<ol class="simple">
<li><p>Determine a probability matrix <span class="math notranslate nohighlight">\(P\)</span>, whose entries <span class="math notranslate nohighlight">\(p_{ij}\)</span> are probabilities.</p></li>
<li><p>For each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> an <span class="math notranslate nohighlight">\(j\)</span>:</p>
<ul class="simple">
<li><p>Obtain a weighted coin <span class="math notranslate nohighlight">\((i,j)\)</span> which has a probability <span class="math notranslate nohighlight">\(p_{ij}\)</span> of landing on heads, and a <span class="math notranslate nohighlight">\(1 - p_{ij}\)</span> probability of landing on tails.</p></li>
<li><p>Flip the <span class="math notranslate nohighlight">\((i,j)\)</span> coin, andd if it lands on heads, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the adjacency matrix is <span class="math notranslate nohighlight">\(1\)</span>. If the coin lands on tails, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> is <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
</ul>
</li>
<li><p>The adjacency matrix we produce, <span class="math notranslate nohighlight">\(A\)</span>, is a realization of an <span class="math notranslate nohighlight">\(IER_n(P)\)</span> random network.</p></li>
</ol>
</div>
<p>It is important to realize that all of the networks we have described so far are also <span class="math notranslate nohighlight">\(IER_n(P)\)</span> random networks. The previous single network models we have described to date simply place restrictions on the way in which we acquire <span class="math notranslate nohighlight">\(P\)</span>. For instance, in an <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random network, all entries <span class="math notranslate nohighlight">\(p_{ij}\)</span> of <span class="math notranslate nohighlight">\(P\)</span> are equal to <span class="math notranslate nohighlight">\(p\)</span>. To see that an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network is also <span class="math notranslate nohighlight">\(IER_n(P)\)</span>, we can construct the probability matrix <span class="math notranslate nohighlight">\(P\)</span> such that <span class="math notranslate nohighlight">\(p_{ij} = b_{kl}\)</span> of the block matrix <span class="math notranslate nohighlight">\(B\)</span> when the community of node <span class="math notranslate nohighlight">\(i\)</span> <span class="math notranslate nohighlight">\(z_i = k\)</span> and the community of node <span class="math notranslate nohighlight">\(j\)</span> <span class="math notranslate nohighlight">\(z_j = l\)</span>. To see that an <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random network is also <span class="math notranslate nohighlight">\(IER_n(P)\)</span>, we can construct the probability matrix <span class="math notranslate nohighlight">\(P\)</span> such that <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. This shows that the IER random network is the most general of the single network models we have studied so far. Next, let’s see how the IER random network can help us address this heterogeneity.</p>
</div>
</div>
<div class="section" id="common-subspace-independent-edge-cosie-model">
<h4><span class="section-number">2.5.2. </span>Common Subspace Independent Edge (COSIE) Model<a class="headerlink" href="#common-subspace-independent-edge-cosie-model" title="Permalink to this headline">¶</a></h4>
<p>In our example on email networks, notice that the Monday and Wednesday emails looked relatively similar, but had an important difference: on Wednesday, there was an administrative meeting, and the employees on the administrative team exchanged far more emails than usual amongst one another. It turns out that, in fact, the random networks <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(3)}\)</span> which underly the email networks <span class="math notranslate nohighlight">\(A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(A^{(3)}\)</span> were also different, because the probability matrices were different:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">P1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P^{(1)}$ Monday Prob. Matrix&quot;</span><span class="p">)</span>
<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">P3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P^{(3)}$ Wednesday Prob. Matrix&quot;</span><span class="p">)</span>
<span class="n">plot_prob_block_annot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">P1</span> <span class="o">-</span> <span class="n">P3</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|P^{(1)} - P^{(3)}|$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_9_0.png" src="_images/multi-network-models_9_0.png" />
</div>
</div>
<p>Notice that the difference in the probability of an email being exchanged between the members of the administrative team is <span class="math notranslate nohighlight">\(0.6\)</span> higher in the probability matrix for Wednesday than for Monday. This is because the random networks <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(3)}\)</span> are heterogeneous. To reflect this heterogeneity, we will need to turn to the COmmon Subspace Independent Edge (COSIE) model.</p>
<div class="section" id="the-cosie-model-is-defined-by-a-collection-of-score-matrices-and-a-shared-low-rank-subspace">
<h5><span class="section-number">2.5.2.1. </span>The COSIE Model is defined by a collection of score matrices and a shared low-rank subspace<a class="headerlink" href="#the-cosie-model-is-defined-by-a-collection-of-score-matrices-and-a-shared-low-rank-subspace" title="Permalink to this headline">¶</a></h5>
<p>Even though <span class="math notranslate nohighlight">\(P^{(1)}\)</span> and <span class="math notranslate nohighlight">\(P^{(3)}\)</span> are not <em>identical</em>, we can see they still share <em>some</em> structure: the employee teams are the same between the two email networks, and much of the probability matrix is unchanged. For this reason, it will be useful for us to have a network model which allows us to convey <em>some</em> shared structure, but still lets us convey aspects of the different networks which are <em>unique</em>. The COSIE model will accomplish this using a <em>shared latent position matrix</em>, and unique <em>score matrices</em> for each of the random networks.</p>
<div class="section" id="the-shared-latent-position-matrix-describes-similarities">
<h6><span class="section-number">2.5.2.1.1. </span>The Shared Latent Position Matrix Describes Similarities<a class="headerlink" href="#the-shared-latent-position-matrix-describes-similarities" title="Permalink to this headline">¶</a></h6>
<p>The <em>shared latent position matrix</em> for the COSIE model is quite similar to the latent position matrix for an RDPG. Like the latent position matrix, the shared latent position matrix <span class="math notranslate nohighlight">\(V\)</span> is a matrix with <span class="math notranslate nohighlight">\(n\)</span> rows (one for each node) and <span class="math notranslate nohighlight">\(d\)</span> columns. The <span class="math notranslate nohighlight">\(d\)</span> columns behave very similarly to the <span class="math notranslate nohighlight">\(d\)</span> columns ffor the latent position matrix of an RDPG, and <span class="math notranslate nohighlight">\(d\)</span> is referred to as the <em>latent dimensionality</em> of the COSIE random networks. Like before, each row of the shared latent position matrix <span class="math notranslate nohighlight">\(v_i\)</span> will be referred to as the shared latent position vector for node <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>We will also add an additional restriction to <span class="math notranslate nohighlight">\(V\)</span>: it will be a matrix with orthonormal columns. What this means is that for each column of <span class="math notranslate nohighlight">\(V\)</span>, the dot product of the column with itself is <span class="math notranslate nohighlight">\(1\)</span>, and the dot product of the column with any other column is <span class="math notranslate nohighlight">\(0\)</span>. This has the implication that <span class="math notranslate nohighlight">\(V^\top V = I\)</span>, the identity matrix.</p>
<p>The shared latent position matrix conveys the <em>common structure</em> between the COSIE random networks, and will be a parameter for each of the neworks. Remember that with the <span class="math notranslate nohighlight">\(JRDPG_n(X)\)</span> model, we were able to capture the homogeneity of the email networks on Monday and Tuesday, but we could not capture the heterogeneity of the email nework on Wednesday. However, we want the shared latent position matrix <span class="math notranslate nohighlight">\(V\)</span> to convey the commonality amongst the three email networks; that is, that the employees are are always working on the same employee teams. Let’s take a look at the shared latent position matrix <span class="math notranslate nohighlight">\(V\)</span> for the email example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">MultipleASE</span> <span class="k">as</span> <span class="n">MASE</span>

<span class="n">embedder</span> <span class="o">=</span> <span class="n">MASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">P1</span><span class="p">,</span> <span class="n">P2</span><span class="p">,</span> <span class="n">P3</span><span class="p">])</span>

<span class="n">plot_latent</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">34</span><span class="p">,</span><span class="mi">44</span><span class="p">],</span> <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;35&quot;</span><span class="p">,</span> <span class="s2">&quot;45&quot;</span><span class="p">],</span>
            <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$V$, Shared Latent Positions&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_11_0.png" src="_images/multi-network-models_11_0.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\(V\)</span> reflects the fact that the first <span class="math notranslate nohighlight">\(25\)</span> employees are the ML experts, and all <span class="math notranslate nohighlight">\(25\)</span> have the same latent position vector. The next <span class="math notranslate nohighlight">\(10\)</span> employees are the administrative members, and share a different latent position vector from the ML experts. Finally, the last <span class="math notranslate nohighlight">\(10\)</span> employees are the marketing members, share a different latent position vector from the ML experts and the administrative team. In this way, the orthonormal matrix <span class="math notranslate nohighlight">\(V\)</span> has conveyed the community structure (the employee roles and who they tend to email amongst) that is shared across all three days.</p>
</div>
<div class="section" id="score-matrices-describe-differences">
<h6><span class="section-number">2.5.2.1.2. </span>Score Matrices Describe Differences<a class="headerlink" href="#score-matrices-describe-differences" title="Permalink to this headline">¶</a></h6>
<p>The <em>score matrices</em> for the COSIE random networks essentially tell us how to assemble the shared latent position matrix to obtain the unique probability matrix for each network. The score matrix <span class="math notranslate nohighlight">\(R^{(m)}\)</span> for a random network <span class="math notranslate nohighlight">\(m\)</span> is a matrix with <span class="math notranslate nohighlight">\(d\)</span> columns and <span class="math notranslate nohighlight">\(d\)</span> rows. Therefore, it is a square matrix whose number of dimensions is equal to the latent dimensionality of the COSIE random networks.</p>
<p>The probability matrix for each network under the COSIE model is the matrix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P^{(m)} &amp;= VR^{(m)}V^\top
\end{align*}\]</div>
<p>In our email example, we want the score matrices to reflect that Monday and Tuesday share a probability matrix, but Monday and Wednesday do not. Consequently, we would expect that the score matrices from Monday and Tuesday should be the same, but the score matrix for Wednesday will be different:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">blockname</span><span class="o">=</span><span class="s2">&quot;Employee Group&quot;</span><span class="p">,</span> <span class="n">blocktix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span>
               <span class="n">blocklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ML&quot;</span><span class="p">,</span> <span class="s2">&quot;Adm.&quot;</span><span class="p">,</span> <span class="s2">&quot;Mark.&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">vmin</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">scores_</span><span class="p">);</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">R1</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:]</span>
<span class="n">R2</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,:]</span>
<span class="n">R3</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="mi">2</span><span class="p">,:,:]</span>

<span class="n">plot_score</span><span class="p">(</span><span class="n">R1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$R^{(1)}$ Monday Scores&quot;</span><span class="p">)</span>
<span class="n">plot_score</span><span class="p">(</span><span class="n">R2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$R^{(2)}$ Tuesday Scores&quot;</span><span class="p">)</span>
<span class="n">plot_score</span><span class="p">(</span><span class="n">R3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$R^{(3)}$ Wednesday Scores&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_13_0.png" src="_images/multi-network-models_13_0.png" />
</div>
</div>
<p>As we can see, the scores <span class="math notranslate nohighlight">\(R^{(1)}\)</span> and <span class="math notranslate nohighlight">\(R^{(2)}\)</span> are the same for Monday and Tuesday, but different for <span class="math notranslate nohighlight">\(R^{(3)}\)</span> Wednesday.</p>
<p>Finally, let’s relate all of this back to the COSIE model. The way we can think about the COSIE model is that for each random network <span class="math notranslate nohighlight">\(m\)</span> of our <span class="math notranslate nohighlight">\(N\)</span> total networks, the probability matrix <span class="math notranslate nohighlight">\(P^{((m)}\)</span> depends on the shared latent position matrix <span class="math notranslate nohighlight">\(V\)</span> and the score matrix <span class="math notranslate nohighlight">\(R^{(m)}\)</span>. The probability matrix <span class="math notranslate nohighlight">\(P^{(m)}\)</span> for the <span class="math notranslate nohighlight">\(m^{th}\)</span> random network is defined so that <span class="math notranslate nohighlight">\(P^{(m)} = VR^{(m)}V^\top\)</span>. This means that each entry <span class="math notranslate nohighlight">\(p_{ij}^{(m)} = \vec v_i^\top R^{(m)} \vec v_j\)</span>. We say that a collection of random networks <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\)</span> with <span class="math notranslate nohighlight">\(n\)</span> nodes is <span class="math notranslate nohighlight">\(COSIE_n\left(V, \left\{R^{(1)},...,R^{(N)}\right\}\right)\)</span> if each random network <span class="math notranslate nohighlight">\(\mathbf A^{(m)}\)</span> is <span class="math notranslate nohighlight">\(IER(P^{(m)})\)</span>. Stated another way, each of the <span class="math notranslate nohighlight">\(N\)</span> random networks share the same orthonormal matrix <span class="math notranslate nohighlight">\(V\)</span>, but a unique score matrix <span class="math notranslate nohighlight">\(R^{(m)}\)</span>. This allows the random networks to share some underlying structure (which is conveyed by <span class="math notranslate nohighlight">\(V\)</span>) but each random network still has a combination of this shared structure (conveyed by <span class="math notranslate nohighlight">\(R^{(m)}\)</span>).</p>
<p>Since the probability matrix <span class="math notranslate nohighlight">\(P^{(m)} = VR^{(m)}V^\top\)</span>, we can see that two random networks with the same score matrix will be homogeneous, and two random networks with different score matrices will be heterogeneous. In this way, we were able to capture the homogeneity between the random networks for Monday and Tuesday emails, while also capturing the heterogeneity between the random networks for Monday and Wednesday emails.</p>
</div>
</div>
</div>
<div class="section" id="correlated-network-models">
<h4><span class="section-number">2.5.3. </span>Correlated Network Models<a class="headerlink" href="#correlated-network-models" title="Permalink to this headline">¶</a></h4>
<p>Finally, we get to a special case of network models, known as correlated network models. Let’s say that we have a group of people in a city, and we know that each people in our group have both a Facebook and a Twitter. The nodes in our network are the people we have. The first network consists of Facebook connections amongst the people, where an edge exists between two people if they are friends on Facebook. The second network consists of Twitter connections amongst the people, where an edge exists between two people if they follow one another on Twitter. We think that if two people are friends on Facebook, there is a good chance that they follow one another on Twitter, and vice versa. How do we reflect this similarity through a multiple network model?</p>
<p>At a high level, network correlation between a pair of networks describes the property that the existence of edges in one network provides us with some level of information about edges in the other network, much like the Facebook/Twitter example we just discussed. In this book, we will focus on the <span class="math notranslate nohighlight">\(\rho\)</span>-<em>correlated</em> network models. What the <span class="math notranslate nohighlight">\(\rho\)</span>-correlated network models focus on is that given two random networks with the same number of nodes, each edge has a correlation of <span class="math notranslate nohighlight">\(\rho\)</span> between the two networks. To define this a little more rigorously, a pair of random networks <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(2)}\)</span> are called <span class="math notranslate nohighlight">\(\rho\)</span>-<strong>correlated</strong> if all of the edges across both networks are mutually independent, except that for all pairs of indices <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(corr(\mathbf a_{ij}^{(1)}, \mathbf a_{ij}^{(2)}) = \rho\)</span>, where <span class="math notranslate nohighlight">\(corr(\mathbf x, \mathbf y)\)</span> is the Pearson correlation between two random variables <span class="math notranslate nohighlight">\(\mathbf x\)</span> and <span class="math notranslate nohighlight">\(\mathbf y\)</span>. In our example, this means that whether two people are friends on Facebook is <em>correlated</em> with whether they are following one another on Twitter.</p>
<p>At a high level, the Pearson correlation describes whether one variable being large/small gives information that the other variable is large/small (positive correlation, between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>) or whether one variable being large/small gives information that the other variable will be small/large (negative correlation, between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(0\)</span>). If the two networks are positively correlated and we know that one of the edges <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(1)}\)</span> has a value of one, then we have information that <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(2)}\)</span> might also be one, and vice-versa for taking values of zero. If the two networks are negatively correlated and we know that one of the edges <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(1)}\)</span> has a value of one, then we have information that <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(2)}\)</span> might be zero, and vice-versa. If the two networks are not correlated (<span class="math notranslate nohighlight">\(\rho = 0\)</span>) we do not learn anything about edges of network two by looking at edges from network one.</p>
<div class="section" id="rho-correlated-rdpg">
<h5><span class="section-number">2.5.3.1. </span><span class="math notranslate nohighlight">\(\rho\)</span>-Correlated RDPG<a class="headerlink" href="#rho-correlated-rdpg" title="Permalink to this headline">¶</a></h5>
<p>The <span class="math notranslate nohighlight">\(\rho\)</span>-correlated RDPG is the most general correlated network model we will need for the purposes of this book. Remembering that both ER and SBM random networks are special cases of the RDPG (for a given choice of the latent position matrix), the <span class="math notranslate nohighlight">\(\rho\)</span>-correlated RDPG can therefore be used to construct <span class="math notranslate nohighlight">\(\rho\)</span>-correlated ER and <span class="math notranslate nohighlight">\(\rho\)</span>-correlated SBMs, too. The way we can think about the <span class="math notranslate nohighlight">\(\rho\)</span>-correlated RDPG is that like for the normal RDPG, a latent position matrix <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(n\)</span> rows and a latent dimensionality of <span class="math notranslate nohighlight">\(d\)</span> is used to define the edge-existence probabilities for the networks <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(2)}\)</span>. We begin by defining that <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> is <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span>. Next, we define the second network as follows. We use a coin for each edge <span class="math notranslate nohighlight">\((i, j)\)</span>, which has a probability that depends on the values that the first network takes. If the edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(1)}\)</span> takes the value of one, then we use a coin which has a probability of landing on heads of <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j + \rho(1 - \vec x_i^\top \vec x_j)\)</span>. If the edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(1)}\)</span> takes the value of zero, then we use a coin which has a probability of landing on heads of <span class="math notranslate nohighlight">\((1 - \rho)\vec x_i^\top \vec x_j\)</span>. We flip this coin, and if it lands on heads, then the edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(2)}\)</span> takes the value of one. If it lands on tails, then the edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(2)}\)</span> takes the value of zero. If <span class="math notranslate nohighlight">\(\mathbf A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(A^{(2)}\)</span> are random networks which are <span class="math notranslate nohighlight">\(\rho\)</span>-correlated RDPGs with latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, we say that the pair <span class="math notranslate nohighlight">\(\left\{\mathbf A^{(1)}, A^{(2)}\right\}\)</span> are <span class="math notranslate nohighlight">\(\rho-RDPG_n(X)\)</span>.</p>
<div class="admonition-simulating-realizations-of-rho-correlated-rdpgs admonition">
<p class="admonition-title">Simulating realizations of <span class="math notranslate nohighlight">\(\rho\)</span>-correlated RDPGs</p>
<ol class="simple">
<li><p>Determine a latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, where rows <span class="math notranslate nohighlight">\(\vec x_i\)</span> are the latent position vectors for the nodes <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>Determine a correlation between the two networks of <span class="math notranslate nohighlight">\(\rho\)</span>, where <span class="math notranslate nohighlight">\(\rho \geq -1\)</span> and <span class="math notranslate nohighlight">\(\rho \leq 1\)</span>.</p></li>
<li><p>Simulate a realization <span class="math notranslate nohighlight">\(A^{(1)}\)</span> which is a realization  of an <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random network.</p></li>
<li><p>For each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>:</p>
<ul class="simple">
<li><p>If the edge <span class="math notranslate nohighlight">\(a_{ij}^{(1)}\)</span> has a value of one, obtain a coin which has a probability of landing on heads of <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j + \rho(1 - \vec x_i^\top \vec x_j)\)</span>. If the edge <span class="math notranslate nohighlight">\(a_{ij}^{(2)}\)</span> has a value of zero, obtain a coin which has a probability of landing on heads of <span class="math notranslate nohighlight">\((1 - \rho)\vec x_i^\top \vec x_j\)</span>.</p></li>
<li><p>Flip the <span class="math notranslate nohighlight">\((i,j)\)</span> coin, andd if it lands on heads, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}^{(2)}\)</span> in the adjacency matrix is <span class="math notranslate nohighlight">\(1\)</span>. If the coin lands on tails, the corresponding entry <span class="math notranslate nohighlight">\(a_{ij}^{(2)}\)</span> is <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
</ul>
</li>
<li><p>The adjacency matrices <span class="math notranslate nohighlight">\(A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(A^{(2)}\)</span> are realizations of <span class="math notranslate nohighlight">\(\rho-RDPG_n(X)\)</span> random networks.</p></li>
</ol>
</div>
<p>Fortunately, graspologic makes sampling <span class="math notranslate nohighlight">\(\rho\)</span>-correlated RDPGs relatively simple. Let’s say that in our Facebook/Twitter example, we have <span class="math notranslate nohighlight">\(100\)</span> people across two schools, like our standard example from the SBM section. The first <span class="math notranslate nohighlight">\(50\)</span> students attend school one, and the second <span class="math notranslate nohighlight">\(50\)</span> students attend school two. To recap, the latent position matrix looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># deine a probability matrix for a stochastic block model with two communities</span>
<span class="c1"># where the first 50 students are from community one and the second 50 students are</span>
<span class="c1"># from community two</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">P</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">3</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
<span class="n">P</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>

<span class="c1"># use the singular value decomposition to obtain the corresponding latent</span>
<span class="c1"># position matrix</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="k">def</span> <span class="nf">plot_latent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">,</span>
                <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimtix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dimlabs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">lim_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="n">lim_max</span><span class="p">;</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">lim_max</span>
        <span class="n">X_annot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;U6&#39;</span><span class="p">)</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">X_annot</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;divergent&quot;</span><span class="p">],</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">,</span>
                        <span class="n">annot</span><span class="o">=</span><span class="n">X_annot</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">ylabel</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">dimtix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dimlabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">dimtix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">dimlabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_latent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span>
              <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">dimtix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="n">dimlabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_17_0.png" src="_images/multi-network-models_17_0.png" />
</div>
</div>
<p>To sample two networks which are <span class="math notranslate nohighlight">\(\rho\)</span>-correlated SBMs, let’s assume that the correlation between the two networks is high, so we will assume <span class="math notranslate nohighlight">\(\rho = 0.7\)</span>. We use the graspologic function to obtain a realization for each network. We show the two networks, as well as the edges which are different between the two networks. We summarize this edge difference plot with <span class="math notranslate nohighlight">\(diff(A^{(F)} - A^{(T)})\)</span>, which simply counts the number of edges which are different:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg_corr</span>

<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">A_facebook</span><span class="p">,</span> <span class="n">A_twitter</span> <span class="o">=</span> <span class="n">rdpg_corr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">rho</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;2&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_facebook</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$A^{(F)}$ Facebook Network&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_twitter</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$A^{(T)}$ Twitter Network&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A_facebook</span> <span class="o">-</span> <span class="n">A_twitter</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$diff(A^{(F)} - A^{(T)}) = </span><span class="si">%d</span><span class="s2">, </span><span class="se">\\</span><span class="s2">rho = 0.7$&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A_facebook</span> <span class="o">-</span> <span class="n">A_twitter</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> 
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_20_0.png" src="_images/multi-network-models_20_0.png" />
</div>
</div>
<p>On the other hand, if the correlation were <span class="math notranslate nohighlight">\(\rho = 0\)</span> (the two networks are uncorrelated), we can see that the number of edges which are different is much higher:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rho</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">A_facebook</span><span class="p">,</span> <span class="n">A_twitter</span> <span class="o">=</span> <span class="n">rdpg_corr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">rho</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;2&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_facebook</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$A^{(F)}$ Facebook Network&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_twitter</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$A^{(T)}$ Twitter Network&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A_facebook</span> <span class="o">-</span> <span class="n">A_twitter</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$diff(A^{(F)} - A^{(T)}) = </span><span class="si">%d</span><span class="s2">, </span><span class="se">\\</span><span class="s2">rho = 0$&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A_facebook</span> <span class="o">-</span> <span class="n">A_twitter</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multi-network-models_23_0.png" src="_images/multi-network-models_23_0.png" />
</div>
</div>
</div>
</div>
</div>
<span id="document-representations/ch5/models-with-covariates"></span><div class="section" id="network-models-with-covariates">
<h3><span class="section-number">2.6. </span>Network Models with Covariates<a class="headerlink" href="#network-models-with-covariates" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-representations/ch5/single-network-models_theory"></span><div class="section" id="single-network-model-theory">
<h3><span class="section-number">2.7. </span>Single network model theory<a class="headerlink" href="#single-network-model-theory" title="Permalink to this headline">¶</a></h3>
<div class="section" id="foundation">
<h4><span class="section-number">2.7.1. </span>Foundation<a class="headerlink" href="#foundation" title="Permalink to this headline">¶</a></h4>
<p>To understand network models, it is crucial to understand the concept of a network as a random quantity, taking a probability distribution. We have a realization <span class="math notranslate nohighlight">\(A\)</span>, and we think that this realization is random in some way. Stated another way, we think that there exists a network-valued random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> that governs the realizations we get to see. Since <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random variable, we can describe it using a probability distribution. The distribution of the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the function <span class="math notranslate nohighlight">\(\mathbb P\)</span> which assigns probabilities to every possible configuration that <span class="math notranslate nohighlight">\(\mathbf A\)</span> could take. Notationally, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P\)</span>, which is read in words as “the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is distributed according to <span class="math notranslate nohighlight">\(\mathbb P\)</span>.”</p>
<p>In the preceding description, we made a fairly substantial claim: <span class="math notranslate nohighlight">\(\mathbb P\)</span> assigns probabilities to every possible configuration that realizations of <span class="math notranslate nohighlight">\(\mathbf A\)</span>, denoted by <span class="math notranslate nohighlight">\(A\)</span>, could take. How many possibilities are there for a network with <span class="math notranslate nohighlight">\(n\)</span> nodes? Let’s limit ourselves to simple networks: that is, <span class="math notranslate nohighlight">\(A\)</span> takes values that are unweighted (<span class="math notranslate nohighlight">\(A\)</span> is <em>binary</em>), undirected (<span class="math notranslate nohighlight">\(A\)</span> is <em>symmetric</em>), and loopless (<span class="math notranslate nohighlight">\(A\)</span> is <em>hollow</em>). In words, <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> is the set of all possible adjacency matrices <span class="math notranslate nohighlight">\(A\)</span> that correspond to simple networks with <span class="math notranslate nohighlight">\(n\)</span> nodes. Stated another way: every <span class="math notranslate nohighlight">\(A\)</span> that is found in <span class="math notranslate nohighlight">\(\mathcal A\)</span> is a <em>binary</em> <span class="math notranslate nohighlight">\(n \times n\)</span> matrix (<span class="math notranslate nohighlight">\(A \in \{0, 1\}^{n \times n}\)</span>), <span class="math notranslate nohighlight">\(A\)</span> is symmetric (<span class="math notranslate nohighlight">\(A = A^\top\)</span>), and <span class="math notranslate nohighlight">\(A\)</span> is <em>hollow</em> (<span class="math notranslate nohighlight">\(diag(A) = 0\)</span>, or <span class="math notranslate nohighlight">\(A_{ii} = 0\)</span> for all <span class="math notranslate nohighlight">\(i = 1,...,n\)</span>). We describe <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal A_n = \left\{A : A \textrm{ is an $n \times n$ matrix with $0$s and $1$s}, A\textrm{ is symmetric}, A\textrm{ is hollow}\right\}
\end{align*}\]</div>
<p>To summarize the statement that <span class="math notranslate nohighlight">\(\mathbb P\)</span> assigns probabilities to every possible configuration that realizations of <span class="math notranslate nohighlight">\(\mathbf A\)</span> can take, we write that <span class="math notranslate nohighlight">\(\mathbb P : \mathcal A_n \rightarrow [0, 1]\)</span>. This means that for any <span class="math notranslate nohighlight">\(A \in \mathcal A_n\)</span> which is a possible realization of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>, that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf A = A)\)</span> is a probability (it takes a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>). If it is completely unambiguous what the random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> refers to, we might abbreviate <span class="math notranslate nohighlight">\(\mathbb P(\mathbf A = A)\)</span> with <span class="math notranslate nohighlight">\(\mathbb P(A)\)</span>. This statement can alternatively be read that the probability that the random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> takes the value <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(\mathbb P(A)\)</span>. Finally, let’s address that question we had in the previous paragraph. How many possible adjacency matrices are in <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>?</p>
<p>Let’s imagine what just one <span class="math notranslate nohighlight">\(A \in \mathcal A_n\)</span> can look like. Note that each matrix <span class="math notranslate nohighlight">\(A\)</span> has <span class="math notranslate nohighlight">\(n \times n = n^2\)</span> possible entries, in total, since <span class="math notranslate nohighlight">\(A\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix. There are <span class="math notranslate nohighlight">\(n\)</span> possible self-loops for a network, but since <span class="math notranslate nohighlight">\(\mathbf A\)</span> is simple, it is loopless. This means that we can subtract <span class="math notranslate nohighlight">\(n\)</span> possible edges from <span class="math notranslate nohighlight">\(n^2\)</span>, leaving us with <span class="math notranslate nohighlight">\(n^2 - n = n(n-1)\)</span> possible edges that might not be unconnected. If we think in terms of a realization <span class="math notranslate nohighlight">\(A\)</span>, this means that we are ignoring the diagonal entries <span class="math notranslate nohighlight">\(a_{ii}\)</span>, for all <span class="math notranslate nohighlight">\(i \in [n]\)</span>.  Remember that a simple network is also undirected. In terms of the realization <span class="math notranslate nohighlight">\(A\)</span>, this means that for every pair <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, that <span class="math notranslate nohighlight">\(a_{ij} = a_{ji}\)</span>. If we were to learn about an entry in the upper triangle of <span class="math notranslate nohighlight">\(A\)</span> where <span class="math notranslate nohighlight">\(a_{ij}\)</span> is such that <span class="math notranslate nohighlight">\(j &gt; i\)</span>, note that we have also learned what <span class="math notranslate nohighlight">\(a_{ji}\)</span> is, too. This symmetry of <span class="math notranslate nohighlight">\(A\)</span> means that of the <span class="math notranslate nohighlight">\(n(n-1)\)</span> entries that are not on the diagonal of <span class="math notranslate nohighlight">\(A\)</span>, we would, in fact, “double count” the possible number of unique values that <span class="math notranslate nohighlight">\(A\)</span> could have. This means that <span class="math notranslate nohighlight">\(A\)</span> has a total of <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span> possible entries which are <em>free</em>, which is equal to the expression <span class="math notranslate nohighlight">\(\binom{n}{2}\)</span>. Finally, note that for each entry of <span class="math notranslate nohighlight">\(A\)</span>, that the adjacency can take one of two possible values: <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. To write this down formally, for every possible edge which is randomly determined, we have <em>two</em> possible values that edge could take. Let’s think about building some intuition here:</p>
<ol class="simple">
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(2 \times 2\)</span>, there are <span class="math notranslate nohighlight">\(\binom{2}{2} = 1\)</span> unique entry of <span class="math notranslate nohighlight">\(A\)</span>, which takes one of <span class="math notranslate nohighlight">\(2\)</span> values. There are <span class="math notranslate nohighlight">\(2\)</span> possible ways that <span class="math notranslate nohighlight">\(A\)</span> could look:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \begin{bmatrix}
        0 &amp; 1 \\
        1 &amp; 0
    \end{bmatrix}\textrm{ or }
    \begin{bmatrix}
        0 &amp; 0 \\
        0 &amp; 0
    \end{bmatrix}
\end{align*}\]</div>
<ol class="simple">
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(3 \times 3\)</span>, there are <span class="math notranslate nohighlight">\(\binom{3}{2} = \frac{3 \times 2}{2} = 3\)</span> unique entries of <span class="math notranslate nohighlight">\(A\)</span>, each of which takes one of <span class="math notranslate nohighlight">\(2\)</span> values. There are <span class="math notranslate nohighlight">\(8\)</span> possible ways that <span class="math notranslate nohighlight">\(A\)</span> could look:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{bmatrix}
    0 &amp; 1 &amp; 1 \\
    1 &amp; 0 &amp; 1 \\
    1 &amp; 1 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 1 &amp; 0 \\
    1 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 1 \\
    0 &amp; 0 &amp; 1 \\
    1 &amp; 1 &amp; 0
    \end{bmatrix}
    \textrm{ or }\\
&amp;\begin{bmatrix}
    0 &amp; 1 &amp; 1 \\
    1 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 1 \\
    0 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0
    \end{bmatrix}\textrm{ or }\\
&amp;\begin{bmatrix}
    0 &amp; 1 &amp; 0 \\
    1 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0
    \end{bmatrix}
\end{align*}\]</div>
<p>How do we generalize this to an arbitrary choice of <span class="math notranslate nohighlight">\(n\)</span>? The answer is to use <em>combinatorics</em>. Basically, the approach is to look at each entry of <span class="math notranslate nohighlight">\(A\)</span> which can take different values, and multiply the total number of possibilities by <span class="math notranslate nohighlight">\(2\)</span> for every element which can take different values. Stated another way, if there are <span class="math notranslate nohighlight">\(2\)</span> choices for each one of <span class="math notranslate nohighlight">\(x\)</span> possible items, we have <span class="math notranslate nohighlight">\(2^x\)</span> possible ways in which we could select those <span class="math notranslate nohighlight">\(x\)</span> items. But we already know how many different elements there are in <span class="math notranslate nohighlight">\(A\)</span>, so we are ready to come up with an expression for the number. In total, there are <span class="math notranslate nohighlight">\(2^{\binom n 2}\)</span> unique adjacency matrices in <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>. Stated another way, the <em>cardinality</em> of <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>, described by the expression <span class="math notranslate nohighlight">\(|\mathcal A_n|\)</span>, is <span class="math notranslate nohighlight">\(2^{\binom n 2}\)</span>. The <strong>cardinality</strong> here just means the number of elements that the set <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> contains. When <span class="math notranslate nohighlight">\(n\)</span> is just <span class="math notranslate nohighlight">\(15\)</span>, note that <span class="math notranslate nohighlight">\(\left|\mathcal A_{15}\right| = 2^{\binom{15}{2}} = 2^{105}\)</span>, which when expressed as a power of <span class="math notranslate nohighlight">\(10\)</span>, is more than <span class="math notranslate nohighlight">\(10^{30}\)</span> possible networks that can be realized with just <span class="math notranslate nohighlight">\(15\)</span> nodes! As <span class="math notranslate nohighlight">\(n\)</span> increases, how many unique possible networks are there? In the below figure, look at the value of <span class="math notranslate nohighlight">\(|\mathcal A_n| = 2^{\binom n 2}\)</span> as a function of <span class="math notranslate nohighlight">\(n\)</span>. As we can see, as <span class="math notranslate nohighlight">\(n\)</span> gets big, <span class="math notranslate nohighlight">\(|\mathcal A_n|\)</span> grows really really fast!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">comb</span>


<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span>
<span class="n">logAn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">comb</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">ni</span> <span class="ow">in</span> <span class="n">n</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">logAn</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of Nodes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Possible Graphs $|A_n|$ (log scale)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">350</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;$10^{{</span><span class="si">{pow:d}</span><span class="s2">}}$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">pow</span><span class="o">=</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">350</span><span class="p">]])</span>
<span class="n">ax</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_theory_2_0.png" src="_images/single-network-models_theory_2_0.png" />
</div>
</div>
<p>So, now we know that we have probability distributions on networks, and a set <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> which defines all of the adjacency matrices that every probability distribution must assign a probability to. Now, just what is a network model? A <strong>network model</strong> is a set <span class="math notranslate nohighlight">\(\mathcal P\)</span> of probability distributions on <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>. Stated another way, we can describe <span class="math notranslate nohighlight">\(\mathcal P\)</span> to be:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal P &amp;\subseteq \{\mathbb P: \mathbb P\textrm{ is a probability distribution on }\mathcal A_n\}
\end{align*}\]</div>
<p>In general, we will simplify <span class="math notranslate nohighlight">\(\mathcal P\)</span> through something called <em>parametrization</em>. We define <span class="math notranslate nohighlight">\(\Theta\)</span> to be the set of all possible parameters of the random network model, and <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> is a particular parameter choice that governs the parameters of a specific network-valued random variaable <span class="math notranslate nohighlight">\(\mathbf A\)</span>. In this case, we will write <span class="math notranslate nohighlight">\(\mathcal P\)</span> as the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal P(\Theta) &amp;= \left\{\mathbb P_\theta : \theta \in \Theta\right\}
\end{align*}\]</div>
<p>If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network that follows a network model, we will write that <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P_\theta\)</span>, for some choice <span class="math notranslate nohighlight">\(\theta\)</span>. We will often use the shorthand <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P\)</span>.</p>
<p>If you are used to traditional univariate or multivariate statistical modelling, an extremely natural choice for when you have a discrete sample space (like <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>, which is discrete because we can count it) would be to use a categorical model. In the categorical model, we would have a single parameter for all possible configurations of an <span class="math notranslate nohighlight">\(n\)</span>-node network; that is, <span class="math notranslate nohighlight">\(|\theta| = \left|\mathcal A_n\right| = 2^{\binom n 2}\)</span>. What is wrong with this model? The limitations are two-fold:</p>
<ol class="simple">
<li><p>As we explained previously, when <span class="math notranslate nohighlight">\(n\)</span> is just <span class="math notranslate nohighlight">\(15\)</span>, we would need over <span class="math notranslate nohighlight">\(10^{30}\)</span> bits of storage just to define <span class="math notranslate nohighlight">\(\theta\)</span>. This amounts to more than <span class="math notranslate nohighlight">\(10^{8}\)</span> zetabytes, which exceeds the storage capacity of <em>the entire world</em>.</p></li>
<li><p>With a single network observed (or really, any number of networks we could collect in the real world) we would never be able to get a reasonable estimate of <span class="math notranslate nohighlight">\(2^{\binom n 2}\)</span> parameters for any reasonably non-trivial number of nodes <span class="math notranslate nohighlight">\(n\)</span>. For the case of one observed network <span class="math notranslate nohighlight">\(A\)</span>, an estimate of <span class="math notranslate nohighlight">\(\theta\)</span> (referred to as <span class="math notranslate nohighlight">\(\hat\theta\)</span>) would simply be for <span class="math notranslate nohighlight">\(\hat\theta\)</span> to have a <span class="math notranslate nohighlight">\(1\)</span> in the entry corresponding to our observed network, and a <span class="math notranslate nohighlight">\(0\)</span> everywhere else. Inferentially, this would imply that the network-valued random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> which governs realizations <span class="math notranslate nohighlight">\(A\)</span> is deterministic, even if this is not the case. Even if we collected potentially <em>many</em> observed networks, we would still (with very high probability) just get <span class="math notranslate nohighlight">\(\hat \theta\)</span> as a series of point masses on the observed networks we see, and <span class="math notranslate nohighlight">\(0\)</span>s everywhere else. This would mean our parameter estimates <span class="math notranslate nohighlight">\(\hat\theta\)</span> would not generalize to new observations at <em>all</em>, with high probability.</p></li>
</ol>
<p>So, what are some more reasonable descriptions of <span class="math notranslate nohighlight">\(\mathcal P\)</span>? We explore some choices below. Particularly, we will be most interested in the <em>independent-edge</em> networks. These are the families of networks in which the generative procedure which governs the random networks assume that the edges of the network are generated <em>independently</em>. <strong>Statistical Independence</strong> is a property which greatly simplifies many of the modelling assumptions which are crucial for proper estimation and rigorous statistical inference, which we will learn more about in the later chapters.</p>
<div class="section" id="equivalence-classes">
<h5><span class="section-number">2.7.1.1. </span>Equivalence Classes<a class="headerlink" href="#equivalence-classes" title="Permalink to this headline">¶</a></h5>
<p>In all of the below models, we will explore the concept of the <strong>probability equivalence class</strong>, or an <em>equivalence class</em>, for short. The probability is a function which in general, describes how effective a particular observation can be described by a random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> with parameters <span class="math notranslate nohighlight">\(\theta\)</span>, written <span class="math notranslate nohighlight">\(\mathbf A \sim F(\theta)\)</span>. The probability will be used to describe the probability <span class="math notranslate nohighlight">\(\mathbb P_\theta(\mathbf A)\)</span> of observing the realization <span class="math notranslate nohighlight">\(A\)</span> if the underlying random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> has parameters <span class="math notranslate nohighlight">\(\theta\)</span>.  Why does this matter when it comes to equivalence classes? An equivalence class is a subset of the sample space <span class="math notranslate nohighlight">\(E \subseteq \mathcal A_n\)</span>, which has the following properties. Holding the parameters <span class="math notranslate nohighlight">\(\theta\)</span> fixed:</p>
<ol class="simple">
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A'\)</span> are members of the same equivalence class <span class="math notranslate nohighlight">\(E\)</span> (written <span class="math notranslate nohighlight">\(A, A' \in E\)</span>), then <span class="math notranslate nohighlight">\(\mathbb P_\theta(A) = \mathbb P_\theta(A')\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A''\)</span> are members of different equivalence classes; that is, <span class="math notranslate nohighlight">\(A \in E\)</span> and <span class="math notranslate nohighlight">\(A'' \in E'\)</span> where <span class="math notranslate nohighlight">\(E, E'\)</span> are equivalence classes, then <span class="math notranslate nohighlight">\(\mathbb P_\theta(A) \neq \mathbb P_\theta(A'')\)</span>.</p></li>
<li><p>Using points 1 and 2, we can establish that if <span class="math notranslate nohighlight">\(E\)</span> and <span class="math notranslate nohighlight">\(E'\)</span> are two different equivalence classes, then <span class="math notranslate nohighlight">\(E \cap E' = \varnothing\)</span>. That is, the equivalence classes are <strong>mutually disjoint</strong>.</p></li>
<li><p>We can use the preceding properties to deduce that given the sample space <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> and a probability function <span class="math notranslate nohighlight">\(\mathbb P_\theta\)</span>, we can define a partition of the sample space into equivalence classes <span class="math notranslate nohighlight">\(E_i\)</span>, where <span class="math notranslate nohighlight">\(i \in \mathcal I\)</span> is an arbitrary indexing set. A <strong>partition</strong> of <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> is a sequence of sets which are mutually disjoint, and whose union is the whole space. That is, <span class="math notranslate nohighlight">\(\bigcup_{i \in \mathcal I} E_i = \mathcal A_n\)</span>.</p></li>
</ol>
<p>We will see more below about how the equivalence classes come into play with network models, and in a later section, we will see their relevance to the estimation of the parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</div>
<div class="section" id="independent-edge-random-networks">
<span id="representations-whyuse-networkmodels-iern"></span><h5><span class="section-number">2.7.1.2. </span>Independent-Edge Random Networks<a class="headerlink" href="#independent-edge-random-networks" title="Permalink to this headline">¶</a></h5>
<p>The below models are all special families of something called <strong>independent-edge random networks</strong>. An independent-edge random network is a network-valued random variable, in which the collection of edges are all independent. In words, this means that for every adjacency <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> of the network-valued random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is independent of <span class="math notranslate nohighlight">\(\mathbf a_{i'j'}\)</span>, any time that <span class="math notranslate nohighlight">\((i,j) \neq (i',j')\)</span>. When the networks are simple, the easiest thing to do is to assume that each edge <span class="math notranslate nohighlight">\((i,j)\)</span> is connected with some probability (which might be different for each edge) <span class="math notranslate nohighlight">\(p_{ij}\)</span>. We use the <span class="math notranslate nohighlight">\(ij\)</span> subscript to denote that this probability is not necessarily the same for each edge. This simple model can be described as <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> has the distribution <span class="math notranslate nohighlight">\(Bern(p_{ij})\)</span>, for every <span class="math notranslate nohighlight">\(j &gt; i\)</span>, and is independent of every other edge in <span class="math notranslate nohighlight">\(\mathbf A\)</span>. We only look at the entries <span class="math notranslate nohighlight">\(j &gt; i\)</span>, since our networks are simple. This means that knowing a realization of <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> also gives us the realizaaion of <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> (and thus <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> is a <em>deterministic</em> function of <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>). Further, we know that the random network is loopless, which means that every <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>. We will call the matrix <span class="math notranslate nohighlight">\(P = (p_{ij})\)</span> the <strong>probability matrix</strong> of the network-valued random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span>. In general, we will see a common theme for the probabilities of a realization <span class="math notranslate nohighlight">\(A\)</span> of a network-valued random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span>, which is that it will greatly simplify our computation. Remember that if <span class="math notranslate nohighlight">\(\mathbf x\)</span> and <span class="math notranslate nohighlight">\(\mathbf y\)</span> are binary variables which are independent, that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf x = x, \mathbf y = y) = \mathbb P(\mathbf x = x) \mathbb P(\mathbf y = y)\)</span>. Using this fact:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf A = A) &amp;= \mathbb P(\mathbf a_{11} = a_{11}, \mathbf a_{12} = a_{12}, ..., \mathbf a_{nn} = a_{nn}) \\
    &amp;= \mathbb P(\mathbf a_{ij} = a_{ij} \text{ for all }j &gt; i) \\
    &amp;= \prod_{j &gt; i}\mathbb P(\mathbf a_{ij} = a_{ij}), \;\;\;\;\textrm{Independence Assumption}
\end{align*}\]</div>
<p>Next, we will use the fact that if a random variable <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> has the Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf a_{ij} = a_{ij}) = p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - p_{ij}}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \prod_{j &gt; i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - p_{ij}}
\end{align*}\]</div>
<p>Now that we’ve specified a probability and a very generalizable model, we’ve learned the full story behind network models and are ready to skip to estimating parameters, right? <em>Wrong!</em> Unfortunately, if we tried too estimate anything about each <span class="math notranslate nohighlight">\(p_{ij}\)</span> individually, we would obtain that <span class="math notranslate nohighlight">\(p_{ij} = a_{ij}\)</span> if we only have one realization <span class="math notranslate nohighlight">\(A\)</span>. Even if we had many realizations of <span class="math notranslate nohighlight">\(\mathbf A\)</span>, this still would not be very interesting, since we have a <em>lot</em> of <span class="math notranslate nohighlight">\(p_{ij}\)</span>s to estimate, and we’ve ignored any sort of structural model that might give us deeper insight into <span class="math notranslate nohighlight">\(\mathbf A\)</span>. In the below sections, we will learn successively less restrictive (and hence, <em>more expressive</em>) assumptions about <span class="math notranslate nohighlight">\(p_{ij}\)</span>s, which will allow us to convey fairly complex random networks, but <em>still</em> enable us with plenty of intteresting things to learn about later on.</p>
</div>
</div>
<div class="section" id="erdos-renyi-er-random-networks">
<h4><span class="section-number">2.7.2. </span>Erdös-Rényi (ER) Random Networks<a class="headerlink" href="#erdos-renyi-er-random-networks" title="Permalink to this headline">¶</a></h4>
<p>The Erdös Rényi model formalizes this relatively simple situation with a single parameter and an <span class="math notranslate nohighlight">\(iid\)</span> assumption:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([0, 1]\)</span></p></td>
<td><p>Probability that an edge exists between a pair of nodes, which is identical for all pairs of nodes</p></td>
</tr>
</tbody>
</table>
<p>From here on out, when we talk about an Erdös Rényi random variable, we will simply call it an ER network. In an ER network, each pair of nodes is connected with probability <span class="math notranslate nohighlight">\(p\)</span>, and therefore not connected with probability <span class="math notranslate nohighlight">\(1-p\)</span>. Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> for every pair of nodes where <span class="math notranslate nohighlight">\(j &gt; i\)</span> (in terms of the adjacency matrix, this means all of the edges in the <em>upper right</em> triangle), that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <em>Bernoulli</em> distribution with probability <span class="math notranslate nohighlight">\(p\)</span>. The word “independent” means that edges in the network occurring or not occurring do not affect one another. For instance, this means that if we knew a student named Alice was friends with Bob, and Alice was also friends with Chadwick, that we do not learn any information about whether Bob is friends with Chadwick. The word “identical” means that every edge in the network has the same probability <span class="math notranslate nohighlight">\(p\)</span> of being connected. If Alice and Bob are friends with probability <span class="math notranslate nohighlight">\(p\)</span>, then Alice and Chadwick are friends with probability <span class="math notranslate nohighlight">\(p\)</span>, too. We assume here that the networks are undirected, which means that if an edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> exists from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, then the edge <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> also exists from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>. We also assume that the networks are loopless, which means that no edges <span class="math notranslate nohighlight">\(\mathbf a_{ii}\)</span> can go from node <span class="math notranslate nohighlight">\(i\)</span> to itself. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an ER network with probability <span class="math notranslate nohighlight">\(p\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim ER_n(p)\)</span>.</p>
<p>Next, let’s formalize an example of one of the limitations of an ER random network. Remember that we said that ER random networks are often too simple. Well, one way in which they are simple is called <strong>degree homogeneity</strong>, which is a property in which <em>all</em> of the nodes in an ER network have the <em>exact</em> same expected node degree! What this means is that if we were to take an ER random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>, we would expect that <em>all</em> of the nodes in the network had the same degree. Let’s see how this works:</p>
<div class="admonition-working-out-the-expected-degree-in-an-erd-ouml-s-r-eacute-nyi-network admonition">
<p class="admonition-title">Working Out the Expected Degree in an Erdös-Rényi Network</p>
<p>Suppose that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a simple network which is random. The network has <span class="math notranslate nohighlight">\(n\)</span> nodes <span class="math notranslate nohighlight">\(\mathcal V = (v_i)_{i = 1}^n\)</span>. Recall that the in a simple network, the node degree is <span class="math notranslate nohighlight">\(deg(v_i) = \sum_{j = 1}^n \mathbf a_{ij}\)</span>. What is the expected degree of a node <span class="math notranslate nohighlight">\(v_i\)</span> of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is Erdös-Rényi?</p>
<p>To describe this, we will compute the expectated value of the degree <span class="math notranslate nohighlight">\(deg(v_i)\)</span>, written <span class="math notranslate nohighlight">\(\mathbb E\left[deg(v_i)\right]\)</span>. Let’s see what happens:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i)\right] &amp;= \mathbb E\left[\sum_{j = 1}^n \mathbf a_{ij}\right] \\
    &amp;= \sum_{j = 1}^n \mathbb E[\mathbf a_{ij}]
\end{align*}\]</div>
<p>We use the <em>linearity of expectation</em> in the line above, which means that the expectation of a sum with a finite number of terms being summed over (<span class="math notranslate nohighlight">\(n\)</span>, in this case) is the sum of the expectations. Finally, by definition, all of the edges <span class="math notranslate nohighlight">\(A_{ij}\)</span> have the same distribution: <span class="math notranslate nohighlight">\(Bern(p)\)</span>. The expected value of a random quantity which takes a Bernoulli distribution is just the probability <span class="math notranslate nohighlight">\(p\)</span>. This means every term <span class="math notranslate nohighlight">\(\mathbb E[\mathbf a_{ij}] = p\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i)\right] &amp;= \sum_{j = 1}^n p = n\cdot p
\end{align*}\]</div>
<p>Since all of the <span class="math notranslate nohighlight">\(n\)</span> terms being summed have the same expected value. This holds for <em>every</em> node <span class="math notranslate nohighlight">\(v_i\)</span>, which means that the expected degree of all nodes is an undirected ER network is the same number, <span class="math notranslate nohighlight">\(n \cdot p\)</span>.</p>
</div>
<div class="section" id="probability">
<h5><span class="section-number">2.7.2.1. </span>Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h5>
<p>What is the probability for realizations of Erdös-Rényi networks? Remember that for Independent-edge graphs, that the probability can be written:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_{\theta}(A) &amp;= \prod_{j &gt; i} \mathbb P_\theta(\mathbf{a}_{ij} = a_{ij})
\end{align*}\]</div>
<p>Next, we recall that by assumption of the ER model, that the probability matrix <span class="math notranslate nohighlight">\(P = (p)\)</span>, or that <span class="math notranslate nohighlight">\(p_{ij} = p\)</span> for all <span class="math notranslate nohighlight">\(i,j\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \prod_{j &gt; i} p^{a_{ij}}(1 - p)^{1 - a_{ij}} \\
    &amp;= p^{\sum_{j &gt; i} a_{ij}} \cdot (1 - p)^{\binom{n}{2} - \sum_{j &gt; i}a_{ij}} \\
    &amp;= p^{m} \cdot (1 - p)^{\binom{n}{2} - m}
\end{align*}\]</div>
<p>This means that the probability <span class="math notranslate nohighlight">\(\mathbb P_\theta(A)\)</span> is a function <em>only</em> of the number of edges <span class="math notranslate nohighlight">\(m = \sum_{j &gt; i}a_{ij}\)</span> in the network represented by adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>. The equivalence class on the Erdös-Rényi networks are the sets:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E_{i} &amp;= \left\{A \in \mathcal A_n : m = i\right\}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> index from <span class="math notranslate nohighlight">\(0\)</span> (the minimum number of edges possible) all the way up to <span class="math notranslate nohighlight">\(n^2\)</span> (the maximum number of edges possible). All of the relationships for equivalence classes discussed above apply to the sets <span class="math notranslate nohighlight">\(E_i\)</span>.</p>
</div>
</div>
<div class="section" id="network-models-for-networks-which-aren-t-simple">
<h4><span class="section-number">2.7.3. </span>Network Models for networks which aren’t simple<a class="headerlink" href="#network-models-for-networks-which-aren-t-simple" title="Permalink to this headline">¶</a></h4>
<p>To make the discussions a little more easy to handle, in the above descriptions and all our successive descriptions, we will describe network models for <strong>simple networks</strong>. To recap, networks which are simple are binary networks which are both loopless and undirected. Stated another way, simple networks are networks whose adjacency matrices are only <span class="math notranslate nohighlight">\(0\)</span>s and <span class="math notranslate nohighlight">\(1\)</span>s, they are hollow (the diagonal is entirely <em>0</em>), and symmetric (the lower and right triangles of the adjacency matrix are the <em>same</em>). What happens our networks don’t quite look this way?</p>
<p>For now, we’ll keep the assumption that the networks are binary, but we will discuss non-binary network models in a later chapter. We have three possibilities we can consider, and we will show how the “relaxations” of the assumptions change a description of a network model. A <em>relaxation</em>, in statistician speak, means that we are taking the assumptions that we had (in this case, that the networks are <em>simple</em>), and progressively making the assumptions weaker (more <em>relaxed</em>) so that they apply to other networks, too. We split these out so we can be as clear as possible about how the generative model changes with each relaxation step.</p>
<p>We will compare each relaxation to the statement about the generative model for the ER generative model. To recap, for a simple network, we wrote:</p>
<p>“Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> for every pair of nodes where <span class="math notranslate nohighlight">\(j &gt; i\)</span> (in terms of the adjacency matrix, this means all of the nodes in the <em>upper right</em> triangle), that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <em>Bernoulli</em> distribution with probability <span class="math notranslate nohighlight">\(p\)</span>….  We assume here that the networks are undirected, which means that if an edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> exists from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, then the edge <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> also exists from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>. We also assume that the networks are loopless, which means that no edges <span class="math notranslate nohighlight">\(\mathbf a_{ii}\)</span> can go from node <span class="math notranslate nohighlight">\(i\)</span> to itself.”</p>
<p>Any additional parts that are added are expressed in <strong><font color='green'>green</font></strong> font. Omitted parts are struck through with <font color='red'><strike>red</strike></font> font.</p>
<p>Note that these generalizations apply to <em>any</em> of the successive networks which we describe in the Network Models section, and not just the ER model!</p>
<div class="section" id="binary-network-model-which-has-loops-but-is-undirected">
<h5><span class="section-number">2.7.3.1. </span>Binary network model which has loops, but is undirected<a class="headerlink" href="#binary-network-model-which-has-loops-but-is-undirected" title="Permalink to this headline">¶</a></h5>
<p>Here, all we want to do is relax the assumption that the network is loopless. We simply ignore the statement that edges <span class="math notranslate nohighlight">\(\mathbf a_{ii}\)</span> cannot exist, and allow that the <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> which follow a Bernoulli distribution (with some probability which depends on the network model choice) <em>now</em> applies to <span class="math notranslate nohighlight">\(j \geq i\)</span>, and not just <span class="math notranslate nohighlight">\(j &gt; i\)</span>. We keep that an edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> existing implies that <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> also exists, which maintains the symmetry of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (and consequently, the undirectedness of the network).</p>
<p>Our description of the ER network changes to:</p>
<p>Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> for every pair of nodes where <span class="math notranslate nohighlight">\(\mathbf{\color{green}{j \geq i}}\)</span> (in terms of the adjacency matrix, this means all of the nodes in the <em>upper right</em> triangle <strong><font color='green'>and the diagonal</font></strong>), that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <em>Bernoulli</em> distribution with probability <span class="math notranslate nohighlight">\(p\)</span>….  We assume here that the networks are undirected, which means that if an edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> exists from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, then the edge <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> also exists from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>. <font color='red'><strike>We also assume that the networks are loopless, which means that no edges <span class="math notranslate nohighlight">\(\mathbf a_{ii}\)</span> can go from node <span class="math notranslate nohighlight">\(i\)</span> to itself.</strike></font></p>
</div>
<div class="section" id="binary-network-model-which-is-loopless-but-directed">
<h5><span class="section-number">2.7.3.2. </span>Binary network model which is loopless, but directed<a class="headerlink" href="#binary-network-model-which-is-loopless-but-directed" title="Permalink to this headline">¶</a></h5>
<p>Like above, we simply ignore the statement that <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span>, which removes the symmetry of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (and consequently, removes the undirectedness of the network). We allow that the <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> which follows a Bernoulli distribution now apply to <span class="math notranslate nohighlight">\(j \neq i\)</span>, and not just <span class="math notranslate nohighlight">\(j &gt; i\)</span>. We keep that <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>, which maintains the hollowness of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (and consequently, the undirectedness of the network).</p>
<p>Our description of the ER network changes to:</p>
<p>Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> for every pair of nodes where <span class="math notranslate nohighlight">\(\mathbf{\color{green}{j \neq i}}\)</span> (in terms of the adjacency matrix, this means all of the nodes <strike><font color='red'>in the <em>upper right</em> triangle</font></strike><strong><font color='green'>which are not along the diagonal</font></strong>), that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <em>Bernoulli</em> distribution with probability <span class="math notranslate nohighlight">\(p\)</span>….  <font color='red'><strike>We assume here that the networks are undirected, which means that if an edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> exists from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, then the edge <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> also exists from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>.</strike></font> We also assume that the networks are loopless, which means that no edges <span class="math notranslate nohighlight">\(\mathbf a_{ii}\)</span> can go from node <span class="math notranslate nohighlight">\(i\)</span> to itself.</p>
</div>
<div class="section" id="binary-network-model-which-is-has-loops-and-is-directed">
<h5><span class="section-number">2.7.3.3. </span>Binary network model which is has loops and is directed<a class="headerlink" href="#binary-network-model-which-is-has-loops-and-is-directed" title="Permalink to this headline">¶</a></h5>
<p>Finally, for a network which has loops and is directed, we combine the above two approaches. We ignore the statements that <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span>, and the statement that <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>.</p>
<p>Our descriptiomn of the ER network changes to:</p>
<p>Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span>  <font color='red'><strike>where <span class="math notranslate nohighlight">\(j &gt; i\)</span> (in terms of the adjacency matrix, this means all of the nodes in the <em>upper right</em> triangle)</strike></font>, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <em>Bernoulli</em> distribution with probability <span class="math notranslate nohighlight">\(p\)</span>, <font color='green'>for all possible combinations of nodes <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(i\)</span></font>. <font color='red'><strike>We assume here that the networks are undirected, which means that if an edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> exists from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, then the edge <span class="math notranslate nohighlight">\(\mathbf a_{ji}\)</span> also exists from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>. We also assume that the networks are loopless, which means that no edges <span class="math notranslate nohighlight">\(\mathbf a_{ii}\)</span> can go from node <span class="math notranslate nohighlight">\(i\)</span> to itself.</strike></font></p>
</div>
</div>
<div class="section" id="a-priori-stochastic-block-model">
<h4><span class="section-number">2.7.4. </span><em>A Priori</em> Stochastic Block Model<a class="headerlink" href="#a-priori-stochastic-block-model" title="Permalink to this headline">¶</a></h4>
<p>The <em>a priori</em> SBM is an SBM in which we know ahead of time (<em>a priori</em>) which nodes are in which communities. Here, we will use the variable <span class="math notranslate nohighlight">\(K\)</span> to denote the maximum number of different communities. The ordering of the communities does not matter; the community we call <span class="math notranslate nohighlight">\(1\)</span> versus <span class="math notranslate nohighlight">\(2\)</span> versus <span class="math notranslate nohighlight">\(K\)</span> is largely a symbolic distinction (the only thing that matters is that they are <em>different</em>). The <em>a priori</em> SBM has the following parameter:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{K \times K}\)</span></p></td>
<td><p>The block matrix, which assigns edge probabilities for pairs of communities</p></td>
</tr>
</tbody>
</table>
<p>To describe the <em>A Priori</em> SBM, we will designate the community each node is a part of using a vector, which has a single community assignment for each node in the network. We will call this <strong>node assignment vector</strong> <span class="math notranslate nohighlight">\(\vec{\tau}\)</span>, and it is a <span class="math notranslate nohighlight">\(n\)</span>-length vector (one element for each node) with elements which can take values from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(K\)</span>. In symbols, we would say that <span class="math notranslate nohighlight">\(\vec\tau \in \{1, ..., K\}^n\)</span>. What this means is that for a given element of <span class="math notranslate nohighlight">\(\vec \tau\)</span>, <span class="math notranslate nohighlight">\(\tau_i\)</span>, that <span class="math notranslate nohighlight">\(\tau_i\)</span> is the community assignment (either <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(2\)</span>, so on and so forth up to <span class="math notranslate nohighlight">\(K\)</span>) for the <span class="math notranslate nohighlight">\(i^{th}\)</span> node. If there we hahd an example where there were <span class="math notranslate nohighlight">\(2\)</span> communities (<span class="math notranslate nohighlight">\(K = 2\)</span>) for instance, and the first two nodes are in community <span class="math notranslate nohighlight">\(1\)</span> and the second two in community <span class="math notranslate nohighlight">\(2\)</span>, then <span class="math notranslate nohighlight">\(\vec\tau\)</span> would be a vector which looks like:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec\tau &amp;= \begin{bmatrix}1 &amp; 1 &amp; 2 &amp; 2\end{bmatrix}^\top
\end{align*}\]</div>
<p>Next, let’s discuss the matrix <span class="math notranslate nohighlight">\(B\)</span>, which is known as the <strong>block matrix</strong> of the SBM. We write down that <span class="math notranslate nohighlight">\(B \in [0, 1]^{K \times K}\)</span>, which means that the block matrix is a matrix with <span class="math notranslate nohighlight">\(K\)</span> rows and <span class="math notranslate nohighlight">\(K\)</span> columns. If we have a pair of nodes and know which of the <span class="math notranslate nohighlight">\(K\)</span> communities each node is from, the block matrix tells us the probability that those two nodes are connected. If our networks are simple, the matrix <span class="math notranslate nohighlight">\(B\)</span> is also symmetric, which means that if <span class="math notranslate nohighlight">\(b_{kk'} = p\)</span> where <span class="math notranslate nohighlight">\(p\)</span> is a probability, that <span class="math notranslate nohighlight">\(b_{k'k} = p\)</span>, too. The requirement of <span class="math notranslate nohighlight">\(B\)</span> to be symmetric exists <em>only</em> if we are dealing with undirected networks.</p>
<p>Finally, let’s think about how to write down the generative model for the <em>a priori</em> SBM. Intuitionally what we want to reflect is, if we know that node <span class="math notranslate nohighlight">\(i\)</span> is in community <span class="math notranslate nohighlight">\(k'\)</span> and node <span class="math notranslate nohighlight">\(j\)</span> is in community <span class="math notranslate nohighlight">\(k\)</span>, that the <span class="math notranslate nohighlight">\((k', k)\)</span> entry of the block matrix is the probability that <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are connected. We say that given  <span class="math notranslate nohighlight">\(\tau_i = k'\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(b_{k' k})\)</span> distribution for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>. Note that the adjacencies <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> are not <em>necessarily</em> identically distributed, because the probability depends on the community of edge <span class="math notranslate nohighlight">\((i,j)\)</span>. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> SBM network with parameter <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(\vec{\tau}\)</span> is a realization of the node-assignment vector, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim SBM_{n,\vec \tau}(B)\)</span>.</p>
<div class="section" id="id1">
<h5><span class="section-number">2.7.4.1. </span>Probability<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>What does the probability for the <em>a priori</em> SBM look like? In our previous description, we admittedly simplified things to an extent to keep the wording down. In truth, we model the <em>a priori</em> SBM using a <em>latent variable</em> model, which means that the node assignment vector, <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, is treated as <em>random</em>. For the case of the <em>a priori</em> SBM, it just so happens that we <em>know</em> the specific value that this latent variable <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> takes, <span class="math notranslate nohighlight">\(\vec \tau\)</span>, ahead of time.</p>
<p>Fortunately, since <span class="math notranslate nohighlight">\(\vec \tau\)</span> is a <em>parameter</em> of the <em>a priori</em> SBM, the probability is a bit simpler than for the <em>a posteriori</em> SBM. This is because the <em>a posteriori</em> SBM requires an integration over potential realizations of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, whereas the <em>a priori</em> SBM does not, since we already know that <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> was realized as <span class="math notranslate nohighlight">\(\vec\tau\)</span>.</p>
<p>Putting these steps together gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;= \mathbb P_{\theta}(\mathbf A = A | \vec{\pmb \tau} = \vec\tau) \\
&amp;= \prod_{j &gt; i} \mathbb P_\theta(\mathbf a_{ij} = a_{ij} | \vec{\pmb \tau} = \vec\tau),\;\;\;\;\textrm{Independence Assumption}
\end{align*}\]</div>
<p>Next, for the <em>a priori</em> SBM, we know that each edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> only <em>actually</em> depends on the community assignments of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, so we know that <span class="math notranslate nohighlight">\(\mathbb P_{\theta}(\mathbf a_{ij} = a_{ij} | \vec{\pmb \tau} = \vec\tau) = \mathbb P(\mathbf a_{ij} = a_{ij} | \tau_i = k', \tau_j = k)\)</span>, where <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span> are any of the <span class="math notranslate nohighlight">\(K\)</span> possible communities. This is because the community assignments of nodes that are not nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> do not matter for edge <span class="math notranslate nohighlight">\(ij\)</span>, due to the independence assumption.</p>
<p>Next, let’s think about the probability matrix <span class="math notranslate nohighlight">\(P = (p_{ij})\)</span> for the <em>a priori</em> SBM. We know that, given that <span class="math notranslate nohighlight">\(\tau_i = k'\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>,  each adjacency <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently and identically from a <span class="math notranslate nohighlight">\(Bern(b_{k',k})\)</span> distribution. This means that <span class="math notranslate nohighlight">\(p_{ij} = b_{k',k}\)</span>. Completing our analysis from above:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \prod_{j &gt; i} b_{k'k}^{a_{ij}}(1 - b_{k'k})^{1 - a_{ij}} \\
    &amp;= \prod_{k,k' \in [K]}b_{k'k}^{m_{k'k}}(1 - b_{k'k})^{n_{k'k} - m_{k'k}}
\end{align*}\]</div>
<p>Where <span class="math notranslate nohighlight">\(n_{k' k}\)</span> denotes the total number of edges possible between nodes assigned to community <span class="math notranslate nohighlight">\(k'\)</span> and nodes assigned to community <span class="math notranslate nohighlight">\(k\)</span>. That is, <span class="math notranslate nohighlight">\(n_{k' k} = \sum_{j &gt; i} \mathbb 1_{\tau_i = k'}\mathbb 1_{\tau_j = k}\)</span>. Further, we will use <span class="math notranslate nohighlight">\(m_{k' k}\)</span> to denote the total number of edges observed between these two communities. That is, <span class="math notranslate nohighlight">\(m_{k' k} = \sum_{j &gt; i}\mathbb 1_{\tau_i = k'}\mathbb 1_{\tau_j = k}a_{ij}\)</span>. Note that for a single <span class="math notranslate nohighlight">\((k',k)\)</span> community pair, that the probability is analogous to the probability of a realization of an ER random variable.</p>
<!--- We can formalize this a bit more explicitly. If we let $A^{\ell k}$ be defined as the subgraph *induced* by the edges incident nodes in community $\ell$ and those in community $k$, then we can say that $A^{\ell k}$ is a directed ER random network, --->
<p>Like the ER model, there are again equivalence classes of the sample space <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> in terms of their probability. For a two-community setting, with <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> given, the equivalence classes are the sets:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E_{a,b,c}(\vec \tau, B) &amp;= \left\{A \in \mathcal A_n : m_{11} = a, m_{21}=m_{12} = b, m_{22} = c\right\}
\end{align*}\]</div>
<p>The number of equivalence classes possible scales with the number of communities, and the manner in which nodes are assigned to communities (particularly, the number of nodes in each community).</p>
</div>
</div>
<div class="section" id="a-posteriori-stochastic-block-model">
<h4><span class="section-number">2.7.5. </span><em>A Posteriori</em> Stochastic Block Model<a class="headerlink" href="#a-posteriori-stochastic-block-model" title="Permalink to this headline">¶</a></h4>
<p>In the <em>a posteriori</em> Stochastic Block Model (SBM), we consider that node assignment to one of <span class="math notranslate nohighlight">\(K\)</span> communities is a random variable, that we <em>don’t</em> know already like te <em>a priori</em> SBM. We’re going to see a funky word come up, that you’re probably not familiar with, the <strong><span class="math notranslate nohighlight">\(K\)</span> probability simplex</strong>. What the heck is a probability simplex?</p>
<p>The intuition for a simplex is probably something you’re very familiar with, but just haven’t seen a word describe. Let’s say I have a vector, <span class="math notranslate nohighlight">\(\vec\pi = (\pi_k)_{k \in [K]}\)</span>, which has a total of <span class="math notranslate nohighlight">\(K\)</span> elements. <span class="math notranslate nohighlight">\(\vec\pi\)</span> will be a vector, which indicates the <em>probability</em> that a given node is assigned to each of our <span class="math notranslate nohighlight">\(K\)</span> communities, so we need to impose some additional constraints. Symbolically, we would say that, for all <span class="math notranslate nohighlight">\(i\)</span>, and for all <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \pi_k = \mathbb P(\pmb\tau_i = k)
\end{align*}\]</div>
<p>The <span class="math notranslate nohighlight">\(\vec \pi\)</span> we’re going to use has a very special property: all of its elements are non-negative: for all <span class="math notranslate nohighlight">\(\pi_k\)</span>, <span class="math notranslate nohighlight">\(\pi_k \geq 0\)</span>. This makes sense since <span class="math notranslate nohighlight">\(\pi_k\)</span> is being used to represent the probability of a node <span class="math notranslate nohighlight">\(i\)</span> being in group <span class="math notranslate nohighlight">\(k\)</span>, so it certainly can’t be negative. Further, there’s another thing that we want our <span class="math notranslate nohighlight">\(\vec\pi\)</span> to have: in order for each element <span class="math notranslate nohighlight">\(\pi_k\)</span> to indicate the probability of something to be assigned to <span class="math notranslate nohighlight">\(k\)</span>, we need all of the <span class="math notranslate nohighlight">\(\pi_k\)</span>s to sum up to one. This is because of something called the Law of Total Probability. If we have <span class="math notranslate nohighlight">\(K\)</span> total values that <span class="math notranslate nohighlight">\(\pmb \tau_i\)</span> could take, then it is the case that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{k=1}^K \mathbb P(\pmb \tau_i = k) = \sum_{k = 1}^K \pi_k = 1
\end{align*}\]</div>
<p>So, back to our question: how does a probability simplex fit in? Well, the <span class="math notranslate nohighlight">\(K\)</span> probability simplex describes all of the possible values that our vector <span class="math notranslate nohighlight">\(\vec\pi\)</span> could take! In symbols, the <span class="math notranslate nohighlight">\(K\)</span> probability simplex is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\left\{\vec\pi : \text{for all $k$ }\pi_k \geq 0, \sum_{k = 1}^K \pi_k = 1 \right\}
\end{align*}\]</div>
<p>So the <span class="math notranslate nohighlight">\(K\)</span> probability simplex is just the space for all possible vectors which could indicate assignment probabilities to one of <span class="math notranslate nohighlight">\(K\)</span> communities.</p>
<p>What does the probability simplex look like? Below, we take a look at the <span class="math notranslate nohighlight">\(2\)</span>-probability simplex (2-d <span class="math notranslate nohighlight">\(\vec\pi\)</span>s) and the <span class="math notranslate nohighlight">\(3\)</span>-probability simplex (3-dimensional <span class="math notranslate nohighlight">\(\vec\pi\)</span>s):</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d.art3d</span> <span class="kn">import</span> <span class="n">Poly3DCollection</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">fig</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figaspect</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Probability Simplexes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$\pi_1$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$\pi_2$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;2-probability simplex&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">verts</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span><span class="p">))]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_collection3d</span><span class="p">(</span><span class="n">Poly3DCollection</span><span class="p">(</span><span class="n">verts</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">azim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$\pi_1$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$\pi_2$&quot;</span><span class="p">)</span>
<span class="n">h</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;$\pi_3$&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;3-probability simplex&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_theory_8_0.png" src="_images/single-network-models_theory_8_0.png" />
</div>
</div>
<p>The values of <span class="math notranslate nohighlight">\(\vec\pi = (\pi)\)</span> that are in the <span class="math notranslate nohighlight">\(K\)</span>-probability simplex are indicated by the shaded region of each figure. This comprises the <span class="math notranslate nohighlight">\((\pi_1, \pi_2)\)</span> pairs that fall along a diagonal line from <span class="math notranslate nohighlight">\((0,1)\)</span> to <span class="math notranslate nohighlight">\((1,0)\)</span> for the <span class="math notranslate nohighlight">\(2\)</span>-simplex, and the <span class="math notranslate nohighlight">\((\pi_1, \pi_2, \pi_3)\)</span> tuples that fall on the surface of the triangular shape above with nodes at <span class="math notranslate nohighlight">\((1,0,0)\)</span>, <span class="math notranslate nohighlight">\((0,1,0)\)</span>, and <span class="math notranslate nohighlight">\((0,0,1)\)</span>.</p>
<p>This model has the following parameters:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vec \pi\)</span></p></td>
<td><p>the <span class="math notranslate nohighlight">\(K\)</span> probability simplex</p></td>
<td><p>The probability of a node being assigned to community <span class="math notranslate nohighlight">\(K\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{K \times K}\)</span></p></td>
<td><p>The block matrix, which assigns edge probabilities for pairs of communities</p></td>
</tr>
</tbody>
</table>
<p>The <em>a posteriori</em> SBM is a bit more complicated than the <em>a priori</em> SBM. We will think about the <em>a posteriori</em> SBM as a variation of the <em>a priori</em> SBM, where instead of the node-assignment vector being treated as a known fixed value (the community assignments), we will treat it as <em>unknown</em>. <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> is called a <em>latent variable</em>, which means that it is a quantity that is never actually observed, but which will be useful for describing our model. In this case, <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> takes values in the space <span class="math notranslate nohighlight">\(\{1,...,K\}^n\)</span>. This means that for a given realization of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, denoted by <span class="math notranslate nohighlight">\(\vec \tau\)</span>, that for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes in the network, we suppose that an integer value between <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(K\)</span> indicates which community a node is from. Statistically, we write that the node assignment for node <span class="math notranslate nohighlight">\(i\)</span>, denoted by <span class="math notranslate nohighlight">\(\pmb \tau_i\)</span>, is sampled independently and identically from <span class="math notranslate nohighlight">\(Categorical(\vec \pi)\)</span>. Stated another way, the vector <span class="math notranslate nohighlight">\(\vec\pi\)</span> indicates the probability <span class="math notranslate nohighlight">\(\pi_k\)</span> of assignment to each community <span class="math notranslate nohighlight">\(k\)</span> in the network.</p>
<p>The matrix <span class="math notranslate nohighlight">\(B\)</span> behaves exactly the same as it did with the <em>a posteriori</em> SBM. Finally, let’s think about how to write down the generative model in the <em>a posteriori</em> SBM. The model for the <em>a posteriori</em> SBM is, in fact, nearly the same as for the <em>a priori</em> SBM: we still say that given <span class="math notranslate nohighlight">\(\tau_i = k'\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> are independent <span class="math notranslate nohighlight">\(Bern(b_{k'k})\)</span>. Here, however, we also describe that <span class="math notranslate nohighlight">\(\pmb \tau_i\)</span> are sampled independent and identically from <span class="math notranslate nohighlight">\(Categorical(\vec\pi)\)</span>, as we learned above. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an <em>a posteriori</em> SBM network with parameters <span class="math notranslate nohighlight">\(\vec \pi\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim SBM_n(\vec \pi, B)\)</span>.</p>
<div class="section" id="id2">
<h5><span class="section-number">2.7.5.1. </span>Probability<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p>What does the probability for the <em>a posteriori</em> SBM look like? In this case, <span class="math notranslate nohighlight">\(\theta = (\vec \pi, B)\)</span> are the parameters for the model, so the probability for a realization <span class="math notranslate nohighlight">\(A\)</span> of <span class="math notranslate nohighlight">\(\mathbf A\)</span> is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;= \mathbb P_\theta(\mathbf A = A)
\end{align*}\]</div>
<p>Next, we use the fact that the probability that <span class="math notranslate nohighlight">\(\mathbf A = A\)</span> is, in fact, the <em>integration</em> (over realizations of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>) of the joint <span class="math notranslate nohighlight">\((\mathbf A, \vec{\pmb \tau})\)</span>. In this case, we will let <span class="math notranslate nohighlight">\(\mathcal T = \{1,...,K\}^n\)</span> be the space of all possible realizations that <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> could take:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9c16a564-ea42-456d-a5ee-f3a10fedbeb1">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-9c16a564-ea42-456d-a5ee-f3a10fedbeb1" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathbb P_\theta(A)&amp;= \sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A, \vec{\pmb \tau} = \vec \tau) 
\end{align}\]</div>
<p>Next, remember that by definition of a conditional probability for a random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> taking value <span class="math notranslate nohighlight">\(x\)</span> conditioned on random variable <span class="math notranslate nohighlight">\(\mathbf y\)</span> taking the value <span class="math notranslate nohighlight">\(y\)</span>, that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf x = x | \mathbf y = y) = \frac{\mathbb P(\mathbf x = x, \mathbf y = y)}{\mathbb P(\mathbf y = y)}\)</span>. Note that by multiplying through by <span class="math notranslate nohighlight">\(\mathbf P(\mathbf y = y)\)</span>, we can see that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf x = x, \mathbf y = y) = \mathbb P(\mathbf x = x| \mathbf y = y)\mathbb P(\mathbf y = y)\)</span>. Using this logic for <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;=\sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A| \vec{\pmb \tau} = \vec \tau)\mathbb P(\vec{\pmb \tau} = \vec \tau)
\end{align*}\]</div>
<p>Intuitively, for each term in the sum, we are treating <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> as taking a fixed value, <span class="math notranslate nohighlight">\(\vec\tau\)</span>, to evaluate this probability statement.</p>
<p>We will start by describing <span class="math notranslate nohighlight">\(\mathbb P(\vec{\pmb \tau} = \vec\tau)\)</span>. Remember that for <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, that each entry <span class="math notranslate nohighlight">\(\pmb \tau_i\)</span> is sampled <em>independently and identically</em> from <span class="math notranslate nohighlight">\(Categorical(\vec \pi)\)</span>.The probability mass for a <span class="math notranslate nohighlight">\(Categorical(\vec \pi)\)</span>-valued random variable is <span class="math notranslate nohighlight">\(\mathbb P(\pmb \tau_i = \tau_i; \vec \pi) = \pi_{\tau_i}\)</span>. Finally, note that if we are taking the products of <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(\pi_{\tau_i}\)</span> terms, that many of these values will end up being the same. Consider, for instance, if the vector <span class="math notranslate nohighlight">\(\tau = [1,2,1,2,1]\)</span>. We end up with three terms of <span class="math notranslate nohighlight">\(\pi_1\)</span>, and two terms of <span class="math notranslate nohighlight">\(\pi_2\)</span>, and it does not matter which order we multiply them in. Rather, all we need to keep track of are the counts of each <span class="math notranslate nohighlight">\(\pi\)</span> term. Written another way, we can use the indicator that <span class="math notranslate nohighlight">\(\tau_i = k\)</span>, given by <span class="math notranslate nohighlight">\(\mathbb 1_{\tau_i = k}\)</span>, and a running counter over all of the community probability assignments <span class="math notranslate nohighlight">\(\pi_k\)</span> to make this expression a little more sensible. We will use the symbol <span class="math notranslate nohighlight">\(n_k = \sum_{i = 1}^n \mathbb 1_{\tau_i = k}\)</span> to denote this value, which is the number of nodes in community <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(\vec{\pmb \tau} = \vec \tau) &amp;= \prod_{i = 1}^n \mathbb P_\theta(\pmb \tau_i = \tau_i),\;\;\;\;\textrm{Independence Assumption} \\
&amp;= \prod_{i = 1}^n \pi_{\tau_i} ,\;\;\;\;\textrm{p.m.f. of a Categorical R.V.}\\
&amp;= \prod_{k = 1}^K \pi_{k}^{n_k},\;\;\;\;\textrm{Reorganizing what we are taking products of}
\end{align*}\]</div>
<p>Next, let’s think about the conditional probability term, <span class="math notranslate nohighlight">\(\mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau)\)</span>. Remember that the entries are all independent conditional on <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> taking the value <span class="math notranslate nohighlight">\(\vec\tau\)</span>. It turns out this is exactly the same result that we obtained for the <em>a priori</em> SBM:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau)
&amp;= \prod_{k',k} b_{\ell k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}
\end{align*}\]</div>
<p>Combining these into the integrand gives:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;= \sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau) \mathbb P_\theta(\vec{\pmb \tau} = \vec \tau) \\
&amp;= \sum_{\vec \tau \in \mathcal T} \prod_{k = 1}^K \left[\pi_k^{n_k}\cdot \prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\right]
\end{align*}\]</div>
<p>Evaluating this sum explicitly proves to be relatively tedious and is a bit outside of the scope of this book, so we will omit it here.</p>
</div>
</div>
<div class="section" id="degree-corrected-stochastic-block-model-dcsbm">
<h4><span class="section-number">2.7.6. </span>Degree-Corrected Stochastic Block Model (DCSBM)<a class="headerlink" href="#degree-corrected-stochastic-block-model-dcsbm" title="Permalink to this headline">¶</a></h4>
<p>Let’s think back to our school example for the Stochastic Block Model. Remember, we had 100 students, each of whom could go to one of two possible schools: school one or school two. Our network had 100 nodes, representing each of the students. We said that the school for which each student attended was represented by their node assignment <span class="math notranslate nohighlight">\(\tau_i\)</span> to one of two possible communities. The matrix <span class="math notranslate nohighlight">\(B\)</span> was the block probaability matrix, where <span class="math notranslate nohighlight">\(b_{11}\)</span> was the probability that students in school one were friends, <span class="math notranslate nohighlight">\(b_{22}\)</span> was the probability that students in school two were friends, and <span class="math notranslate nohighlight">\(b_{12} = b_{21}\)</span> was the probability that students were friends if they did not go to the same school. In this case, we said that <span class="math notranslate nohighlight">\(\mathbf A\)</span> was an <span class="math notranslate nohighlight">\(SBM_n(\tau, B)\)</span> random network.</p>
<p>When would this setup not make sense? Let’s say that Alice and Bob both go to the same school, but Alice is more popular than Bob. In general since Alice is more popular than Bob, we might want to say that for any clasasmate, Alice gets an additional “popularity benefit” to her probability of being friends with the other classmate, and Bob gets an “unpopularity penalty.” The problem here is that within a single community of an SBM, the SBM assumes that the <strong>node degree</strong> (the number of nodes each nodes is connected to) is the <em>same</em> for all nodes within a single community. This means that we would be unable to reflect this benefit/penalty system to Alice and Bob, since each student will have the same number of friends, on average. This problem is referred to as <strong>community degree homogeneity</strong> in a Stochastic Block Model Network. Community degree homogeneity just means that the node degree is <em>homogeneous</em>, or the same, for all nodes within a community.</p>
<div class="admonition-degree-homogeneity-in-a-stochastic-block-model-network admonition">
<p class="admonition-title">Degree Homogeneity in a Stochastic Block Model Network</p>
<p>Suppose that <span class="math notranslate nohighlight">\(\mathbf A \sim SBM_{n, \vec\tau}(B)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf A\)</span> has <span class="math notranslate nohighlight">\(K=2\)</span> communities. What is the node degree of each node in <span class="math notranslate nohighlight">\(\mathbf A\)</span>?</p>
<p>For an arbitrary node <span class="math notranslate nohighlight">\(v_i\)</span> which is in community <span class="math notranslate nohighlight">\(k\)</span> (either one or two), we will compute the expectated value of the degree <span class="math notranslate nohighlight">\(deg(v_i)\)</span>, written <span class="math notranslate nohighlight">\(\mathbb E\left[deg(v_i); \tau_i = k\right]\)</span>. We will let <span class="math notranslate nohighlight">\(n_k\)</span> represent the number of nodes whose node assignments <span class="math notranslate nohighlight">\(\tau_i\)</span> are to community <span class="math notranslate nohighlight">\(k\)</span>. Let’s see what happens:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &amp;= \mathbb E\left[\sum_{j = 1}^n \mathbf a_{ij}\right] \\
    &amp;= \sum_{j = 1}^n \mathbb E[\mathbf a_{ij}]
\end{align*}\]</div>
<p>We use the <em>linearity of expectation</em> again to get from the top line to the second line. Next, instead of summing over all the nodes, we’ll break the sum up into the nodes which are in the same community as node <span class="math notranslate nohighlight">\(i\)</span>, and the ones in the <em>other</em> community <span class="math notranslate nohighlight">\(k'\)</span>. We use the notation <span class="math notranslate nohighlight">\(k'\)</span> to emphasize that <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span> are different values:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &amp;= \sum_{j : i \neq j, \tau_j = k} \mathbb E\left[\mathbf a_{ij}\right] + \sum_{j : \tau_j =k'} \mathbb E[\mathbf a_{ij}]
\end{align*}\]</div>
<p>In the first sum, we have <span class="math notranslate nohighlight">\(n_k-1\)</span> total edges (the number of nodes that aren’t node <span class="math notranslate nohighlight">\(i\)</span>, but are in the same community), and in the second sum, we have <span class="math notranslate nohighlight">\(n_{k'}\)</span> total edges (the number of nodes that are in the other community). Finally, we will use that the probability of an edge in the same community is <span class="math notranslate nohighlight">\(b_{kk}\)</span>, but the probability of an edge between the communities is <span class="math notranslate nohighlight">\(b_{k' k}\)</span>. Finally, we will use that the expected value of an adjacency <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> which is Bernoulli distributed is its probability:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &amp;= \sum_{j : i \neq j, \tau_j = k} b_{kk} + \sum_{j : \tau_j = \ell} b_{kk'},\;\;\;\;\mathbf a_{ij}\textrm{ are Bernoulli distributed} \\
    &amp;= (n_k - 1)b_{kk} + n_{k'} b_{kk'}
\end{align*}\]</div>
<p>This holds for any node <span class="math notranslate nohighlight">\(i\)</span> which is in community <span class="math notranslate nohighlight">\(k\)</span>. Therefore, the expected node degree is the same, or <strong>homogeneous</strong>, within a community of an SBM.</p>
</div>
<p>To address this limitation, we turn to the Degree-Corrected Stochastic Block Model, or DCSBM. As with the Stochastic Block Model, there is both a <em>a priori</em> and <em>a posteriori</em> DCSBM.</p>
<div class="section" id="a-priori-dcsbm">
<h5><span class="section-number">2.7.6.1. </span><em>A Priori</em> DCSBM<a class="headerlink" href="#a-priori-dcsbm" title="Permalink to this headline">¶</a></h5>
<p>Like the <em>a priori</em> SBM, the <em>a priori</em> DCSBM is where we know which nodes are in which communities ahead of time. Here, we will use the variable <span class="math notranslate nohighlight">\(K\)</span> to denote the number of different communiies. The <em>a priori</em> DCSBM has the following two parameters:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{K \times K}\)</span></p></td>
<td><p>The block matrix, which assigns edge probabilities for pairs of communities</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\vec\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb R^n_+\)</span></p></td>
<td><p>The degree correction vector, which adjusts the degree for pairs of nodes</p></td>
</tr>
</tbody>
</table>
<p>The latent community assignment vector <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> with a known <em>a priori</em> realization <span class="math notranslate nohighlight">\(\vec{\tau}\)</span> and the block matrix <span class="math notranslate nohighlight">\(B\)</span> are exactly the same for the <em>a priori</em> DCSBM as they were for the <em>a priori</em> SBM.</p>
<p>The vector <span class="math notranslate nohighlight">\(\vec\theta\)</span> is the degree correction vector. Each entry <span class="math notranslate nohighlight">\(\theta_i\)</span> is a positive scalar. <span class="math notranslate nohighlight">\(\theta_i\)</span> defines how much more (or less) edges associated with node <span class="math notranslate nohighlight">\(i\)</span> are connected due to their association with node <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Finally, let’s think about how to write down the generative model for the <em>a priori</em> DCSBM. We say that <span class="math notranslate nohighlight">\(\tau_i = k'\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(\theta_i \theta_j b_{k'k})\)</span> distribution for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>. As we can see, <span class="math notranslate nohighlight">\(\theta_i\)</span> in a sense is “correcting” the probabilities of each adjacency to node <span class="math notranslate nohighlight">\(i\)</span> to be higher, or lower, depending on the value of <span class="math notranslate nohighlight">\(\theta_i\)</span> that that which is given by the block probabilities <span class="math notranslate nohighlight">\(b_{\ell k}\)</span>. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> DCSBM network with parameters and <span class="math notranslate nohighlight">\(B\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim DCSBM_{n,\vec\tau}(\vec \theta, B)\)</span>.</p>
<div class="section" id="id3">
<h6><span class="section-number">2.7.6.1.1. </span>Probability<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h6>
<p>The derivation for the probability is the same as for the <em>a priori</em> SBM, with the change that <span class="math notranslate nohighlight">\(p_{ij} = \theta_i \theta_j b_{k'k}\)</span> instead of just <span class="math notranslate nohighlight">\(b_{k'k}\)</span>. This gives that the probability turns out to be:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \prod_{j &gt; i} \left(\theta_i \theta_j b_{k'k}\right)^{a_{ij}}\left(1 - \theta_i \theta_j b_{k'k}\right)^{1 - a_{ij}}
\end{align*}\]</div>
<p>The expression doesn’t simplify much more due to the fact that the probabilities are dependent on the particular <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, so we can’t just reduce the statement in terms of <span class="math notranslate nohighlight">\(n_{k'k}\)</span> and <span class="math notranslate nohighlight">\(m_{k'k}\)</span> like for the SBM.</p>
</div>
</div>
<div class="section" id="a-posteriori-dcsbm">
<h5><span class="section-number">2.7.6.2. </span><em>A Posteriori</em> DCSBM<a class="headerlink" href="#a-posteriori-dcsbm" title="Permalink to this headline">¶</a></h5>
<p>The <em>a posteriori</em> DCSBM is to the <em>a posteriori</em> SBM what the <em>a priori</em> DCSBM was to the <em>a priori</em> SBM. The changes are very minimal, so we will omit explicitly writing it all down here so we can get this section wrapped up, with the idea that the preceding section on the <em>a priori</em> DCSBM should tell you what needs to change. We will leave it as an exercise to the reader to write down a model and probability statement for realizations of the DCSBM.</p>
</div>
</div>
<div class="section" id="random-dot-product-graph-rdpg">
<h4><span class="section-number">2.7.7. </span>Random Dot Product Graph (RDPG)<a class="headerlink" href="#random-dot-product-graph-rdpg" title="Permalink to this headline">¶</a></h4>
<div class="section" id="a-priori-rdpg">
<h5><span class="section-number">2.7.7.1. </span><em>A Priori</em> RDPG<a class="headerlink" href="#a-priori-rdpg" title="Permalink to this headline">¶</a></h5>
<p>The <em>a priori</em> Random Dot Product Graph is an RDPG in which we know <em>a priori</em> the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. The <em>a priori</em> RDPG has the following parameter:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(X\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \mathbb R^{n \times d}\)</span></p></td>
<td><p>The matrix of latent positions for each node <span class="math notranslate nohighlight">\(n\)</span>.</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(X\)</span> is called the <strong>latent position matrix</strong> of the RDPG. We write that <span class="math notranslate nohighlight">\(X \in \mathbb R^{n \times d}\)</span>, which means that it is a matrix with real values, <span class="math notranslate nohighlight">\(n\)</span> rows, and <span class="math notranslate nohighlight">\(d\)</span> columns. We will use the notation <span class="math notranslate nohighlight">\(\vec x_i\)</span> to refer to the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of <span class="math notranslate nohighlight">\(X\)</span>. <span class="math notranslate nohighlight">\(\vec x_i\)</span> is referred to as the <strong>latent position</strong> of a node <span class="math notranslate nohighlight">\(i\)</span>. This looks something like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X = \begin{bmatrix}
     \vec x_{1}^\top \\
     \vdots \\
     \vec x_n^\top
    \end{bmatrix}
\end{align*}\]</div>
<p>Noting that <span class="math notranslate nohighlight">\(X\)</span> has <span class="math notranslate nohighlight">\(d\)</span> columns, this implies that <span class="math notranslate nohighlight">\(\vec x_i \in  \mathbb R^d\)</span>, or that each node’s latent position is a real-valued <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector.</p>
<p>What is the generative model for the <em>a priori</em> RDPG? As we discussed above, given <span class="math notranslate nohighlight">\(X\)</span>, for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(\vec x_i^\top \vec x_j)\)</span> independently. If <span class="math notranslate nohighlight">\(i &lt; j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (the network is <em>undirected</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> (the network is <em>loopless</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> RDPG with parameter <span class="math notranslate nohighlight">\(X\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim RDPG_n(X)\)</span>.</p>
<!-- TODO: return to add equivalence classes --><div class="section" id="id4">
<h6><span class="section-number">2.7.7.1.1. </span>Probability<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h6>
<p>Given <span class="math notranslate nohighlight">\(X\)</span>, the probability for an RDPG is relatively straightforward, as an RDPG is another Independent-Edge Random Graph. The independence assumption vastly simplifies our resulting expression. We will also use many of the results we’ve identified above, such as the p.m.f. of a Bernoulli random variable. Finally, we’ll note that the probability matrix <span class="math notranslate nohighlight">\(P = (\vec x_i^\top \vec x_j)\)</span>, so <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec x_j\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \mathbb P_\theta(A) \\
    &amp;= \prod_{j &gt; i}\mathbb P(\mathbf a_{ij} = a_{ij}),\;\;\;\; \textrm{Independence Assumption} \\
    &amp;= \prod_{j &gt; i}(\vec x_i^\top \vec x_j)^{a_{ij}}(1 - \vec x_i^\top \vec x_j)^{1 - a_{ij}},\;\;\;\; a_{ij} \sim Bern(\vec x_i^\top \vec x_j)
\end{align*}\]</div>
<p>Unfortunately, the probability equivalence classes are a bit harder to understand intuitionally here compared to the ER and SBM examples so we won’t write them down here, but they still exist!</p>
</div>
</div>
<div class="section" id="a-posteriori-rdpg">
<h5><span class="section-number">2.7.7.2. </span><em>A Posteriori</em> RDPG<a class="headerlink" href="#a-posteriori-rdpg" title="Permalink to this headline">¶</a></h5>
<p>Like for the <em>a posteriori</em> SBM, the <em>a posteriori</em> RDPG introduces another strange set: the <strong>intersection of the unit ball and the non-negative orthant</strong>. Huh? This sounds like a real mouthful, but it turns out to be rather straightforward. You are probably already very familiar with a particular orthant: in two-dimensions, an orthant is called a quadrant. Basically, an orthant just extends the concept of a quadrant to spaces which might have more than <span class="math notranslate nohighlight">\(2\)</span> dimensions. The non-negative orthant happens to be the orthant where all of the entries are non-negative. We call the <strong><span class="math notranslate nohighlight">\(K\)</span>-dimensional non-negative orthant</strong> the set of points in <span class="math notranslate nohighlight">\(K\)</span>-dimensional real space, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \left\{\vec x \in \mathbb R^K : x_k \geq 0\text{ for all $k$}\right\}
\end{align*}\]</div>
<p>In two dimensions, this is the traditional upper-right portion of the standard coordinate axis. To give you a picture, the <span class="math notranslate nohighlight">\(2\)</span>-dimensional non-negative orthant is the blue region of the following figure:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axisartist</span> <span class="kn">import</span> <span class="n">SubplotZero</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patch</span>

<span class="k">class</span> <span class="nc">myAxes</span><span class="p">():</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xlim</span> <span class="o">=</span> <span class="n">xlim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span> <span class="o">=</span> <span class="n">ylim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">figsize</span>  <span class="o">=</span> <span class="n">figsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__scale_arrows</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">__drawArrow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> 
            <span class="n">color</span>       <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>
            <span class="n">clip_on</span>     <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
            <span class="n">head_width</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span><span class="p">,</span> 
            <span class="n">head_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_length</span>
        <span class="p">)</span> 
        
    <span class="k">def</span> <span class="nf">__scale_arrows</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Make the arrows look good regardless of the axis limits &quot;&quot;&quot;</span>
        <span class="n">xrange</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">yrange</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span>  <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">xrange</span><span class="o">/</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">yrange</span><span class="o">/</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__drawAxis</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Draws the 2D cartesian axis</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># A subplot with two additional axis, &quot;xzero&quot; and &quot;yzero&quot;</span>
        <span class="c1"># corresponding to the cartesian axis</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">SubplotZero</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
        
        <span class="c1"># make xzero axis (horizontal axis line through y=0) visible.</span>
        <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;xzero&quot;</span><span class="p">,</span><span class="s2">&quot;yzero&quot;</span><span class="p">]:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># make the other axis (left, bottom, top, right) invisible</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="s2">&quot;bottom&quot;</span><span class="p">,</span> <span class="s2">&quot;top&quot;</span><span class="p">]:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            
        <span class="c1"># Plot limits</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="c1"># Draw the arrows</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__drawArrow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># x-axis arrow</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__drawArrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span> <span class="c1"># y-axis arrow</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
        
    <span class="k">def</span> <span class="nf">draw</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># First draw the axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">figsize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__drawAxis</span><span class="p">()</span>

<span class="n">axes</span> <span class="o">=</span> <span class="n">myAxes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

<span class="n">rectangle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rectangle</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_theory_18_0.png" src="_images/single-network-models_theory_18_0.png" />
</div>
</div>
<p>Now, what is the unit ball? You are probably familiar with the idea of the unit ball, even if you haven’t heard it called that specifically. Remember that the Euclidean norm for a point <span class="math notranslate nohighlight">\(\vec x\)</span> which has coordinates <span class="math notranslate nohighlight">\(x_i\)</span> for <span class="math notranslate nohighlight">\(i=1,...,K\)</span> is given by the expression:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \left|\left|\vec x\right|\right|_2 = \sqrt{\sum_{i = 1}^K x_i^2}
\end{align*}\]</div>
<p>The Euclidean unit ball is just the set of points whose Euclidean norm is at most <span class="math notranslate nohighlight">\(1\)</span>. To be more specific, the <strong>closed unit ball</strong> with the Euclidean norm is the set of points:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \left\{\vec x \in \mathbb R^K :\left|\left|\vec x\right|\right|_2 \leq 1\right\}
\end{align*}\]</div>
<p>We draw the <span class="math notranslate nohighlight">\(2\)</span>-dimensional unit ball with the Euclidean norm below, where the points that make up the unit ball are shown in red:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axes</span> <span class="o">=</span> <span class="n">myAxes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

<span class="n">circle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_theory_20_0.png" src="_images/single-network-models_theory_20_0.png" />
</div>
</div>
<p>Now what is their intersection? Remember that the intersection of two sets <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A \cap B &amp;= \{x : x \in A, x \in B\}
\end{align*}\]</div>
<p>That is, each element must be in <em>both</em> sets to be in the intersection. The interesction of the unit ball and the non-negative orthant will be the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
   \mathcal X_K = \left\{\vec x \in \mathbb R^K :\left|\left|\vec x\right|\right|_2 \leq 1, x_k \geq 0 \textrm{ for all $k$}\right\}
\end{align*}\]</div>
<p>visually, this will be the set of points in the <em>overlap</em> of the unit ball and the non-negative orthant, which we show below in purple:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axes</span> <span class="o">=</span> <span class="n">myAxes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

<span class="n">circle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
<span class="n">rectangle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rectangle</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-network-models_theory_22_0.png" src="_images/single-network-models_theory_22_0.png" />
</div>
</div>
<p>This space has an <em>incredibly</em> important corollary. It turns out that if <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> are both elements of <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>, that <span class="math notranslate nohighlight">\(\left\langle \vec x, \vec y \right \rangle = \vec x^\top \vec y\)</span>, the <strong>inner product</strong>, is at most <span class="math notranslate nohighlight">\(1\)</span>, and at least <span class="math notranslate nohighlight">\(0\)</span>. Without getting too technical, this is because of something called the Cauchy-Schwartz inequality and the properties of <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>. If you remember from linear algebra, the Cauchy-Schwartz inequality states that <span class="math notranslate nohighlight">\(\left\langle \vec x, \vec y \right \rangle\)</span> can be at most the product of <span class="math notranslate nohighlight">\(\left|\left|\vec x\right|\right|_2\)</span> and <span class="math notranslate nohighlight">\(\left|\left|\vec y\right|\right|_2\)</span>. Since <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> have norms both less than or equal to <span class="math notranslate nohighlight">\(1\)</span> (since they are on the <em>unit ball</em>), their inner-product is at most <span class="math notranslate nohighlight">\(1\)</span>. Further, since <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> are in the non-negative orthant, their inner product can never be negative. This is because both <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> have entries which are not negative, and therefore their element-wise products can never be negative.</p>
<p>The <em>a posteriori</em> RDPG is to the <em>a priori</em> RDPG what the <em>a posteriori</em> SBM was to the <em>a priori</em> SBM. We instead suppose that we do <em>not</em> know the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, but instead know how we can characterize the individual latent positions. We have the following parameter:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>F</p></td>
<td><p>inner-product distributions</p></td>
<td><p>A distribution which governs each latent position.</p></td>
</tr>
</tbody>
</table>
<p>The parameter <span class="math notranslate nohighlight">\(F\)</span> is what is known as an <strong>inner-product distribution</strong>. In the simplest case, we will assume that <span class="math notranslate nohighlight">\(F\)</span> is a distribution on a subset of the possible real vectors that have <span class="math notranslate nohighlight">\(d\)</span>-dimensions with an important caveat: for any two vectors within this subset, their inner product <em>must</em> be a probability. We will refer to the subset of the possible real vectors as <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>, which we learned about above. This means that for any <span class="math notranslate nohighlight">\(\vec x_i, \vec x_j\)</span> that are in <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>, it is always the case that <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span> is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. This is essential because like previously, we will describe the distribution of each edge in the adjacency matrix using <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span> to represent a probability. Next, we will treat the latent position matrix as a matrix-valued random variable which is <em>latent</em> (remember, <em>latent</em> means that we don’t get to see it in our real data). Like before, we will call <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i\)</span> the random latent positions for the nodes of our network. In this case, each <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span> is sampled independently and identically from the inner-product distribution <span class="math notranslate nohighlight">\(F\)</span> described above. The latent-position matrix is the matrix-valued random variable <span class="math notranslate nohighlight">\(\mathbf X\)</span> whose entries are the latent vectors <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span>, for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes.</p>
<p>The model for edges of the <em>a posteriori</em> RDPG can be described by conditioning on this unobserved latent-position matrix. We write down that, conditioned on <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i = \vec x\)</span> and <span class="math notranslate nohighlight">\(\vec {\mathbf x}_j = \vec y\)</span>, that if <span class="math notranslate nohighlight">\(j &gt; i\)</span>, then <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(\vec x^\top \vec y)\)</span> distribution. As before, if <span class="math notranslate nohighlight">\(i &lt; j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (the network is <em>undirected</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> (the network is <em>loopless</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an <em>a posteriori</em> RDPG with parameter <span class="math notranslate nohighlight">\(F\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim RDPG_n(F)\)</span>.</p>
<div class="section" id="id5">
<h6><span class="section-number">2.7.7.2.1. </span>Probability<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h6>
<p>The probability for the <em>a posteriori</em> RDPG is fairly complicated. This is because, like the <em>a posteriori</em> SBM, we do not actually get to see the latent position matrix <span class="math notranslate nohighlight">\(\mathbf X\)</span>, so we need to use <em>integration</em> to obtain an expression for the probability. Here, we are concerned with realizations of <span class="math notranslate nohighlight">\(\mathbf X\)</span>. Remember that <span class="math notranslate nohighlight">\(\mathbf X\)</span> is just a matrix whose rows are <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span>, each of which individually have have the distribution <span class="math notranslate nohighlight">\(F\)</span>; e.g., <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i \sim F\)</span> independently. For simplicity, we will assume that <span class="math notranslate nohighlight">\(F\)</span> is a disrete distribution on <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>. This makes the logic of what is going on below much simpler since the notation gets less complicated, but does not detract from the generalizability of the result (the only difference is that sums would be replaced by multivariate integrals, and probability mass functions replaced by probability density functions).</p>
<p>We will let <span class="math notranslate nohighlight">\(p\)</span> denote the probability mass function (p.m.f.) of this discrete distribution function <span class="math notranslate nohighlight">\(F\)</span>. The strategy will be to use the independence assumption, followed by integration over the relevant rows of <span class="math notranslate nohighlight">\(\mathbf X\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;= \mathbb P_\theta(\mathbf A = A) \\
&amp;= \prod_{j &gt; i} \mathbb P(\mathbf a_{ij} = a_{ij}), \;\;\;\;\textrm{Independence Assumption} \\
\mathbb P(\mathbf a_{ij} = a_{ij})&amp;= \sum_{\vec x \in \mathcal X_K}\sum_{\vec y \in \mathcal X_K}\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y),\;\;\;\;\textrm{integration over }\vec {\mathbf x}_i \textrm{ and }\vec {\mathbf x}_j
\end{align*}\]</div>
<p>Next, we will simplify this expression a little bit more, using the definition of a conditional probability like we did before for the SBM:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\\
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;= \mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) \mathbb P(\vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Further, remember that if <span class="math notranslate nohighlight">\(\mathbf a\)</span> and <span class="math notranslate nohighlight">\(\mathbf b\)</span> are independent, then <span class="math notranslate nohighlight">\(\mathbb P(\mathbf a = a, \mathbf b = b) = \mathbb P(\mathbf a = a)\mathbb P(\mathbf b = b)\)</span>. Using that <span class="math notranslate nohighlight">\(\vec x_i\)</span> and <span class="math notranslate nohighlight">\(\vec x_j\)</span> are independent, by definition:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;= \mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Which means that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;=  \mathbb P(\mathbf a_{ij} = a_{ij} | \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y)\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Finally, we that conditional on <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i = \vec x_i\)</span> and <span class="math notranslate nohighlight">\(\vec{\mathbf x}_j = \vec x_j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is <span class="math notranslate nohighlight">\(Bern(\vec x_i^\top \vec x_j)\)</span>. This means that in terms of our probability matrix, each entry <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec x_j\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;= (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}
\end{align*}\]</div>
<p>This implies that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;=  (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>So our complete expression for the probability is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;= \prod_{j &gt; i}\sum_{\vec x \in \mathcal X_K}\sum_{\vec y \in \mathcal X_K} (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
</div>
</div>
</div>
<div class="section" id="generalized-random-dot-product-graph-grdpg">
<h4><span class="section-number">2.7.8. </span>Generalized Random Dot Product Graph (GRDPG)<a class="headerlink" href="#generalized-random-dot-product-graph-grdpg" title="Permalink to this headline">¶</a></h4>
<p>The Generalized Random Dot Product Graph, or GRDPG, is the most general random network model we will consider in this book. Note that for the RDPG, the probability matrix <span class="math notranslate nohighlight">\(P\)</span> had entries <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec x_j\)</span>. What about <span class="math notranslate nohighlight">\(p_{ji}\)</span>? Well, <span class="math notranslate nohighlight">\(p_{ji} = \vec x_j^\top \vec x_i\)</span>, which is exactly the same as <span class="math notranslate nohighlight">\(p_{ij}\)</span>! This means that even if we were to consider a directed RDPG, the probabilities that can be captured are <em>always</em> going to be symmetric. The generalized random dot product graph, or GRDPG, relaxes this assumption. This is achieved by using <em>two</em> latent positin matrices, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, and letting <span class="math notranslate nohighlight">\(P = X Y^\top\)</span>. Now, the entries <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec y_j\)</span>, but <span class="math notranslate nohighlight">\(p_{ji} = \vec x_j^\top \vec y_i\)</span>, which might be different.</p>
<div class="section" id="a-priori-grdpg">
<h5><span class="section-number">2.7.8.1. </span><em>A Priori</em> GRDPG<a class="headerlink" href="#a-priori-grdpg" title="Permalink to this headline">¶</a></h5>
<p>The <em>a priori</em> GRDPG is a GRDPG in which we know <em>a priori</em> the latent position matrices <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. The <em>a priori</em> GRDPG has the following parameters:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(X\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \mathbb R^{n \times d}\)</span></p></td>
<td><p>The matrix of left latent positions for each node <span class="math notranslate nohighlight">\(n\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(Y\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \mathbb R^{n \times d}\)</span></p></td>
<td><p>The matrix of right latent positions for each node <span class="math notranslate nohighlight">\(n\)</span>.</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> behave nearly the same as the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> for the <em>a priori</em> RDPG, with the exception that they will be called the <strong>left latent position matrix</strong> and the <strong>right latent position matrix</strong> respectively. Further, the vectors <span class="math notranslate nohighlight">\(\vec x_i\)</span> will be the left latent positions, and <span class="math notranslate nohighlight">\(\vec y_i\)</span> will be the right latent positions, for a given node <span class="math notranslate nohighlight">\(i\)</span>, for each node <span class="math notranslate nohighlight">\(i=1,...,n\)</span>.</p>
<p>What is the generative model for the <em>a priori</em> GRDPG? As we discussed above, given <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, for all <span class="math notranslate nohighlight">\(j \neq i\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(\vec x_i^\top \vec y_j)\)</span> independently. If we consider only loopless networks, <span class="math notranslate nohighlight">\(\mathbf a_{ij} = 0\)</span>. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> GRDPG with left and right latent position matrices <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim GRDPG_n(X, Y)\)</span>.</p>
</div>
<div class="section" id="a-posteriori-grdpg">
<h5><span class="section-number">2.7.8.2. </span><em>A Posteriori</em> GRDPG<a class="headerlink" href="#a-posteriori-grdpg" title="Permalink to this headline">¶</a></h5>
<p>The <em>A Posteriori</em> GRDPG is very similar to the <em>a posteriori</em> RDPG. We have two parameters:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>F</p></td>
<td><p>inner-product distributions</p></td>
<td><p>A distribution which governs the left latent positions.</p></td>
</tr>
<tr class="row-odd"><td><p>G</p></td>
<td><p>inner-product distributions</p></td>
<td><p>A distribution which governs the right latent positions.</p></td>
</tr>
</tbody>
</table>
<p>Here, we treat the left and right latent position matrices as latent variable matrices, like we did for <em>a posteriori</em> RDPG. That is, the left latent positions are sampled independently and identically from <span class="math notranslate nohighlight">\(F\)</span>, and the right latent positions <span class="math notranslate nohighlight">\(\vec y_i\)</span> are sampled independently and identically from <span class="math notranslate nohighlight">\(G\)</span>.</p>
<p>The model for edges of the <em>a posteriori</em> RDPG can be described by conditioning on the unobserved left and right latent-position matrices. We write down that, conditioned on <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i = \vec x\)</span> and <span class="math notranslate nohighlight">\(\vec {\mathbf y}_j = \vec y\)</span>, that if <span class="math notranslate nohighlight">\(j \neq i\)</span>, then <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(\vec x^\top \vec y)\)</span> distribution. As before, assuming the network is loopless, <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an <em>a posteriori</em> RDPG with parameter <span class="math notranslate nohighlight">\(F\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim GRDPG_n(F, G)\)</span>.</p>
</div>
</div>
<div class="section" id="inhomogeneous-erdos-renyi-ier">
<h4><span class="section-number">2.7.9. </span>Inhomogeneous Erdös-Rényi (IER)<a class="headerlink" href="#inhomogeneous-erdos-renyi-ier" title="Permalink to this headline">¶</a></h4>
<p>In the preceding models, we typically made assumptions about how we could characterize the edge-existence probabilities using fewer than <span class="math notranslate nohighlight">\(\binom n 2\)</span> different probabilities (one for each edge). The reason for this is that in general, <span class="math notranslate nohighlight">\(n\)</span> is usually relatively large, so attempting to actually learn <span class="math notranslate nohighlight">\(\binom n 2\)</span> different probabilities is not, in general, going to be very feasible (it is <em>never</em> feasible when we have a single network, since a single network only one observation for each independent edge). Further, it is relatively difficult to ask questions for which assuming edges share <em>nothing</em> in common (even if they don’t share the same probabilities, there may be properties underlying the probabilities, such as the <em>latent positions</em> that we saw above with the RDPG, that we might still want to characterize) is actually favorable.</p>
<p>Nonetheless, the most general model for an independent-edge random network is known as the Inhomogeneous Erdös-Rényi (IER) Random Network. An IER Random Network is characterized by the following parameters:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(P\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{n \times n}\)</span></p></td>
<td><p>The edge probability matrix.</p></td>
</tr>
</tbody>
</table>
<p>The probability matrix <span class="math notranslate nohighlight">\(P\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix, where each entry <span class="math notranslate nohighlight">\(p_{ij}\)</span> is a probability (a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>). Further, if we restrict ourselves to the case of simple networks like we have done so far, <span class="math notranslate nohighlight">\(P\)</span> will also be symmetric (<span class="math notranslate nohighlight">\(p_{ij} = p_{ji}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>). The generative model is similar to the preceding models we have seen: given the <span class="math notranslate nohighlight">\((i, j)\)</span> entry of <span class="math notranslate nohighlight">\(P\)</span>, denoted <span class="math notranslate nohighlight">\(p_{ij}\)</span>, the edges <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> are independent <span class="math notranslate nohighlight">\(Bern(p_{ij})\)</span>, for any <span class="math notranslate nohighlight">\(j &gt; i\)</span>. Further, <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> (the network is <em>loopless</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (the network is <em>undirected</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency maatrix for an IER network with probability matarix <span class="math notranslate nohighlight">\(P\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim IER_n(P)\)</span>.</p>
<p>It is worth noting that <em>all</em> of the preceding models we have discussed so far are special cases of the IER model. This means that, for instance, if we were to consider only the probability matrices where all of the entries are the same, we could represent the ER models. Similarly, if we were to only to consider the probability matrices <span class="math notranslate nohighlight">\(P\)</span> where <span class="math notranslate nohighlight">\(P = XX^\top\)</span>, we could represent any RDPG.</p>
<p>The IER Random Network can be thought of as the limit of Stochastic Block Models, as the number of communities equals the number of nodes in the network. Stated another way, an SBM Random Network where each node is in its own community is equivalent to an IER Random Network. Under this formulation, note that the block matarix for such an SBM, <span class="math notranslate nohighlight">\(B\)</span>, would have <span class="math notranslate nohighlight">\(n \times n\)</span> unique entries. Taking <span class="math notranslate nohighlight">\(P\)</span> to be this block matrix shows that the IER is a limiting case of SBMs.</p>
<div class="section" id="id6">
<h5><span class="section-number">2.7.9.1. </span>Probability<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h5>
<p>The probability for a network which is IER is very straightforward. We use the independence assumption, and the p.m.f. of a Bernoulli-distributed random-variable <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \mathbb P(\mathbf A = A) \\
    &amp;= \prod_{j &gt; i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - a_{ij}}
\end{align*}\]</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-representations/ch6/ch6"></span><div class="section" id="learning-network-representations">
<h2><span class="section-number">3. </span>Learning Network Representations<a class="headerlink" href="#learning-network-representations" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-representations/ch6/estimating-parameters_mle"></span><div class="section" id="estimating-parameters-in-network-models-via-mle">
<h3><span class="section-number">3.1. </span>Estimating Parameters in Network Models via MLE<a class="headerlink" href="#estimating-parameters-in-network-models-via-mle" title="Permalink to this headline">¶</a></h3>
<p>When we learned about random networks which can be described using single network models, one of the key things we covered were the <em>parameters</em> that define the underlying random networks. If we see a network which is a realization of a random network, we do <em>not</em>, in practice, know what those parameters that describe the random network are. However, we have a slight problem, because learning about the underlying random network <em>requires</em> us to have some understanding of the parameters that define it. What are we to do?</p>
<p>To overcome this hurdle, we must <em>estimate</em> the parameters of the underlying random network. At a very high level, <strong>estimation</strong> is a procedure to calculate properties about a random variable (or a set of random variables) using only the data we are given: finitely many (in network statistics, often just one) samples which we assume are realizations of the random variable we want to learn about. Here, what we want to obtain are ways in which we can <em>estimate</em> the parameters of the underlying random network, when we have a realization of a random network.</p>
<div class="section" id="erdos-renyi-er">
<h4><span class="section-number">3.1.1. </span>Erdös-Rényi (ER)<a class="headerlink" href="#erdos-renyi-er" title="Permalink to this headline">¶</a></h4>
<p>Recall that the Erdös-Rényi (ER) network has a single parameter: the probability of each edge existing, which we termed <span class="math notranslate nohighlight">\(p\)</span>. Due to the simplicity of a random network which is ER, we can resort to the Maximum Likelihood technique we described above, and it turns out we obtain virtually the same result. We find that the best estimate of the probability of an edge existing in an ER random network is just the ratio of the total number of edges in the network, <span class="math notranslate nohighlight">\(m\)</span>, divided by the total number of edges possible in the network, which is <span class="math notranslate nohighlight">\(\binom n 2\)</span>! Our result is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat p &amp;= \frac{m}{\binom n 2}.
\end{align*}\]</div>
<p>Intuitively, the estimate of the probability <span class="math notranslate nohighlight">\(p\)</span> is the ratio of how many edges we see in the network, <span class="math notranslate nohighlight">\(m\)</span>, and how many edges we could have seen <span class="math notranslate nohighlight">\(\binom n 2\)</span>! To bring this back to our coin flip example, this is like we are saying that there is a single coin. We flip the coin once for every possible edge between those pairs of communities, <span class="math notranslate nohighlight">\(\binom n 2\)</span>. When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads we obtained, <span class="math notranslate nohighlight">\(m\)</span>, and divide by the number of coin flips we made, <span class="math notranslate nohighlight">\(\binom n 2\)</span>.</p>
<p>Let’s work on an example. We will use a realization of a random network which is ER, with <span class="math notranslate nohighlight">\(40\)</span> nodes and an edge probability of <span class="math notranslate nohighlight">\(0.2\)</span>. We begin by simulating and visualizing the appropriate network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Simulated ER(0.2)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_mle_2_0.png" src="_images/estimating-parameters_mle_2_0.png" />
</div>
</div>
<p>Next, we fit the appropriate model, from graspologic, and plot the estimated probability matrix <span class="math notranslate nohighlight">\(\hat P\)</span> against the true probability matrix <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.models</span> <span class="kn">import</span> <span class="n">EREstimator</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">EREstimator</span><span class="p">(</span><span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Phat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">p_mat_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Phat</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\hat P_</span><span class="si">{ER}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">P</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>  <span class="c1"># default entries to 0.2</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">P</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P_</span><span class="si">{ER}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Phat</span> <span class="o">-</span> <span class="n">P</span><span class="p">),</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|\hat P_</span><span class="si">{ER}</span><span class="s2"> - P_</span><span class="si">{ER}</span><span class="s2">|$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_mle_5_0.png" src="_images/estimating-parameters_mle_5_0.png" />
</div>
</div>
<p>Not half bad! The estimated probability matrix <span class="math notranslate nohighlight">\(\hat P\)</span> looks extremely similar to the true probability matrix <span class="math notranslate nohighlight">\(P\)</span>.</p>
</div>
<div class="section" id="stochastic-block-model">
<h4><span class="section-number">3.1.2. </span>Stochastic Block Model<a class="headerlink" href="#stochastic-block-model" title="Permalink to this headline">¶</a></h4>
<p>The Stochastic Block Model also has a single parameter: the block matrix, <span class="math notranslate nohighlight">\(B\)</span>, whose entries <span class="math notranslate nohighlight">\(b_{kk'}\)</span> denote the probabilities of edges existing or not existing between pairs of communities in the Stochastic Block Model. When we apply the method of MLE to the SBM, what we find is that, where <span class="math notranslate nohighlight">\(m_{kk'}\)</span> is the total number of edges between nodes in communities <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span>, and <span class="math notranslate nohighlight">\(n_{kk'}\)</span> is the number of edges possible between nodes in communities <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat b_{kk'} = \frac{m_{kk'}}{n_{kk'}}.
\end{align*}\]</div>
<p>Intuitively, the estimate of the block probability <span class="math notranslate nohighlight">\(b_{kk'}\)</span> is the ratio of how many edges we see between communities <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span> <span class="math notranslate nohighlight">\(m_{kk'}\)</span> and how many edges we could have seen <span class="math notranslate nohighlight">\(n_{kk'}\)</span>! To bring this back to our coin flip example, this is like we are saying that there is one coin called coin <span class="math notranslate nohighlight">\((k, k')\)</span> for each pair of communities in our network. We flip each coin once for every possible edge between those pairs of communities, <span class="math notranslate nohighlight">\(n_{kk'}\)</span>. When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads we obtained, <span class="math notranslate nohighlight">\(m_{kk'}\)</span>, and divide by the number of coin flips we made, <span class="math notranslate nohighlight">\(n_{kk'}\)</span>.</p>
<p>Let’s work through an example network, with 20 nodes in each community, and a block matrix of:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    B &amp;= \begin{bmatrix}
        .8 &amp; .2 \\
        .2 &amp; .8
    \end{bmatrix}
\end{align*}\]</div>
<p>Which corresponds to a probability matrix <span class="math notranslate nohighlight">\(P\)</span> where each entry is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p_{ij} &amp;= \begin{cases}
    0.8 &amp; i, j \leq 20 \text{ or }i, j \geq 20 \\
    0.2 &amp; \text{otherwise}
    \end{cases}
\end{align*}\]</div>
<p>We begin by simulating an appropriate SBM:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">],</span>
     <span class="p">[</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">]]</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Simulated SBM(B)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_mle_8_0.png" src="_images/estimating-parameters_mle_8_0.png" />
</div>
</div>
<p>Next, let’s fit an appropriate SBM, and investigate the estimate of <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.models</span> <span class="kn">import</span> <span class="n">SBMEstimator</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SBMEstimator</span><span class="p">(</span><span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">Bhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">block_p_</span>
<span class="n">Phat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">p_mat_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Bhat</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\hat B_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$B_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Bhat</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">)),</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|\hat B_</span><span class="si">{SBM}</span><span class="s2"> - B_</span><span class="si">{SBM}</span><span class="s2">|$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_mle_11_0.png" src="_images/estimating-parameters_mle_11_0.png" />
</div>
</div>
<p>And our estimate <span class="math notranslate nohighlight">\(\hat B\)</span> is very similar to the true block matrix <span class="math notranslate nohighlight">\(B\)</span>. This is further reflected by looking at the probability matrix, like we did for the ER example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Phat</span><span class="p">,</span>
        <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\hat P_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">P</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">n</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">n</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>  <span class="c1"># default entries to 0.2</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>  <span class="c1"># B11</span>
<span class="n">P</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>  <span class="c1"># B22</span>
<span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># loopless</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span>
        <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Phat</span> <span class="o">-</span> <span class="n">P</span><span class="p">),</span>
        <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|\hat P_</span><span class="si">{SBM}</span><span class="s2"> - P_</span><span class="si">{SBM}</span><span class="s2">|$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_mle_13_0.png" src="_images/estimating-parameters_mle_13_0.png" />
</div>
</div>
</div>
</div>
<span id="document-representations/ch6/why-embed-networks"></span><div class="section" id="why-embed-networks">
<h3><span class="section-number">3.2. </span>Why embed networks?<a class="headerlink" href="#why-embed-networks" title="Permalink to this headline">¶</a></h3>
<p>Networks by themselves can have interesting properties, but a network is not how we traditionally organize data in machine learning. In almost any ML algorithm - whether you’re using a neural network or a decision tree, whether your goal is to classify observations or to predict values using regression - you’ll see data organized into a matrix, where the rows represent observations and the columns represent features, or variables. Each observation, or row of the matrix, is traditionally represented as a single point in <span class="math notranslate nohighlight">\(d\)</span>-dimensional space (if there are <span class="math notranslate nohighlight">\(d\)</span> columns in the matrix). If you have two columns, for instance, you could represent data organized in this way on an x/y coordinate plane. The first column would represent the x-axis, and the second column would represent the y-axis.</p>
<p>For example, the data below is organized traditionally. On the left is the data matrix; each observation has its own row, with two features across the columns. The x-column contains the first feature for each observation, and the y-column contains the second feature for each observation. We can see the two clusters of data numerically, through the color mapping.</p>
<p>On the right is the same data, but plotted in Euclidean space. Each column of the data matrix gets its own axis in the plot. The x and y axis location of the <span class="math notranslate nohighlight">\(i^{th}\)</span> point in the scatterplot is the same as the x and y values of the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of the data matrix. We can see the two clusters of data geometrically, through the location of the points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 1

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># make the data</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> 
                    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">X</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                  <span class="n">centers</span><span class="o">=</span><span class="n">centers</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># convert data into a DataFrame</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.gridspec</span> <span class="kn">import</span> <span class="n">GridSpec</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">GraphColormap</span><span class="p">,</span> <span class="n">draw_cartesian</span>

<span class="c1"># setup</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;divergent&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">color</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">axm</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="c1"># plot left</span>
<span class="n">hm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axm</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)),</span> <span class="o">*</span><span class="n">hm</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># plot right</span>
<span class="n">draw_cartesian</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>

<span class="c1"># lines</span>
<span class="n">max_</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plot</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">max_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">max_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># ticks</span>
<span class="n">plot</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plot</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>

<span class="c1"># set axis bounds</span>
<span class="n">lim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">max_</span><span class="p">,</span> <span class="n">max_</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="n">lim</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="n">lim</span><span class="p">)</span>

<span class="c1"># title, etc</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Euclidean data represented as a data matrix and represented in Euclidean space&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/why-embed-networks_3_0.png" src="_images/why-embed-networks_3_0.png" />
</div>
</div>
<p>It’s often useful for our data to be organized like this, since it opens the door to a wide variety of machine learning methods. With the data above, for example, we could use scikit-learn to perform simple K-Means Clustering to find the two clusters of observations. Below, we import scikit-learn’s K-Means clustering algorithm. K-Means finds a pre-determined number of clusters in your data by setting randomly determined starting-points, and then iterating to get those points closer to the true cluster means. It outputs the community membership labels for each datapoint, which you can see below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted labels: &quot;</span><span class="p">,</span> <span class="n">predicted_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted labels:  [0 0 0 0 0 1 1 1 1 1]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>

<span class="c1"># plot right</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> 
                       <span class="n">hue</span><span class="o">=</span><span class="n">predicted_labels</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;qualitative&quot;</span><span class="p">])</span>

<span class="c1"># lines</span>
<span class="n">plot</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">max_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">max_</span><span class="p">,</span> <span class="n">max_</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># ticks</span>
<span class="n">plot</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plot</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># title</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Clustered data after K-Means&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/why-embed-networks_6_0.png" src="_images/why-embed-networks_6_0.png" />
</div>
</div>
<p>Network-valued data are different. Take the Stochastic Block Model below, shown as both a layout plot and an adjacency matrix. Say your goal is to view the nodes as particular observations, and you’d like to cluster the data in the same way you clustered the Euclidean data above. Intuitively, you’d expect to find two groups: one for the first set of heavily connected nodes, and one for the second set. Unfortunately, traditional machine learning algorithms won’t work on data represented as a network: it doesn’t live in the traditional rows-as-observations, columns-as-features format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">]])</span>
<span class="n">A</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="n">p</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">aimport</span> graphbook_code
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;A Network With Two Groups&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/why-embed-networks_9_0.png" src="_images/why-embed-networks_9_0.png" />
</div>
</div>
<p>You, of course, <em>can</em> make up methods which work directly on networks - algorithms which run by traversing along edges, for instance, or which use network statistics like node degree to learn, and so on - and data scientists have developed many algorithms like this. But to be able to use the entire toolbox that machine learning offers, you’d like to be able to figure out a way to <em>represent</em> networks in Euclidean space as tabular data. This is why having good embedding methods, like Spectral Embedding (which we’ll learn about soon), is useful. There’s another problem with networks that make embedding into lower-dimensional space useful.</p>
<div class="section" id="high-dimensionality-of-network-data">
<h4><span class="section-number">3.2.1. </span>High Dimensionality of Network Data<a class="headerlink" href="#high-dimensionality-of-network-data" title="Permalink to this headline">¶</a></h4>
<p>The other problem with network data is its high dimensionality. You could view each element of an adjacency matrix as its own (binary, for unweighted networks) dimension, for instance – although you could also make the argument that talking about dimensionality doesn’t even make <em>sense</em> with network data, since it doesn’t live in Euclidean space. Regardless, if you were to view the elements of the adjacency matrix as their own dimensions, you can get to a fairly unmanageable number of dimensions fairly quickly. Many dimensions can generally be unmanageable largely because of a machine learning concept called the <em>curse of dimensionality</em>, described below.</p>
<div class="admonition-the-curse-of-dimensionality admonition">
<p class="admonition-title">The Curse of Dimensionality</p>
<p>Our intuition often fails when observations have a lot of features – meaning, observations that, when you think of them geometrically, are points in very high-dimensional space.</p>
<p>For example, pick a point randomly in a 10,000-dimensional unit hypercube (meaning, a <span class="math notranslate nohighlight">\(1 \times 1 \times \dots \times 1\)</span> cube, with ten thousand 1s). You can also just think of this point as a vector with 10,000 elements, each of which has a value between 0 and 1. There’s a probability greater than 99.999999% that the point will be located a distance less than .001 from a border of the hypercube. This probability is only 0.4% in a unit square. This actually makes intuitive sense: if you think about measuring a lot of attributes of an object, there’s a decent chance it’ll be extreme in at least one of those attributes. Take yourself, for example. You’re probably normal in a lot of ways, but I’m sure you can think of a part of yourself which is extreme compared to other people.</p>
<p>An even bigger shocker: if you pick two random points in a unit square with two dimensions, they’ll be on average 0.52 units of distance away from each other. However, if you pick two random points in a unit hypercube with a million dimensions, they’ll be around 408 units away from each other. This implies that, on average, any set of points that you generate from some random process when you’re in high dimensions will be extremely far away from each other.</p>
<p>What this comes down to is that almost every point in ultra-high dimensions is extremely lonely, hugging the edge of the space it lives in, all by itself. These facts mess with many traditional machine learning methods which use relative distances, or averages (very few points in high-dimensional space will actually be anywhere near their average!) <span id="id1"></span></p>
</div>
<p>This is where network embedding methods come into play. Because networks represented as adjacency matrices are extremely high-dimensional, they run into many of the issues described above. Embedding, much like traditional dimensionality reduction methods in machine learning like Principle Component Analysis (PCA), allows us to move down to a more manageable number of dimensions while still preserving useful information about the network.</p>
</div>
<div class="section" id="we-often-embed-to-estimate-latent-positions">
<h4><span class="section-number">3.2.2. </span>We Often Embed To Estimate Latent Positions<a class="headerlink" href="#we-often-embed-to-estimate-latent-positions" title="Permalink to this headline">¶</a></h4>
<p>The embedding methods which we’ll explore the most in this book are the spectral methods. These methods pull heavily from linear algebra to keep only the information about our network which is useful - and use that information to place nodes in Euclidean space. We’ll explore other methods as well. It’s worth it to know a bit of linear algebra review here, particularly on concepts like eigenvectors and eigenvalues, as well as the properties of symmetric matrices. We’ll guide you as clearly as possible through the math in future sections.</p>
<p>Spectral embedding methods in particular, which we’ll talk about in the next section, will estimate an embedding called the latent position matrix. This is an <span class="math notranslate nohighlight">\(n \times d\)</span> matrix (where this are <span class="math notranslate nohighlight">\(n\)</span> rows, one for each node, and <span class="math notranslate nohighlight">\(d\)</span> dimensions for each row). The latent position matrix is thus organized like a traditional data table, with nodes corresponding to observations, and you could plot the rows as points in Euclidean space.</p>
</div>
<div class="section" id="what-the-heck-is-the-latent-position-matrix-anyway">
<h4><span class="section-number">3.2.3. </span>What The Heck Is The Latent Position Matrix, Anyway?<a class="headerlink" href="#what-the-heck-is-the-latent-position-matrix-anyway" title="Permalink to this headline">¶</a></h4>
<p>What actually is a latent position? How can we interpret a latent position matrix?</p>
<p>Well, assuming you’re viewing your network as some type of random dot product graph (remember that this can include SBMs, ER networks, and more), you can think of every node as being secretly associated with a position in Euclidean space. This position (relative to the positions associated with other nodes) tells you the probability that one node will have an edge with another node.</p>
<p>Let’s call the latent position matrix, <span class="math notranslate nohighlight">\(X\)</span>. Remember that <span class="math notranslate nohighlight">\(X\)</span> has <span class="math notranslate nohighlight">\(n\)</span> rows (the number of nodes) and <span class="math notranslate nohighlight">\(d\)</span> columns (the number of dimensions). Although in practice you almost never know what the latent position matrix <em>actually</em> is, you can <em>estimate it</em> by embedding your network.</p>
<p>We’re going to cheat a bit and use an embedding method (in this case, adjacency spectral embedding) before we’ve discussed it, just to show what this looks like. In the next section, you’ll learn how this embedding is happening, but for now, just think of it as a way to estimate the latent positions for the nodes of a network and move from network space to Euclidean space.</p>
<p>Below we make a network, which in this case is an SBM. From the network, we can estimate a set of latent positions, where <span class="math notranslate nohighlight">\(n=20\)</span> rows for each node and <span class="math notranslate nohighlight">\(d=2\)</span> dimensions. Usually when something is an estimation for something else in statistics, you put a hat over it: <span class="math notranslate nohighlight">\(\hat{X}\)</span>. We’ll do that here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># make a network</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">A</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># embed</span>
<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span><span class="p">,</span> <span class="n">plot_latents</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;A Set Of Estimated Latent Positions $\hat</span><span class="si">{X}</span><span class="s2">$ </span><span class="se">\n</span><span class="s2">(Matrix Representation)&quot;</span><span class="p">,</span> 
             <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">});</span>

<span class="n">ax_eucl</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;A Set of Estimated Latent Positions $\hat</span><span class="si">{X}</span><span class="s2">$ (Euclidean Representation)&quot;</span><span class="p">,</span> 
             <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/why-embed-networks_18_0.png" src="_images/why-embed-networks_18_0.png" />
</div>
</div>
<p>It’s good to emphasize here that we’re modeling our networks as <em>random dot-product graphs</em> (RDPGs). One implication is that we can think of a network as having some underlying probability distribution, and any specific network is one of many possible realizations of that distribution. It also means that each edge in our network has some <em>probability</em> of existing: nodes 0 and 3, for instance, may or may not have an edge. The concept of a latent position only works under the assumption that the network is drawn from an RDPG.</p>
<div class="section" id="the-latent-position-matrix-tells-you-about-edge-probabilities">
<h5><span class="section-number">3.2.3.1. </span>The Latent Position Matrix Tells You About Edge Probabilities<a class="headerlink" href="#the-latent-position-matrix-tells-you-about-edge-probabilities" title="Permalink to this headline">¶</a></h5>
<p>We mentioned before that the relative locations of latent positions tell you about edge probabilities, but it’s good to be a bit more specific. If you take the dot product (or the weighted sum) of row <span class="math notranslate nohighlight">\(i\)</span> of the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> with row <span class="math notranslate nohighlight">\(j\)</span>, you’ll get the probability that nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> have an edge between them. Incidentally, this means that the dot product between any two rows of the latent position matrix has to be bound between 0 and 1.</p>
<div class="section" id="making-a-block-probability-matrix-from-the-latent-positions">
<h6><span class="section-number">3.2.3.1.1. </span>Making A Block Probability Matrix From The Latent Positions<a class="headerlink" href="#making-a-block-probability-matrix-from-the-latent-positions" title="Permalink to this headline">¶</a></h6>
<p>Similarly, you can find the block probability matrix <span class="math notranslate nohighlight">\(P\)</span> for your network using the latent positions. How would you generate <span class="math notranslate nohighlight">\(P\)</span> from <span class="math notranslate nohighlight">\(X\)</span>?</p>
<p>Well, you’d just multiply it by its transpose: <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. This operation will take the dot product between every row of <span class="math notranslate nohighlight">\(X\)</span> and put it in the result. <span class="math notranslate nohighlight">\((XX^\top)_{ij}\)</span> will just be the dot product between rows <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of the latent position matrix (which is the probability that nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> will be connected). So, <span class="math notranslate nohighlight">\(XX^\top\)</span> is just the <span class="math notranslate nohighlight">\(n \times n\)</span> block probability matrix - and if you’ve estimated your latent positions using real-world data, you can also estimate the block probability matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">text</span>


<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
<span class="n">B0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">)</span>
<span class="n">B1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># block probability matrix</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">([[</span><span class="n">B0</span><span class="p">,</span> <span class="n">B1</span><span class="p">],</span>
              <span class="p">[</span><span class="n">B1</span><span class="p">,</span> <span class="n">B0</span><span class="p">]])</span>



<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Estimated block </span><span class="se">\n</span><span class="s2">probability matrix&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Actual block </span><span class="se">\n</span><span class="s2">probability matrix&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># text</span>
<span class="n">text</span><span class="p">(</span><span class="s2">&quot;.8&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">)</span>
<span class="n">text</span><span class="p">(</span><span class="s2">&quot;.8&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">)</span>
<span class="n">text</span><span class="p">(</span><span class="s2">&quot;.1&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">)</span>
<span class="n">text</span><span class="p">(</span><span class="s2">&quot;.1&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/why-embed-networks_24_0.png" src="_images/why-embed-networks_24_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="thinking-about-latent-positions-geometrically">
<h5><span class="section-number">3.2.3.2. </span>Thinking About Latent Positions Geometrically<a class="headerlink" href="#thinking-about-latent-positions-geometrically" title="Permalink to this headline">¶</a></h5>
<p>You can also think about this stuff geometrically. The dot product between any two vectors <span class="math notranslate nohighlight">\(u_i\)</span> and <span class="math notranslate nohighlight">\(u_j\)</span>, geometrically, is their lengths multiplied together and then weighted by the cosine of the angle between them. Smaller angles have cosines close to 1, and larger angles have cosines close to 0. So, nodes whose latent positions have larger angles between them tend to have lower edge probabilities, and nodes whose latent positions have smaller angles between them tend to have higher edge probabilities. This is the core intuition you need to understand why you can find communities and do downstream inference with latent position matrices: two nodes whose latent positions are further apart will have a smaller probability of having an edge between them!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>


<span class="c1">####### First Ax</span>
<span class="c1"># background plot</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;tab10&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;bottom&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>

<span class="c1"># plot vector arrows</span>
<span class="n">u_i</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">u_j</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u_i</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u_i</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u_j</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u_j</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="c1"># Text and text arrow</span>
<span class="n">style</span> <span class="o">=</span> <span class="s2">&quot;Simple, tail_width=0.5, head_width=4, head_length=8&quot;</span>
<span class="n">kw</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="n">style</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span>
<span class="n">text_arrow</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">FancyArrowPatch</span><span class="p">((</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">),</span> <span class="p">(</span><span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="o">.</span><span class="mi">01</span><span class="p">),</span> <span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3, rad=.2&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
<span class="n">txt</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">angle close to 90°, cos(angle) close to 0, so </span>
<span class="s2">dot product = probability of edge smaller</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.22</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="n">txt</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">text_arrow</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Positions In Different Communities </span><span class="se">\n</span><span class="s2">Have A Lower Dot Product&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">);</span>

<span class="c1">####### Second Ax</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;tab10&quot;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;bottom&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">(</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>

<span class="c1"># plot vector arrows</span>
<span class="n">u_i</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">u_j</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u_j</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u_i</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u_j</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u_j</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="c1"># Text and text arrow</span>
<span class="n">style</span> <span class="o">=</span> <span class="s2">&quot;Simple, tail_width=0.5, head_width=4, head_length=8&quot;</span>
<span class="n">kw</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="n">style</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span>
<span class="n">text_arrow</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">FancyArrowPatch</span><span class="p">((</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">.</span><span class="mi">15</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3, rad=.7&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
<span class="n">txt</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">angle close to 0°, cos(angle) close to 1, so </span>
<span class="s2">dot product = probability of edge larger</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.22</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="n">txt</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">text_arrow</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Positions In The Same Community </span><span class="se">\n</span><span class="s2">Have A Higher Dot Product&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/why-embed-networks_27_0.png" src="_images/why-embed-networks_27_0.png" />
</div>
</div>
<p>If you have an <em>estimate</em> for the latent positions, there’s math that shows that you get a pretty good estimate for the block probability matrix as well. In practice, that’s what you’re actually doing: getting an estimate of the latent positions with spectral embedding, then using those to do more downstream tasks or estimating block probability matrices.</p>
</div>
</div>
</div>
<span id="document-representations/ch6/spectral-embedding"></span><div class="section" id="spectral-embedding-methods">
<h3><span class="section-number">3.3. </span>Spectral Embedding Methods<a class="headerlink" href="#spectral-embedding-methods" title="Permalink to this headline">¶</a></h3>
<p>One of the primary embedding tools we’ll use in this book is a set of methods called <em>spectral embedding</em> <span id="id1"></span>. You’ll see spectral embedding and variations on it repeatedly, both throughout this section and when we get into applications, so it’s worth taking the time to understand spectral embedding deeply. If you’re familiar with Principal Component Analysis (PCA), this method has a lot of similarities. We’ll need to get into a bit of linear algebra to understand how it works.</p>
<p>Remember that the basic idea behind any network embedding method is to take the network and put it into Euclidean space - meaning, a nice data table with rows as observations and columns as features (or dimensions), which you can then plot on an x-y axis. In this section, you’ll see the linear algebra-centric approach that spectral embedding uses to do this.</p>
<p>Spectral methods are based on a bit of linear algebra, but hopefully a small enough amount to still be understandable. The overall idea has to do with eigenvectors, and more generally, something called “singular vectors” - a generalization of eigenvectors. It turns out that the biggest singular vectors of a network’s adjacency matrix contain the most information about that network - and as the singular vectors get smaller, they contain less information about the network (we’re glossing over what ‘information’ means a bit here, so just think about this as a general intuition). So if you represent a network in terms of its singular vectors, you can drop the smaller ones and still retain most of the information. This is the essence of what spectral embedding is about (here “biggest” means “the singular vector corresponding to the largest singular value”).</p>
<div class="admonition-singular-values-and-singular-vectors admonition">
<p class="admonition-title">Singular Values and Singular Vectors</p>
<p>If you don’t know what singular values and singular vectors are, don’t worry about it. You can think of them as a generalization of eigenvalues/vectors (it’s also ok if you don’t know what those are): all matrices have singular values and singular vectors, but not all matrices have eigenvalues and eigenvectors. In the case of square, symmetric matrices with positive eigenvalues, the eigenvalues/vectors and singular values/vectors are the same thing.</p>
<p>If you want some more background information on eigenstuff and singularstuff, there are some explanations in the Math Refresher section in the introduction. They’re an important set of vectors associated with matrices with a bunch of interesting properties. A lot of linear algebra is built around exploring those properties.</p>
</div>
<p>You can see visually how Spectral Embedding works below. We start with a 20-node Stochastic Block Model with two communities, and then found its singular values and vectors. It turns out that because there are only two communities, only the first two singular vectors contain information – the rest are just noise! (you can see this if you look carefully at the first two columns of the eigenvector matrix). So, we took these two columns and scaled them by the first two singular vectors of the singular value matrix <span class="math notranslate nohighlight">\(D\)</span>. The final embedding is that scaled matrix, and the plot you see takes the rows of that matrix and puts them into Euclidean space (an x-y axis) as points. This matrix is called the <em>latent position matrix</em>, and the embeddings for the nodes are called the <em>latent positions</em>. Underneath the figure is a list that explains how the algorithm works, step-by-step.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span><span class="p">,</span> <span class="n">cmaps</span><span class="p">,</span> <span class="n">plot_latents</span>
<span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">to_laplacian</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>

<span class="k">def</span> <span class="nf">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># Make network</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">A</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">to_laplacian</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">Ut</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Uc</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span>
<span class="n">Ec</span> <span class="o">=</span> <span class="n">E</span><span class="p">[:</span><span class="n">n_components</span><span class="p">]</span>
<span class="n">latents</span> <span class="o">=</span> <span class="n">Uc</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Ec</span><span class="p">)</span>
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">06</span><span class="p">,</span> <span class="o">-.</span><span class="mi">06</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span> 
<span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network Representation&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>


<span class="c1"># add arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># add joint matrix</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Left Singular vector matrix $U$&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.55</span><span class="p">,</span> <span class="o">-.</span><span class="mi">06</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">E</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Singular value matrix $S$&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-.</span><span class="mi">06</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">Ut</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Right singular vector matrix $V^T$&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
    
<span class="c1"># add second arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">style</span> <span class="o">=</span> <span class="s2">&quot;Simple, tail_width=10, head_width=40, head_length=20&quot;</span>
<span class="n">kw</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="n">style</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">text_arrow</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">FancyArrowPatch</span><span class="p">((</span><span class="mf">0.33</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">),</span> <span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3, rad=-.55&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
<span class="n">arrow_ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">text_arrow</span><span class="p">)</span>


<span class="c1"># Embedding</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">185</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Positions </span><span class="se">\n</span><span class="s2">(matrix representation)&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;First two scaled columns of $U$&quot;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">185</span><span class="o">+.</span><span class="mi">45</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Positions (Euclidean representation)&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Plotting the rows of U as points in space&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;The Spectral Embedding Algorithm&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mf">1.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_2_0.png" src="_images/spectral-embedding_2_0.png" />
</div>
</div>
<div class="admonition-the-spectral-embedding-algorithm admonition">
<p class="admonition-title">The Spectral Embedding Algorithm</p>
<ol class="simple">
<li><p>Take a network’s adjacency matrix. Optionally take its Laplacian as a network representation.</p></li>
<li><p>Decompose it into a a singular vector matrix, a singular value matrix, and the singular vector matrix’s transpose.</p></li>
<li><p>Remove every column of the singular vector matrix except for the first <span class="math notranslate nohighlight">\(k\)</span> vectors, corresponding to the <span class="math notranslate nohighlight">\(k\)</span> largest singular values.</p></li>
<li><p>Scale the <span class="math notranslate nohighlight">\(k\)</span> remaining columns by their corresponding singular values to create the embedding.</p></li>
<li><p>The rows of this embedding matrix are the locations in Euclidean space for the nodes of the network (called the latent positions). The embedding matrix is an estimate of the latent position matrix (which we talked about in the ‘why embed networks’ section)</p></li>
</ol>
</div>
<p>We need to dive into a few specifics to understand spectral embedding better. We need to figure out how to find our network’s singular vectors, for instance, and we also need to understand why those singular vectors can be used to form a representation of our network. To do this, we’ll explore a few concepts from linear algebra like matrix rank, and we’ll see how understanding these concepts connects to understanding spectral embedding.</p>
<p>Let’s scale down and make a simple network, with only six nodes. We’ll take its Laplacian just to show what that optional step looks like, and then we’ll find its singular vectors with a technique we’ll explore called Singular Value Decomposition. Then, we’ll explore why we can use the first <span class="math notranslate nohighlight">\(k\)</span> singular values and vectors to find an embedding. Let’s start with creating the simple network.</p>
<div class="section" id="a-simple-network">
<h4><span class="section-number">3.3.1. </span>A Simple Network<a class="headerlink" href="#a-simple-network" title="Permalink to this headline">¶</a></h4>
<p>Say we have the simple network below. There are six nodes total, numbered 0 through 5, and there are two distinct connected groups (called “connected components” in network theory land). Nodes 0 through 2 are all connected to each other, and nodes 3 through 5 are also all connected to each other.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">add_edge</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">edge</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add an edge to an undirected graph.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">edge</span>
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">A</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">combinations</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">add_edge</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">edge</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">combinations</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">add_edge</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">edge</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see the adjacency matrix and network below. Notice that there are two distrinct blocks in the adjacency matrix: in its upper-left, you can see the edges between the first three nodes, and in the bottom right, you can see the edges between the second three nodes.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>

<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">nx</span><span class="o">.</span><span class="n">kamada_kawai_layout</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Our Simple Network&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_9_0.png" src="_images/spectral-embedding_9_0.png" />
</div>
</div>
</div>
<div class="section" id="the-laplacian-matrix">
<h4><span class="section-number">3.3.2. </span>The Laplacian Matrix<a class="headerlink" href="#the-laplacian-matrix" title="Permalink to this headline">¶</a></h4>
<p>With spectral embedding, we’ll either find the singular vectors of the Laplacian or the singular vectors of the Adjacency Matrix itself (For undirected Laplacians, the singular vectors are the same thing as the eigenvectors). Since we already have the adjacency matrix, let’s take the Laplacian just to see what that looks like.</p>
<p>Remember from chapter four that there are a few different types of Laplacian matrices. By default, for undirected networks, Graspologic uses the normalized Laplacian <span class="math notranslate nohighlight">\(L = D^{-1/2} A D^{-1/2}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the degree matrix. Remember that the degree matrix has the degree, or number of edges, of each node along the diagonals. Variations on the normalized Laplacian are generally what we use in practice, but for simplicity and illustration, we’ll just use the basic, cookie-cutter version of the Laplacian <span class="math notranslate nohighlight">\(L = D - A\)</span>.</p>
<p>Here’s the degree matrix <span class="math notranslate nohighlight">\(D\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build the degree matrix D</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">degrees</span><span class="p">)</span>
<span class="n">D</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[2, 0, 0, 0, 0, 0],
       [0, 2, 0, 0, 0, 0],
       [0, 0, 2, 0, 0, 0],
       [0, 0, 0, 2, 0, 0],
       [0, 0, 0, 0, 2, 0],
       [0, 0, 0, 0, 0, 2]])
</pre></div>
</div>
</div>
</div>
<p>And here’s the Laplacian matrix, written out in full.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build the Laplacian matrix L</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">D</span><span class="o">-</span><span class="n">A</span>
<span class="n">L</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 2., -1., -1.,  0.,  0.,  0.],
       [-1.,  2., -1.,  0.,  0.,  0.],
       [-1., -1.,  2.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  2., -1., -1.],
       [ 0.,  0.,  0., -1.,  2., -1.],
       [ 0.,  0.,  0., -1., -1.,  2.]])
</pre></div>
</div>
</div>
</div>
<p>Below, you can see these matrices visually.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">Normalize</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">GraphColormap</span>
<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># First axis (Degree)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Degree Matrix $D$&quot;</span><span class="p">)</span>

<span class="c1"># Second axis (-)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">y</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> 
            <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Third axis (Adjacency matrix)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Adjacency Matrix $A$&quot;</span><span class="p">)</span>

<span class="c1"># Third axis (=)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">y</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
            <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Fourth axis</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Laplacian Matrix $L$&quot;</span><span class="p">)</span>

<span class="c1"># Colorbar</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">color</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;The Laplacian is just a function of the adjacency matrix&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_17_0.png" src="_images/spectral-embedding_17_0.png" />
</div>
</div>
</div>
<div class="section" id="finding-singular-vectors-with-singular-value-decomposition">
<h4><span class="section-number">3.3.3. </span>Finding Singular Vectors With Singular Value Decomposition<a class="headerlink" href="#finding-singular-vectors-with-singular-value-decomposition" title="Permalink to this headline">¶</a></h4>
<p>Now that we have a Laplacian matrix, we’ll want to find its singular vectors. To do this, we’ll need to use a technique called <em>Singular Value Decomposition</em>, or SVD.</p>
<p>SVD is a way to break a single matrix apart (also known as factorizing) into three distinct new matrices – In our case, the matrix will be the Laplacian we just built. These three new matrices correspond to the singular vectors and singular values of the original matrix: the algorithm will collect all of the singular vectors as columns of one matrix, and the singular values as the diagonals of another matrix.</p>
<p>In the case of the Laplacian (as with all symmetric matrices that have real, positive eigenvalues), remember that the singular vectors/values and the eigenvectors/values are the same thing. For more technical and generalized details on how SVD works, or for explicit proofs, we would recommend a Linear Algebra textbook [Trefethan, LADR]. Here, we’ll look at the SVD with a bit more detail here in the specific case where we start with a matrix which is square, symmetric, and has real eigenvalues.</p>
<p><strong>Singular Value Decomposition</strong> Suppose you have a square, symmetrix matrix <span class="math notranslate nohighlight">\(X\)</span> with real eigenvalues. In our case, <span class="math notranslate nohighlight">\(X\)</span> corresponds to the Laplacian <span class="math notranslate nohighlight">\(L\)</span> (or the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>).</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\begin{bmatrix}
    x_{11} &amp; &amp; &amp; &quot; \\
    &amp; x_{22} &amp; &amp; \\
    &amp; &amp; \ddots &amp; \\
    &quot; &amp; &amp; &amp; x_{nn}
    \end{bmatrix}
\end{align*}\]</div>
<p>Then, you can find three matrices - one which rotates vectors in space, one which scales them along each coordinate axis, and another which rotates them back - which, when you multiply them all together, recreate the original matrix <span class="math notranslate nohighlight">\(X\)</span>. This is the essence of singular value decomposition: you can break down any linear transformation into a rotation, a scaling, and another rotation. Let’s call the matrix which rotates <span class="math notranslate nohighlight">\(U\)</span> (this type of matrix is called “orthogonal”), and the matrix that scales <span class="math notranslate nohighlight">\(S\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X &amp;= U S V^T
\end{align*}\]</div>
<p>Since <span class="math notranslate nohighlight">\(U\)</span> is a matrix that just rotates any vector, all of its column-vectors are orthogonal (all at right angles) from each other and they all have the unit length of 1. These columns are more generally called the <strong>singular vectors</strong> of X. In some specific cases, these are also called the eigenvectors. Since <span class="math notranslate nohighlight">\(S\)</span> just scales, it’s a diagonal matrix: there are values on the diagonals, but nothing (0) on the off-diagonals. The amount that each coordinate axis is scaled are the values on the diagonal entries of <span class="math notranslate nohighlight">\(S\)</span>, <span class="math notranslate nohighlight">\(\sigma_{i}\)</span>. These are <strong>singular values</strong> of the matrix <span class="math notranslate nohighlight">\(X\)</span>, and, also when some conditions are met, these are also the eigenvalues. Assuming our network is undirected, this will be the case with the Laplacian matrix, but not necessarily the adjacency matrix.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X &amp;= \begin{bmatrix}
    \uparrow &amp; \uparrow &amp;  &amp; \uparrow \\
    u_1 &amp; \vec u_2 &amp; ... &amp; \vec u_n \\
    \downarrow &amp; \downarrow &amp;  &amp; \downarrow
    \end{bmatrix}\begin{bmatrix}
    \sigma_1 &amp; &amp;  &amp; \\
    &amp; \sigma_2 &amp;  &amp; \\
    &amp; &amp; \ddots &amp; \\
    &amp; &amp; &amp; \sigma_n
    \end{bmatrix}\begin{bmatrix}
    \leftarrow &amp; \vec u_1^T &amp; \rightarrow \\
    \leftarrow &amp; \vec u_2^T &amp; \rightarrow \\
    &amp; \vdots &amp; \\
    \leftarrow &amp; \vec u_n^T &amp; \rightarrow \\
    \end{bmatrix}
\end{align*}\]</div>
</div>
<div class="section" id="breaking-down-our-network-s-laplacian-matrix">
<h4><span class="section-number">3.3.4. </span>Breaking Down Our Network’s Laplacian matrix<a class="headerlink" href="#breaking-down-our-network-s-laplacian-matrix" title="Permalink to this headline">¶</a></h4>
<p>Now we know how to break down any random matrix into singular vectors and values with SVD, so let’s apply it to our toy network. We’ll break down our Laplacian matrix into <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(S\)</span>, and <span class="math notranslate nohighlight">\(V^\top\)</span>. The Laplacian is a special case where the singular values and singular vectors are the same as the eigenvalues and eigenvectors, so we’ll just refer to them as eigenvalues and eigenvectors from here on, since those terms are more common. For similar (actually the same) reasons, in this case <span class="math notranslate nohighlight">\(V^\top = U^\top\)</span>.</p>
<p>Here, the leftmost column of <span class="math notranslate nohighlight">\(U\)</span> (and the leftmost eigenvalue in <span class="math notranslate nohighlight">\(S\)</span>) correspond to the eigenvector with the highest eigenvalue, and they’re organized in descending order (this is standard for Singular Value Decomposition).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># First axis (Laplacian)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$L$&quot;</span><span class="p">)</span>

<span class="c1"># Second axis (=)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">y</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> 
            <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Third axis (U)</span>
<span class="n">U_ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U$&quot;</span><span class="p">)</span>
<span class="n">U_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Columns of eigenvectors&quot;</span><span class="p">)</span>

<span class="c1"># Third axis (S)</span>
<span class="n">E_ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$S$&quot;</span><span class="p">)</span>
<span class="n">E_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Eigenvalues on diagonal&quot;</span><span class="p">)</span>

<span class="c1"># Fourth axis (V^T)</span>
<span class="n">Ut_ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">Vt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$V^T$&quot;</span><span class="p">)</span>
<span class="n">Ut_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Rows of eigenvectors&quot;</span><span class="p">)</span>

<span class="c1"># Colorbar</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">color</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Decomposing our simple Laplacian into eigenvectors and eigenvalues with SVD&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_23_0.png" src="_images/spectral-embedding_23_0.png" />
</div>
</div>
<p>So now we have a collection of eigenvectors organized into a matrix with <span class="math notranslate nohighlight">\(U\)</span>, and a collection of their corresponding eigenvalues organized into a matrix with <span class="math notranslate nohighlight">\(S\)</span>. Remember that with Spectral Embedding, we keep only the largest eigenvalues/vectors and “clip” columns off of <span class="math notranslate nohighlight">\(U\)</span>.</p>
<p>Why exactly do these matrices reconstruct our Laplacian when multiplied together? Why does the clipped version of <span class="math notranslate nohighlight">\(U\)</span> give us a lower-dimensional representation of our network? To answer that question, we’ll need to start talking about a concept in linear algebra called the <em>rank</em> of a matrix.</p>
<p>The essential idea is that you can turn each eigenvector/eigenvalue pair into a low-information matrix instead of a vector and number. Summing all of these matrices lets you reconstruct <span class="math notranslate nohighlight">\(L\)</span>. Summing only a few of these matrices lets you get <em>close</em> to <span class="math notranslate nohighlight">\(L\)</span>. In fact, if you were to unwrap the two matrices into single vectors, the vector you get from summing is as close in Euclidean space as you possibly can get to <span class="math notranslate nohighlight">\(L\)</span> given the information you deleted when you removed the smaller eigenvectors.</p>
<p>Let’s dive into it!</p>
</div>
<div class="section" id="why-we-care-about-taking-eigenvectors-matrix-rank">
<h4><span class="section-number">3.3.5. </span>Why We Care About Taking Eigenvectors: Matrix Rank<a class="headerlink" href="#why-we-care-about-taking-eigenvectors-matrix-rank" title="Permalink to this headline">¶</a></h4>
<p>When we embed anything to create a new representation, we’re essentially trying to find a simpler version of that thing which preserves as much information as possible. This leads us to the concept of <strong>matrix rank</strong>.</p>
<p><strong>Matrix Rank</strong>: The rank of a matrix <span class="math notranslate nohighlight">\(X\)</span>, defined <span class="math notranslate nohighlight">\(rank(X)\)</span>, is the number of linearly independent rows and columns of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>At a very high level, we can think of the matrix rank as telling us just how “simple” <span class="math notranslate nohighlight">\(X\)</span> is. A matrix which is rank <span class="math notranslate nohighlight">\(1\)</span> is very simple: all of its rows or columns can be expressed as a weighted sum of just a single vector. On the other hand, a matrix which has “full rank”, or a rank equal to the number of rows (or columns, whichever is smaller), is a bit more complex: no row nor column can be expressed as a weighted sum of other rows or columns.</p>
<p>There are a couple ways that the rank of a matrix and the singular value decomposition interact which are critical to understand: First, you can make a matrix from your singular vectors and values (eigenvectors and values, in our Laplacian’s case), and summing all of them recreates your original, full-rank matrix. Each matrix that you add to the sum increases the rank of the result by one. Second, summing only a few of them gets you to the best estimation of the original matrix that you can get to, given the low-rank result. Let’s explore this with a bit more depth.</p>
<p>We’ll be using the Laplacian as our examples, which has the distinctive quality of having its eigenvectors be the same as its singular vectors. For the adjacency matrix, this theory all still works, but you’d just have to replace <span class="math notranslate nohighlight">\(\vec u_i \vec u_i^\top\)</span> with <span class="math notranslate nohighlight">\(\vec u_i \vec v_i^\top\)</span> throughout (the adjacency matrices’ SVD is <span class="math notranslate nohighlight">\(A = U S V^\top\)</span>, since the right singular vectors might be different than the left singular vectors).</p>
<div class="section" id="summing-rank-1-matrices-recreates-the-original-matrix">
<h5><span class="section-number">3.3.5.1. </span>Summing Rank 1 Matrices Recreates The Original Matrix<a class="headerlink" href="#summing-rank-1-matrices-recreates-the-original-matrix" title="Permalink to this headline">¶</a></h5>
<p>You can actually create an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix using any one of the original Laplacian’s eigenvectors <span class="math notranslate nohighlight">\(\vec u_i\)</span> by taking its outer product <span class="math notranslate nohighlight">\(\vec{u_i} \vec{u_i}^T\)</span>. This creates a rank one matrix which only contains the information stored in the first eigenvector. Scale it by its eigenvalue <span class="math notranslate nohighlight">\(\sigma_i\)</span> and you have something that feels suspiciously similar to how we take the first few singular vectors of <span class="math notranslate nohighlight">\(U\)</span> and scale them in the spectral embedding algorithm.</p>
<p>It turns out that we can express any matrix <span class="math notranslate nohighlight">\(X\)</span> as the sum of all of these rank one matrices.
Take the <span class="math notranslate nohighlight">\(i^{th}\)</span> column of <span class="math notranslate nohighlight">\(U\)</span>. Remember that we’ve been calling this <span class="math notranslate nohighlight">\(\vec u_i\)</span>: the <span class="math notranslate nohighlight">\(i^{th}\)</span> eigenvector of our Laplacian. Its corresponding eigenvalue is the <span class="math notranslate nohighlight">\(i^{th}\)</span> element of the diagonal eigenvalue matrix <span class="math notranslate nohighlight">\(E\)</span>. You can make a rank one matrix from this eigenvalue/eigenvector pair by taking the outer product and scaling the result by the eigenvalue: <span class="math notranslate nohighlight">\(\sigma_i \vec u_i \vec u_i^T\)</span>.</p>
<p>It turns out that when we take the sum of all of these rank <span class="math notranslate nohighlight">\(1\)</span> matrices–each one corresponding to a particular eigenvalue/eigenvector pair–we’ll recreate the original matrix.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X &amp;= \sum_{i = 1}^n \sigma_i \vec u_i \vec u_i^T = \sigma_1 \begin{bmatrix}\uparrow \\ \vec u_1 \\ \downarrow\end{bmatrix}\begin{bmatrix}\leftarrow &amp; \vec u_1^T &amp; \rightarrow \end{bmatrix} + 
    \sigma_2 \begin{bmatrix}\uparrow \\ \vec u_2 \\ \downarrow\end{bmatrix}\begin{bmatrix}\leftarrow &amp; \vec u_2^T &amp; \rightarrow \end{bmatrix} + 
    ... + 
    \sigma_n \begin{bmatrix}\uparrow \\ \vec u_n \\ \downarrow\end{bmatrix}\begin{bmatrix}\leftarrow &amp; \vec u_n^T &amp; \rightarrow \end{bmatrix}
\end{align*}\]</div>
<p>Here are all of the <span class="math notranslate nohighlight">\(\sigma_i \vec u_i \vec u_i^T\)</span> for our Laplacian L. Since there were six nodes in the original network, there are six eigenvalue/vector pairs, and six rank 1 matrices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_nodes</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># For each eigenvector/value,</span>
<span class="c1"># find its outer product,</span>
<span class="c1"># and append it to a list.</span>
<span class="n">low_rank_matrices</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">):</span>
    <span class="n">ui</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span> <span class="n">node</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">vi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">Vt</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span> <span class="n">node</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">low_rank_matrix</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">*</span> <span class="n">ui</span> <span class="o">@</span> <span class="n">vi</span><span class="o">.</span><span class="n">T</span>
    <span class="n">low_rank_matrices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">low_rank_matrix</span><span class="p">)</span>
    
<span class="c1"># Take the elementwise sum of every matrix in the list.</span>
<span class="n">laplacian_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">low_rank_matrices</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see the result of the sum below. On the left are all of the low-rank matrices - one corresponding to each eigenvector - and on the right is the sum of all of them. You can see that the sum is just our Laplacian!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.gridspec</span> <span class="kn">import</span> <span class="n">GridSpec</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">ax_laplacian</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:])</span>

<span class="c1"># Plot low-rank matrices</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">])</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;$\sigma_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> u_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> v_</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">^T$&quot;</span>
        <span class="n">heatmap</span><span class="p">(</span><span class="n">low_rank_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        
<span class="c1"># Plot Laplacian</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">laplacian_sum</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax_laplacian</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$L = \sum_{i = 1}^n \sigma_i u_i v_i^T$&quot;</span><span class="p">)</span>

<span class="c1"># # Colorbar</span>
<span class="n">cax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">04</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">laplacian_sum</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">laplacian_sum</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">color</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">use_gridspec</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>


<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;We can recreate our simple Laplacian by summing all the low-rank matrices&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_31_0.png" src="_images/spectral-embedding_31_0.png" />
</div>
</div>
<p>Next up, we’ll estimate the Laplacian by only taking a few of these matrices. You can already kind of see in the figure above that this’ll work - the last two matrices don’t even have anything in them (they’re just 0)!</p>
</div>
<div class="section" id="we-can-approximate-our-simple-laplacian-by-only-summing-a-few-of-the-low-rank-matrices">
<h5><span class="section-number">3.3.5.2. </span>We can approximate our simple Laplacian by only summing a few of the low-rank matrices<a class="headerlink" href="#we-can-approximate-our-simple-laplacian-by-only-summing-a-few-of-the-low-rank-matrices" title="Permalink to this headline">¶</a></h5>
<p>When you sum the first few of these low-rank <span class="math notranslate nohighlight">\(\sigma_i u_i u_i^T\)</span>, you can <em>approximate</em> your original matrix.</p>
<p>This tells us something interesting about Spectral Embedding: the information in the first few eigenvectors of a high rank matrix lets us find a more simple approximation to it. You can take a matrix that’s extremely complicated (high-rank) and project it down to something which is much less complicated (low-rank).</p>
<p>Look below. In each plot, we’re summing more and more of these low-rank matrices. By the time we get to the fourth sum, we’ve totally recreated the original Laplacian.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">current</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">new</span> <span class="o">=</span> <span class="n">low_rank_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">current</span> <span class="o">+=</span> <span class="n">new</span>
    <span class="n">heatmap</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
            <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$\sum_</span><span class="se">{{</span><span class="s2">i = 1</span><span class="se">}}</span><span class="s2">^</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> \sigma_i u_i u_i^T$&quot;</span><span class="p">)</span>
    
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Each of these is the sum of an </span><span class="se">\n</span><span class="s2">increasing number of low-rank matrices&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_35_0.png" src="_images/spectral-embedding_35_0.png" />
</div>
</div>
</div>
<div class="section" id="approximating-becomes-extremely-useful-when-we-have-a-bigger-now-regularized-laplacian">
<h5><span class="section-number">3.3.5.3. </span>Approximating becomes extremely useful when we have a bigger (now regularized) Laplacian<a class="headerlink" href="#approximating-becomes-extremely-useful-when-we-have-a-bigger-now-regularized-laplacian" title="Permalink to this headline">¶</a></h5>
<p>This becomes even more useful when we have huge networks with thousands of nodes, but only a few communities. It turns out, especially in this situation, we can usually sum a very small number of low-rank matrices and get to an excellent approximation for our network that uses much less information.</p>
<p>Take the network below, for example. It’s generated from a Stochastic Block Model with 1000 nodes total (500 in one community, 500 in another). We took its normalized Laplacian (remember that this means <span class="math notranslate nohighlight">\(L = D^{-1/2} A D^{-1/2}\)</span>), decomposed it, and summed the first two low-rank matrices that we generated from the eigenvector columns.</p>
<p>The result is not exact, but it looks pretty close. And we only needed the information from the first two singular vectors instead of all of the information in our full <span class="math notranslate nohighlight">\(n \times n\)</span> matrix!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">to_laplacian</span>

<span class="c1"># Make network</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span>
<span class="n">A2</span><span class="p">,</span> <span class="n">labels2</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Form new laplacian</span>
<span class="n">L2</span> <span class="o">=</span> <span class="n">to_laplacian</span><span class="p">(</span><span class="n">A2</span><span class="p">)</span>

<span class="c1"># decompose</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">U2</span><span class="p">,</span> <span class="n">E2</span><span class="p">,</span> <span class="n">Ut2</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">L2</span><span class="p">)</span>

<span class="n">k_matrices</span> <span class="o">=</span> <span class="n">U2</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span>
<span class="n">low_rank_approximation</span> <span class="o">=</span> <span class="n">U2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">E2</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">])</span> <span class="o">@</span> <span class="n">Ut2</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">,</span> <span class="p">:])</span>


<span class="c1"># Plotting</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">l2_hm</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">L2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$L$&quot;</span><span class="p">)</span>
<span class="n">l2approx_hm</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">low_rank_approximation</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\sum_{{i = 1}}^</span><span class="si">{2}</span><span class="s2"> \sigma_i u_i u_i^T$&quot;</span><span class="p">)</span>

<span class="n">l2_hm</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Full-rank Laplacian for a 50-node matrix&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">})</span>
<span class="n">l2approx_hm</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Sum of only two low-rank matrices&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">});</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Summing only two low-rank matrices approximates the normalized Laplacian pretty well!&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_38_0.png" src="_images/spectral-embedding_38_0.png" />
</div>
</div>
<p>This is where a lot of the power of an SVD comes from: you can approximate extremely complicated (high-rank) matrices with extremely simple (low-rank) matrices.</p>
</div>
</div>
<div class="section" id="how-this-matrix-rank-stuff-helps-us-understand-spectral-embedding">
<h4><span class="section-number">3.3.6. </span>How This Matrix Rank Stuff Helps Us Understand Spectral Embedding<a class="headerlink" href="#how-this-matrix-rank-stuff-helps-us-understand-spectral-embedding" title="Permalink to this headline">¶</a></h4>
<p>Remember the actual spectral embedding algorithm: we take a network, decompose it with Singular Value Decomposition into its singular vectors and values, and then cut out everything but the top <span class="math notranslate nohighlight">\(k\)</span> singular vector/value pairs. Once we scale the columns of singular vectors by their corresponding values, we have our embedding. That embedding is called the latent position matrix, and the locations in space for each of our nodes are called the latent positions.</p>
<p>Let’s go back to our original, small (six-node) network and make an estimate of the latent position matrix from it. We’ll embed down to three dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">U_cut</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
<span class="n">E_cut</span> <span class="o">=</span> <span class="n">E</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>

<span class="n">latents_small</span> <span class="o">=</span> <span class="n">U_cut</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">E_cut</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">latents_small</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">xticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Eigenvector&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Position Matrix&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_43_0.png" src="_images/spectral-embedding_43_0.png" />
</div>
</div>
<p>How does what we just talked about help us understand spectral embedding?</p>
<p>Well, each column of the latent position matrix is the <span class="math notranslate nohighlight">\(i^{th}\)</span> eigenvector scaled by the <span class="math notranslate nohighlight">\(i^{th}\)</span> eigenvalue: <span class="math notranslate nohighlight">\(\sigma_i \vec{u_i}\)</span>. If we right-multiplied one of those columns by its unscaled transpose <span class="math notranslate nohighlight">\(\vec{u_i}^\top\)</span>, we’d have one of our rank one matrices. This means that you can think of our rank-one matrices as essentially just fancy versions of the columns of a latent position matrix (our embedding). They contain all the same information - they’re just matrices instead of vectors!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># First axis (Degree)</span>
<span class="n">first_col</span> <span class="o">=</span> <span class="n">E</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">latents_small</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">first_mat</span> <span class="o">=</span> <span class="n">first_col</span> <span class="o">@</span> <span class="n">first_col</span><span class="o">.</span><span class="n">T</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">first_col</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">xticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;First Eigenvector&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;First column of </span><span class="se">\n</span><span class="s2">latent position matrix $u_0$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>

<span class="c1"># Third axis (Adjacency matrix)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">first_col</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">xticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Node&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;First column of latent position matrix $u_0^T$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>

<span class="c1"># Third axis (=)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">y</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
            <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Fourth axis</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">first_mat</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;First low-rank </span><span class="se">\n</span><span class="s2">matrix $\sigma_0 u_0 u_0^T$&quot;</span><span class="p">)</span>

<span class="c1"># Colorbar</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">L</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">color</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Our low-rank matrices contain the same information</span><span class="se">\n</span><span class="s2"> as the columns of the latent position matrix&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_45_0.png" src="_images/spectral-embedding_45_0.png" />
</div>
</div>
<p>In fact, you can express the sum we did earlier - our lower-rank estimation of L - with just our latent position matrix! Remember that <span class="math notranslate nohighlight">\(U_k\)</span> is the first <span class="math notranslate nohighlight">\(k\)</span> eigenvectors of our Laplacian, and <span class="math notranslate nohighlight">\(S_k\)</span> is the diagonal matrix with the first <span class="math notranslate nohighlight">\(k\)</span> eigenvalues (and that we named them <span class="math notranslate nohighlight">\(\sigma_1\)</span> through <span class="math notranslate nohighlight">\(\sigma_k\)</span>).</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">matplotlib.transforms</span> <span class="kn">import</span> <span class="n">Affine2D</span>
<span class="kn">import</span> <span class="nn">mpl_toolkits.axisartist.floating_axes</span> <span class="k">as</span> <span class="nn">floating_axes</span>

<span class="c1"># First axis (sum matrix)</span>
<span class="n">current</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">new</span> <span class="o">=</span> <span class="n">low_rank_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">current</span> <span class="o">+=</span> <span class="n">new</span>
    
<span class="n">heatmap</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\sum_{i=1}^2 \sigma_i u_i u_i^T$&quot;</span><span class="p">)</span>

<span class="c1"># Second axis (=)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">y</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
            <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Third axis (Uk)</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Uk</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
<span class="n">Ek</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">E</span><span class="p">)[:</span><span class="n">k</span><span class="p">,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Uk</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">xticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_box_aspect</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Eigenvector&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;$U_k$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>


<span class="c1"># Ek</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Ek</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">xticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;$S_k$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="c1"># Uk^T</span>
<span class="c1"># TODO: make this the same size as Uk, just rotated (currently too small)</span>
<span class="c1"># Will probably involve revamping all this code to make subplots differently,</span>
<span class="c1"># because the reason it&#39;s that size is that the dimensions are constrained by the `plt.subplots` call.</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">Affine2D</span><span class="p">()</span><span class="o">.</span><span class="n">rotate_deg</span><span class="p">(</span><span class="mi">90</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">set_transform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Uk</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">xticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_box_aspect</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;$U_k^T$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_47_0.png" src="_images/spectral-embedding_47_0.png" />
</div>
</div>
<p>This helps gives an intuition for why our latent position matrix gives a representation of our network. You can take columns of it, turn those columns into matrices, and sum those matrices, and then estimate the Laplacian for the network. That means the columns of our embedding network contain all of the information necessary to estimate the network!</p>
</div>
<div class="section" id="figuring-out-how-many-dimensions-to-embed-your-network-into">
<h4><span class="section-number">3.3.7. </span>Figuring Out How Many Dimensions To Embed Your Network Into<a class="headerlink" href="#figuring-out-how-many-dimensions-to-embed-your-network-into" title="Permalink to this headline">¶</a></h4>
<p>One thing we haven’t addressed is how to figure out how many dimensions to embed down to. We’ve generally been embedding into two dimensions throughout this chapter (mainly because it’s easier to visualize), but you can embed into as many dimensions as you want.</p>
<p>If you don’t have any prior information about the “true” dimensionality of your latent positions, by default you’d just be stuck guessing. Fortunately, there are some rules-of-thumb to make your guess better, and some methods people have developed to make fairly decent guesses automatically.</p>
<p>The most common way to pick the number of embedding dimensions is with something called a scree plot. Essentially, the intuition is this: the top singular vectors of an adjacency matrix contain the most useful information about your network, and as the singular vectors have smaller and smaller singular values, they contain less important and so are less important (this is why we’re allowed to cut out the smallest <span class="math notranslate nohighlight">\(n-k\)</span> singular vectors in the spectral embedding algorithm).</p>
<p>The scree plot just plots the singular values by their indices: the first (biggest) singular value is in the beginning, and the last (smallest) singular value is at the end.</p>
<p>You can see the scree plot for the Laplacian we made earlier below. We’re only plotting the first ten singular values for demonstration purposes.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># from graspologic.plot import screeplot</span>
<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Circle</span>
<span class="kn">from</span> <span class="nn">matplotlib.patheffects</span> <span class="kn">import</span> <span class="n">withStroke</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1.anchored_artists</span> <span class="kn">import</span> <span class="n">AnchoredDrawingArea</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">svdvals</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># eigval plot</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">svdvals</span><span class="p">(</span><span class="n">L2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">D</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Singular value index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Singular value&quot;</span><span class="p">)</span>

<span class="c1"># plot circle</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="o">.</span><span class="mi">15</span><span class="p">,</span> <span class="o">.</span><span class="mi">15</span>
<span class="n">radius</span> <span class="o">=</span> <span class="o">.</span><span class="mi">15</span>
<span class="n">ada</span> <span class="o">=</span> <span class="n">AnchoredDrawingArea</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower left&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">circle</span> <span class="o">=</span> <span class="n">Circle</span><span class="p">((</span><span class="mi">105</span><span class="p">,</span> <span class="mi">35</span><span class="p">),</span> <span class="mi">20</span><span class="p">,</span> <span class="n">clip_on</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">0125</span><span class="p">),</span>
                <span class="n">path_effects</span><span class="o">=</span><span class="p">[</span><span class="n">withStroke</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">foreground</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)])</span>
<span class="n">ada</span><span class="o">.</span><span class="n">da</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ada</span><span class="p">)</span>

<span class="c1"># add text</span>
<span class="k">def</span> <span class="nf">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">backgroundcolor</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span>
            <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    
<span class="n">text</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">19</span><span class="p">,</span> <span class="s2">&quot;Elbow&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_51_0.png" src="_images/spectral-embedding_51_0.png" />
</div>
</div>
<p>You’ll notice that there’s a marked area called the “elbow”. This is an area where singular values stop changing in magnitude as much when they get smaller: before the elbow, singular values change rapidly, and after the elbow, singular values barely change at all. (It’s called an elbow because the plot kind of looks like an arm, viewed from the side!)</p>
<p>The location of this elbow gives you a rough indication for how many “true” dimensions your latent positions have. The singular values after the elbow are quite close to each other and have singular vectors which are largely noise, and don’t tell you very much about your data. It looks from the scree plot that we should be embedding down to two dimensions, and that adding more dimensions would probably just mean adding noise to our embedding.</p>
<p>One drawback to this method is that a lot of the time, the elbow location is pretty subjective - real data will rarely have a nice, pretty elbow like the one you see above. The advantage is that it still generally works pretty well; embedding into a few more dimensions than you need isn’t too bad, since you’ll only have a few noies dimensions and there still may be <em>some</em> signal there.</p>
<p>In any case, Graspologic automates the process of finding an elbow using a popular method developed in 2006 by Mu Zhu and Ali Ghodsi at the University of Waterloo. We won’t get into the specifics of how it works here, but you can usually find fairly good elbows automatically.</p>
</div>
<div class="section" id="using-graspologic-to-embed-networks">
<h4><span class="section-number">3.3.8. </span>Using Graspologic to embed networks<a class="headerlink" href="#using-graspologic-to-embed-networks" title="Permalink to this headline">¶</a></h4>
<p>It’s pretty straightforward to use graspologic’s API to embed a network. The setup works like an SKlearn class: you instantiate an AdjacencySpectralEmbed class, and then you use it to transform data. You set the number of dimensions to embed to (the number of eigenvector columns to keep!) with <code class="docutils literal notranslate"><span class="pre">n_components</span></code>.</p>
<div class="section" id="adjacency-spectral-embedding">
<h5><span class="section-number">3.3.8.1. </span>Adjacency Spectral Embedding<a class="headerlink" href="#adjacency-spectral-embedding" title="Permalink to this headline">¶</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>

<span class="c1"># Generate a network from an SBM</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> 
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span>
<span class="n">A</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Instantiate an ASE model and find the embedding</span>
<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_latents</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Adjacency Spectral Embedding&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/spectral-embedding_57_0.png" src="_images/spectral-embedding_57_0.png" />
</div>
</div>
</div>
<div class="section" id="laplacian-spectral-embedding">
<h5><span class="section-number">3.3.8.2. </span>Laplacian Spectral Embedding<a class="headerlink" href="#laplacian-spectral-embedding" title="Permalink to this headline">¶</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">LaplacianSpectralEmbed</span> <span class="k">as</span> <span class="n">LSE</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="n">LSE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_latents</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Laplacian Spectral Embedding&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;left&#39;:&#39;Laplacian Spectral Embedding&#39;}&gt;
</pre></div>
</div>
<img alt="_images/spectral-embedding_60_1.png" src="_images/spectral-embedding_60_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="when-should-you-use-ase-and-when-should-you-use-lse">
<h4><span class="section-number">3.3.9. </span>When should you use ASE and when should you use LSE?<a class="headerlink" href="#when-should-you-use-ase-and-when-should-you-use-lse" title="Permalink to this headline">¶</a></h4>
<p>Throughout this article, we’ve primarily used LSE, since Laplacians have some nice properties (such as having singular values being the same as eigenvalues) that make stuff like SVD easier to explain. However, you can embed the same network with either ASE or LSE, and you’ll get two different (but equally true) embeddings.</p>
<p>Since both embeddings will give you a reasonable clustering, how are they different? When should you use one compared to the other?</p>
<p>Well, it turns out that LSE and ASE capture different notions of “clustering”. Carey Priebe and collaborators at Johns Hopkins University investigated this recently - in 2018 - and discovered that LSE lets you capture “affinity” structure, whereas ASE lets you capture “core-periphery” structure (their paper is called “On a two-truths phenomenon in spectral graph clustering” - it’s an interesting read for the curious). The difference between the two types of structure is shown in the image below.</p>
<div class="figure align-default" id="two-truths">
<a class="reference internal image-reference" href="_images/two-truths.jpeg"><img alt="_images/two-truths.jpeg" src="_images/two-truths.jpeg" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Affinity vs. Core-periphery Structure</span><a class="headerlink" href="#two-truths" title="Permalink to this image">¶</a></p>
</div>
<p>The “affinity” structure - the one that LSE is good at finding - means that you have two groups of nodes which are well-connected within the groups, and aren’t very connected with each other. Think of a friend network in two schools, where people within the same school are much more likely to be friends than people in different schools. This is a type of structure we’ve seen a lot in this book in our Stochastic Block Model examples. If you think the communities in your data look like this, you should apply LSE to your network.</p>
<p>The name “core-periphery” is a good description for this type of structure (which ASE is good at finding). In this notion of clustering, you have a core group of well-connected nodes surrounded by a bunch of “outlier” nodes which just don’t have too many edges with anything in general. Think of a core of popular, well-liked, and charismatic kids at a high school, with a periphery of loners or people who prefer not to socialize as much.</p>
</div>
</div>
<span id="document-representations/ch6/estimating-parameters_spectral"></span><div class="section" id="estimating-parameters-in-network-models-via-spectral-methods">
<h3><span class="section-number">3.4. </span>Estimating Parameters in Network Models via Spectral Methods<a class="headerlink" href="#estimating-parameters-in-network-models-via-spectral-methods" title="Permalink to this headline">¶</a></h3>
<p>Now that we have the ability to estimate parameters via MLE for the SBM, you might think we are done discussing SBM estimation. Unfortunately, we have skipped a relatively fundamental problem with SBM parameter estimation. You will notice that everything we have covered about the SBM to date assumes that we the assignments to one of <span class="math notranslate nohighlight">\(K\)</span> possible communities for each node, which is given by the node assignment variable <span class="math notranslate nohighlight">\(\tau_i\)</span> for each node in the network. Why is this problematic? Well, quite simply, when we are learning about <em>many</em> different networks we might come across, we might not actually <em>observe</em> the communities of each node.</p>
<p>Consider, for instance, the school example we have worked extensively with. In the context of the SBM, it makes sense to guess that two individuals will have a higher chaance of being friends if they attend the same school than if they did not go to the same school. Remember, when we knew what school each student was from and could <em>order</em> the students by school ahead of time, the network looked like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>  <span class="c1"># network with 100 nodes</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]</span>  <span class="c1"># block matrix</span>

<span class="c1"># sample a single simple adjacency matrix from SBM(z, B)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">zs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">zs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$SBM_n(z, B)$ Simulation, nodes ordered by school&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_2_0.png" src="_images/estimating-parameters_spectral_2_0.png" />
</div>
</div>
<p>The block structure is <em>completely obvious</em>, and it seems like we could almost just guess which nodes are from which communities by looking at the adjacency matrix (with the proper ordering). Ant therein lies the issue: if we did not know which school each student was from, we would have <em>no way</em> of actually using the technique we described in the preceding chapter to estimate parameters for our SBM. If our nodes were just randomly ordered, we might see something instead like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a reordering of the n nodes</span>
<span class="n">vtx_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ns</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">Aperm</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">vtx_perm</span><span class="p">])]</span> <span class="p">[:,</span><span class="n">vtx_perm</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_multiplot</span><span class="p">(</span><span class="n">Aperm</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$SBM_n(z, B)$ Simulation, random node order&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_5_0.png" src="_images/estimating-parameters_spectral_5_0.png" />
</div>
</div>
<p>It is extremely unclear which nodes are in which community, because there is no immediate block structure to the network. So, what <em>can</em> we do?</p>
<p>In fact, the way we will proceed is to devise a technique for estimation for RDPGs, and then we will use this technique for RDPG estimation to produce meaningful community assignments for our nodes. Effectively, what we will do is we will use a clever technique we discussed in the preceding sections, called an <em>embedding</em>, to learn not only about each edge, but to <em>learn about each node in relation to all of the other nodes</em>. What do we mean by this? What we mean is that, while looking at a single edge in our network, we only have two possible choices: the edge exists or it does not exist. However, if we instead consider nodes in <em>relation</em> to one another, we can start to deduce patterns about how our nodes might be organized in the community sense. While seeing that two students Bill and Stephanie were friends will not tell us whether Bill and Stephanie were in the same school, if we knew that Bill and Stephanie also shared many other friends (such as Denise, Albert, and Kwan), and those friends also tended to be friends, that piece of information might tell us that they all might happen to be school mates.</p>
<p>The embedding technique we employ, the <em>spectral embedding</em>, allows us to pick up on these <em>community</em> sorts of tendencies. When we call a set of nodes a <strong>community</strong> in the context of a network, what we mean is that these nodes tend to be more connected (more edges exist between and amongst them) than with other nodes in the network. The spectral embeddings will help us to identify these communities of nodes, and hopefully, when we review the communities of nodes we learn, we will be able to derive some sort of insight. For instance, in our school example, ideally, we might pick up on two communities of nodes, one for each school.</p>
<div class="section" id="random-dot-product-graph">
<h4><span class="section-number">3.4.1. </span>Random Dot Product Graph<a class="headerlink" href="#random-dot-product-graph" title="Permalink to this headline">¶</a></h4>
<p>The Random Dot Product Graph has a single parameter, the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. Remember that the latent position matrix has <span class="math notranslate nohighlight">\(n\)</span> rows, one for each node in the network, and <span class="math notranslate nohighlight">\(d\)</span> columns, one for each latent dimension in the network.</p>
<p>Remember that when we see a network which we think might be a realization of an RDPG random network, we do not actually have the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> ahead of time. For this reason, we must <em>estimate</em> <span class="math notranslate nohighlight">\(X\)</span>. Unfortunately, we cannot just use MLE like we did for the ER and SBM networks. Instead, we will use spectral methods.</p>
<p>In order to produce an estimate of <span class="math notranslate nohighlight">\(X\)</span>, we also need to know the number of latent dimensions of <span class="math notranslate nohighlight">\(\pmb A\)</span>, <span class="math notranslate nohighlight">\(d\)</span>. We might have a reasonable ability to “guess” what <span class="math notranslate nohighlight">\(d\)</span> is ahead of time, but this will often not be the case. For this reason, we can instead estimate <span class="math notranslate nohighlight">\(d\)</span> using the strategy described in <span class="xref myst">6.3.7</span>. Once we have an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> and a number of latent dimensions <span class="math notranslate nohighlight">\(d\)</span> (or an estimate of the number of latent dimensions, which we will call <span class="math notranslate nohighlight">\(\hat d\)</span>), producing an estimate of the latent position matrix is very straightforward. We simply use the adjacency spectral embedding to embed the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> into <span class="math notranslate nohighlight">\(d\)</span> (or, <span class="math notranslate nohighlight">\(\hat d\)</span>) dimensions. The resulting embedded adjacency matrix <em>is</em> the estimate of the latent position matrix. We will call the estimate of the latent position matrix <span class="math notranslate nohighlight">\(\hat X\)</span>.</p>
<p>Let’s try an example of an <em>a priori</em> RDPG. We will use the same example that we used in the <a class="reference external" href="#link?">section on RDPGs</a>, where we had <span class="math notranslate nohighlight">\(100\)</span> people living along a <span class="math notranslate nohighlight">\(100\)</span> mile road, with each person one mile apart. The nodes represented people, and two people were connected if they were friends. The people at the ends of the street hosted parties, and invite everyone along the street to their parties. If a person was closer to one party host, they would tend to go to that host’s parties more than the other party host. The latent positions for each person <span class="math notranslate nohighlight">\(i\)</span> were the vectors:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x_i &amp;= \begin{bmatrix}
        \frac{100 - i}{100} \\ \frac{i}{100}
    \end{bmatrix}
\end{align*}\]</div>
<p>In this case, since each <span class="math notranslate nohighlight">\(\vec x_i\)</span> is <span class="math notranslate nohighlight">\(2\)</span>-dimensional, the number of latent dimensions in <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(d=2\)</span>. Let’s simulate an example network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># the number of nodes in our network</span>

<span class="c1"># design the latent position matrix X according to </span>
<span class="c1"># the rules we laid out previously</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">[((</span><span class="n">n</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">),</span> <span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">n</span><span class="p">)]</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Simulated RDPG(X)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_8_0.png" src="_images/estimating-parameters_spectral_8_0.png" />
</div>
</div>
<p>What happens when we fit a <code class="docutils literal notranslate"><span class="pre">rdpg</span></code> model to <span class="math notranslate nohighlight">\(A\)</span>? We will evaluate the performance of the RDPG estimator by comparing the estimated probability matrix, <span class="math notranslate nohighlight">\(\hat P = \hat X \hat X^\top\)</span>, to the true probability matrix, <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. We can do this using the <code class="docutils literal notranslate"><span class="pre">RDPGEstimator</span></code> object, provided directly by graspologic:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.models</span> <span class="kn">import</span> <span class="n">RDPGEstimator</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RDPGEstimator</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># number of latent dimensions is 2</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Xhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">latent_</span>
<span class="n">Phat</span> <span class="o">=</span> <span class="n">Xhat</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Xhat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Phat</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Estimated probability matrix&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;True probability matrix&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_11_0.png" src="_images/estimating-parameters_spectral_11_0.png" />
</div>
</div>
<p>Note that our estimated probability matrix tends to preserve the pattern in the true probability matrix, where the probabilities are highest for pairs of nodes which are closer together, but lower for pairs of nodes which are farther apart.</p>
<p>What if we did not know that there were two latent dimensions ahead of time? The RDPG Estimator handles this situation just as well, and we can estimate the number of latent dimensions with <span class="math notranslate nohighlight">\(\hat d\)</span> instead:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RDPGEstimator</span><span class="p">(</span><span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># number of latent dimensions is not known</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Xhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">latent_</span>
<span class="n">Phat</span> <span class="o">=</span> <span class="n">Xhat</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Xhat</span><span class="p">)</span>
<span class="n">dhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">latent_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Estimated number of latent dimensions: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dhat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Estimated number of latent dimensions: 2
</pre></div>
</div>
</div>
</div>
<p>So we can see that choosing the best-fit elbow instead yielded <span class="math notranslate nohighlight">\(\hat d = 3\)</span>; that is, the number of latent dimensions are estimated to be <span class="math notranslate nohighlight">\(3\)</span>. Again, looking at the estimated and true probability matrices:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Phat</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Estimated probability matrix&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;True probability matrix&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_15_0.png" src="_images/estimating-parameters_spectral_15_0.png" />
</div>
</div>
<p>Which also is a decent estimate of the true probability matrix <span class="math notranslate nohighlight">\(P\)</span>.</p>
</div>
<div class="section" id="stochastic-block-model-with-unknown-communities">
<h4><span class="section-number">3.4.2. </span>Stochastic Block Model with unknown communities<a class="headerlink" href="#stochastic-block-model-with-unknown-communities" title="Permalink to this headline">¶</a></h4>
<p>Finally, we can return to our original goal, which was to estimate the parameters of an Stochastic Block Model when we don’t know the communities different nodes are in.</p>
<div class="section" id="number-of-communities-k-is-known">
<h5><span class="section-number">3.4.2.1. </span>Number of communities <span class="math notranslate nohighlight">\(K\)</span> is known<a class="headerlink" href="#number-of-communities-k-is-known" title="Permalink to this headline">¶</a></h5>
<p>When the number of communities is known (even if we don’t know which community each node is in), the procedure for fitting a Stochastic Block Model to a network is relatively straightforward. Let’s consider a similar example to the scenario we had <a class="reference external" href="#link?">in our introduction</a>, but with <span class="math notranslate nohighlight">\(3\)</span> communities instead of <span class="math notranslate nohighlight">\(2\)</span>. We will have a block matrix given by:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    B &amp;= \begin{bmatrix}
        0.8 &amp; 0.2 &amp; 0.2 \\
        0.2 &amp; 0.8 &amp; 0.2 \\
        0.2 &amp; 0.2 &amp; 0.8
    \end{bmatrix}
\end{align*}\]</div>
<p>Which is a Stochastic block model in which the within-community edge probability is <span class="math notranslate nohighlight">\(0.8\)</span>, and exceeds the between-community edge probability of <span class="math notranslate nohighlight">\(0.2\)</span>. We will let the probability of each node being assigned to different blocks be equal, and we will produce a matrix with <span class="math notranslate nohighlight">\(100\)</span> nodes in total.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="n">pi_vec</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># randomly assign nodes to one of three communities by sampling</span>
<span class="c1"># node counts</span>
<span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">pi_vec</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]]</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">B</span><span class="p">)</span>

<span class="c1"># the true community labels</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">2</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Simulated SBM($\pi$, B)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_19_0.png" src="_images/estimating-parameters_spectral_19_0.png" />
</div>
</div>
<p>Remember, however, that we do not <em>actually</em> know the community labels of each node in <span class="math notranslate nohighlight">\(A\)</span>, so this problem is a little more difficult than it might seem. If we reordered the nodes, the coommunity each node is assigned to would not be as visually obvious as it is here in this example, as we showed back in <span class="xref myst">Chapter 5</span>. In real data, the nodes might not actually be ordered in a manner which makes the community structure as readily apparent.</p>
<p>Our goal is to learn about the block matrix, <span class="math notranslate nohighlight">\(B\)</span>, which is the parameter that we care about for the SBM. However, we cannot just plug <span class="math notranslate nohighlight">\(A\)</span> into the <code class="docutils literal notranslate"><span class="pre">SBMEstimator</span></code> class like we did back when we <span class="xref myst">fit an SBM using MLE</span>. This is because the <code class="docutils literal notranslate"><span class="pre">SBMEstimator</span></code> uses node community assignments, which we do not have. Instead, what we will do is turn again to the adjacency spectral embedding, as follows:</p>
<ol class="simple">
<li><p>We begin by reduce the observed network <span class="math notranslate nohighlight">\(A\)</span> to a an estimated latent position matrix, <span class="math notranslate nohighlight">\(\hat X\)</span> using the adjacency spectral embedding.</p></li>
<li><p>We will use K-Means clustering (or an alternative clustering technique, such as Gaussian Mixture Model) to assign each node’s latent position to a particular community. These will be called our estimates of the community labels for the nodes.</p></li>
<li><p>Finally, we will use the communities to which each node is assigned to estimate the block matrix, <span class="math notranslate nohighlight">\(B\)</span>, by using the estimated community labels in conjunction with the <code class="docutils literal notranslate"><span class="pre">SBMEstimator</span></code> class.</p></li>
</ol>
<p>We will demonstrate how to use K-means clustering to infer block labels here. We begin by first embedding <span class="math notranslate nohighlight">\(A\)</span> to estimate a latent position matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span>

<span class="n">ase</span> <span class="o">=</span> <span class="n">AdjacencySpectralEmbed</span><span class="p">()</span>  <span class="c1"># adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking</span>
<span class="n">Xhat</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>How do we explore the estimated latent position matrix <span class="math notranslate nohighlight">\(\hat X\)</span> to figure out whether we might be able to uncover useful community assignments for the nodes in our network?</p>
</div>
<div class="section" id="pairs-plots">
<h5><span class="section-number">3.4.2.2. </span>Pairs Plots<a class="headerlink" href="#pairs-plots" title="Permalink to this headline">¶</a></h5>
<p>When embedding a matrix using any embedding technique in <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>, it is important to figure out how good that embedding is. One particularly useful way to figure out this “latent structure” (community assignments which are present, but <em>unknown</em> by us ahead of time) from a network we suspect might be well-fit by a Stochastic Block Model is known as a “pairs plot”. In a pairs plot, we investigate how effectively the embedding “separates” nodes within the dataset into individual “clusters”. We will ultimately exploit these “clusters” that appear in the latent positions to generate community assignments for each node. To demonstrate the case where the “pairs plot” shows obvious latent community structure, we will use the predicted latent position matrix we just produced, from an adjacency matrix which is a realization of a random network which is truly a Stochastic Block Model. The pairs plot looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">pairplot</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Pairs plot for network with communities&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_23_0.png" src="_images/estimating-parameters_spectral_23_0.png" />
</div>
</div>
<p>As we can see, the pairs plot is a <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">x</span> <span class="pre">d</span></code> matrix of plots, where <code class="docutils literal notranslate"><span class="pre">d</span></code> is the total number of features of the matrix for which a pairs plot is being produced. For each off-diagonal plot (the plots with the dots), the <span class="math notranslate nohighlight">\(k^{th}\)</span> row and <span class="math notranslate nohighlight">\(l^{th}\)</span> column scatter plot has the points <span class="math notranslate nohighlight">\((x_{ik}, x_{il})\)</span> for each node <span class="math notranslate nohighlight">\(i\)</span> in the network. Stated another way, the off-diagonal plot is a scatter plot for each node of the <span class="math notranslate nohighlight">\(k^{th}\)</span> dimension and the <span class="math notranslate nohighlight">\(l^{th}\)</span> dimension of the latent position matrix. That these scatter plots indicate that the points appear to be separated into individual clusters provides evidence that there might be community structure in the network.</p>
<p>The diagonal elements of the pairs plot simply represent histograms of latent positions for each dimension. Higher bars indicate that more points have latent position estimates in that range. For instance, the top-left histogram indicates a histogram of the first latent dimension for all nodes, the middle histogram is a histogram of the second latent dimension for all nodes, so on and so forth.</p>
<p>Next, we will show a brief example of what happens when adjacency spectral embedding does not indicate that there is latent community structure. Our example network here will be a realization of a random network which is ER, with a probability of <span class="math notranslate nohighlight">\(0.5\)</span> for an edge existing between any pair of nodes. As an ER network does not have community structure, we wouldn’t expect the pairs plot to indicate that there are obvious clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>

<span class="n">A_er</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A_er</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;ER(0.5)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_25_0.png" src="_images/estimating-parameters_spectral_25_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ase_er</span> <span class="o">=</span> <span class="n">AdjacencySpectralEmbed</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking</span>
<span class="n">Xhat_er</span> <span class="o">=</span> <span class="n">ase_er</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A_er</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat_er</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Pairs plot for network without communities&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_26_0.png" src="_images/estimating-parameters_spectral_26_0.png" />
</div>
</div>
<p>Unlike with the SBM, we can’t see any obvious clusters in this pairs plot.</p>
<p>Next, let’s return to our SBM example and obtain some predicted community assignments for our points. Since we do not have any information as to which cluster each node is assigned to, we must use an unsupervised clustering method. We will use the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> function from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s cluster module to do so. Since we know that the SBM has 3 communities, we will use 3 clusters for the KMeans algorithm. The clusters produced by the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> algorithm will be our “predicted” community assignments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">labels_kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">Xhat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Since we have simulated data, we have the benefit of being able to evaluate the quality of our predicted community assignments to the true community assignments. We will use the Adjusted Rand Index (ARI), which is a measure of the clustering accuracy. A high ARI (near <span class="math notranslate nohighlight">\(1\)</span>) indicates a that the predicted community assignments are good relative the true community assignments, and a low ARI (near <span class="math notranslate nohighlight">\(0\)</span>) indicates that the predicted community assignments are not good relative the true community assignments. The ARI is agnostic to the names of the different communities, which means that even if the community labels assigned by unsupervised learning do not match the community labels in the true realized network, the ARI is still a legitimate statistic we can investigate. We will look more at the implications of this in the following paragraph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">adjusted_rand_score</span>

<span class="n">ari_kmeans</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_kmeans</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ARI(predicted communities, true communities) = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ari_kmeans</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ARI(predicted communities, true communities) = 1.0
</pre></div>
</div>
</div>
</div>
<p>The ARI of <span class="math notranslate nohighlight">\(1\)</span> indicates that the true communities and the predicted communities are in complete agreement!</p>
<p>When using unsupervised learning to learn about labels (such as, in this case, community assignments) for a given set of points (such as, in this case, the latent positions of each of the <span class="math notranslate nohighlight">\(n\)</span> <em>nodes</em> of our realized network), a truly unsupervised approach knows <em>nothing</em> about the true labels for the set of points. This has the implication that the assigned community labels may not make sense in the context of the true labels, or may not align. For instance, a predicted community of <span class="math notranslate nohighlight">\(2\)</span> may not mean the same thing as the true community being <span class="math notranslate nohighlight">\(2\)</span>, since the true community assignments did not have any <em>Euclidean</em> relevance to the set of points we clustered. This means that we may have to remap the labels from the unsupervised learning predictions to better match the true labels so that we can do further diagnostics. For this reason, the <code class="docutils literal notranslate"><span class="pre">graspologic</span></code> package offers the <code class="docutils literal notranslate"><span class="pre">remap_labels</span></code> utility function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">remap_labels</span>

<span class="n">labels_kmeans_remap</span> <span class="o">=</span> <span class="n">remap_labels</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">labels_kmeans</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can use these remapped labels to understand when <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> is, or is not, producing reasonable labels for our investigation. We begin by first looking at a pairs plot, which now will color the points by their assigned community:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span>
         <span class="n">labels</span><span class="o">=</span><span class="n">labels_kmeans_remap</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;KMeans on embedding, ARI: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ari_kmeans</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
         <span class="n">legend_name</span><span class="o">=</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">,</span>
         <span class="n">height</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span>
         <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;muted&#39;</span><span class="p">,);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_34_0.png" src="_images/estimating-parameters_spectral_34_0.png" />
</div>
</div>
<p>The final utility of the pairs plot is that we can investigate which points, if any, the clustering technique is getting wrong. We can do this by looking at the classification error of the nodes to each community:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">labels_kmeans_remap</span>  <span class="c1"># compute which assigned labels from labels_kmeans_remap differ from the true labels y</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">error</span> <span class="o">!=</span> <span class="mi">0</span>  <span class="c1"># if the difference between the community labels is non-zero, an error has occurred</span>
<span class="n">er_rt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>  <span class="c1"># error rate is the frequency of making an error</span>

<span class="n">palette</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Right&#39;</span><span class="p">:(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.2</span><span class="p">),</span>
           <span class="s1">&#39;Wrong&#39;</span><span class="p">:(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)}</span>

<span class="n">error_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;Right&#39;</span><span class="p">])</span>  <span class="c1"># initialize numpy array for each node</span>
<span class="n">error_label</span><span class="p">[</span><span class="n">error</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Wrong&#39;</span>  <span class="c1"># add label &#39;Wrong&#39; for each error that is made</span>

<span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span>
         <span class="n">labels</span><span class="o">=</span><span class="n">error_label</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Error from KMeans, Error rate: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">er_rt</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
         <span class="n">legend_name</span><span class="o">=</span><span class="s1">&#39;Error label&#39;</span><span class="p">,</span>
         <span class="n">height</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span>
         <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">,);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_36_0.png" src="_images/estimating-parameters_spectral_36_0.png" />
</div>
</div>
<p>Great! Our classification has not made any errors.</p>
<p>To learn about the block matrix <span class="math notranslate nohighlight">\(B\)</span>, we can now use the <code class="docutils literal notranslate"><span class="pre">SBMEstimator</span></code> class, with our predicted labels:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.models</span> <span class="kn">import</span> <span class="n">SBMEstimator</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SBMEstimator</span><span class="p">(</span><span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">labels_kmeans_remap</span><span class="p">)</span>
<span class="n">Bhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">block_p_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Bhat</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\hat B_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$B_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Bhat</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">)),</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|\hat B_</span><span class="si">{SBM}</span><span class="s2"> - B_</span><span class="si">{SBM}</span><span class="s2">|$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_40_0.png" src="_images/estimating-parameters_spectral_40_0.png" />
</div>
</div>
<div class="admonition-recap-of-inference-for-stochastic-block-model-with-known-number-of-communities admonition">
<p class="admonition-title">Recap of inference for Stochastic Block Model with known number of communities</p>
<ol class="simple">
<li><p>We learned that the adjacency spectral embedding is a key algorithm for making sense of networks which are realizations of SBM random networks. The estimates of latent positions produced by ASE are critical for learning community assignments.</p></li>
<li><p>We learned that unsuperised learning (such as K-means) allows us to ues the estimated latent positions to learn community assignments for each node in our network.</p></li>
<li><p>We can use <code class="docutils literal notranslate"><span class="pre">remap_labels</span></code> to align predicted labels with true labels, if true labels are known. This is useful for benchmarking techniques on networks with known community labels.</p></li>
<li><p>We evaluate the quality of unsupervised learning by plotting the predicted node labels and (if we know the true labels) the errorfully classified nodes. The ARI and the error rate summarize how effective our unsupervised learning techniques performed.</p></li>
</ol>
</div>
</div>
<div class="section" id="number-of-communities-k-is-not-known">
<h5><span class="section-number">3.4.2.3. </span>Number of communities <span class="math notranslate nohighlight">\(K\)</span> is not known<a class="headerlink" href="#number-of-communities-k-is-not-known" title="Permalink to this headline">¶</a></h5>
<p>In real data, we almost never have the beautiful canonical modular structure obvious to us from a Stochastic Block Model. This means that it is <em>extremely infrequently</em> going to be the case that we know exactly how we should set the number of communities, <span class="math notranslate nohighlight">\(K\)</span>, ahead of time.</p>
<p>Let’s first remember back to the single network models section, when we took a Stochastic block model with obvious community structure, and let’s see what happens when we just move the nodes of the adjacency matrix around. We begin with a similar adjacency matrix to <span class="math notranslate nohighlight">\(A\)</span> given above, for the <span class="math notranslate nohighlight">\(3\)</span>-community SBM example, but with the within and between-community edge probabilities a bit closer together so that we can see what happens when we experience errors. The communities are still slightly apparent, but less so than before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]]</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">B</span><span class="p">)</span>

<span class="c1"># the true community labels</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">2</span><span class="p">])]</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Simulated SBM($\pi$, B)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_42_0.png" src="_images/estimating-parameters_spectral_42_0.png" />
</div>
</div>
<p>Next, we permute the nodes around to reorder the realized adjacency matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a reordering of the n nodes</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">vtx_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">A_permuted</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">vtx_perm</span><span class="p">])]</span> <span class="p">[:,</span><span class="n">vtx_perm</span><span class="p">]</span>
<span class="n">y_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)[</span><span class="n">vtx_perm</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span> <span class="k">as</span> <span class="n">hm_code</span> 
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_layout_plot</span> <span class="k">as</span> <span class="n">lp_code</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># heatmap</span>
<span class="n">hm</span> <span class="o">=</span> <span class="n">hm_code</span><span class="p">(</span>
    <span class="n">A_permuted</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span>
    <span class="n">xticklabels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">yticklabels</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>

<span class="c1"># layout plot</span>
<span class="n">lp_code</span><span class="p">(</span><span class="n">A_permuted</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pos</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">y_perm</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s2">&quot;qualitative&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Simulated SBM($\pi, B$), reordered vertices&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
    
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_45_0.png" src="_images/estimating-parameters_spectral_45_0.png" />
</div>
</div>
<p>We only get to see the adjacency matrix in the <em>left</em> panel; the panel in the <em>right</em> is constructed by using the true labels (which we do <em>not</em> have!). This means that we proceed for statistical inference about the random network underlying our realized network using <em>only</em> the heatmap we have at right. It is not immediately obvious that this is the realization of a random network which is an SBM with <span class="math notranslate nohighlight">\(3\)</span> communities.</p>
<p>Our procedure is <em>very</em> similar to what we did previously <a class="reference external" href="#link?">when the number of communities was known</a>. We again embed using the “elbow picking” technique:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ase_perm</span> <span class="o">=</span> <span class="n">AdjacencySpectralEmbed</span><span class="p">()</span>  <span class="c1"># adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking</span>
<span class="n">Xhat_permuted</span> <span class="o">=</span> <span class="n">ase_perm</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A_permuted</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We examine the pairs plot, <em>just</em> like in the section on <a class="reference external" href="#link?">pairs plots</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat_permuted</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;SBM adjacency spectral embedding&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_49_0.png" src="_images/estimating-parameters_spectral_49_0.png" />
</div>
</div>
<p>We can still see that there is some level of latent community structure apparent in the pairs plot. This is evident from, for instance, the plots of Dimension 2 against Dimension 3, where we can see that the latent positions of respective nodes <em>appear</em> to be clustering in some way.</p>
<p>Next, we have the biggest difference with the approach we took previously. Since we do <em>not</em> know the optimal number of clusters <span class="math notranslate nohighlight">\(K\)</span> <em>nor</em> the true community assignments, we must choose an unsupervised clustering technique which allows us to <em>compare</em> clusterings with different choices of clusters. We can again perform this using the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> algorithm that we used previously. Here, we will compare the quality of a clustering with one number of clusters to the quality of a clustering with a <em>different</em> number of clusters using the silhouette score. The optimal clustering is selected to be the clustering which has the largest silhouette score across all attempted numbers of clusters.</p>
<p>This feature is implemented automatically in the <code class="docutils literal notranslate"><span class="pre">KMeansCluster</span></code> function of <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>. We will select the number of clusters which maximizes the silhouette score, and will allow at most <span class="math notranslate nohighlight">\(10\)</span> clusters total to be produced:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.cluster</span> <span class="kn">import</span> <span class="n">KMeansCluster</span>

<span class="n">km_clust</span> <span class="o">=</span> <span class="n">KMeansCluster</span><span class="p">(</span><span class="n">max_clusters</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">km_clust</span> <span class="o">=</span> <span class="n">km_clust</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xhat_permuted</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we visualize the silhouette score as a function of the number of clusters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span> <span class="k">as</span> <span class="n">df</span>

<span class="n">nclusters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>  <span class="c1"># graspologic nclusters goes from 2 to max_clusters</span>
<span class="n">silhouette</span> <span class="o">=</span> <span class="n">km_clust</span><span class="o">.</span><span class="n">silhouette_</span>  <span class="c1"># obtain the respective silhouette scores</span>

<span class="n">silhouette_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">({</span><span class="s2">&quot;Number of Clusters&quot;</span><span class="p">:</span> <span class="n">nclusters</span><span class="p">,</span> <span class="s2">&quot;Silhouette Score&quot;</span><span class="p">:</span> <span class="n">silhouette</span><span class="p">})</span>  <span class="c1"># place into pandas dataframe</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">silhouette_df</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Number of Clusters&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Silhouette Score&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Silhouette Analysis of KMeans Clusterings&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_53_0.png" src="_images/estimating-parameters_spectral_53_0.png" />
</div>
</div>
<p>As we can see, Silhouette Analysis has indicated the best number of clusters as <span class="math notranslate nohighlight">\(3\)</span> (which, is indeed, <em>correct</em> since we are performing a simulation where we know the right answer). Next, let’s take a look at the pairs plot for the optimal classifier. We begin by producing the predicted labels for each of our nodes, and remapping to the true community assignment labels, exactly as we did previously for further analysis:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels_autokmeans</span> <span class="o">=</span> <span class="n">km_clust</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">Xhat_permuted</span><span class="p">)</span>
<span class="n">labels_autokmeans</span> <span class="o">=</span> <span class="n">remap_labels</span><span class="p">(</span><span class="n">y_perm</span><span class="p">,</span> <span class="n">labels_autokmeans</span><span class="p">)</span>

<span class="n">ari_kmeans</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_autokmeans</span><span class="p">,</span> <span class="n">y_perm</span><span class="p">)</span>

<span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat_permuted</span><span class="p">,</span>
         <span class="n">labels</span><span class="o">=</span><span class="n">labels_autokmeans</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;KMeans on embedding, ARI: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ari_kmeans</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
         <span class="n">legend_name</span><span class="o">=</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">,</span>
         <span class="n">height</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span>
         <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;muted&#39;</span><span class="p">,);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_55_0.png" src="_images/estimating-parameters_spectral_55_0.png" />
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">KMeans</span></code> was still able to find relatively stable clusters, which align quite well (ARI of <span class="math notranslate nohighlight">\(0.855\)</span>, which is not perfect but closer to <span class="math notranslate nohighlight">\(1\)</span> than to <span class="math notranslate nohighlight">\(0\)</span>!) with the true labels! Next, we will look at which points <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> tends to get <em>wrong</em> to see if any patterns arise:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">error</span> <span class="o">=</span> <span class="n">y_perm</span> <span class="o">-</span> <span class="n">labels_autokmeans</span>  <span class="c1"># compute which assigned labels from labels_kmeans_remap differ from the true labels y</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">error</span> <span class="o">!=</span> <span class="mi">0</span>  <span class="c1"># if the difference between the community labels is non-zero, an error has occurred</span>
<span class="n">er_rt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>  <span class="c1"># error rate is the frequency of making an error</span>

<span class="n">palette</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Right&#39;</span><span class="p">:(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.2</span><span class="p">),</span>
           <span class="s1">&#39;Wrong&#39;</span><span class="p">:(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)}</span>

<span class="n">error_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;Right&#39;</span><span class="p">])</span>  <span class="c1"># initialize numpy array for each node</span>
<span class="n">error_label</span><span class="p">[</span><span class="n">error</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Wrong&#39;</span>  <span class="c1"># add label &#39;Wrong&#39; for each error that is made</span>

<span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat_permuted</span><span class="p">,</span>
         <span class="n">labels</span><span class="o">=</span><span class="n">error_label</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Error from KMeans, Error rate: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">er_rt</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
         <span class="n">legend_name</span><span class="o">=</span><span class="s1">&#39;Error label&#39;</span><span class="p">,</span>
         <span class="n">height</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span>
         <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">,);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_spectral_57_0.png" src="_images/estimating-parameters_spectral_57_0.png" />
</div>
</div>
<p>And there do not appear to be any dramatic issues in our clustering which woul suggest systematic errors are present. To learn about <span class="math notranslate nohighlight">\(B\)</span>, we would proceed exactly as we did previously, by using these labels with the <code class="docutils literal notranslate"><span class="pre">SBMEstimator</span></code> class to perform inference:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SBMEstimator</span><span class="p">(</span><span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A_permuted</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">labels_autokmeans</span><span class="p">)</span>
<span class="n">Bhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">block_p_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Bhat</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$\hat B_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$B_</span><span class="si">{SBM}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Bhat</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">)),</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$|\hat B_</span><span class="si">{SBM}</span><span class="s2"> - B_</span><span class="si">{SBM}</span><span class="s2">|$&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ValueError</span><span class="g g-Whitespace">                                </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">34</span><span class="o">-</span><span class="n">a112421e2ac5</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span>         <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> 
<span class="ne">---&gt; </span><span class="mi">17</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Bhat</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">)),</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span>         <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span>         <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

<span class="ne">ValueError</span>: operands could not be broadcast together with shapes (2,2) (3,3) 
</pre></div>
</div>
<img alt="_images/estimating-parameters_spectral_60_1.png" src="_images/estimating-parameters_spectral_60_1.png" />
</div>
</div>
<p>Which appears very close to the true <span class="math notranslate nohighlight">\(B\)</span>.</p>
<div class="admonition-recap-of-inference-for-stochastic-block-model-with-unknown-number-of-communities admonition">
<p class="admonition-title">Recap of inference for Stochastic Block Model with unknown number of communities</p>
<ol class="simple">
<li><p>The Adjacency Spectral Embedding is used to generate <em>estimates of latent positions</em>. We verify the quality of these estimates using pairs plots.</p></li>
<li><p>We use unsupervised learning with an objective quality metric (such as the silhoutte score) to learn both the number of communities <em>and</em> the community assignment for nodes within our network. The use of an objective quality metric that allows us to evaluate classification performance across different numbers of communities is the key difference between how we perform inference with unknown community labels when we knew vs did not know the number of communities in our network.</p></li>
<li><p>We <em>align</em> the labels produced by unsupervised learning with true labels for our network using <code class="docutils literal notranslate"><span class="pre">remap</span> <span class="pre">labels</span></code>.</p></li>
<li><p>We evaluate the nuances of the unsupervised learning technique using pairs plots colored with the predicted labels, and the classification errors. Again, we ue the ARI and the error rate to evaluate classifier performance.</p></li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-representations/ch6/random-walk-diffusion-methods"></span><div class="section" id="random-walk-and-diffusion-based-methods">
<h3><span class="section-number">3.5. </span>Random-Walk and Diffusion-based Methods<a class="headerlink" href="#random-walk-and-diffusion-based-methods" title="Permalink to this headline">¶</a></h3>
<p>Although this book puts a heavy emphasis on spectral methods, there are many ways to learn lower-dimensional representations for networks which don’t involve spectral clustering in any way. These methods might be more computationally efficient on large networks, or they might be more easily parallelizable; they might avoid problems like nonidentifiability, or they could provide representations which are better for the particular downstream machine learning task that you want to use the representation for. Maybe you want to cluster in a way that groups together different types of nodes than spectral methods. Regardless of your reasoning, this chapter is intended to give you a some insight into the world of alternative ways to represent networks.</p>
<div class="section" id="node2vec">
<h4><span class="section-number">3.5.1. </span>node2vec<a class="headerlink" href="#node2vec" title="Permalink to this headline">¶</a></h4>
<p>node2vec is one such method. Instead of relying on taking eigenvectors or eigenvalues, like a Laplacian, node2vec uses a random walk to preserve the relationships between nodes and their <em>local neighborhoods</em>: all of the nodes which you can get to by walking along a small number of edges from your starting node. For example, take</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">node2vec_embed</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>

<span class="c1"># Start with some simple parameters</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># Total number of nodes</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">3</span>  <span class="c1"># Nodes per community</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">],</span>
              <span class="p">[</span><span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">]])</span>  <span class="c1"># Our block probability matrix</span>

<span class="c1"># Make our Stochastic Block Model</span>
<span class="n">A</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:&gt;
</pre></div>
</div>
<img alt="_images/random-walk-diffusion-methods_4_1.png" src="_images/random-walk-diffusion-methods_4_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>

<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">latents</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="n">pairplot</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x164e99580&gt;
</pre></div>
</div>
<img alt="_images/random-walk-diffusion-methods_5_1.png" src="_images/random-walk-diffusion-methods_5_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">pairplot</span>

<span class="n">networkx_sbm</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">nodes</span> <span class="o">=</span> <span class="n">node2vec_embed</span><span class="p">(</span><span class="n">networkx_sbm</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">walk_length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">pairplot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance
WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words&#39; for smoother alpha decay
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x165a39f40&gt;
</pre></div>
</div>
<img alt="_images/random-walk-diffusion-methods_6_2.png" src="_images/random-walk-diffusion-methods_6_2.png" />
</div>
</div>
</div>
</div>
<span id="document-representations/ch6/graph-neural-networks"></span><div class="section" id="graph-neural-networks">
<h3><span class="section-number">3.6. </span>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-representations/ch6/multigraph-representation-learning"></span><div class="section" id="multiple-network-representation-learning">
<h3><span class="section-number">3.7. </span>Multiple-Network Representation Learning<a class="headerlink" href="#multiple-network-representation-learning" title="Permalink to this headline">¶</a></h3>
<div class="section" id="aliens-and-humans">
<h4><span class="section-number">3.7.1. </span>Aliens and Humans<a class="headerlink" href="#aliens-and-humans" title="Permalink to this headline">¶</a></h4>
<p>Say you’re a brain researcher, and you have a bunch of scans of brains - some are scans of people, and some are scans of aliens. You have some code that estimates networks from your scans, so you turn all your scans into networks. The nodes represent the brain regions which are common to both humans and aliens (isn’t evolution amazing?), and the edges represent communication between these brain regions. You want to know if the human and alien networks share a common grouping of regions (your research topic is titled, “Do Alien Brains Have The Same Hemispheres That We Do?”). What do you do? How do you even deal with situations in which you have a lot of networks whose nodes all represent the same objects, but whose edges might come from totally different distributions?</p>
<p>Well, if your goal is to find the shared grouping of regions between the human and alien networks, you could try embedding your networks and then seeing what those embeddings look like. This would serve the dual purpose of having less stuff to deal with and having some way to directly compare all of your networks in the same space. Finding an embedding is also simply useful in general, because embedding a network or group of networks opens the door to machine learning methods designed for tabular data.</p>
<p>For example, say you have four alien networks and four human networks. Since alien brain networks aren’t currently very accessible, we’ll just simulate our human and alien networks with Stochastic Block Models. The communities that we’re trying to group all of the brain regions into are the two hemispheres of the brain. We’ll design the human brains to have strong connections within hemispheres, and we’ll design the alien brains to have strong connections between hemispheres – but the same regions still correspond to the same hemispheres.</p>
<p>we’ll use a relatively small number of nodes and fairly small block probabilities. You can see the specific parameters in the code below.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="o">%</span><span class="k">load_ext</span> autoreload
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="c1"># Generate networks from an SBM, given some parameters</span>
<span class="k">def</span> <span class="nf">make_sbm</span><span class="p">(</span><span class="o">*</span><span class="n">probs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">pa</span><span class="p">,</span> <span class="n">pb</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">pd</span> <span class="o">=</span> <span class="n">probs</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">pa</span><span class="p">,</span> <span class="n">pb</span><span class="p">],</span> 
                  <span class="p">[</span><span class="n">pc</span><span class="p">,</span> <span class="n">pd</span><span class="p">]])</span>
    
    <span class="k">return</span> <span class="n">sbm</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="n">P</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="n">return_labels</span><span class="p">)</span>

<span class="c1"># make nine human networks</span>
<span class="c1"># and nine alien networks</span>
<span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">p3</span> <span class="o">=</span> <span class="o">.</span><span class="mi">12</span><span class="p">,</span> <span class="o">.</span><span class="mi">06</span><span class="p">,</span> <span class="o">.</span><span class="mi">03</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">n</span>
<span class="n">humans</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_sbm</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
<span class="n">aliens</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_sbm</span><span class="p">(</span><span class="n">p3</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>The human and alien networks come from very different distributions. As you can see from the Stochastic Block Model structure below, the regions in the human and the alien brains can both be separated into two communities. These communities represent the two hemispheres of the brain (who knew aliens also have bilateralized brains!). Although both humans and aliens have the same regions belonging to their respective hemispheres, as we planned, the alien networks have a strange property: their brain regions have more connections with regions in the opposite hemisphere than the same one.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1</span> <span class="kn">import</span> <span class="n">ImageGrid</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">adjplot</span><span class="p">,</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">lined_heatmap</span><span class="p">,</span> <span class="n">add_legend</span>

<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;



<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="n">grid1</span> <span class="o">=</span> <span class="n">ImageGrid</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="mi">121</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axes_pad</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">share_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">grid2</span> <span class="o">=</span> <span class="n">ImageGrid</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="mi">122</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axes_pad</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">share_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">axi</span><span class="p">,</span> <span class="n">axj</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grid1</span><span class="p">,</span> <span class="n">grid2</span><span class="p">)):</span>
    <span class="n">hmn</span> <span class="o">=</span> <span class="n">lined_heatmap</span><span class="p">(</span><span class="n">humans</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axi</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">outline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hma</span> <span class="o">=</span> <span class="n">lined_heatmap</span><span class="p">(</span><span class="n">aliens</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axj</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">outline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    

<span class="n">grid1</span><span class="o">.</span><span class="n">axes_all</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Human Brain Networks&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">grid2</span><span class="o">.</span><span class="n">axes_all</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Alien Brain Networks&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>

<span class="n">add_legend</span><span class="p">(</span><span class="n">grid2</span><span class="o">.</span><span class="n">axes_all</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">w_pad</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_6_0.png" src="_images/multigraph-representation-learning_6_0.png" />
</div>
</div>
</div>
<div class="section" id="different-ways-to-embed-the-networks">
<h4><span class="section-number">3.7.2. </span>Different ways to Embed the Networks<a class="headerlink" href="#different-ways-to-embed-the-networks" title="Permalink to this headline">¶</a></h4>
<p>Remember, our goal is to find community structure common to both humans and aliens, and in our case that community structure is the brain hemispheres. We’re going to try to to embed our brain networks into some lower-dimensional space - that way, we can use standard clustering methods from machine learning to figure out which regions are grouped. Try to think about how you might find a lower-dimensional embedding where the location of each node’s latent positions uses information from all of the networks.</p>
<div class="section" id="averaging-separately">
<h5><span class="section-number">3.7.2.1. </span>Averaging Separately<a class="headerlink" href="#averaging-separately" title="Permalink to this headline">¶</a></h5>
<p>The first idea you might come up with is to average your networks together, and then embed the result of that averaging with Spectral Embedding. It turns out that this is actually the right idea in the very special case where all of your networks come from the same probability distribution. In our case, we’ll try averaging our groups of networks separately: we’ll treat the human networks as one group, and the alien networks as another group, and we’ll average each independently. In the end, we’ll have two separate embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>

<span class="c1"># Compute the average adjacency matrix for </span>
<span class="c1"># human brains and alien brains</span>
<span class="n">human_mean_network</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">humans</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">alien_mean_network</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">aliens</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Embed both matrices</span>
<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">human_latents</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">human_mean_network</span><span class="p">)</span>
<span class="n">alien_latents</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">alien_mean_network</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Below, you can see what happens when we embed the averaged human and alien networks separately. Like all of our embedding plots, each dot represents the latent positions for a particular node.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">human_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedding when we average the human </span><span class="se">\n</span><span class="s2">networks&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">alien_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedding when we average the alien </span><span class="se">\n</span><span class="s2">networks&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_13_0.png" src="_images/multigraph-representation-learning_13_0.png" />
</div>
</div>
<p>Both of these embeddings have clear clustering: there are two communities of nodes in both the human and the alien networks. We can recover the labels for these communities fairly easily using our pick of unsupervised clustering method. We know that the latent positions in each community of an Adjacency Spectral Embedding are normally distributed under this simulation setting, and we have two communities. That means that the above embeddings are distributed according to a Gaussian Mixture. Here, “Gaussian” just means “normal”, and a gaussian mixture just means that we have groups of normally distributed data clusters. As a result, it makes sense to cluster these data using scikit-learn’s GaussianMixture implementation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span> <span class="k">as</span> <span class="n">GMM</span>

<span class="c1"># Predict labels for the human and alien brains</span>
<span class="n">human_labels</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">human_latents</span><span class="p">)</span>
<span class="n">alien_labels</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">alien_latents</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see a plot that predicts our community structure below. Success! When we embed the human and the alien networks separately, averaging them clearly lets us cluster the brain regions by hemisphere. However, as you can see, the colors are flipped: the communities are in different places relative to each other. This is because the alien networks are drawn from a different distribution than the human networks.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plot_latents</span><span class="p">(</span><span class="n">human_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Clustering our averaged human network </span><span class="se">\n</span><span class="s2">embedding with a GMM&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">human_labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">alien_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Clustering our averaged alien network </span><span class="se">\n</span><span class="s2">embedding with a GMM&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">alien_labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.15</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-large&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="p">,</span>
           <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_17_0.png" src="_images/multigraph-representation-learning_17_0.png" />
</div>
</div>
</div>
<div class="section" id="averaging-together">
<h5><span class="section-number">3.7.2.2. </span>Averaging Together<a class="headerlink" href="#averaging-together" title="Permalink to this headline">¶</a></h5>
<p>But what if you wanted to embed <em>all</em> of the networks into the same space, both the human and the alien networks, so that there’s only one plot? Let’s try it. We’ll take all of the networks and then average them together, and then do an Adjacency Spectral Embedding. This will result in a single plot, with each point representing a single brain region. Do you think we’ll still find this nice community separation?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total_mean_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">humans</span> <span class="o">+</span> <span class="n">aliens</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">all_latents</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">total_mean_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_latents</span><span class="p">(</span><span class="n">all_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedding when we average everything together&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_21_0.png" src="_images/multigraph-representation-learning_21_0.png" />
</div>
</div>
<p>Nope, bummer. Our community separation into discrete hemispheres is gone - the human networks and the alien networks cancelled each other out. As far as anybody can tell, our latent positions have just become meaningless noise, so we can’t cluster and find communities like we did before.</p>
<div class="section" id="why-did-averaging-together-fail">
<h6><span class="section-number">3.7.2.2.1. </span>Why Did Averaging Together Fail?<a class="headerlink" href="#why-did-averaging-together-fail" title="Permalink to this headline">¶</a></h6>
<p>Why did this happen? Well, let’s go back and compare one human brain network with one alien brain network.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">hmn</span> <span class="o">=</span> <span class="n">lined_heatmap</span><span class="p">(</span><span class="n">humans</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;One Human Brain Network&quot;</span><span class="p">)</span>
<span class="n">hma</span> <span class="o">=</span> <span class="n">lined_heatmap</span><span class="p">(</span><span class="n">aliens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;One Alien Brain Network&quot;</span><span class="p">)</span>

<span class="n">add_legend</span><span class="p">(</span><span class="n">humans</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_25_0.png" src="_images/multigraph-representation-learning_25_0.png" />
</div>
</div>
<p>The human network has more edges in the upper-left and lower-left quadrants of the heatmap. This implies that two regions in the same hemisphere are more likely to be connected for humans than two regions in opposite hemispheres.</p>
<p>The alien network tells a different story. For aliens, two regions in opposite hemispheres are more likely to be connected than two regions in the same hemisphere.</p>
<p>But what happens when you average these two adjacency matrices together?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">humans</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">aliens</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">averaged</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;Greys&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">hm</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">averaged</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Averaged Brain Network&quot;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">hm</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># # colorbar</span>
<span class="n">add_legend</span><span class="p">(</span><span class="n">hm</span><span class="p">,</span> <span class="n">legend_labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Edge in no networks&quot;</span><span class="p">,</span> <span class="s2">&quot;Edge in one network&quot;</span><span class="p">,</span> <span class="s2">&quot;Edge in both networks&quot;</span><span class="p">],</span>
           <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="s2">&quot;black&quot;</span><span class="p">],</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.3</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_28_0.png" src="_images/multigraph-representation-learning_28_0.png" />
</div>
</div>
<p>By averaging, we’ve lost all of the community structure used to exist. That’s why our big averaged embedding failed.</p>
<p>We’ve just discovered that even though it’s oten a great idea to simply average all of your networks together - for example, if they were drawn from the same distribution - it’s often a horrible idea to average all of your networks if they might come from different distributions. This is a case of averaging networks which are “heterogeneous”: Not only are your networks slightly different, but they’re <em>should</em> to be different because their edge probabilities aren’t the same. Sampling a lot of heterogenous networks and then averaging them, as you can see from our exploration above, can result in losing the community signal you might have had.</p>
<p>We’d like to find a way to compare these heterogeneous networks directly, so that we can embed all of our networks into the same space and still keep that nice community structure. Figuring out the best way to do this is a topic under active research, and the set of techniques and tools that have developed as a result are together called multiple-network representation learning.</p>
</div>
</div>
</div>
<div class="section" id="different-types-of-multiple-network-representation-learning">
<h4><span class="section-number">3.7.3. </span>Different Types of Multiple-Network Representation Learning<a class="headerlink" href="#different-types-of-multiple-network-representation-learning" title="Permalink to this headline">¶</a></h4>
<p>Let’s take a moment to explore some of the possible general approaches we could take in multiple-network representation learning. At some point we need to combine the many individual representations of our networks into one, and there are at least three possible places where we could do this: combining the networks together, combining the networks separately, and combining the embeddings. Each of these eventually results in a latent position representation for our networks. It’s important to note that in all of these approaches, we’re simply learning representations for our groups of networks. You can do whatever you want with these representations; in our case, we’ll illustrate that we can use them to classify our nodes.</p>
<div class="section" id="combining-the-networks-together">
<h5><span class="section-number">3.7.3.1. </span>Combining the Networks Together<a class="headerlink" href="#combining-the-networks-together" title="Permalink to this headline">¶</a></h5>
<p>With this approach, you’ll start with a set of networks, and then you’ll combine them all into a single network prior to doing anything else. You can then embed and classify this network directly. What we did before, averaging the human and alien networks, was an example of combining our networks – we just averaged all of our adjacency matrices, and then we embedded the result.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">MultipleASE</span> <span class="k">as</span> <span class="n">MASE</span>
<span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">OmnibusEmbed</span> <span class="k">as</span> <span class="n">OMNI</span>
<span class="kn">from</span> <span class="nn">graspologic.embed.omni</span> <span class="kn">import</span> <span class="n">_get_omni_matrix</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">binary_heatmap</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span>

<span class="k">def</span> <span class="nf">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span>

<span class="c1"># add stack of heatmaps</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span> 
    <span class="n">ax</span> <span class="o">=</span> <span class="n">binary_heatmap</span><span class="p">(</span><span class="n">humans</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Adjacency Matrices&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>


<span class="c1"># add arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># add joint matrix</span>
<span class="n">omni_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">human_mean_network</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">a_hm</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">omni_ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">a_hm</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Joint Matrix&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">a_hm</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
<span class="c1"># add second arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.75</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># add averaged embedding</span>
<span class="n">omni_embed_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">55</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">human_latents</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">omni_embed_ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Joint Embedding&quot;</span><span class="p">,</span> 
             <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">omni_embed_ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># add third arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">2.7</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># classify</span>
<span class="n">mase_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">3.05</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">55</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">human_latents</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">mase_ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Classification&quot;</span><span class="p">,</span> 
             <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">},</span> <span class="n">labels</span><span class="o">=</span><span class="n">human_labels</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>

<span class="c1"># plt.suptitle(&quot;Combining the Networks&quot;, x=2, y=1.1, fontsize=26);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div>
</div>
<img alt="_images/multigraph-representation-learning_34_1.png" src="_images/multigraph-representation-learning_34_1.png" />
</div>
</div>
</div>
<div class="section" id="combining-the-networks-separately">
<h5><span class="section-number">3.7.3.2. </span>Combining The Networks Separately<a class="headerlink" href="#combining-the-networks-separately" title="Permalink to this headline">¶</a></h5>
<p>The above approach is nice for collapsing our information into a single embedding – with each point in our final embedding representing a single node of our network. However, there are situations in which we might want to keep our embeddings separate, but make sure that they’re in the same latent space – meaning, the embeddings aren’t rotations of each other. That way, we can directly compare the embeddings of our separate embeddings.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">MultipleASE</span> <span class="k">as</span> <span class="n">MASE</span>
<span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">OmnibusEmbed</span> <span class="k">as</span> <span class="n">OMNI</span>
<span class="kn">from</span> <span class="nn">graspologic.embed.omni</span> <span class="kn">import</span> <span class="n">_get_omni_matrix</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="k">def</span> <span class="nf">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span>

<span class="c1"># add stack of heatmaps</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span> 
    <span class="n">ax</span> <span class="o">=</span> <span class="n">binary_heatmap</span><span class="p">(</span><span class="n">humans</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Adjacency Matrices&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># add arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># add joint matrix</span>
<span class="n">omni_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">_get_omni_matrix</span><span class="p">(</span><span class="n">humans</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="n">aliens</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">a_hm</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">omni_ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">a_hm</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Joint Matrix&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">a_hm</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
<span class="c1"># add second arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.75</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># add omni embedding</span>
<span class="n">latents_omni</span> <span class="o">=</span> <span class="n">OMNI</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">humans</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="n">aliens</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">latents_omni</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">2.1</span><span class="o">+.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">.</span><span class="mi">55</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Separate Combination&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">embedding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">embedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
                       <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
    <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_37_0.png" src="_images/multigraph-representation-learning_37_0.png" />
</div>
</div>
</div>
<div class="section" id="combining-the-embeddings">
<h5><span class="section-number">3.7.3.3. </span>Combining the embeddings<a class="headerlink" href="#combining-the-embeddings" title="Permalink to this headline">¶</a></h5>
<p>The final approach to multiple-network representation learning that we’ll talk about is combining the embeddings themselves. With this approach, you’re waiting until you’ve already embnedded all of your networks separately before you combine them, either with Adjacency Spectral Embedding or with some other single-network embedding method. Multiple Adjacency Spectral Embedding, which we’ll be talking about soon, is an example of this approach.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="c1"># add stack of heatmaps</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">])</span> 
    <span class="n">ax</span> <span class="o">=</span> <span class="n">binary_heatmap</span><span class="p">(</span><span class="n">humans</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Adjacency Matrices&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">)</span>
    <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># add arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># add stack of latent plots</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">8</span><span class="o">+.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">.</span><span class="mi">35</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Separate Embeddings&quot;</span><span class="p">)</span>
    <span class="n">latents</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">humans</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">latents</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">latents</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
                       <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
    <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
<span class="c1"># add second arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># add group embeddings</span>
<span class="n">mase</span> <span class="o">=</span> <span class="n">MASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">latents_mase</span> <span class="o">=</span> <span class="n">mase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">humans</span> <span class="o">+</span> <span class="n">aliens</span><span class="p">)</span>
<span class="n">mase_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.57</span><span class="p">,</span> <span class="o">-.</span><span class="mi">03</span><span class="p">,</span> <span class="o">.</span><span class="mi">35</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">mase_ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Joint Embedding&quot;</span><span class="p">)</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">mase_ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># add third arrow</span>
<span class="n">arrow_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">1.95</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">rm_ticks</span><span class="p">(</span><span class="n">arrow_ax</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dy</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">width</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span> 

<span class="c1"># classify</span>
<span class="n">labels_normal</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">human_latents</span><span class="p">)</span>
<span class="n">mase_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">2.27</span><span class="p">,</span> <span class="o">-.</span><span class="mi">03</span><span class="p">,</span> <span class="o">.</span><span class="mi">35</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">mase_ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Classification&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">labels_normal</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Combining the Embeddings&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mf">1.4</span><span class="p">,</span> <span class="n">y</span><span class="o">=.</span><span class="mi">7</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_40_0.png" src="_images/multigraph-representation-learning_40_0.png" />
</div>
</div>
<p>For the rest of this section, we’ll explore the strengths and weaknesses of different particular techniques which use these approaches. The first we’ll look at is combines the embeddings, like above. It’s called Multiple Adjacency Spectral Embedding, or MASE for short.</p>
</div>
</div>
<div class="section" id="multiple-adjacency-spectral-embedding">
<h4><span class="section-number">3.7.4. </span>Multiple Adjacency Spectral Embedding<a class="headerlink" href="#multiple-adjacency-spectral-embedding" title="Permalink to this headline">¶</a></h4>
<p>MASE is a technique which combines embeddings by concatennating and re-embedding the separate latent positions into a single space. It’s nice because you don’t actually need each network to be generated from the same distribution - you only need the nodes of the different networks to be aligned and for them to belong to the same communities.</p>
<p>MASE is probably the easiest to understand if you know how Adjacency Spectral Embeddings work. Say you have some number of networks, and (like we said above) their nodes are aligned. The goal of MASE is to embed the networks into a single space, with each point in that space representing a single node - but, unlike simply averaging, MASE lets you combine networks which aren’t necessarily drawn from the same distribution. MASE is based on the common subspace independent-edge (COSIE) model from the multi-network models section of chapter 5, so we’re operating under the assumption that there <em>is</em> some low-dimensional space common to all of our networks that we can embed into in the first place.</p>
<p>Let’s go back to our group of human and alien brains and try using MASE to embed them rather than averaging. Then, we’ll dive deeper into what’s going on under the hood. First, we’ll instantiate a MASE classifier and embed down to two dimensions. Then we’ll create a combined list of the human and alien brains, and use MASE to find the latent positions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">MultipleASE</span> <span class="k">as</span> <span class="n">MASE</span>

<span class="c1"># Use MASE to embed everything</span>
<span class="n">mase</span> <span class="o">=</span> <span class="n">MASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">latents_mase</span> <span class="o">=</span> <span class="n">mase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">humans</span> <span class="o">+</span> <span class="n">aliens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">,</span> 
             <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedding when we use MASE on the group </span><span class="se">\n</span><span class="s2">of all human and alien networks&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_45_0.png" src="_images/multigraph-representation-learning_45_0.png" />
</div>
</div>
<p>Unlike the disastrous results from simply averaging all of our networks together, MASE manages to keep the community structure that we found when we averaged our networks separately. Let’s see what’s under the hood.</p>
<div class="section" id="how-does-mase-work">
<h5><span class="section-number">3.7.4.1. </span>How Does MASE Work?<a class="headerlink" href="#how-does-mase-work" title="Permalink to this headline">¶</a></h5>
<p>Below, you can see how MASE works. We start with networks, drawn as nodes in space connected to each other. We turn them into adjacency matrices, and then we embed the adjacency matrices of a bunch of networks separately, using our standard Adjacency Spectral Embedding. Then, we take all of those embeddings, concatenate horizontally into a single matrix, and embed the entire concatenated matrix. The colors are the true communities each node belongs to: there’s a red and an orange community. MASE is an unsupervised learning technique and so it doesn’t need any information about the true communities to embed, but they’re useful to see.</p>
<div class="figure align-default" id="mase-fig">
<a class="reference internal image-reference" href="_images/mase1.jpeg"><img alt="_images/mase1.jpeg" src="_images/mase1.jpeg" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2 </span><span class="caption-text">The MASE algorithm</span><a class="headerlink" href="#mase-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="a-collection-of-networks">
<h6><span class="section-number">3.7.4.1.1. </span>A Collection of Networks<a class="headerlink" href="#a-collection-of-networks" title="Permalink to this headline">¶</a></h6>
<p>We’ll illustrate what’s happening in the MASE algorithm by running through all of its steps ourselves, with a set of example networks.</p>
<p>Suppose we have a set of networks generated from Stochastic Block Models with two communities in each network. The networks have aligned nodes – meaning that the <span class="math notranslate nohighlight">\(i_{th}\)</span> row of all of their adjacency matrices represent the edges for the same node <span class="math notranslate nohighlight">\(i\)</span>. The nodes also all belong to the same communities. However, edge probabilities might change depending on the network. In the first network, you might have nodes in the same community having a high chance of connecting to each other, whereas in the second network, nodes are much more likely to be connected to other nodes in different communities. You want to end up with a classification that distinctly groups the nodes into their respective communities, using the information from all of the networks. Because MASE takes approach of combining the embeddings, we start by embedding each network separately with an Adjacency Spectral Embedding.</p>
<p>Below is Python code which generates four networks with Stochastic Block Models. Each of the networks is drawn from a different distribution (the block probability matrices are different), but the labels are the same across the networks (which means that nodes have a consistent community no matter which network you’re looking at). If you’re interested in the particular parameters used to generate these SBMs, you can see them in the code below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">p3</span> <span class="o">=</span> <span class="o">.</span><span class="mi">12</span><span class="p">,</span> <span class="o">.</span><span class="mi">06</span><span class="p">,</span> <span class="o">.</span><span class="mi">03</span>
<span class="n">A1</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">make_sbm</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> 
                      <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">make_sbm</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p2</span><span class="p">)</span>
<span class="n">A3</span> <span class="o">=</span> <span class="n">make_sbm</span><span class="p">(</span><span class="n">p3</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">p3</span><span class="p">)</span>
<span class="n">A4</span> <span class="o">=</span> <span class="n">make_sbm</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="n">p3</span><span class="p">)</span>

<span class="n">networks</span> <span class="o">=</span> <span class="p">[</span><span class="n">A1</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">A3</span><span class="p">,</span> <span class="n">A4</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">networks</span><span class="p">)):</span>
    <span class="n">hmap</span> <span class="o">=</span> <span class="n">binary_heatmap</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;network </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hmap</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">hmap</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Four different networks&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=.</span><span class="mi">05</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=.</span><span class="mi">05</span><span class="p">)</span>

<span class="n">add_legend</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_53_0.png" src="_images/multigraph-representation-learning_53_0.png" />
</div>
</div>
</div>
<div class="section" id="embedding-our-networks">
<h6><span class="section-number">3.7.4.1.2. </span>Embedding our networks<a class="headerlink" href="#embedding-our-networks" title="Permalink to this headline">¶</a></h6>
<p>Next, we embed each of the four networks separately using Adjacency Spectral Embedding. This step is pretty straightforward, so we won’t dive into it too much: remember, we’re combining the embeddings, not the networks, so we’re not doing anything fancy. The python code below just groups the four networks into a list, and then loops through the list, embedding each network into two dimensions and saving the resulting embeddings into a variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>

<span class="n">networks</span> <span class="o">=</span> <span class="p">[</span><span class="n">A1</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">A3</span><span class="p">,</span> <span class="n">A4</span><span class="p">]</span>
<span class="n">latents_mase</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">network</span> <span class="ow">in</span> <span class="n">networks</span><span class="p">:</span>
    <span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">latent</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
    <span class="n">latents_mase</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latent</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">autoreload</span> 2
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Embedding for network </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                 <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Adjacency Spectral Embedding for our four networks&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>

<span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center right&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">),</span> 
           <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="s1">&#39;x-large&#39;</span><span class="p">);</span>

<span class="n">fig</span><span class="o">.</span><span class="n">supxlabel</span><span class="p">(</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supylabel</span><span class="p">(</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_57_0.png" src="_images/multigraph-representation-learning_57_0.png" />
</div>
</div>
<p>It’s important to keep in mind that these embeddings don’t live in the same <em>latent space</em>. What this means is that averaging these networks together would result in essentially meaningless noise. This is because of the rotational invariance of latent positions: you can only recover the latent positions of any network up to a rotation.</p>
</div>
<div class="section" id="combining-our-embeddings">
<h6><span class="section-number">3.7.4.1.3. </span>Combining our embeddings<a class="headerlink" href="#combining-our-embeddings" title="Permalink to this headline">¶</a></h6>
<p>Now comes the interesting part. Our goal is to find some way to take each of these individual embeddings and combine them. We want to find a reasonable way of doing this.</p>
<p>We can visualize each of our four embeddings a different way. Instead of the using the two latent position dimensions as the x-axis and the y-axis of our plot, we can just visualize our latent position matrices directly. Each latent position now corresponds to rows in one of these matrices. The two columns are the two latent position dimensions, and the two colors in each row corresponds to the latent position value. We’re essentially substituting location for color.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">Normalize</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">GraphColormap</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;divergent&quot;</span><span class="p">,</span> <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">palette</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">hm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
                     <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">hm</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding for network </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>
    
<span class="n">fig</span><span class="o">.</span><span class="n">supxlabel</span><span class="p">(</span><span class="s2">&quot;Dimension&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=.</span><span class="mi">42</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supylabel</span><span class="p">(</span><span class="s2">&quot;Latent Position&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=.</span><span class="mi">005</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Latent position matrices for our four embeddings&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=.</span><span class="mi">42</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">w_pad</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_61_0.png" src="_images/multigraph-representation-learning_61_0.png" />
</div>
</div>
<p>Because the rows of these matrices are all aligned - meaning, row 0 corresponds to node 0 for all four matrices - we can actually think of each node as having (in this case) eight latent position dimensions: two for each of our four networks. Eight is a somewhat arbitrary number here: each network contributes two dimensions simply because we originally chose to embed all of our networks down to two dimensions with ASE, and the number of networks is of course even more arbitrary. You’ll usually have more than four.</p>
<p>In the more general sense, we can think of each node as having <span class="math notranslate nohighlight">\(m \times d\)</span> latent position dimensions, where <span class="math notranslate nohighlight">\(m\)</span> is the number of networks, and <span class="math notranslate nohighlight">\(d\)</span> is the number of dimensions we embed each network into. We don’t actually need separate matrices to express this idea: the natural thing to do would be to just concatenate all of the matrices horizontally into a single <span class="math notranslate nohighlight">\(m \times d\)</span> matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Concatenate our four matrices horizontally into a single m by d matrix</span>
<span class="n">concatenated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">hm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">concatenated</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">50</span><span class="p">);</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Combined embedding for all four networks&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">});</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dimension&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Latent Position&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_64_0.png" src="_images/multigraph-representation-learning_64_0.png" />
</div>
</div>
</div>
<div class="section" id="embedding-our-combination-to-create-a-joint-embedding">
<h6><span class="section-number">3.7.4.1.4. </span>Embedding our Combination To Create a Joint Embedding<a class="headerlink" href="#embedding-our-combination-to-create-a-joint-embedding" title="Permalink to this headline">¶</a></h6>
<p>So now we have a combined representation for our separate embeddings, but we have a new problem: our latent positions suddenly have way too many dimensions. In this example they have eight (the number of columns in our combined matrix), but remember that in general we’d have <span class="math notranslate nohighlight">\(m \times d\)</span>. This somewhat defeats the purpose of an embedding: we took a bunch of high-dimensional objects and turned them all into a single high-dimensional object. Big whoop. We can’t see what our combined embedding look like in euclidean space, unless we can somehow visualize <span class="math notranslate nohighlight">\(m \times d\)</span> dimensional space (hint: we can’t). We’d like to just have <code class="docutils literal notranslate"><span class="pre">d</span></code> dimensions - that was the whole point of using <code class="docutils literal notranslate"><span class="pre">d</span></code> components for each of our Adjacency Spectral Embeddings in the first place!</p>
<p>There’s an obvious solution here: why don’t we just embed <em>again</em>? Nothing stops us from doing a Singular Value Decomposition on a nonsquare matrix, and so we can just create a joint embedding of our combined matrix and go back down to a healthy <span class="math notranslate nohighlight">\(d\)</span> columns.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">select_svd</span>
<span class="n">joint_embedding</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">select_svd</span><span class="p">(</span><span class="n">concatenated</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.gridspec</span> <span class="kn">import</span> <span class="n">GridSpec</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>

<span class="c1"># TODO: add legend</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">axm</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="c1"># Matrix representation</span>
<span class="n">hm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axm</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">cbar_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;shrink&quot;</span><span class="p">:</span> <span class="o">.</span><span class="mi">91</span><span class="p">})</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matrix visualization of our </span><span class="se">\n</span><span class="s2">Joint Embedding&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">14</span><span class="p">},</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dimension&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Latent Positions&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>

<span class="c1"># Euclidean representation</span>
<span class="n">splot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">joint_embedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                        <span class="n">palette</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;qualitative&quot;</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">splot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dimension 0&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
<span class="n">splot</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
<span class="n">splot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Euclidean visualization of our Joint Embedding&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fontsize&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
<span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">splot</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">splot</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Community&#39;</span><span class="p">,</span> <span class="n">handles</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">])</span>

<span class="c1"># fig title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Two Visualizations For Our Joint Embedding&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_68_0.png" src="_images/multigraph-representation-learning_68_0.png" />
</div>
</div>
<p>Looks like this idea worked well - Our nodes are clearly grouped into two distinct communities, and all of our networks were drawn from the same distribution! To reiterate, what we did was:</p>
<ol class="simple">
<li><p>Embed each of our four networks separately into two-dimensional space</p></li>
<li><p>Think of all of the resulting latent positions for a particular node as a single vector</p></li>
<li><p>With the intuition from 2, horizontally concatenate our four latent position matrices into a single matrix</p></li>
<li><p>embed that new matrix down to 2 dimensions</p></li>
</ol>
</div>
</div>
<div class="section" id="using-graspologic">
<h5><span class="section-number">3.7.4.2. </span>Using Graspologic<a class="headerlink" href="#using-graspologic" title="Permalink to this headline">¶</a></h5>
<p>In practice, you don’t actually have to implement any of this stuff yourself. Graspologic’s MultipleASE class implements it all for you under the hood. You can see the embedding below - you give MultipleASE a list of networks, and it spits out a set of joint latent positions. Graspologic’s implementation of MASE is doing pretty much exactly what we just did: it embeds all of the networks you pass in, concatenates them horizontally, and then re-embeds the concatenated matrix. You can see this in the figure – MASE’s embedding looks just like the one we made above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">MultipleASE</span> <span class="k">as</span> <span class="n">MASE</span>

<span class="n">mase</span> <span class="o">=</span> <span class="n">MASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">latents</span> <span class="o">=</span> <span class="n">mase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">networks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_latents</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;MASE embedding&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_73_0.png" src="_images/multigraph-representation-learning_73_0.png" />
</div>
</div>
</div>
<div class="section" id="score-matrices">
<h5><span class="section-number">3.7.4.3. </span>Score Matrices*<a class="headerlink" href="#score-matrices" title="Permalink to this headline">¶</a></h5>
<p>Exactly how is the joint embedding we created related to all of separate, original networks? Well, to understand this, we need to introduce the concept of <em>score matrices</em>.</p>
<p>In MASE, each network is associated with its own score matrix. Just like the joint embedding describes how the networks are similar, the score matrices describe how each network is different.</p>
<p>Suppose we have a set of networks with adjacency matrices <span class="math notranslate nohighlight">\(A^{(1)}, ..., A^{(m)}\)</span>, with each network being unweighted. In the joint embedding we made before, for instance, we had <span class="math notranslate nohighlight">\(m=4\)</span>.</p>
<p>Now, we run MASE using the method described above, and we get a joint embedding <span class="math notranslate nohighlight">\(V\)</span>. Then each adjacency matrix, <span class="math notranslate nohighlight">\(A^{(i)}\)</span>, can be decomposed into <span class="math notranslate nohighlight">\(VR^{(i)} V^\top\)</span>, where <span class="math notranslate nohighlight">\(R^{(i)}\)</span> is the score matrix corresponding to the <span class="math notranslate nohighlight">\(i_{th}\)</span> network:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A^{(i)} = VR^{(i)} V^\top
\end{align*}\]</div>
<p>This is how the score matrix of a particular network <span class="math notranslate nohighlight">\(R^{(i)}\)</span> and the single joint embedding <span class="math notranslate nohighlight">\(V\)</span> is related to the original network <span class="math notranslate nohighlight">\(A^{(i)}\)</span>.</p>
<div class="section" id="finding-score-matrices">
<h6><span class="section-number">3.7.4.3.1. </span>Finding Score Matrices<a class="headerlink" href="#finding-score-matrices" title="Permalink to this headline">¶</a></h6>
<p>Any particular score matrix, <span class="math notranslate nohighlight">\(R^{(i)}\)</span>, is square and <span class="math notranslate nohighlight">\(d \times d\)</span>. The dimension, <span class="math notranslate nohighlight">\(d\)</span>, corresponds to the number of embedding dimensions – so if we wanted to embed down to two dimensions, each <span class="math notranslate nohighlight">\(R^{(i)}\)</span> would be a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix.</p>
<p>Now, here’s the interesting part: how do we find our score matrices? Well, there’s a theorem in linear algebra about matrices which are <em>orthogonal</em>, meaning that the columns all perpendicular to each other. This theorem says that the inverse of an orthogonal matrix is its transpose. So, for an orthogonal matrix <span class="math notranslate nohighlight">\(O\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    O^\top = O^{-1}
\end{align*}\]</div>
<p>Interestingly, the column-vectors of our joint embedding matrix (let’s call it <span class="math notranslate nohighlight">\(V\)</span>) are all perpendicular. Since definitionally, what it means for two vectors to be perpendicular is that they have a dot product of 0, we can check this below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="n">joint_embedding</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Take the dot product of the columns of our joint latent position matrix</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">V</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">V</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>What this all means is that <span class="math notranslate nohighlight">\(V^\top V\)</span> is just the identity matrix <span class="math notranslate nohighlight">\(I\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="nd">@V</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 0.],
       [0., 1.]])
</pre></div>
</div>
</div>
</div>
<p>and so, finally, we can use the above two facts to find the score matrix for a particular network. We just take our original formula <span class="math notranslate nohighlight">\(A^{(i)} = VR^{(i)} V^\top\)</span>, left-multiply by <span class="math notranslate nohighlight">\(V^\top\)</span>, and right-multiply by <span class="math notranslate nohighlight">\(V\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A^{(i)} &amp;= VR^{(i)} V^\top \\
    V^{\top} A^{(i)} V &amp;= (V^\top V) R^{(i)} (V^\top V) \\
    V^\top A^{(i)} V &amp;= R^{(i)} 
\end{align*}\]</div>
<p>Below, we turn the list of four networks we already embedded into a 3-D numpy array, and then do the above multiplication to get a new 3D numpy array of scores matrices. Because we embedded into two dimensions, each score matrix is <span class="math notranslate nohighlight">\(2 \times 2\)</span>, and the four score matrices are “slices” along the 0th axis of the numpy array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">networks_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">networks</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">networks_array</span> <span class="o">@</span> <span class="n">V</span>
<span class="n">scores</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 2, 2)
</pre></div>
</div>
</div>
</div>
<p>Now, here’s something interesting: it turns out that we can estimate the edge probability matrix which generated any graph with <span class="math notranslate nohighlight">\( P^{(i)} = V R^{(i)} V^\top\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P_0</span> <span class="o">=</span> <span class="n">V</span> <span class="o">@</span> <span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>Below and to the left, you can see the original adjacency matrix for the first matrix. In the center, you can see the heatmap for the first network’s score matrix. Next to it, you can see the recreation of the first network. Remember that we only used the score matrix to recreate it. The first network has a block probability matrix of</p>
<div class="amsmath math notranslate nohighlight" id="equation-4e7701b0-7d2e-42da-b41b-e765a9d92127">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-4e7701b0-7d2e-42da-b41b-e765a9d92127" title="Permalink to this equation">¶</a></span>\[\begin{align}
\begin{bmatrix}
.12 &amp; .03 \\
.03 &amp; .06 \\
\end{bmatrix}
\end{align}\]</div>
<p>and so we should expect the edges in the top-left block of our adjacency matrix to be more connected, the edges in the two off-diagonal blocks to not be very connected, and the edges in the bottom-right block to be kind of connected.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">lined_heatmap</span><span class="p">(</span><span class="n">networks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;The original adjacency matrix </span><span class="se">\n</span><span class="s2">for the first network&quot;</span><span class="p">,</span>
               <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">lined_heatmap</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Score matrix for the first network&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">lined_heatmap</span><span class="p">(</span><span class="n">P_0</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Estimated edge probabilities </span><span class="se">\n</span><span class="s2">for the first network&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_86_0.png" src="_images/multigraph-representation-learning_86_0.png" />
</div>
</div>
<p>So we’ve learned that MASE is useful when you want a joint embedding that combines all of your networks together, and when you want to estimate edge probabilities for one of your networks. What if we wanted to keep our separate embeddings, but put them all in the same space? That’s what the Omnibus Embedding gives, and what we’ll explore now.</p>
</div>
</div>
</div>
<div class="section" id="omnibus-embedding">
<h4><span class="section-number">3.7.5. </span>Omnibus Embedding<a class="headerlink" href="#omnibus-embedding" title="Permalink to this headline">¶</a></h4>
<p>The Omnibus Embedding combines networks separately to put them all into the same latent space. What this means is that the embeddings for each network after the omnibus embedding are <em>directly comparable</em>: none of the embeddings are rotations of each other, and distances between nodes across embeddings actually means something. You can use the omnibus embedding to answer a variety of questions about the interacting properties of a collection of networks. For example, you could figure out which nodes or subgraphs are responsible for similarities or differences across your networks, or you could determine whether subcommunities in your networks are statistically similar or different. You could try to figure out which underlying parameters of your network are the same, and which are different.</p>
<p>In the next section, we’ll explore how the Omnibus Embedding works. Sections in future chapters will explore some the things you can do with your separate embeddings to learn about your networks.</p>
<div class="section" id="omni-on-our-four-networks">
<h5><span class="section-number">3.7.5.1. </span>OMNI on our four networks<a class="headerlink" href="#omni-on-our-four-networks" title="Permalink to this headline">¶</a></h5>
<p>We’ll begin with an example. Let’s go back to the four networks we created in the MASE section and look at their embeddings. Notice that the way the blue cluster of points and the red cluster of points is rotated is somewhat arbitrary across the embeddings for our different networks - this is because of the nonidentifiability problem in spectral embeddings.</p>
<div class="admonition-non-identifiability admonition">
<p class="admonition-title">Non-Identifiability</p>
<p>Let’s take a network generated from an RDPG with <span class="math notranslate nohighlight">\(n\)</span> nodes. Each of these <span class="math notranslate nohighlight">\(n\)</span> nodes is associated with a latent position vector, corresponding to that node’s row in the network’s embedding. What it means for a node to have a latent position vector is that the probability for an edge to exist between two nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> is the dot product of their latent position vectors.</p>
<p>More specifically, if <span class="math notranslate nohighlight">\(\textbf{P}\)</span> is a matrix of edge probabilities, and <span class="math notranslate nohighlight">\(\textbf{X}\)</span> is our latent position matrix, then <span class="math notranslate nohighlight">\(\textbf{P} = \textbf{X} \textbf{X}^\top\)</span>.</p>
<p>The nonidentifiability problem is as follows: Take any orthogonal matrix (a matrix which only rotates or flips other matrices). Call it <span class="math notranslate nohighlight">\(\textbf{W}\)</span>. By definition, the transpose of any orthogonal matrix is its inverse: <span class="math notranslate nohighlight">\(\textbf{W} \textbf{W}^\top = \textbf{I}\)</span>, where <span class="math notranslate nohighlight">\(\textbf{I}\)</span> is the identity matrix. So,</p>
<div class="amsmath math notranslate nohighlight" id="equation-a3d94e9d-ac1b-44d1-a6b5-0dd9990ceb32">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-a3d94e9d-ac1b-44d1-a6b5-0dd9990ceb32" title="Permalink to this equation">¶</a></span>\[\begin{align}
P &amp;= \textbf{X} \textbf{X}^\top \\
  &amp;= \textbf{X} \textbf{I} \textbf{X}^\top \\
  &amp;= (\textbf{X} \textbf{W}) (\textbf{W}^\top \textbf{X}^\top) \\
  &amp;= (\textbf{X} \textbf{W}) (\textbf{X} \textbf{W})^\top \\
\end{align}\]</div>
<p>What this means is that you can take any latent position matrix and rotate it, and the rotated version will still generate the same matrix of edge probabilities. So, when you try to estimate latent positions, separate estimations can produce rotated versions of each other.</p>
<p>You need to be aware of this in situations where you’re trying to directly compare more than one embedding. You wouldn’t be able to figure out the average position of a node, for instance, when you have multiple embeddings of that node.</p>
</div>
<p>You can see the nonidentifiability problem in action below. The embeddings for network 1 and for network 2 are particularly illustrative; community 0 is generally top in network 1, but on the right in network two. There isn’t a way to compare any two nodes directly. Another way to say this is that, right now, all of our embeddings live in different <em>latent spaces</em>: direct comparison between embeddings for nodes in network 1 and nodes in network 2 isn’t possible. You can also see the latent position corresponding to the first node as a big red circle in each network so that you can track a single point - you can see that the red points are likely to be rotated or flipped across networks.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Embedding for network </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                 <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">_x</span><span class="p">,</span> <span class="n">_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">latents_mase</span><span class="p">)[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Adjacency Spectral Embedding for our four networks&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>

<span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center right&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">),</span> 
           <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="s1">&#39;x-large&#39;</span><span class="p">);</span>

<span class="n">fig</span><span class="o">.</span><span class="n">supxlabel</span><span class="p">(</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supylabel</span><span class="p">(</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_94_0.png" src="_images/multigraph-representation-learning_94_0.png" />
</div>
</div>
</div>
<div class="section" id="omni-on-our-four-heterogeneous-networks">
<h5><span class="section-number">3.7.5.2. </span>OMNI on our four heterogeneous networks<a class="headerlink" href="#omni-on-our-four-heterogeneous-networks" title="Permalink to this headline">¶</a></h5>
<p>Let’s see what happens when, instead of embedding our networks separately as above, we find their latent positions with an Omnibus Embedding. Again, we’ll plot a particular node with a circle so that we can track it across embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">OmnibusEmbed</span>

<span class="n">omni</span> <span class="o">=</span> <span class="n">OmnibusEmbed</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">latents_omni</span> <span class="o">=</span> <span class="n">omni</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">networks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_omni</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;OMNI Embedding for network </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                 <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">_x</span><span class="p">,</span> <span class="n">_y</span> <span class="o">=</span> <span class="n">latents_omni</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">markeredgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Omnibus Embedding for our four networks&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
<span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center right&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">),</span> 
           <span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="p">,</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="s1">&#39;x-large&#39;</span><span class="p">);</span>
    
<span class="n">fig</span><span class="o">.</span><span class="n">supxlabel</span><span class="p">(</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supylabel</span><span class="p">(</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_98_0.png" src="_images/multigraph-representation-learning_98_0.png" />
</div>
</div>
<p>Unlike when we embedded the four networks separately, the clusters created by the Omnibus Embedding <em>live in the same space</em>: you don’t have to rotate or flip your points to line them up across embeddings. The cluster of blue points is always in the top left, and the cluster of red points is always in the bottom right. This means that we can compare points directly; the relative location of the node in your network corresponding to the red circle, for instance, now means something across the four networks, and we can do stuff like measure the distance of the red circle in network 1 to the red circle in network two to gain information.</p>
</div>
<div class="section" id="how-does-omni-work">
<h5><span class="section-number">3.7.5.3. </span>How Does OMNI work?<a class="headerlink" href="#how-does-omni-work" title="Permalink to this headline">¶</a></h5>
<p>At a high level, the omnibus embedding is fairly simple. It:</p>
<ol class="simple">
<li><p>Combines the adjacency matrices for all of our networks into a single, giant matrix (the Omnibus Matrix)</p></li>
<li><p>Embeds that matrix using a standard Adjacency or Laplacian Spectral Embedding.</p></li>
</ol>
<p>The omnibus matrix itself just has every original adjacency or laplacian matrix along its diagonal, and the elementwise average of every pair of original matrices on the off-diagonals. This means that the Omnibus Matrix is <em>huge</em>: if you have <span class="math notranslate nohighlight">\(m\)</span> networks, each of which has <span class="math notranslate nohighlight">\(n\)</span> nodes, the Omnibus Matrix will be a <span class="math notranslate nohighlight">\(mn \times mn\)</span> matrix.</p>
<p>For example, say we only have two networks. Let’s name their adjacency matrices <span class="math notranslate nohighlight">\(A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(A^{(2)}\)</span>. Then, the omnibus embedding looks like this:</p>
<div class="amsmath math notranslate nohighlight" id="equation-bc82a287-3c5e-42ed-85ba-d7904e618257">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-bc82a287-3c5e-42ed-85ba-d7904e618257" title="Permalink to this equation">¶</a></span>\[\begin{align}
\begin{bmatrix}
A^{(1)} &amp; \frac{A^{(1)} + A^{(2)}}{2} \\
\frac{A^{(2)} + A^{(1)}}{2} &amp; A^{(2)} \\
\end{bmatrix}
\end{align}\]</div>
<p>where each entry on the diagonal is itself a matrix. In general, when we have <span class="math notranslate nohighlight">\(m\)</span> networks, the <span class="math notranslate nohighlight">\(i_{th}\)</span> diagonal entry is <span class="math notranslate nohighlight">\(A^{(i)}\)</span> and the <span class="math notranslate nohighlight">\((i, j)_{th}\)</span> entry is <span class="math notranslate nohighlight">\(\frac{A^{(i)} + A^{(j)}}{2}\)</span>. What this means is that you just stick each of your adjacency matrices on the diagonal of a large matrix, and you fill in the off-diagonals with the averages of each pair of two adjacency matrices.</p>
<p>You can see this in code below. Below, we just use numpy’s block function to generate our simple Omnibus Matrix from two networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a0</span><span class="p">,</span> <span class="n">a1</span> <span class="o">=</span> <span class="n">networks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">networks</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">omni</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">([[</span><span class="n">a0</span><span class="p">,</span> <span class="p">(</span><span class="n">a0</span><span class="o">+</span><span class="n">a1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">],</span>
                 <span class="p">[(</span><span class="n">a1</span><span class="o">+</span><span class="n">a0</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">a1</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>Below you can see the resulting Omnibus Matrix. The first and second networks are shown as heatmaps on the left, and their Omnibus Matrix is shown on the right.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">text</span>

<span class="c1"># fig, axs = plt.subplots(1, 3, figsize=(15, 5))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSpec</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax0</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax_omni</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>

<span class="c1"># first two</span>
<span class="n">omni_cmap</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;PuOr_r&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">))[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">],</span> <span class="p">[</span><span class="n">a0</span><span class="p">,</span> <span class="n">a1</span><span class="p">])):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;First network ($A_1$)&quot;</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="sa">r</span><span class="s2">&quot;Second network ($A_2$)&quot;</span>
    <span class="n">hm</span> <span class="o">=</span> <span class="n">lined_heatmap</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> 
                       <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="n">omni_cmap</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">omni_cmap</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span>


<span class="c1"># big one</span>
<span class="n">hm</span> <span class="o">=</span> <span class="n">lined_heatmap</span><span class="p">(</span><span class="n">omni</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax_omni</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">omni_cmap</span><span class="p">,</span>
                   <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Omnibus Matrix for first </span><span class="se">\n</span><span class="s2">and second network&quot;</span><span class="p">,</span>
                   <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># outline</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax_omni</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># separating lines</span>
<span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="o">.</span><span class="mi">25</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">]:</span>
    <span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">6</span><span class="p">)</span>
    
<span class="c1"># text</span>
<span class="n">text</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$A_1$&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">)</span>
<span class="n">text</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$A_2$&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">)</span>
<span class="n">text</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\frac{(A_2 + A_1)}</span><span class="si">{2}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">)</span>
<span class="n">text</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\frac{(A_1 + A_2)}</span><span class="si">{2}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">)</span>

<span class="c1"># legend</span>
<span class="n">omni_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span>
<span class="n">add_legend</span><span class="p">(</span><span class="n">legend_labels</span><span class="o">=</span><span class="n">omni_labels</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">omni_cmap</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_104_0.png" src="_images/multigraph-representation-learning_104_0.png" />
</div>
</div>
<div class="section" id="creating-the-omnibus-matrix-for-all-four-networks">
<h6><span class="section-number">3.7.5.3.1. </span>Creating the Omnibus Matrix For All Four Networks<a class="headerlink" href="#creating-the-omnibus-matrix-for-all-four-networks" title="Permalink to this headline">¶</a></h6>
<p>Here’s the Omnibus Matrix for all four of our networks. You can see adjacency matrices for the original four networks on the diagonal blocks, highlighted in blue, and all possible pairs of averages of adjacency matrices on the off-diagonal blocks, highlighted in orange.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed.omni</span> <span class="kn">import</span> <span class="n">_get_omni_matrix</span>
<span class="n">omni</span> <span class="o">=</span> <span class="n">_get_omni_matrix</span><span class="p">(</span><span class="n">networks</span><span class="p">)</span>

<span class="n">hm</span> <span class="o">=</span> <span class="n">lined_heatmap</span><span class="p">(</span><span class="n">omni</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">omni_cmap</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Full omnibus matrix for all four networks&quot;</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">hm</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">:</span>
    <span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="mi">8</span><span class="p">:</span>
    <span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">omni</span><span class="p">)</span>

<span class="c1"># vertical solids</span>
<span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">*.</span><span class="mi">25</span><span class="p">,</span> <span class="n">n</span><span class="o">*.</span><span class="mi">75</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="o">*.</span><span class="mi">75</span><span class="p">,</span> <span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># horizontal solids</span>
<span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">*.</span><span class="mi">25</span><span class="p">,</span> <span class="n">n</span><span class="o">*.</span><span class="mi">75</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hm</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">n</span><span class="o">*.</span><span class="mi">75</span><span class="p">,</span> <span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_107_0.png" src="_images/multigraph-representation-learning_107_0.png" />
</div>
</div>
</div>
<div class="section" id="embedding-the-omnibus-matrix">
<h6><span class="section-number">3.7.5.3.2. </span>Embedding the Omnibus Matrix<a class="headerlink" href="#embedding-the-omnibus-matrix" title="Permalink to this headline">¶</a></h6>
<p>You should understand the next step fairly well by now. We embed the Omnibus Matrix normally, using ASE, as if it were just a normal adjacency matrix. This will create an <span class="math notranslate nohighlight">\(nm \times d\)</span> sized latent position matrix (where, remember, <span class="math notranslate nohighlight">\(n\)</span> is the number of nodes in each network, <span class="math notranslate nohighlight">\(m\)</span> is the number of networks, and <span class="math notranslate nohighlight">\(d\)</span> is the number of embedding dimensions). Here, since each of our four networks has 200 nodes, <span class="math notranslate nohighlight">\(mn\)</span> is 800, and we chose to embed down to two dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">select_svd</span>

<span class="n">U</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">select_svd</span><span class="p">(</span><span class="n">omni</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">joint_embedding</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>

<span class="n">joint_embedding</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(800, 2)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-separate-latent-positions-in-the-same-latent-space">
<h6><span class="section-number">3.7.5.3.3. </span>Creating Separate Latent Positions In The Same Latent Space<a class="headerlink" href="#creating-separate-latent-positions-in-the-same-latent-space" title="Permalink to this headline">¶</a></h6>
<p>Now, the only question we have remaining is how to actually pull the separate latent positions for each network from this matrix. It turns out that the individual latent positions for each network are actually stacked on top of each other: the first <span class="math notranslate nohighlight">\(n\)</span> rows of the joint matrix we just made correspond to the nodes of the first network, the second <span class="math notranslate nohighlight">\(n\)</span> rows correspond to the nodes of the second network, and so on.</p>
<p>If we want, we can pull out the separate latent positions for each network explicitly. Below, we reshape our 2-dimensional <span class="math notranslate nohighlight">\(mn \times d\)</span> numpy array for the omnbus embedding into a <span class="math notranslate nohighlight">\(m \times n \times d\)</span> array: the embeddings for each network are now simply stacked on top of each other on the third dimension (and the first axis of our numpy array).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">networks</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">networks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">latent_networks</span> <span class="o">=</span> <span class="n">joint_embedding</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">latent_networks</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 200, 2)
</pre></div>
</div>
</div>
</div>
<p>Below, you can see the embeddings we just created. On the left is the full <span class="math notranslate nohighlight">\(mn \times d\)</span> omnibus matrix, and on the right are the slices of the <span class="math notranslate nohighlight">\(m \times n \times d\)</span> 3-D array we created above. If you look carefully, you can see that the top two blocks of colors (row-wise) in the larger embedding correspond to the latent positions for network 1, the second two blocks correspond to the latent positions for network 2, and so on. They’re a bit squished, so that everything lines up nicely, but they’re there.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSpec</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">hm_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">hm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">hm_ax</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
<span class="n">hm</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Omnibus Embedding&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="c1"># hm.set(xlabel=&quot;Dimension&quot;, ylabel=&quot;Latent Positions&quot;)</span>

<span class="n">ax0</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">3</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">]):</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">latent_networks</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> 
                       <span class="n">yticklabels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Latent positions </span><span class="se">\n</span><span class="s2">for network </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">in</span> <span class="p">{</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">}:</span>
        <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">in</span> <span class="p">{</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax3</span><span class="p">}:</span>
        <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>


<span class="c1"># labels</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supxlabel</span><span class="p">(</span><span class="s2">&quot;Dimension&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=.</span><span class="mi">42</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supylabel</span><span class="p">(</span><span class="s2">&quot;Latent Position&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=.</span><span class="mi">005</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># colorbar</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">hm_ax</span><span class="p">,</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">]));</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;omnibus_latent_fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_115_1.png" src="_images/multigraph-representation-learning_115_1.png" />
</div>
</div>
<div class="figure align-default" id="fig-omnibus" style="width: 300px">
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_115_0.png" src="_images/multigraph-representation-learning_115_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 3.3 </span><span class="caption-text">This is a <strong>caption</strong>, with an embedded <code class="docutils literal notranslate"><span class="pre">{glue:text}</span></code> element: !</span><a class="headerlink" href="#fig-omnibus" title="Permalink to this image">¶</a></p>
</div>
<p>And finally, below is the above embeddings, plotted in Euclidean space. Each point is a row of the embedding above, and the dots are colored according to their class label. The big matrix on the left (the joint OMNI embedding) just contains every latent position we have, across all of our networks. This means that, on the lefthand plot, there will be four points for every node (remember that we’re operating under the assumption that we have the same set of nodes across all of our networks).</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set up</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_gridspec</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">flat</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
    
<span class="c1"># plot individual networks</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:,</span> <span class="o">-</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">plot_latents</span><span class="p">(</span><span class="n">latent_networks</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Latent positions for network </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                 <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># average and plot average</span>
<span class="n">joint_ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
             <span class="n">ax</span><span class="o">=</span><span class="n">joint_ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Joint OMNI embedding&quot;</span><span class="p">)</span>

<span class="c1"># layout stuff</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supxlabel</span><span class="p">(</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=-.</span><span class="mi">03</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">supylabel</span><span class="p">(</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=-.</span><span class="mi">01</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_118_0.png" src="_images/multigraph-representation-learning_118_0.png" />
</div>
</div>
</div>
</div>
</div>
<div class="section" id="how-can-you-use-the-omnibus-embedding">
<h4><span class="section-number">3.7.6. </span>How Can You Use The Omnibus Embedding?<a class="headerlink" href="#how-can-you-use-the-omnibus-embedding" title="Permalink to this headline">¶</a></h4>
<p>Fundamentally, the omnibus embedding is useful is because it lets you avoid the somewhat annoying and noise-generating process of figuring out a good way to rotate your separate embeddings to line them up. For instance, say you want to figure out if two networks are generated from the same distribution (This means that the matrix that contains edge probabilities, <span class="math notranslate nohighlight">\(\textbf{P}\)</span>, is the same for both networks). Then, it’s reasonable to assume that their latent positions will be pretty close to each other. Look at the equation below:</p>
<p><span class="math notranslate nohighlight">\(\min_{W} ||{\hat{N_1} - \hat{N_2}W}||_F\)</span></p>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W\)</span> is a matrix that just rotates or flips (called an isometry, or an orthonormal matrix)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{N_1}\)</span> and <span class="math notranslate nohighlight">\(\hat{N_2}\)</span> are the estimated latent positions for networks one and two, respectively</p></li>
</ul>
<p>the <span class="math notranslate nohighlight">\(||X||_F\)</span> syntax means that we’re taking the frobenius norm of <span class="math notranslate nohighlight">\(X\)</span>. Taking the frobenius norm of a matrix is the same as unwrapping the matrix into a giant vector and measuring that vector’s length. So, this equation is saying that the latent position for a given node in network one should be close to the latent position in network two.</p>
<p>But, there’s that <span class="math notranslate nohighlight">\(W\)</span> there, the rotation matrix. We actually wish we didn’t have to find it. We have to because of the same problem we keep running into: you can rotate latent positions and they’ll still have the same dot product relative to each other, and so you can only embed a network up to a rotation. In practice, you can find this matrix and rotate latent positions for separate networks using it to compare them directly, but again, it’s annoying, adds compute power that you probably don’t want to use, and it’ll add noise to any kind of inference you want to do later.</p>
<p>The Omnibus Embedding is fundamentally a solution to this problem. Because the embeddings for all of your networks live in the same space, you don’t have to rotate them manually – and you cut out the noise that gets created when you have to <em>infer</em> a good rotation matrix. We’ll explore all the downstream use cases in future chapters, but below is a sneak peak.</p>
<p>The figure below (adapted from Gopalakrishnan et al. 2021 <span id="id1">[]</span>,  is the omnibus embedding for 32 networks created from a bunch of mouse brains, some of which have been genetically modified. The nodes of these networks represent the regions of a mouse brain and the edges represent how well-connected the neurons in a given pair of regions are. The figure below actually only shows two nodes: the node representing one region in the left hemisphere, and the node representing its corresponding region in the right hemisphere.</p>
<p>So what we’re actually <em>plotting</em> in this embedding is a bit different than normal, because rather than being nodes, the points we plot are <em>networks</em>: one for each of our thirty-two mice. The only reason we’re able to get away with doing this is the omnibus embedding: each network lives in the same space!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">OmnibusEmbed</span>
<span class="kn">from</span> <span class="nn">graspologic.datasets</span> <span class="kn">import</span> <span class="n">load_mice</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>
<span class="kn">from</span> <span class="nn">scipy.stats.distributions</span> <span class="kn">import</span> <span class="n">chi2</span>


<span class="k">def</span> <span class="nf">_get_parameters</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()])</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">cov</span>


<span class="k">def</span> <span class="nf">_get_eigen</span><span class="p">(</span><span class="n">cov</span><span class="p">):</span>
    <span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">eigvals</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">eigvals</span><span class="p">[</span><span class="n">order</span><span class="p">],</span> <span class="n">eigvecs</span><span class="p">[:,</span> <span class="n">order</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span>


<span class="k">def</span> <span class="nf">_get_ellipse_parameters</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">ci</span><span class="p">):</span>

    <span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">_get_eigen</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

    <span class="c1"># Calculate angle of displacement from x-axis</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">eigvecs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="c1"># Calculate scaling factor based on probability</span>
    <span class="n">dof</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ci</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dof</span><span class="p">)</span>
    <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scale</span> <span class="o">*</span> <span class="n">eigvals</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">theta</span>


<span class="k">def</span> <span class="nf">make_ellipse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ci</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">_get_parameters</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="n">_get_ellipse_parameters</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">ci</span><span class="p">)</span>
    <span class="n">ellipse</span> <span class="o">=</span> <span class="n">Ellipse</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ellipse</span>


<span class="k">def</span> <span class="nf">draw_probability_ellipse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ci</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">ellipse</span> <span class="o">=</span> <span class="n">make_ellipse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ci</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>

    <span class="n">plot_kws</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">plot_kws</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">munge_embedding</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">col_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Dimension </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">col_names</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Strain&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">return</span> <span class="n">df</span>


<span class="k">def</span> <span class="nf">_pairgrid</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">munge_embedding</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="n">palette</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#e7298a&quot;</span><span class="p">,</span> <span class="s2">&quot;#1b9e77&quot;</span><span class="p">,</span> <span class="s2">&quot;#d95f02&quot;</span><span class="p">,</span> <span class="s2">&quot;#7570b3&quot;</span><span class="p">]</span>
    <span class="n">plot_kws</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;paper&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Strain&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">,</span> <span class="o">**</span><span class="n">plot_kws</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">g</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>

    <span class="k">return</span> <span class="n">g</span>


<span class="k">def</span> <span class="nf">ellipse_pairgrid</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">ci</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">_pairgrid</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;ci&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ci</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">draw_probability_ellipse</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span>

<span class="c1"># Load the full mouse dataset</span>
<span class="n">mice</span> <span class="o">=</span> <span class="n">load_mice</span><span class="p">()</span>

<span class="c1"># Stack all adjacency matrices in a 3D numpy array</span>
<span class="n">graphs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mice</span><span class="o">.</span><span class="n">graphs</span><span class="p">)</span>

<span class="c1"># Sort the connectomes and genotypic labels so BTBR is first</span>
<span class="n">label_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">mice</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">label_indices</span> <span class="o">=</span> <span class="n">label_indices</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">mice</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">label_indices</span><span class="p">]</span>
<span class="n">mapping</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">BTBR</span><span class="o">=</span><span class="s2">&quot;Genetically Modified Mice&quot;</span><span class="p">,</span> <span class="n">B6</span><span class="o">=</span><span class="s2">&quot;First Normal Group&quot;</span><span class="p">,</span> 
               <span class="n">CAST</span><span class="o">=</span><span class="s2">&quot;Second Normal Group&quot;</span><span class="p">,</span> <span class="n">DBA2</span><span class="o">=</span><span class="s2">&quot;Third Normal Group&quot;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">mapping</span><span class="p">[</span><span class="n">old</span><span class="p">]</span> <span class="k">for</span> <span class="n">old</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
<span class="n">graphs</span> <span class="o">=</span> <span class="n">graphs</span><span class="p">[</span><span class="n">label_indices</span><span class="p">]</span>

<span class="c1"># Jointly embed graphs using omnibus embedding</span>
<span class="n">embedder</span> <span class="o">=</span> <span class="n">OmnibusEmbed</span><span class="p">()</span>
<span class="n">omni_embedding</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">graphs</span><span class="p">)</span>


<span class="c1"># Get index for the input structure</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">mice</span><span class="o">.</span><span class="n">atlas</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Structure == &#39;Corpus_Callosum&#39;&quot;</span><span class="p">)[</span><span class="s2">&quot;ROI&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">index</span> <span class="o">-=</span> <span class="mi">1</span>

<span class="c1"># Make the plot</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">ellipse_pairgrid</span><span class="p">(</span><span class="n">omni_embedding</span><span class="p">[:,</span> <span class="n">index</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">labels</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Mouse Networks Corresponding to a Single Node </span><span class="se">\n</span><span class="s2">After Omnibus Embedding&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.08</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">g</span><span class="o">.</span><span class="n">add_legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Type of Mouse&quot;</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">move_legend</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s2">&quot;center right&quot;</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/multigraph-representation-learning_121_0.png" src="_images/multigraph-representation-learning_121_0.png" />
</div>
</div>
<p>You can clearly see a difference between the genetically modified mice and the normal mice. The genetically modified mice are off in their own cluster; if you’re familiar with classical statistics, you could do a MANOVA here and find that the genetically modified mice are significantly different from the rest - if we wanted to, we could figure out which mice are genetically modified, even without having that information in advance!</p>
</div>
</div>
<span id="document-representations/ch6/joint-representation-learning"></span><div class="section" id="joint-representation-learning">
<h3><span class="section-number">3.8. </span>Joint Representation Learning<a class="headerlink" href="#joint-representation-learning" title="Permalink to this headline">¶</a></h3>
<p>In many problems, our network might be more than just the information contained in its adjacency matrix (called its topology, or its collection of nodes and edges). If we were investigating a social network, we might have access to extra information about each person – their gender, for instance, or their age. If we were investigating a brain network, we might have information about the physical location of neurons, or the volume of a brain region. When we we embed a network, it seems like we should be able to use these extra bits of information - called the “features” or “covariates” of a network - to somehow improve our analysis. The techniques and tools that we’ll explore in this section use both the covariates and the topology of a network to create and learn from new representations of the network. Because they jointly use both the topology of the network and its extra covariate information, these techniques and tools are called joint representation learning.</p>
<p>There are two primary reasons that we might want to explore using node covariates in addition to topological structure. First, they might improve our standard embedding algorithms, like Laplacian and Adjacency Spectral Embedding. For example, if the latent structure of the covariates of a network lines up with the latent structure of its topology, then we might be able to reduce noise when we embed, even if the communities in our network don’t overlap perfectly with the communities in our covariates. Second, figuring out what the clusters of an embedding actually mean can sometimes be difficult and covariates create a natural structure in our network that we can explore. Covariate information in brain networks telling us where in the brain each node is, for instance, might let us better understand the types of characteristics that distinguish between different brain regions.</p>
<p>In this section, we’ll explore different ways to learn from our data when we have access to the covariates of a network in addition to its topological structure. We’ll explore <em>Covariate-Assisted Spectral Embedding</em> (CASE), a variation on Spectral Embedding. In CASE, instead of embedding just the adjacency matrix or its regularized Laplacian, we’ll combine the Laplacian and our covariates into a new matrix and embed that.</p>
<p>A good way to illustrate how using covariates might help us is to use a model in which some of our community information is in the covariates and some is in our topology. Using the Stochastic Block Model, we’ll create a simulation using three communities: the first and second community will be indistinguishable in the topological structure of a network, and the second and third community will be indistinguishable in its covariates. By combining the topology and the covariates, we’ll get a nice embedding that lets us find three distinct community clusters.</p>
<div class="section" id="stochastic-block-model">
<h4><span class="section-number">3.8.1. </span>Stochastic Block Model<a class="headerlink" href="#stochastic-block-model" title="Permalink to this headline">¶</a></h4>
<p>Suppose we have a network generated from a Stochastic Block Model (or, commonly, SBM) with three communities. In our adjacency matrix, which contains only our network’s topological information, we’d like to create a situation where the first two communities are completely indistinguishable: Any random node in the first community will have exactly the same chance of being connected to another node in the first community or to a node in the second community. We’d like the third community to be distinct, with only a small probability that nodes in it will connect to nodes in either of the other two communities.</p>
<p>The Python code below generates a matrix that looks like this. There are 1500 nodes, with 500 nodes per community. Because the <span class="math notranslate nohighlight">\(3 \times 3\)</span> block probability matrix that generated this SBM has the same probability values (.3) in its upper-left <span class="math notranslate nohighlight">\(2 \times 2\)</span> square, a node in community 1 has a 30% chance of being connected to either another node in community 1 or a node in community 2. As a result, in our adjacency matrix, we’ll see the nodes in communities one and two as a single giant block. On the other hand, nodes in community three only have a 15% chance to connect to nodes in the first community. So, the end result is that we’ve created a situation where we have three communities that we’d like to separate into distinct clusters, but the topological structure in the adjacency matrix can’t separate the three groups by itself.</p>
<div class="cell tag_hide-input tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="c1"># Start with some simple parameters</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1500</span>  <span class="c1"># Total number of nodes</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="mi">3</span>  <span class="c1"># Nodes per community</span>
<span class="n">p</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">15</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">15</span><span class="p">],</span>
              <span class="p">[</span><span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">15</span><span class="p">],</span>
              <span class="p">[</span><span class="o">.</span><span class="mi">15</span><span class="p">,</span> <span class="o">.</span><span class="mi">15</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">]])</span>  <span class="c1"># Our block probability matrix</span>

<span class="c1"># Make our Stochastic Block Model</span>
<span class="n">A</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here you can see what our adjacency matrix looks like. Notice the giant block in the top-left: this block contains both nodes in both of the first two communities, and they’re indistinguishable from each other.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="c1"># visualize</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">plot_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;A Stochastic Block Model With 3 Communities&quot;</span><span class="p">,</span> <span class="n">show_cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">])</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                 <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_cbar</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>
        <span class="n">cax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
        <span class="n">colorbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">),</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">)</span>
        <span class="n">colorbar</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
        <span class="n">colorbar</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([</span><span class="s2">&quot;Edge&quot;</span><span class="p">,</span> <span class="s2">&quot;No Edge&quot;</span><span class="p">])</span>
        <span class="n">cax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            
        
<span class="n">plot_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_8_0.png" src="_images/joint-representation-learning_8_0.png" />
</div>
</div>
<p>If we wanted to embed this graph using our Laplacian or Adjacency Spectral Embedding methods, we’d find the first and second communities layered on top of each other (though we wouldn’t be able to figure that out from our embedding if we didn’t cheat by knowing in advance which community each node is supposed to belong to). The python code below embeds our latent positions all the way down to two dimensions. Below it, you can see a plot of the latent positions, with each node color-coded by its true community.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">LaplacianSpectralEmbed</span> <span class="k">as</span> <span class="n">LSE</span>
<span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">to_laplacian</span>

<span class="n">L</span> <span class="o">=</span> <span class="n">to_laplacian</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">form</span><span class="o">=</span><span class="s2">&quot;R-DAD&quot;</span><span class="p">)</span>
<span class="n">L_latents</span> <span class="o">=</span> <span class="n">LSE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">L_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions when we</span><span class="se">\n</span><span class="s2"> only embed the Laplacian&quot;</span><span class="p">,</span> 
                    <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_11_0.png" src="_images/joint-representation-learning_11_0.png" />
</div>
</div>
<p>As you can see, we’d have a tough time clustering this - the first and second community are completely indistinguishable. It would be nice if we could use extra information to more clearly distinguish between them. We don’t have this information in our adjacency matrix: it needs to come from somewhere else.</p>
</div>
<div class="section" id="covariates">
<h4><span class="section-number">3.8.2. </span>Covariates<a class="headerlink" href="#covariates" title="Permalink to this headline">¶</a></h4>
<p>But we’re in luck - we have a set of covariates for each node! These covariates contain the extra information we need that allows us to separate our first and second community. However, with only these extra covariate features, we can no longer distinguish between the last two communities - they contain the same information.</p>
<p>Below is Python code that generates these covariates. Each node is associated with its own group of 30 covariates (thirty being chosen primarily to visualize what’s happening more clearly). We’ll organize this information into a matrix, where the <span class="math notranslate nohighlight">\(i_{th}\)</span> row contains the covariates associated with node <span class="math notranslate nohighlight">\(i\)</span>. Remember that we have 1500 nodes in our network, so there will be 1500 rows. We’ll draw all the covariates for each node from the same Beta distribution (with values ranging from 0 to 1), but the nodes in the first community will be drawn from a different Beta distribution than the nodes in the last two.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="k">def</span> <span class="nf">make_community</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">beta</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">30</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">gen_covariates</span><span class="p">():</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">make_community</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">c2</span> <span class="o">=</span> <span class="n">make_community</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">c3</span> <span class="o">=</span> <span class="n">make_community</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">covariates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">c3</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">covariates</span>
    

<span class="c1"># Generate a covariate matrix</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">gen_covariates</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Here is a visualization of the covariates we just created. The first community is represented by the lighter-colored rows, and the last two are represented by the darker-colored rows.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">GraphColormap</span>

<span class="c1"># Plot and make the axis look nice</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">GraphColormap</span><span class="p">(</span><span class="s2">&quot;sequential&quot;</span><span class="p">,</span> <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">palette</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Visualization of the covariates&quot;</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> 
       <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Nodes (each row is a node)&quot;</span><span class="p">,</span>
       <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Covariates for each node (each column is a covariate)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_17_0.png" src="_images/joint-representation-learning_17_0.png" />
</div>
</div>
<p>We can play almost the same game here as we did with the Laplacian. If we embed the information contained in this matrix of covariates into lower dimensions, we can see the reverse situation as before - the first community is separate, but the last two are overlayed on top of each other.</p>
<p>Below is Python code that embeds the covariates. We’ll use custom embedding code rather than graspologic’s LSE class, because it isn’t technically right to act as if we’re embedding a Laplacian (even though we’re doing essentially the same thing under the hood). Underneath it is a plot of the resulting embedding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.utils.extmath</span> <span class="kn">import</span> <span class="n">randomized_svd</span>

<span class="k">def</span> <span class="nf">embed</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dimension</span><span class="p">):</span>
    <span class="n">latents</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">randomized_svd</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">dimension</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">latents</span>

<span class="n">Y_latents</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As you can see, we’re in a similar situation as we were in with the adjacency matrix, but with different communities: instead of the first and second communities being indistinguishable, now the second and third are. We’d like to see full separation between all three communities, so we need some kind of representation of our network that allows us to use both the information in the topology and the information in the covariates. This is where CASE comes in.</p>
</div>
<div class="section" id="covariate-assisted-spectral-embedding">
<h4><span class="section-number">3.8.3. </span>Covariate-Assisted Spectral Embedding<a class="headerlink" href="#covariate-assisted-spectral-embedding" title="Permalink to this headline">¶</a></h4>
<p><i>Covariate-Assisted Spectral Embedding</i>, or CASE<sup>1</sup>, is a simple way of combining our network and our covariates into a single model. In the most straightforward version of CASE, we combine the network’s regularized Laplacian matrix <span class="math notranslate nohighlight">\(L\)</span> and a function of our covariate matrix <span class="math notranslate nohighlight">\(YY^\top\)</span>. Here, <span class="math notranslate nohighlight">\(Y\)</span> is just our covariate matrix, in which row <span class="math notranslate nohighlight">\(i\)</span> contains the covariates associated with node <span class="math notranslate nohighlight">\(i\)</span>. Notice the word “regularized” - This means (from the Laplacian section earlier) that our Laplacian looks like <span class="math notranslate nohighlight">\(L = L_{\tau} = D_{\tau}^{-1/2} A D_{\tau}^{-1/2}\)</span> (remember, <span class="math notranslate nohighlight">\(D\)</span> is a diagonal matrix with <span class="math notranslate nohighlight">\(D_{ii}\)</span> telling us the degree of node <span class="math notranslate nohighlight">\(i\)</span>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Suppose that <span class="math notranslate nohighlight">\(Y\)</span> only contains 0’s and 1’s. To interpret <span class="math notranslate nohighlight">\(YY^T\)</span>, notice we’re effectively taking the the dot product of each row of <span class="math notranslate nohighlight">\(Y\)</span> with each other row, because the transpose operation turns rows into columns. Now, look at what happens below when we take the dot product of two example vectors with only 0’s and 1’s in them:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7276f209-1403-45e3-a060-ac6cd49c643e">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-7276f209-1403-45e3-a060-ac6cd49c643e" title="Permalink to this equation">¶</a></span>\[\begin{align}
\begin{bmatrix}
1 &amp; 1 &amp; 1
\end{bmatrix} \cdot 
\begin{bmatrix}
0 \\
1 \\
1 \\
\end{bmatrix} = 1\times 0 + 1\times 1 + 1\times 1 = 2
\end{align}\]</div>
<p>If there are two overlapping 1’s in the same position of the left vector and the right vector, then there will be an additional 1 added to their weighted sum. So, in the case of a binary <span class="math notranslate nohighlight">\(YY^T\)</span>, when we matrix-multiply a row of <span class="math notranslate nohighlight">\(Y\)</span> by a column of <span class="math notranslate nohighlight">\(Y^T\)</span>, the resulting value, <span class="math notranslate nohighlight">\((YY^T)_{i, j}\)</span>, will be equal to the number of shared locations in which vectors <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> both have ones.</p>
</div>
<p>A particular value in <span class="math notranslate nohighlight">\(YY^\top\)</span>, <span class="math notranslate nohighlight">\((YY^\top)_{i, j}\)</span>, can be interpreted as measuring the “agreement” or “similarity” between row <span class="math notranslate nohighlight">\(i\)</span> and row <span class="math notranslate nohighlight">\(j\)</span> of our covariate matrix. Since each row represents the covariates for a particular node, the higher the value of <span class="math notranslate nohighlight">\((YY^\top)_{i, j}\)</span>, the more similar the covariates of the <span class="math notranslate nohighlight">\(i_{th}\)</span> and <span class="math notranslate nohighlight">\(j_{th}\)</span> nodes are. The overall result is a matrix that looks fairly similar to our Laplacian!</p>
<p>The following Python code generates our covariate similarity matrix <span class="math notranslate nohighlight">\(YY^\top\)</span>. We’ll also normalize the rows of our covariate matrix to have unit length using scikit-learn - this is because we want the scale for our covariate matrix to be roughly the same as the scale for our adjacency matrix. Later, we’ll weight <span class="math notranslate nohighlight">\(YY^\top\)</span> to help with this as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">normalize</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">YYt</span> <span class="o">=</span> <span class="n">Y</span><span class="nd">@Y</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>Below, you can see the Laplacian we generated earlier next to <span class="math notranslate nohighlight">\(YY^\top\)</span>. Remember, each matrix contains information about our communities that the other doesn’t have - and our goal is to combine them in a way that lets us distinguish between all three communities.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">L_ax</span> <span class="o">=</span> <span class="n">plot_heatmap</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;Regularized Laplacian&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">show_cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">YYt</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;Covariate matrix times its transpose ($YY^\top$)&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_27_0.png" src="_images/joint-representation-learning_27_0.png" />
</div>
</div>
<p>The way we’ll combine the two matrices will simply be a weighted sum of these two matrices - this is what CASE is doing under the hood. The weight (here called <span class="math notranslate nohighlight">\(\alpha\)</span>) is multiplied by <span class="math notranslate nohighlight">\(YY^\top\)</span> - that way, both matrices contribute an equal amount of useful information to the embedding.</p>
<div class="math notranslate nohighlight">
\[
L + \alpha YY^\top
\]</div>
<div class="section" id="exploring-possible-weights">
<h5><span class="section-number">3.8.3.1. </span>Exploring Possible Weights<a class="headerlink" href="#exploring-possible-weights" title="Permalink to this headline">¶</a></h5>
<p>An obvious question here is how to weight the covariates. If we simply summed the two matrices by themselves, we’d unfortunately be in a situation where whichever matrix contained larger numbers would dominate over the other. In our current setup, without a weight on <span class="math notranslate nohighlight">\(YY^\top\)</span>, the covariates of our network would dominate over its topology.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="n">Y</span><span class="nd">@Y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Summing the two matrices </span><span class="se">\n</span><span class="s2">without a weight&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">context</span><span class="o">=</span><span class="s1">&#39;notebook&#39;</span><span class="p">);</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">embed</span><span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="n">Y</span><span class="nd">@Y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedding the summed </span><span class="se">\n</span><span class="s2">matrix without a weight&quot;</span><span class="p">,</span> 
             <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_31_0.png" src="_images/joint-representation-learning_31_0.png" />
</div>
</div>
<p>What do different potential weights look like? Let’s do a comparison. Below you can see the embeddings for 9 possible weights on <span class="math notranslate nohighlight">\(YY^\top\)</span>, ranging between <span class="math notranslate nohighlight">\(10^{-5}\)</span> and 100.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="mf">10e-5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">9</span><span class="p">),</span> <span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">l_</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">L</span><span class="o">+</span><span class="n">a</span><span class="o">*</span><span class="n">YYt</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plot_latents</span><span class="p">(</span><span class="n">l_</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;weight: </span><span class="si">{</span><span class="n">a</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_legend</span><span class="p">()</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Comparison of embeddings for different weights on $YY^\top$&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_33_0.png" src="_images/joint-representation-learning_33_0.png" />
</div>
</div>
<p>It looks like we’d probably want a weight somewhere between 0.01 and 0.5  - then, we’ll have three communities which are fairly distinct from each other, implying that we’re pulling good information from both our network’s topology and its covariates. We could just pick our weight manually, but it would be nice to have some kind of algorithm or equation which lets us pick a reasonable weight automatically.</p>
</div>
<div class="section" id="finding-a-reasonable-weight-automatically">
<h5><span class="section-number">3.8.3.2. </span>Finding a Reasonable Weight Automatically<a class="headerlink" href="#finding-a-reasonable-weight-automatically" title="Permalink to this headline">¶</a></h5>
<p>In general, we’d like to embed in a way that lets us distinguish between communities. This means that if we knew which community each node belonged to, we’d like to be able to correctly retrieve the correct commmunities for each node as possible with a clustering algorithm after embedding. This also implies that the communities will be as distinct as possible.</p>
<p>We already found a range of possible weights, embedded our combined matrix for every value in this range, and then looked at which values produced the best clustering. But, how do we find a weight which lets us consistently use useful information from both the topology and the covariates?</p>
<p>When we embed symmetric matrices, keep in mind that the actual points we’re plotting are the components of the eigenvectors with the biggest eigenvalues. When we embed into two-dimensional space, for instance, the X-axis values of our points are the components of the eigenvector with the biggest eigenvalue, and the Y-axis values are the components of the eigenvector with the second-biggest eigenvalue. This means that we should probably be thinking about how much information the Laplacian and <span class="math notranslate nohighlight">\(YY^\top\)</span> contributes to the biggest eigenvalue/eigenvector pairs.</p>
<p>Thinking about this more, if we have a small weight, <span class="math notranslate nohighlight">\(YY^\top\)</span> will contribute only a small amount to the biggest eigenvalue/vector pair. If we have a large weight, <span class="math notranslate nohighlight">\(YY^\top\)</span> will contribute a large amount to the biggest eigenvalue/vector pair. The weight that causes the Laplacian and <span class="math notranslate nohighlight">\(YY^\top\)</span> to contribute the same amount of information, then, is just the ratio of the biggest eigenvalue of <span class="math notranslate nohighlight">\(L\)</span> and the biggest eigenvalue of <span class="math notranslate nohighlight">\(YY^\top\)</span>:</p>
<div class="math notranslate nohighlight">
\[
weight = \frac{\lambda_1 (L)}{\lambda_1 (YY^\top)}
\]</div>
<p>Let’s check what happens when we combine our Laplacian and covariates matrix using the weight described in the equation above. Our embedding works the same as it does in Laplacian Spectral Embedding: we decompose our combined matrix using Singular Value Decomposition, truncating the columns, and then we visualize the rows of the result. Remember, we’ll be embedding <span class="math notranslate nohighlight">\(L + \alpha YY^\top\)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is our weight. We’ll embed all the way down to two dimensions, just to make visualization simpler.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.sparse.linalg</span> <span class="kn">import</span> <span class="n">eigsh</span>

<span class="c1"># Find the biggest eigenvalues in both of our matrices</span>
<span class="n">leading_eigval_L</span><span class="p">,</span> <span class="o">=</span> <span class="n">eigsh</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">leading_eigval_YYt</span><span class="p">,</span> <span class="o">=</span> <span class="n">eigsh</span><span class="p">(</span><span class="n">YYt</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Per our equation above, we get the weight using</span>
<span class="c1"># the ratio of the two biggest eigenvalues.</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">leading_eigval_L</span> <span class="o">/</span> <span class="n">leading_eigval_YYt</span>

<span class="c1"># Do our weighted sum, then embed</span>
<span class="n">L_</span> <span class="o">=</span> <span class="n">L</span> <span class="o">+</span> <span class="n">weight</span><span class="o">*</span><span class="n">YYt</span>
<span class="n">latents_</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">L_</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">L_</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Our Combined Laplacian and </span><span class="se">\n</span><span class="s2">covariates matrix with a weight of </span><span class="si">{</span><span class="n">weight</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_</span><span class="p">,</span> 
             <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Embedding of the combined matrix&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_39_0.png" src="_images/joint-representation-learning_39_0.png" />
</div>
</div>
<p>Success! We’ve managed to achieve separation between all three communities. Below we can see (from left to right) a comparison of our network’s latent position when we only use its topological information, when we only use the information contained in its covariates, and finally our embedding using the weight we found.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">LaplacianSpectralEmbed</span> <span class="k">as</span> <span class="n">LSE</span>


<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">L_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions when we only use the Laplacian&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">Y_latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions when we only use our covariates&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">latents_</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions when we combine</span><span class="se">\n</span><span class="s2"> our network and its covariates&quot;</span><span class="p">,</span> 
             <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_41_0.png" src="_images/joint-representation-learning_41_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="using-graspologic">
<h4><span class="section-number">3.8.4. </span>Using Graspologic<a class="headerlink" href="#using-graspologic" title="Permalink to this headline">¶</a></h4>
<p>Graspologic’s CovariateAssistedSpectralEmbedding class implements CASE directly. The following code applies CASE to reduce the dimensionality of <span class="math notranslate nohighlight">\(L + aYY^T\)</span> down to two dimensions, and then plots the latent positions to show the clustering.</p>
<div class="admonition-non-assortative-networks admonition">
<p class="admonition-title">Non-Assortative Networks</p>
<p>We don’t always necessarily want to embed <span class="math notranslate nohighlight">\(L + \alpha YY^\top\)</span>. Using the regularized Laplacian by itself, for instance, isn’t always best. If your network is <em>non-assortative</em> - meaning, the between-block probabilities are greater than the within-block probabilities - it’s better to square our Laplacian. This is because the adjacency matrices of non-assortative networks have a lot of negative eigenvalues; squaring the Laplacian gets rid of a lot of annoying negative eigenvalues, and we end up with a better embedding. In the non-assortative case, we end up embedding <span class="math notranslate nohighlight">\(LL + aYY^\top\)</span>. The <code class="docutils literal notranslate"><span class="pre">embedding_alg</span></code> parameter controls this: you can write <code class="docutils literal notranslate"><span class="pre">embedding_alg=&quot;non-assortative&quot;</span></code> if you’re in the non-assortative situation.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">CovariateAssistedEmbed</span> <span class="k">as</span> <span class="n">CASE</span>

<span class="n">casc</span> <span class="o">=</span> <span class="n">CASE</span><span class="p">(</span><span class="n">assortative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">latents</span> <span class="o">=</span> <span class="n">casc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">covariates</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedding our model using graspologic&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_45_0.png" src="_images/joint-representation-learning_45_0.png" />
</div>
</div>
</div>
<div class="section" id="omnibus-joint-embedding">
<h4><span class="section-number">3.8.5. </span>Omnibus Joint Embedding<a class="headerlink" href="#omnibus-joint-embedding" title="Permalink to this headline">¶</a></h4>
<p>If you’ve read the Multiple-Network Representation Learning section, you’ve seen the Omnibus Embedding (if you haven’t read that section, you should go read it before reading this one!). To recap, the way the omnibus embedding works is:</p>
<ol class="simple">
<li><p>Have a bunch of networks</p></li>
<li><p>Combine the adjacency matrices from all of those networks into a single, huge network</p></li>
<li><p>Embed that huge network</p></li>
</ol>
<p>Remember that the Omnibus Embedding is a way to bring all of your networks into the same space (meaning, you don’t run into any rotational nonidentifiability issues when you embed). Once you embed the Omnibus Matrix, it’ll produce a huge latent position matrix, which you can break apart along the rows to recover latent positions for your individual networks.</p>
<p>You might be able to predict where this is going. What if we created an Omnibus embedding not with a set of networks, but with a network and covariates?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">OmnibusEmbed</span>

<span class="c1"># embed with Omni</span>
<span class="n">omni</span> <span class="o">=</span> <span class="n">OmnibusEmbed</span><span class="p">()</span>

<span class="c1"># Normalize the covariates first</span>
<span class="n">YYt</span> <span class="o">=</span> <span class="n">Y</span><span class="nd">@Y</span><span class="o">.</span><span class="n">T</span>
<span class="n">YYt</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">YYt</span><span class="p">)</span>

<span class="c1"># Create a joint embedding</span>
<span class="n">joint_embedding</span> <span class="o">=</span> <span class="n">omni</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">YYt</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># network</span>
<span class="n">network_plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">legend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Omni embedding for topology&quot;</span><span class="p">)</span>

<span class="c1"># covariates</span>
<span class="n">covariates_plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
                               <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Omni embedding for covariates&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">move_legend</span><span class="p">(</span><span class="n">covariates_plot</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;center right&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_49_0.png" src="_images/joint-representation-learning_49_0.png" />
</div>
</div>
<p>There’s a few things going on here. First, we had to normalize the covariates by dividing <span class="math notranslate nohighlight">\(YY^\top\)</span> by its maximum. This is because if we didn’t, the covariates and the adjacency matrix would contribute vastly different amounts of information to the omnibus matrix. You can see that by looking at the average value of <span class="math notranslate nohighlight">\(YY^\top\)</span> compared to the average value of <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Mean of Y@Y.T:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="nd">@Y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Mean of A:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean of Y@Y.T: 0.02
Mean of A: 0.23
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Mean of normalized Y@Y.T:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">YYt</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Mean of A:&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean of normalized Y@Y.T: 0.42
Mean of A: 0.23
</pre></div>
</div>
</div>
</div>
<p>Remember the way this data is set up: you can separate the first community from the last two with the topology, you can separate the last community from the first two with the covariates, but you need both data sources to separate all three.</p>
<p>Here, you can see that <em>both embeddings</em> are able to separate all three communities. This is because the Omnibus Embedding induces dependence on the latent positions it outputs. Remember that the off-diagonals of the Omnibus Matrix contain the averages of pairs of networks fed into it. These off-diagonal elements are responsible for some “information leakage”: so the topology embedding contains information from the covariates, and the covariate embedding contains information from the topology.</p>
</div>
<div class="section" id="mase-joint-embedding">
<h4><span class="section-number">3.8.6. </span>MASE Joint Embedding<a class="headerlink" href="#mase-joint-embedding" title="Permalink to this headline">¶</a></h4>
<p>Just like you can use the OMNI to do a joint embedding, you can also use MASE to do a joint embedding. This fundamentally comes down to the fact that both embeddings fundamentally just eat matrices as their input - whether those matrices are the adjacency matrix or <span class="math notranslate nohighlight">\(YY^\top\)</span> doesn’t really matter.</p>
<p>Just like OMNI, we’ll quickly recap how MASE works here:</p>
<ol class="simple">
<li><p>Have a bunch of networks</p></li>
<li><p>Embed them all separately with ASE or LSE</p></li>
<li><p>Concate those embeddings into a single latent position matrix with a lot more dimensions</p></li>
<li><p>Embed that new matrix</p></li>
</ol>
<p>The difference here is the same as with Omni – we have the adjacency matrix (topology) and its covariates for a single network. So instead of embedding a bunch of adjacency matrices or Laplacians, we embed the adjacency matrix (or Laplacian) and the similarity matrix for the covariates <span class="math notranslate nohighlight">\(YY^\top\)</span> separately, concatenate, and then embed again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">MultipleASE</span> <span class="k">as</span> <span class="n">MASE</span>

<span class="c1"># Remmeber that YY^T is still normalized!</span>
<span class="n">mase</span> <span class="o">=</span> <span class="n">MASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">joint_embedding</span> <span class="o">=</span> <span class="n">mase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">YYt</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># network</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">joint_embedding</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                            <span class="n">title</span><span class="o">=</span><span class="s2">&quot;MASE embedding&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">move_legend</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;center right&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.3</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/joint-representation-learning_57_0.png" src="_images/joint-representation-learning_57_0.png" />
</div>
</div>
<p>As you can see, MASE lets us get fairly clear separation between communities. The covariates are still normalized, as with OMNI, so that they can contribute the same amount to the embedding as the adjacency matrix.</p>
<div class="section" id="references">
<h5><span class="section-number">3.8.6.1. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h5>
<p>[1] N. Binkiewicz, J. T. Vogelstein, K. Rohe, Covariate-assisted spectral clustering, Biometrika, Volume 104, Issue 2, June 2017, Pages 361–377, https://doi.org/10.1093/biomet/asx008<br />
[2] Lloyd, S. (1982). Least squares quantization in PCM. IEEE transactions on information theory, 28(2), 129-137.<br />
[3] https://scikit-learn.org/stable/modules/clustering.html#k-means<br />
[4] Hotelling, H. (1936). Relations between two sets of variates. Biometrika 28, 321–77.</p>
</div>
</div>
</div>
<span id="document-representations/ch6/estimating-parameters_theory"></span><div class="section" id="model-estimation-theory">
<h3><span class="section-number">3.9. </span>Model Estimation Theory<a class="headerlink" href="#model-estimation-theory" title="Permalink to this headline">¶</a></h3>
<p>Throughout Chapter 5, we spent a lot of attention developing intuition for many of the network models that are essential to understanding random networks. Recall that the notation that we use for a random network (more specifically, a network-valued random variable), <span class="math notranslate nohighlight">\(\mathbf A\)</span>, does <em>not</em> refer to any network we could ever hope to see (or as we introduced in the previous chapter, <em>realize</em>) in the real world. This issue is extremely important in network machine learning, so we will try to drive it home one more time: no matter how much data we collected (unless we could get infinite data, which we <em>can’t</em>), we can never hope to understand the true distribution of <span class="math notranslate nohighlight">\(\mathbf A\)</span>. As network scientists, this leaves us with a bit of a problem: what, then, can we do to make useful claims about <span class="math notranslate nohighlight">\(\mathbf A\)</span>, if we can’t actually see <span class="math notranslate nohighlight">\(\mathbf A\)</span> nor its distribution?</p>
<p>This is where statistics, particularly, <strong>estimation</strong>, comes into play. At a very high level, estimation is a procedure to calculate properties about a random variable (or a set of random variables) using <em>only</em> the data we are given: finitely many (in network statistics, often just <em>one</em>) samples which we assume are <em>realizations</em> of the random variable we want to learn about. The properties of the random variable that we seek to learn about are called <strong>estimands</strong>, and  In the case of our network models, in particular, we will attempt to obtain reasonable estimates of the parameters (our <em>estimands</em>) associated with random networks.</p>
<p>The most useful property that we will leverage which was developed in Chapter <span class="math notranslate nohighlight">\(5\)</span> is the independent-edge assumption. As we discussed, when working with independent-edge random network models, we will assume that edges in our random network are <em>independent</em>. This means that the probability of observing a particular realization of a random network is, in fact, the product of the probabilities of observing each edge in the random network. Notationally, what this means is that if <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network with <span class="math notranslate nohighlight">\(n\)</span> nodes and edges <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>, and <span class="math notranslate nohighlight">\(A\)</span> is a realization of that random network with edges <span class="math notranslate nohighlight">\(a_{ij}\)</span>, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(\mathbf A = A) &amp;= \mathbb P(\mathbf a_{11} = a_{11}, \mathbf a_{12} = a_{12}, ..., \mathbf a_{nn} = a_{nn}), \\
    &amp;= \prod_{i, j} \mathbb P_\theta(\mathbf a_{ij} = a_{ij}).
\end{align*}\]</div>
<p>In the special case where our networks are simple (undirected and loopless), this simplifies to:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(\mathbf A = A) &amp;= \prod_{i &lt; j} \mathbb P_\theta(\mathbf a_{ij} = a_{ij}).
\end{align*}\]</div>
<p>for any network realization <span class="math notranslate nohighlight">\(A\)</span> which is simple. This is because if <span class="math notranslate nohighlight">\(\mathbf a_{ij} = a\)</span>, then we also know that <span class="math notranslate nohighlight">\(\mathbf a_{ji} = a\)</span>, and we only need to worry about one of the edges (we chose the edges in the upper right triangle of the adjacency matrix arbitrarily).  Further, since <span class="math notranslate nohighlight">\(A\)</span> is also simple, then we know hat <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>; that is, no nodes have loops, so we don’t need to worry about the case where <span class="math notranslate nohighlight">\(i = j\)</span> either.</p>
<p>We will set the scene for the later examples using a common example. Let’s say we flip a coin <span class="math notranslate nohighlight">\(10\)</span> times, and see <span class="math notranslate nohighlight">\(6\)</span> heads. What is the probability that the coin lands on heads? Intuitively, the answer is rather simple! It feels like it should just be <span class="math notranslate nohighlight">\(\frac{6}{10}\)</span>. And in one particular way, that really is the <em>best</em> guess we could make!</p>
<p>Below, we discuss the nitty-gritty technical details of how we learn about random networks using a particular method known as Maximum Likelihood Estimation (MLE). Maximum likelihood estimation is why <span class="math notranslate nohighlight">\(\frac{6}{10}\)</span> is a great guess for our coin flip example. Finding MLEs can be pretty difficult, so we leave the details in starred sections. If you aren’t familiar with MLE, you can skip these, and still learn how to use the results!</p>
<div class="section" id="the-method-of-maximum-likelihood-estimation-mle">
<h4><span class="section-number">3.9.1. </span>The Method of Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#the-method-of-maximum-likelihood-estimation-mle" title="Permalink to this headline">¶</a></h4>
<p>Let’s think about what exactly this means using an example that you are likely familiar with. I have a single coin, and I want to know the probability of the outcome of a roll of that coin being a heads. For sake of argument, we will call this coin <em>fair</em>, which means that the true probability it lands on heads (or tails) is <span class="math notranslate nohighlight">\(0.5\)</span>. In this case, I would call the outcome of the <span class="math notranslate nohighlight">\(i^{th}\)</span> coin flip the random variable <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>, and it can produce realizations which take one of two possible values: a heads (an outcome of a <span class="math notranslate nohighlight">\(1\)</span>) or a tails (an outcome of a <span class="math notranslate nohighlight">\(0\)</span>). We will say that we see <span class="math notranslate nohighlight">\(10\)</span> total coin flips. We will number these realizations as <span class="math notranslate nohighlight">\(x_i\)</span>, where <span class="math notranslate nohighlight">\(i\)</span> goes from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(10\)</span>. To recap, the boldfaced <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> denotes the random variable, and the unbolded <span class="math notranslate nohighlight">\(x_i\)</span> denotes the realization which we actually see. Our question of interest is: how do we estimate the probability of the coin landing on a heads, if we don’t know anything about the true probability value <span class="math notranslate nohighlight">\(p\)</span>, other than the outcomes of the coin flips we got to observe?</p>
<p>Here, since <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> takes the value <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(0\)</span> each with probability <span class="math notranslate nohighlight">\(0.5\)</span>, we would say that <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> is a <span class="math notranslate nohighlight">\(Bernoulli(0.5)\)</span> random variable. This means that the random variable <span class="math notranslate nohighlight">\(\mathbf x\)</span> has the Bernoulli distribution, and the probability of a heads, <span class="math notranslate nohighlight">\(p\)</span>, is <span class="math notranslate nohighlight">\(0.5\)</span>. All <span class="math notranslate nohighlight">\(10\)</span> of our <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> are called <em>identically distributed</em>, since they all have the same <span class="math notranslate nohighlight">\(Bernoulli(0.5)\)</span> distribution.</p>
<p>We will also assume that the outcomes of the coin flips are mutually independent, which is explained in the terminology section.</p>
<p>For any one coin flip, the probability of observing the outcome <span class="math notranslate nohighlight">\(i\)</span> is, by definition of the Bernoulli distribution:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(\mathbf x_i = x_i) = p^{x_i} (1 - p)^{1 - x_i}.
\end{align*}\]</div>
<p>Note that we use the notation <span class="math notranslate nohighlight">\(\mathbb P_\theta\)</span> to indicate that the probability is a function of the parameter set <span class="math notranslate nohighlight">\(\theta\)</span> for the random variable <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>. Here, since the only parameter for each <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> is a probability <span class="math notranslate nohighlight">\(p\)</span>, then <span class="math notranslate nohighlight">\(\theta = p\)</span>.</p>
<p>If we saw <span class="math notranslate nohighlight">\(n\)</span> total outcomes, the probability is, using the definition of mutual independence:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(\mathbf x_1 = x_1, ..., \mathbf x_{n} = x_{n}; p) &amp;= \prod_{i = 1}^{n}\mathbb P(\mathbf x_i = x_i), \\
    &amp;= \prod_{i = 1}^n p^{x_i}(1 - p)^{1 - x_i}, \\
    &amp;= p^{\sum_{i = 1}^n x_i}(1 - p)^{n - \sum_{i = 1}^n x_i}.
\end{align*}\]</div>
<p>What if we saw <span class="math notranslate nohighlight">\(10\)</span> coin flips, and <span class="math notranslate nohighlight">\(6\)</span> were heads? Can we take a “guess” at what <span class="math notranslate nohighlight">\(p\)</span> might be? Intuitively your first reaction might be to say a good guess of <span class="math notranslate nohighlight">\(p\)</span>, which we will abbreviate <span class="math notranslate nohighlight">\(\hat p\)</span>, would be <span class="math notranslate nohighlight">\(0.6\)</span>, which is <span class="math notranslate nohighlight">\(6\)</span> heads of <span class="math notranslate nohighlight">\(10\)</span> outcomes. In many ways, this intuitive guess is spot on. However, in network machine learning, we like to be really specific about why, exactly, this guess makes sense.</p>
<p>Looking at the above equation, one thing we can do is use the technique of <strong>maximum likelihood estimation</strong>. We call the function <span class="math notranslate nohighlight">\(\mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p)\)</span> the <em>likelihood</em> of our sequence, for a given value of <span class="math notranslate nohighlight">\(p\)</span>. Note that we have added the term “<span class="math notranslate nohighlight">\(; p\)</span>” to our notation, which is simply to emphasize the dependence of the likelihood on the probability. So, what we <em>really</em> want to do is find the value that <span class="math notranslate nohighlight">\(p\)</span> could take, which <em>maximizes</em> the likelihood. Let’s see what the likelihood function looks like as a function of different values of <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">.</span><span class="mi">02</span><span class="p">,</span> <span class="o">.</span><span class="mi">98</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">49</span><span class="p">)</span>
<span class="n">nflips</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="n">nheads</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">p</span><span class="o">**</span><span class="p">(</span><span class="n">nheads</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">nflips</span> <span class="o">-</span> <span class="n">nheads</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Bernoulli probability parameter, p&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Likelihood, $P_{</span><span class="se">\\</span><span class="s2">theta}(x_1, ..., x_</span><span class="si">{10}</span><span class="s2">)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_theory_1_0.png" src="_images/estimating-parameters_theory_1_0.png" />
</div>
</div>
<p>As we can see, it turns out that our intuitive answer, that <span class="math notranslate nohighlight">\(p=0.6\)</span>, is in fact the Maximum Likelihood Estimate for the Bernoulli probability parameter <span class="math notranslate nohighlight">\(p\)</span>. Now how do we go about showing this rigorously?</p>
<p>An easier problem, we often will find, is to instead maximize the <em>log likelihood</em> rather than the likelihood itself. This is because the log function is <em>monotone</em>, which means that if <span class="math notranslate nohighlight">\(\mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_1) &lt; \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_2)\)</span>, then <span class="math notranslate nohighlight">\(\log\mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_1) &lt; \log \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_2)\)</span> as well for some choices <span class="math notranslate nohighlight">\(p_1\)</span> and <span class="math notranslate nohighlight">\(p_2\)</span>. Without going too down in the weeds, the idea is that the <span class="math notranslate nohighlight">\(\log\)</span> function does not change any critical points of the likelihood. The log likelihood of the above expression is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\log \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_{n} = x_{n}; p) &amp;= \log \left[p^{\sum_{i = 1}^n x_i}(1 - p)^{n - \sum_{i = 1}^n x_i}\right], \\
&amp;= \sum_{i = 1}^n x_i \log(p) + \left(n - \sum_{i = 1}^n x_i\right)\log(1 - p).
\end{align*}\]</div>
<p>And visually, the log-likelihood now looks instead like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loglikelihood</span> <span class="o">=</span> <span class="n">nheads</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">nflips</span> <span class="o">-</span> <span class="n">nheads</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">loglikelihood</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Bernoulli probability parameter, p&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Log Likelihood, $</span><span class="se">\\</span><span class="s2">log P_{</span><span class="se">\\</span><span class="s2">theta}(x_1, ..., x_</span><span class="si">{10}</span><span class="s2">)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/estimating-parameters_theory_4_0.png" src="_images/estimating-parameters_theory_4_0.png" />
</div>
</div>
<p>Although we can see that the two plots look <em>almost</em> nothing alike, the key is the word <em>almost</em> here. Notice that the absolute maximum is, in fact, the same regardless of whether we use the likelihood or the log-likelihood. Further, notice that at the maximum, the slope of the tangent line is <span class="math notranslate nohighlight">\(0\)</span>. You may recall from calculus that this is how we typically go about finding a critical point of a function. Now, let’s get make our argument a little more technical. Remembering from calculus <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span>, to find a maximal point of the log-likelihood function with respect to some variable <span class="math notranslate nohighlight">\(p\)</span>, our process looks like this:</p>
<ol class="simple">
<li><p>Take the derivative of the log-likelihood with respect to <span class="math notranslate nohighlight">\(p\)</span>,</p></li>
<li><p>Set it equal to <span class="math notranslate nohighlight">\(0\)</span> and solve for the critical point <span class="math notranslate nohighlight">\(\tilde p\)</span>,</p></li>
<li><p>Verify that the critical point <span class="math notranslate nohighlight">\(\tilde p\)</span> is indeed an estimate of a maximum, <span class="math notranslate nohighlight">\(\hat p\)</span>.</p></li>
</ol>
<p>Proceeding using the result we derived above, and using the fact that <span class="math notranslate nohighlight">\(\frac{d}{du} \log(u) = \frac{1}{u}\)</span> and that <span class="math notranslate nohighlight">\(\frac{d}{du} \log(1 - u) = -\frac{1}{1 - u}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{d}{d p}\log \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_{n} = x_{n}; p) &amp;= \frac{\sum_{i = 1}^n x_i}{p} - \frac{n - \sum_{i = 1}^n x_i}{1 - p} = 0, \\
\Rightarrow \frac{\sum_{i = 1}^n x_i}{p} &amp;= \frac{n - \sum_{i = 1}^n x_i}{1 - p}, \\
\Rightarrow (1 - p)\sum_{i = 1}^n x_i &amp;= p\left(n - \sum_{i = 1}^n x_i\right), \\
\sum_{i = 1}^n x_i - p\sum_{i = 1}^n x_i &amp;= pn - p\sum_{i = 1}^n x_i ,\\
\Rightarrow \tilde p &amp;= \frac{1}{n}\sum_{i = 1}^n x_i.
\end{align*}\]</div>
<p>We use the notation <span class="math notranslate nohighlight">\(\tilde p\)</span> here to denote that <span class="math notranslate nohighlight">\(\tilde p\)</span> is a critical point of the function.</p>
<p>Finally, we must check that this is an estimate of a maximum, which we can do by taking the second derivative and checking that the second derivative is negative. We will omit this since it’s a bit intricate and tangential from our argument, but if you work it through, you will find that the second derivative is indeed negative at <span class="math notranslate nohighlight">\(\tilde p\)</span>. This means that <span class="math notranslate nohighlight">\(\tilde p\)</span> is indeed an estimate of a maximum, which we would denote by <span class="math notranslate nohighlight">\(\hat p\)</span>.</p>
<p>Finally, using this result, we find that with <span class="math notranslate nohighlight">\(6\)</span> heads in <span class="math notranslate nohighlight">\(10\)</span> outcomes, we would obtain an estimate:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat p &amp;= \frac{6}{10} = 0.6.
\end{align*}\]</div>
<p>which exactly aligns with our intuition.</p>
<p>So, why do we need estimation tools, if in our example, our intuition gave us the answer a whole lot faster? Unfortunately, the particular scenario we described was one of the <em>simplest possible examples</em> in which a parameter requires estimation. As the scenario grows more complicated, and <em>especially</em> when we extend to network-valued data, figuring out good ways to estimate parameters is extremely difficult. For this reason, we will describe some tools which are very relevant to network machine learning to learn about network parameters.</p>
<div class="section" id="mle-for-er">
<h5><span class="section-number">3.9.1.1. </span>MLE for ER<a class="headerlink" href="#mle-for-er" title="Permalink to this headline">¶</a></h5>
<p>In Chapter 5, we explored the derivation for the probability of observing a realization <span class="math notranslate nohighlight">\(A\)</span> of a given random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is ER, which is equivalent to the likelihood of <span class="math notranslate nohighlight">\(A\)</span>. Recall this was:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= p^{m} \cdot (1 - p)^{\binom{n}{2} - m}.
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(m = \sum_{i &lt; j} a_{ij}\)</span> is the total number of edges in the observed network <span class="math notranslate nohighlight">\(A\)</span>. Our approach here parallels directly the approach for the coin; we begin by taking the log of the probability:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \log \mathbb P_\theta(A) &amp;= \log \left[p^{m} \cdot (1 - p)^{\binom{n}{2} - m}\right], \\
    &amp;= m \log p + \left(\binom n 2 - m\right)\log (1 - p).
\end{align*}\]</div>
<p>Next, we take the derivative with respect to <span class="math notranslate nohighlight">\(p\)</span>, set equal to <span class="math notranslate nohighlight">\(0\)</span>, and we end up with:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{d}{d p}\log \mathbb P_\theta(A) &amp;= \frac{m}{p} - \frac{\binom n 2 - m}{1 - p} = 0, \\
\Rightarrow \tilde p &amp;= \frac{m}{\binom n 2}.
\end{align*}\]</div>
<p>We omitted several detailed steps due to the fact that we show the rigorous derivation above. Checking the second derivative, which we omit since it is rather mathematically tedious, we see that the second derivative at <span class="math notranslate nohighlight">\(\tilde p\)</span> is negative, so we indeed have found an estimate of the maximum, and will be denoted by <span class="math notranslate nohighlight">\(\hat p\)</span>. This gives that the Maximum Likelihood Estimate (or, the MLE, for short) of the probability <span class="math notranslate nohighlight">\(p\)</span> for a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is ER is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat p &amp;= \frac{m}{\binom n 2}.
\end{align*}\]</div>
</div>
<div class="section" id="mle-for-sbm">
<h5><span class="section-number">3.9.1.2. </span>MLE for SBM<a class="headerlink" href="#mle-for-sbm" title="Permalink to this headline">¶</a></h5>
<p>When we derived the probability for a realization <span class="math notranslate nohighlight">\(A\)</span> of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which could be characterized using the <em>a priori</em> Stochasic Block Model, we obtained that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \prod_{k, k' \in [K]}b_{k'k}^{m_{k'k}} \cdot (1 - b_{k'k})^{n_{k'k - m_{k'k}}},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(n_{k'k} = \sum_{i &lt; j}\mathbb 1_{\tau_i = k}\mathbb 1_{\tau_j = k'}\)</span> was the number of possible edges between nodes in community <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span>, and <span class="math notranslate nohighlight">\(m_{k'k} = \sum_{i &lt; j}\mathbb 1_{\tau_i = k}\mathbb 1_{\tau_j = k'}a_{ij}\)</span> was the number of edges in the realization <span class="math notranslate nohighlight">\(A\)</span> between nodes within communities <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span>.</p>
<p>Noting that the log of the product is the sum of the logs, or that <span class="math notranslate nohighlight">\(\log \prod_i x_i = \sum_i \log x_i\)</span>, the log of the probability is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \log \mathbb P_\theta(A) &amp;= \sum_{k, k' \in [K]} m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k}).
\end{align*}\]</div>
<p>We notice a side-note that we mentioned briefly in the network models section: in a lot of ways, the probability (and consequently, the log probability) of a random network which is an <em>a priori</em> SBM behaves very similarly to that of a random network which is ER, with the caveat that the probability term <span class="math notranslate nohighlight">\(p\)</span>, the total number of possible edges <span class="math notranslate nohighlight">\(\binom n 2\)</span>, and the total number of edges <span class="math notranslate nohighlight">\(m\)</span> have been replaced with the probability term <span class="math notranslate nohighlight">\(b_{k'k}\)</span>, the total number of possible edges <span class="math notranslate nohighlight">\(n_{k'k}\)</span>, and the total number of edges <span class="math notranslate nohighlight">\(m_{k'k}\)</span> which <em>apply only to that particular pair of communities</em>. In this sense, the <em>a priori</em> SBM is kind of like a collection of communities of ER networks. Pretty neat right? Well, it doesn’t stop there. When we take the partial derivative of <span class="math notranslate nohighlight">\(\log \mathbb P_\theta(A)\)</span> with respect to any of the probability terms <span class="math notranslate nohighlight">\(b_{l'l}\)</span>, we see an even more direct consequence of this observation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial }{\partial b_{l' l}}\log \mathbb P_\theta(A) &amp;= \frac{\partial}{\partial b_{l'l}}\sum_{k, k' \in [K]} m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k}), \\
    &amp;= \sum_{k, k' \in [K]} \frac{\partial}{\partial b_{l'l}}\left[m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k})\right].
\end{align*}\]</div>
<p>Now what? Notice that any of the summands in which <span class="math notranslate nohighlight">\(k \neq l\)</span> and <span class="math notranslate nohighlight">\(k' \neq l'\)</span>, the partial derivative with respect to <span class="math notranslate nohighlight">\(b_{l'l}\)</span> is in fact exactly <span class="math notranslate nohighlight">\(0\)</span>! Why is this? Well, let’s consider a <span class="math notranslate nohighlight">\(k\)</span> which is different from <span class="math notranslate nohighlight">\(l\)</span>, and a <span class="math notranslate nohighlight">\(k'\)</span> which is different from <span class="math notranslate nohighlight">\(l'\)</span>. Notice that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial}{\partial b_{l'l}}\left[m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k})\right] = 0,
\end{align*}\]</div>
<p>which simply follows since the quantity to the right of the partial derivative is not a funcion of <span class="math notranslate nohighlight">\(b_{l'l}\)</span> at all! Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial }{\partial b_{l' l}}\log \mathbb P_\theta(A) &amp;= 0 + \frac{\partial}{\partial b_{l'l}}\left[m_{l'l}\log b_{l'l} + \left(n_{l'l} - m_{l'l}\right)\log(1 - b_{l'l})\right] \\
    &amp;= \frac{m_{l'l}}{b_{l'l}} - \frac{n_{l'l} - m_{l'l}}{1 - b_{l'l}} = 0, \\
\Rightarrow b_{l'l}^* &amp;= \frac{m_{l'l}}{n_{l'l}}.
\end{align*}\]</div>
<p>Like above, we omit the second derivative test, and conclude that the MLE of the block matrix <span class="math notranslate nohighlight">\(B\)</span> for a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is <em>a priori</em> SBM is the matrix <span class="math notranslate nohighlight">\(\hat B\)</span> with entries:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat b_{l'l} &amp;= \frac{m_{l'l}}{n_{l'l}}.
\end{align*}\]</div>
</div>
</div>
<div class="section" id="spectral-methods">
<h4><span class="section-number">3.9.2. </span>Spectral Methods<a class="headerlink" href="#spectral-methods" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="why-don-t-we-just-use-mle">
<h4><span class="section-number">3.9.3. </span>Why don’t we just use MLE?*<a class="headerlink" href="#why-don-t-we-just-use-mle" title="Permalink to this headline">¶</a></h4>
<p>The a posteriori Stochastic Block Model has a pair of parameters, the block matrix, <span class="math notranslate nohighlight">\(B\)</span>, and the community probability vector, <span class="math notranslate nohighlight">\(\vec \pi\)</span>. If you are keeping up with the log-likelihood derivations in the single network models section, you will recall that the log-likelihood for an a posteriori Stochastic Block Model, we obtain that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \sum_{\vec \tau \in \mathcal T} \prod_{k = 1}^K \left[\pi_k^{n_k}\cdot \prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\right]
\end{align*}\]</div>
<p>That expression, it turns out, is a lot more complicated than what we had to deal with for the <em>a priori</em> Stochastic Block Model. Taking the log gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\log 
    \mathbb P_\theta(A) &amp;= \log\left(\sum_{\vec \tau \in \mathcal T} \prod_{k = 1}^K \left[\pi_k^{n_k}\cdot \prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\right]\right)
\end{align*}\]</div>
<p>Whereas the log of a product of terms is the sum of the logs of the terms, no such easy simplification exists for the log of a <em>sum</em> of terms. This means that we will have to get a bit creative here. Instead, we will turn first to the <em>a priori</em> Random Dot Product Graph, and then figure out how to estimate parameters from an <em>a posteriori</em> SBM using that.</p>
</div>
</div>
</div>
</div>
<span id="document-representations/ch7/ch7"></span><div class="section" id="theoretical-results">
<h2><span class="section-number">4. </span>Theoretical Results<a class="headerlink" href="#theoretical-results" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-representations/ch7/theory-single-network"></span><div class="section" id="theory-for-single-network-models">
<h3><span class="section-number">4.1. </span>Theory for Single Network Models<a class="headerlink" href="#theory-for-single-network-models" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-representations/ch7/theory-multigraph"></span><div class="section" id="theory-for-multiple-network-models">
<h3><span class="section-number">4.2. </span>Theory for Multiple-Network Models<a class="headerlink" href="#theory-for-multiple-network-models" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-representations/ch7/theory-matching"></span><div class="section" id="theory-for-graph-matching">
<h3><span class="section-number">4.3. </span>Theory for Graph Matching<a class="headerlink" href="#theory-for-graph-matching" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-applications/ch8/ch8"></span><div class="section" id="applications-when-you-have-one-network">
<h2><span class="section-number">1. </span>Applications When You Have One Network<a class="headerlink" href="#applications-when-you-have-one-network" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-applications/ch8/community-detection"></span><div class="section" id="community-detection">
<h3><span class="section-number">1.1. </span>Community Detection<a class="headerlink" href="#community-detection" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-applications/ch8/testing-differences"></span><div class="section" id="testing-for-differences-between-communities">
<h3><span class="section-number">1.2. </span>Testing for Differences between Communities<a class="headerlink" href="#testing-for-differences-between-communities" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-applications/ch8/model-selection"></span><div class="section" id="model-selection">
<h3><span class="section-number">1.3. </span>Model Selection<a class="headerlink" href="#model-selection" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-applications/ch8/single-vertex-nomination"></span><div class="section" id="single-network-vertex-nomination">
<h3><span class="section-number">1.4. </span>Single-Network Vertex Nomination<a class="headerlink" href="#single-network-vertex-nomination" title="Permalink to this headline">¶</a></h3>
<p>Say you’re a criminal investigator trying to uncover a human trafficking ring. You build a network of potential suspects: some of the nodes in the network represent human traffickers, and some represent innocent people. The edges of the network represent a working relationship between a given pair of individuals.</p>
<p>Your team has limited resources, and so it’s difficult to scrutinize everybody in the network directly to see if they are human traffickers. Ideally, you’d like to use your network to nominate potential suspects, so that you can prioritize your investigative efforts. You’ve already done some work: you have a list of a few nodes of the network who are known to be traffickers, and you have a list of a few who you know are not. Your goal, then, is to build an ordered list of nodes in the network that are most similar to the nodes you already know belong to human traffickers. Ideally, the first nodes in the list would be more likely to be traffickers, and the nodes would get less and less likely the further down in the list you go.</p>
<p>This is the idea behind <em>single-network vertex nomination</em>. You have a group of “seed nodes” that you know have the right community membership, and then you take the rest of the nodes in your network and order them by their relationship to the seed nodes in terms of that community membership. The nomination task here isn’t just classification: it’s prioritization. You’re prioritizing how important the rest of your nodes are with respect to the seed nodes, with the most important nodes at the top.</p>
<div class="section" id="spectral-vertex-nomination">
<h4><span class="section-number">1.4.1. </span>Spectral Vertex Nomination<a class="headerlink" href="#spectral-vertex-nomination" title="Permalink to this headline">¶</a></h4>
<p>There are a few approaches to vertex nomination. You can take a likelihood-maximization approach, or a bayes-optimal approach - but what we’ll focus on is <em>Spectral Vertex Nomination</em>. The general idea is that you embed your network, and then you just order the latent positions by how close they are to the seed node latent positions. There are a few ways of doing this: you could create a <em>separate</em> set of nominees for each node, for instance. This would correspond to finding the people closest to <em>each</em> human trafficker, rather than finding a single list of nominees. You could also just get a single list of nominees: you could first take the centroid of the latent positions of your seed nodes, and then find the closest nodes to that <em>centroid</em>. There are also a few different ways of defining what it means to be “close” to seed node latent positions. The obvious way is euclidean distance, which is what you’d traditionally think of as the distance between two points, but you could also use something like the Mahalanobis distance, which is essentially Euclidean distance but with a coordinate system and a rescaling defined by the covariance in your data.</p>
<p>In any case, all forms of Spectral Vertex Nomination involve finding embeddings and then taking distances. In contrast to the other approaches, it scales well with very large networks (since you’re essentially just doing an embedding followed by a simple calculation) and doesn’t require any prior knowledge of community membership.</p>
<p>Let’s see what spectral vertex nomination looks like. Below, we see the latent positions for a network with three communities, where two of the communities are more closely linked than the third community. We do a standard adjacency spectral embedding, and we end up with a set of latent positions. Our seed nodes - the ones whose community membership we know - are marked.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>

<span class="c1"># construct network</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">]])</span>

<span class="c1"># Create a network from and SBM, then embed</span>
<span class="n">A</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># Let&#39;s say we know that the first five nodes belong to the first community.</span>
<span class="c1"># We&#39;ll say that those are our seed nodes.</span>
<span class="n">seeds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># grab a set of seed nodes</span>
<span class="n">memberships</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">memberships</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">seed_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">seeds</span><span class="p">))</span>
<span class="n">mask</span><span class="p">[</span><span class="n">seed_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">memberships</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># find the latent positions for the seed nodes</span>
<span class="n">seed_latents</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">memberships</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">memberships</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions and seeds for an SBM </span><span class="se">\n</span><span class="s2">with three communities&quot;</span><span class="p">,</span> 
                    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;#1f77b4&quot;</span><span class="p">,</span> <span class="s2">&quot;#ff0000&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">seed_latents</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">seed_latents</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;latent positions </span><span class="se">\n</span><span class="s2">for seed nodes&quot;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">45</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
              <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">52</span><span class="p">,</span> <span class="o">-.</span><span class="mi">15</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="p">});</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-vertex-nomination_5_0.png" src="_images/single-vertex-nomination_5_0.png" />
</div>
</div>
<p>Now, we’d like to order the rest of the vertices in this network by their degree of similarity to the seed nodes. Remember that we talked about two ways of doing this: we could find a separate set of nominations for each seed node, or we could find a single set of nominations for all of the seed nodes. Let’s start by finding a single set, using the centroid.</p>
<div class="section" id="finding-a-single-set-of-nominations">
<h5><span class="section-number">1.4.1.1. </span>Finding a single set of nominations<a class="headerlink" href="#finding-a-single-set-of-nominations" title="Permalink to this headline">¶</a></h5>
<p>Computing the centroid is as easy as just taking the mean value for the seed latent positions along each coordinate axis. Since our example is in 2 dimensions, we can just take our <span class="math notranslate nohighlight">\(m \times 2\)</span> matrix of seed latent positions and take the mean along the first axis to create a <span class="math notranslate nohighlight">\(1 \times 2\)</span> vector. That vector will be the centroid, and its location in Euclidean space will be right in the middle of the seeds. You can see the centroid (red star) along with the seed latent positions (red circles) below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">centroid</span> <span class="o">=</span> <span class="n">seed_latents</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">memberships</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Centroid for seed latent positions&quot;</span><span class="p">,</span> 
                    <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;#1f77b4&quot;</span><span class="p">,</span> <span class="s2">&quot;#ff0000&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">seed_latents</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">seed_latents</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> 
             <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">centroid</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">centroid</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;Centroid&quot;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">52</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
              <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">centroid</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+.</span><span class="mi">005</span><span class="p">,</span> <span class="n">centroid</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+.</span><span class="mi">05</span><span class="p">),</span> 
              <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="p">});</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-vertex-nomination_10_0.png" src="_images/single-vertex-nomination_10_0.png" />
</div>
</div>
<p>Now, all we do is order the rest of the latent positions (the blue dots in the figure above) by their distance to the centroid. The nodes corresponding to the closer latent positions will be higher up in our nomination list. Scikit-learn has a <code class="docutils literal notranslate"><span class="pre">NearestNeighbors</span></code> classifier, so we’ll just use that. Below, we fit the classifier to our latent positions matrix, then get our nominations using the <code class="docutils literal notranslate"><span class="pre">kneighbors</span></code> function. The latent positions closer to the centroid are more visible, and they get progressively less visible the further from the centroid they are.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

<span class="c1"># Find the nearest neighbors to the seeds, excluding other seeds</span>
<span class="n">neighbors</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">neighbors</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">nominations</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">centroid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nominations</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">distances</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">t</span><span class="o">*</span><span class="n">d</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">t</span><span class="o">*</span><span class="n">d</span><span class="o">**</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">alpha_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">([</span><span class="n">f</span><span class="o">*</span><span class="n">g</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Function made through trial and error to get alpha-values looking right</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;#1f77b4&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">centroid</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">centroid</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;Nominations closer to the </span><span class="se">\n</span><span class="s2">centroid are more visible&quot;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">45</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
              <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">525</span><span class="p">,</span> <span class="o">-.</span><span class="mi">28</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="p">});</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Nomination List&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-vertex-nomination_13_0.png" src="_images/single-vertex-nomination_13_0.png" />
</div>
</div>
<p>Let’s look at the network directly, and see where our nominations tend to be. Below is a network colored by nomination rank: nodes that are higher up in the nomination list are more purple, and nodes that are lower in the nomination list are more white. You can see that the higher up in the nomination list you get (more purple), the more well-connected nodes tend to be to the seed nodes.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">networkplot</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">networkplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">node_hue</span><span class="o">=</span><span class="n">nominations</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> 
            <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span> <span class="n">edge_alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;Higher-ranked nominations tend to be</span><span class="se">\n</span><span class="s2"> in the same group as the seed nodes&quot;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">45</span><span class="p">,</span> <span class="o">.</span><span class="mi">15</span><span class="p">),</span> 
              <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="p">});</span>

<span class="c1"># TODO: add colorbar</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-vertex-nomination_15_0.png" src="_images/single-vertex-nomination_15_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="finding-nominations-for-each-node">
<h4><span class="section-number">1.4.2. </span>Finding Nominations for Each Node<a class="headerlink" href="#finding-nominations-for-each-node" title="Permalink to this headline">¶</a></h4>
<p>Another approach, if we don’t want to combine the information from all of our seed nodes, is to create a different nomination list for each node. This would correspond to finding multiple sets of people close to <em>each</em> human trafficker, rather than finding a single set of people for the <em>group</em> of human traffickers. Graspologic does this natively; the only real difference between the two approaches is that we take the nearest neighbors of the centroid for the first method rather than for each individual. Because of this, we’ll just use graspologic directly, rather than showcasing the algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.nominate</span> <span class="kn">import</span> <span class="n">SpectralVertexNomination</span>

<span class="c1"># Choose the number of nominations we want for each seed node</span>
<span class="n">svn</span> <span class="o">=</span> <span class="n">SpectralVertexNomination</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">svn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># get nominations and distances for each seed index</span>
<span class="n">nominations</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="n">svn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">seed_idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Below you can see the nominations for each node. The first row containes the indices for each seed node, and each subsequent row contains the nearest neighbors for those seed nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nominations</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0,  1,  2,  3,  4],
       [30, 80, 57, 25, 71],
       [96,  7, 39, 27, 23],
       [43, 29,  5, 89, 14],
       [93, 89, 96, 88, 20]])
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;lightgreen&#39;</span><span class="p">,</span> <span class="s1">&#39;gold&#39;</span><span class="p">,</span> <span class="s1">&#39;cyan&#39;</span><span class="p">,</span> <span class="s1">&#39;pink&#39;</span><span class="p">]</span>
<span class="n">seed_color</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;firebrick&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;tan&#39;</span><span class="p">,</span> <span class="s1">&#39;darkblue&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">]</span>


<span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">seed_latents</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">seed_latents</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> 
             <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seed_group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nominations</span><span class="o">.</span><span class="n">T</span><span class="p">):</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">seed_group</span><span class="p">]</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">neighbors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">seed_color</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Nominees for each seed node&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/single-vertex-nomination_21_0.png" src="_images/single-vertex-nomination_21_0.png" />
</div>
</div>
<p>These approaches are each useful in different situations.</p>
</div>
<div class="section" id="references">
<h4><span class="section-number">1.4.3. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h4>
<p>theory papers:</p>
<ul class="simple">
<li><p>theory of consistency and Bayes optimality: On consistent vertex nomination schemes</p></li>
<li><p>dealing with corrupted networks (e.g., not one-to-one correspondence): Vertex Nomination, Consistent Estimation, and Adversarial Modification</p></li>
</ul>
<p>applications papers:</p>
<ul class="simple">
<li><p>social networks: Vertex nomination via local neighborhood matching</p></li>
<li><p>data associated with human trafficking: Vertex nomination schemes for membership prediction</p></li>
</ul>
</div>
</div>
<span id="document-applications/ch8/out-of-sample"></span><div class="section" id="out-of-sample-embedding">
<h3><span class="section-number">1.5. </span>Out-of-sample Embedding<a class="headerlink" href="#out-of-sample-embedding" title="Permalink to this headline">¶</a></h3>
<p>Imagine you have a citation network of scholars publishing papers. The nodes are the scholars, and an edge exists in a given pair of scholars if they’re published a paper together.</p>
<p>You’ve already found a representation using ASE or LSE and you have a set of latent positions, which you then clustered to figure out who came from which university. It took a long time for you to get this representation - there are a lot of people doing research out there!</p>
<p>Now, suppose a new graduate student publishes a paper. Your network gets bigger by a single node, and you’d like to find this person’s latent position (thus adding them to the clustering system). To do that, however, you’d have to get an entirely new representation for the network: your latent position matrix is <span class="math notranslate nohighlight">\(n \times d\)</span>, and it would need to become <span class="math notranslate nohighlight">\((n+1) \times d\)</span>. Re-embedding the entire network with the new node added seems like it should be unecessary - after all, you already know the latent positions for every other node.</p>
<p>This section is all about this problem: how to find the representation for new nodes without the computationally expensive task of re-embedding an entire network. As it turns out, there has been some work done, and there is a solution that can get you pretty close the latent position for the new node that you would have had. For more details and formaility, see the 2013 paper “Out-of-sample extension for latent position graphs”, by Tang et al (although, as with most science, the theory in this paper was built on top of other work from related fields).</p>
<p>Let’s make a network from an SBM, and an additional node that should belong to the first community. Then, we’ll embed the network and explore how to find the latent position for the additional node.</p>
<div class="section" id="a-network-and-an-out-of-sample-node">
<h4><span class="section-number">1.5.1. </span>A Network and an Out-of-Sample Node<a class="headerlink" href="#a-network-and-an-out-of-sample-node" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">remove_vertices</span>

<span class="c1"># Generate parameters</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>

<span class="c1"># Generate both an original network along with community memberships, </span>
<span class="c1"># and an out-of-sample node with the same SBM call</span>
<span class="n">network</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Grab out-of-sample node</span>
<span class="n">oos_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">oos_label</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">oos_idx</span><span class="p">)</span>

<span class="c1"># create our original network</span>
<span class="n">A</span><span class="p">,</span> <span class="n">a_1</span> <span class="o">=</span> <span class="n">remove_vertices</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">oos_idx</span><span class="p">,</span> <span class="n">return_removed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>What we have now is a network and an additional node. You can see the adjacency matrix for the network below, along with the adjacency vector for the additional node (Here, an “adjacency vector”  is a vector with a 1 in every position that the out-of-sample node has an edge with an in-sample node). The heatmap on the left is a network with two communities, with 100 nodes in each community. The vector on the right is purple on row <span class="math notranslate nohighlight">\(i\)</span> if the <span class="math notranslate nohighlight">\(i^{th}\)</span> in-sample node is connected to the out-of-sample node.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">from</span> <span class="nn">graphbook_code.plotting</span> <span class="kn">import</span> <span class="n">cmaps</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># heatmap</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># adjacency vector</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">85</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">a_1</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelright</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;in-sample node index&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s2">&quot;right&quot;</span><span class="p">)</span>

<span class="c1"># title</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Adjacency matrix (left) and vector for additional </span><span class="se">\n</span><span class="s2">node (right)&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">x</span><span class="o">=.</span><span class="mi">19</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/out-of-sample_5_0.png" src="_images/out-of-sample_5_0.png" />
</div>
</div>
<p>After embedding with ASE, we have an embedding for the original network. The rows of this embedding contain the latent position for each original node. We’ll call the embedding <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>

<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ase</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">plot_latents</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions for our original network (X)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/out-of-sample_8_0.png" src="_images/out-of-sample_8_0.png" />
</div>
</div>
</div>
<div class="section" id="the-latent-position-matrix-can-be-used-to-estimate-probability-vectors">
<h4><span class="section-number">1.5.2. </span>The Latent Position Matrix Can be used to Estimate Probability Vectors<a class="headerlink" href="#the-latent-position-matrix-can-be-used-to-estimate-probability-vectors" title="Permalink to this headline">¶</a></h4>
<p>Everything up until now has just been pretty standard stuff. We still haven’t done anything with our new node - all we have is a big vector that tells us which other nodes it’s connected to, and our standard matrix of latent positions. However, it’s time for a bit more exploration into the nature of the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, and what happens when you view it as a linear transformation. This will get us closer to understanding the out-of-sample embedding.</p>
<p>Remember from the section on latent positions that <span class="math notranslate nohighlight">\(X\)</span> can be used to estimate the block probability matrix. When you use ASE on a single network to make <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(XX^\top\)</span> estimates <span class="math notranslate nohighlight">\(P\)</span>: meaning, <span class="math notranslate nohighlight">\((XX^\top)_{ij}\)</span>, the element on the <span class="math notranslate nohighlight">\(i^{(th)}\)</span> row and <span class="math notranslate nohighlight">\(j^{(th)}\)</span> column of <span class="math notranslate nohighlight">\(XX^\top\)</span>, will estimate the probability that node <span class="math notranslate nohighlight">\(i\)</span> has an edge with node <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>Let’s take a single latent position vector - call it <span class="math notranslate nohighlight">\(v_i\)</span> (this will be the <span class="math notranslate nohighlight">\(i_{th}\)</span> row of the latent position matrix). What’ll <span class="math notranslate nohighlight">\(X v_i\)</span> look like? Well, it’ll look the same as grabbing the <span class="math notranslate nohighlight">\(i_{th}\)</span> column of <span class="math notranslate nohighlight">\(XX^\top\)</span>. Meaning, <span class="math notranslate nohighlight">\(X v_i\)</span> will be a single vector whose <span class="math notranslate nohighlight">\(j^{(th)}\)</span> element estimates the probability that node <span class="math notranslate nohighlight">\(i\)</span> will connect to node <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>You can see this in action below. We took the latent position corresponding to the first node out of the latent position matrix (and called it <span class="math notranslate nohighlight">\(v_1\)</span>), and then multiplied it by the latent position matrix itself. What emerged is what you see below: a vector that shows the estimated probability that node 0 has an edge with each other node in the network. The true probabilities for the first half of nodes (the ones in the same community) should be .8, and the true probabilities for the second half of nodes in the other community should be .2. The average values were .775 and .149 - so, pretty close!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">v_1</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">v_est_proba</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">v_1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">v_est_proba</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;average value: </span><span class="si">{</span><span class="n">v_est_proba</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;average value: </span><span class="si">{</span><span class="n">v_est_proba</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node index&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Estimated probability</span><span class="se">\n</span><span class="s2"> vector&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot; for first node $X v_1$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/out-of-sample_12_0.png" src="_images/out-of-sample_12_0.png" />
</div>
</div>
</div>
<div class="section" id="going-in-the-other-direction">
<h4><span class="section-number">1.5.3. </span>Going in the Other Direction<a class="headerlink" href="#going-in-the-other-direction" title="Permalink to this headline">¶</a></h4>
<p>Remember that our goal is to take the adjacency vector for a new node and use it to estimate that node’s latent position without re-embedding the whole network. So far, we’ve essentially figured out how to use the node’s latent position to get an estimated probability vector.</p>
<p>Let’s think about the term “estimated probability vector” for a second. This should be a vector associated to node <span class="math notranslate nohighlight">\(i\)</span> with <span class="math notranslate nohighlight">\(n\)</span> elements, where the <span class="math notranslate nohighlight">\(j^{(th)}\)</span> element of the vector contains the probability that node <span class="math notranslate nohighlight">\(i\)</span> will connect to node <span class="math notranslate nohighlight">\(j\)</span>. The thing we’re starting with for the out-of-sample node, however, is an adjacency vector full of 0’s and 1’s - 0 if there isn’t an edge, 1 if there is an edge.</p>
<p>If you think about it, however, you can think of this adjacency vector as kind of an estimate for edge probabilities. Say you sample a node’s adjacency vector from an RDPG, then you sample again, and again. Averaging all of your samples will get you closer and closer to the actual edge connection probabilities. So you can think of a single adjacency vector as an estimate for the edge probability vector!</p>
<p>The point here is that if you can start with a latent position and then estimate the edge probabilities, it’s somewhat equivalent (albeit going in the other direction) to start with an out-of-sample adjacency vector and estimate a node’s the latent position.</p>
<p>Let’s call the estimated probability vector <span class="math notranslate nohighlight">\(\hat{a_i}\)</span>. We know that <span class="math notranslate nohighlight">\(\hat{a_i} = \hat{X} \hat{v_i}\)</span>: you multiply the latent position matrix by the <span class="math notranslate nohighlight">\(i_{th}\)</span> latent position to estimate the probability vector (remember that the ^ hats above letters means we’re getting an estimate for something, rather than getting the thing itself). How do we isolate the latent position <span class="math notranslate nohighlight">\(\hat{v_i}\)</span>?</p>
<p>Well, if <span class="math notranslate nohighlight">\(X\)</span> were invertible, we could do <span class="math notranslate nohighlight">\(\hat{X}^{-1} \hat{a_i} = \hat{v_i}\)</span>: just invert both sides of the equation to get <span class="math notranslate nohighlight">\(v_i\)</span> by itself. Unfortunately, in practice, <span class="math notranslate nohighlight">\(X\)</span> will almost never be invertible. We’ll have to do the next-best thing, which is to use the <em>pseudoinverse</em>.</p>
<p>We’ll take a brief break in the coming section to talk about the pseudoinverse for a bit, then we’ll come back and use it to estimate the out-of-sample latent position.</p>
</div>
<div class="section" id="the-pseudoinverse-gives-us-our-best-guess-for-inverting-a-matrix">
<h4><span class="section-number">1.5.4. </span>The Pseudoinverse Gives Us our Best Guess For Inverting a Matrix<a class="headerlink" href="#the-pseudoinverse-gives-us-our-best-guess-for-inverting-a-matrix" title="Permalink to this headline">¶</a></h4>
<p>The Moore-Penrose Pseudoinverse is useful to know in general, since it pops up a lot in a lot of different places. Say you have a matrix which isn’t invertible. Call it <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>The pseudoinverse <span class="math notranslate nohighlight">\(T^+\)</span> is the closest approximation you can get to the inverse <span class="math notranslate nohighlight">\(T^{-1}\)</span>. This is best understood visually. Let’s take <span class="math notranslate nohighlight">\(T\)</span> to be a matrix which projects points on the x-y coordinate axis down to the x-axis, then flips them to their negative on the number line. The matrix would look like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    T &amp;=
    \begin{bmatrix}
    -1 &amp; 0 \\
    0 &amp; 0  \\
    \end{bmatrix}
\end{align*}\]</div>
<p>Some information is inherently lost here. Because the second column is all zeroes, any information in the y-axis can’t be recovered. For instance, say we have some vectors with different x-axis and y-axis coordinates:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    v_1 &amp;= \begin{bmatrix} 1 &amp; 1 \end{bmatrix}^\top \\
    v_2 &amp;= \begin{bmatrix} 2 &amp; 2 \end{bmatrix}^\top
\end{align*}\]</div>
<p>When we use <span class="math notranslate nohighlight">\(T\)</span> as a linear transformation to act on <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span>, the y-axis coordinates both collapse to the same thing (0, in this case). Information in the x-axis, however, is preserved.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                 <span class="c1"># v 1.19.2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>    <span class="c1"># v 3.3.2</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_cartesian</span>


<span class="c1"># make axis</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">draw_cartesian</span><span class="p">()</span>

<span class="c1"># Enter x and y coordinates of points and colors</span>
<span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">]</span>

<span class="c1"># Plot points</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>

<span class="c1"># Draw lines connecting points to axes</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>


<span class="c1"># Draw arrows</span>
<span class="n">arrow_fmt</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">clip_on</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;&gt;&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">get_yaxis_transform</span><span class="p">(),</span> <span class="o">**</span><span class="n">arrow_fmt</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xaxis_transform</span><span class="p">(),</span> <span class="o">**</span><span class="n">arrow_fmt</span><span class="p">)</span>

<span class="n">arrow_fmt</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">clip_on</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;&lt;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">arrow_fmt</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;&lt;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">arrow_fmt</span><span class="p">)</span>

<span class="c1"># Draw text</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$v_1$ (1, 1)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.9</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$v_2$ (2, 2)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=-</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">y</span><span class="o">=-.</span><span class="mi">3</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$T v_1$ (-1, 0)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=-</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">y</span><span class="o">=-.</span><span class="mi">6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$T v_2$ (-2, 0)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">));</span>

<span class="c1"># input/output</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">2.3</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;input vectors&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=-</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">y</span><span class="o">=-</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;output vectors&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/out-of-sample_17_0.png" src="_images/out-of-sample_17_0.png" />
</div>
</div>
<p>Our goal is to reverse <span class="math notranslate nohighlight">\(T\)</span> and bring <span class="math notranslate nohighlight">\(Tv_1\)</span> and <span class="math notranslate nohighlight">\(Tv_2\)</span> back to <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span>. Unfortunately, since both <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span> get squished onto zero in the y-axis position after getting passed through <span class="math notranslate nohighlight">\(T\)</span>, we’ve lost all information about what was happening on the y-axis – that’s a lost cause. So it’s impossible to get perfectly back to <span class="math notranslate nohighlight">\(v_1\)</span> or <span class="math notranslate nohighlight">\(v_2\)</span>.</p>
<p>If you restrict your attention to the x-axis, however, you’ll see that <span class="math notranslate nohighlight">\(Tv_1\)</span> and <span class="math notranslate nohighlight">\(Tv_2\)</span> landed in different places (<span class="math notranslate nohighlight">\(v_1\)</span> went to -1, and <span class="math notranslate nohighlight">\(v_2\)</span> went to -2). You can use this information about the x-axis location of <span class="math notranslate nohighlight">\(Tv_1\)</span> and <span class="math notranslate nohighlight">\(Tv_2\)</span> to re-orient the x-axis values back to where they were prior to the vectors getting passed through X, even if it’s impossible to figure out where the y-values were.</p>
<p>That’s what the pseudoinverse does: it reverses what it can, and accepts that some information has vanished.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                 <span class="c1"># v 1.19.2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>    <span class="c1"># v 3.3.2</span>

<span class="c1"># make axis</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">draw_cartesian</span><span class="p">()</span>

<span class="c1"># Enter x and y coordinates of points and colors</span>
<span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">xs_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">ys_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">]</span>


<span class="c1"># Plot points</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs_out</span><span class="p">,</span> <span class="n">ys_out</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>

<span class="c1"># Draw lines connecting points to axes</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">arrow_fmt</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">clip_on</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Draw text</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">9</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$v_1$ (1, 1)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">2.2</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.9</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$v_2$ (2, 2)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">y</span><span class="o">=-.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$T^+ (X v_1$)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">1.8</span><span class="p">,</span> <span class="n">y</span><span class="o">=-.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;$T^+ (X v_2$)&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/out-of-sample_19_0.png" src="_images/out-of-sample_19_0.png" />
</div>
</div>
</div>
<div class="section" id="using-the-pseudoinverse-to-estimate-out-of-sample-latent-positions">
<h4><span class="section-number">1.5.5. </span>Using the Pseudoinverse to Estimate out-of-sample Latent Positions<a class="headerlink" href="#using-the-pseudoinverse-to-estimate-out-of-sample-latent-positions" title="Permalink to this headline">¶</a></h4>
<p>Let’s get back to estimating our out-of-sample latent position.</p>
<p>Remember that we had a nonsquare latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. Like we learned before, we can get the probability vector <span class="math notranslate nohighlight">\(a_i\)</span> (the vector with its probability of connecting with node <span class="math notranslate nohighlight">\(j\)</span> in the <span class="math notranslate nohighlight">\(j_{th}\)</span> position) for a node by passing its latent position (<span class="math notranslate nohighlight">\(v_i\)</span>) through the latent position matrix.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
a_i = X v_i
\end{align*}\]</div>
<p>We can think of <span class="math notranslate nohighlight">\(X\)</span> as a matrix the same way we thought of <span class="math notranslate nohighlight">\(T\)</span>: it’s a linear transformation that eats a vector, and doesn’t necessarily preserve all the information about that vector when it outputs something (In this case, since <span class="math notranslate nohighlight">\(X\)</span> brings lower-dimensional latent positions to higher-dimensional probability vectors, what’s happening is more of a restriction on which high-dimensional vectors you can access than a loss of information, but that’s not particularly important).</p>
<p>The pseudoinverse, <span class="math notranslate nohighlight">\(X^+\)</span>, is the best we can do to bring a higher-dimensional adjacency vector to a lower-dimensional latent position. Since the adjacency vector just approximates the probability vector, we can call it <span class="math notranslate nohighlight">\(\hat{a_i}\)</span>. In practice, the best we can do generally turns out to be a pretty good guess, and so we can get a decent estimation of the latent position <span class="math notranslate nohighlight">\(v_i\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
X^+ \hat{a_i} \approx X^+ (X v_i) \approx v_i
\end{align*}\]</div>
<p>Let’s see it in action. Remember that we already grabbed our out-of-sample latent position and called it <code class="docutils literal notranslate"><span class="pre">a_1</span></code>. We use numpy’s pseudoinverse function to generate the pseudoinverse of the latent position matrix. Finally, we use it to get <code class="docutils literal notranslate"><span class="pre">a_1</span></code>’s estimated latent position, and call it <code class="docutils literal notranslate"><span class="pre">v_1</span></code>. You can see the location of this estimate in Euclidean space below: it falls squarely into the first community, which is where it should be.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">pinv</span>

<span class="c1"># Make the pseudoinverse of the latent position matrix</span>
<span class="n">X_pinverse</span> <span class="o">=</span> <span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Get its estimated latent position</span>
<span class="n">v_1</span> <span class="o">=</span> <span class="n">X_pinverse</span> <span class="o">@</span> <span class="n">a_1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">v_1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.61596041, 0.61217755])
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.gridspec</span> <span class="k">as</span> <span class="nn">gridspec</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># setup</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_gridspec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># adjacency vector</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">a_1</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;average value: </span><span class="si">{</span><span class="n">a_1</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;average value: </span><span class="si">{</span><span class="n">a_1</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Adjacency vector for&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot; the first node $a_1$&quot;</span><span class="p">);</span>

<span class="c1"># latent position plot</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions with out-of-sample estimate&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">v_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">v_1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Estimated latent position for&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot; the first adjacency vector: $X^+ a$&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">v_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+.</span><span class="mi">002</span><span class="p">,</span> <span class="n">v_1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+.</span><span class="mi">008</span><span class="p">),</span> 
            <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">v_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-.</span><span class="mi">02</span><span class="p">,</span> <span class="n">v_1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-.</span><span class="mi">2</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;k&quot;</span><span class="p">})</span>
<span class="n">sns</span><span class="o">.</span><span class="n">move_legend</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="s2">&quot;center right&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/out-of-sample_24_0.png" src="_images/out-of-sample_24_0.png" />
</div>
</div>
</div>
<div class="section" id="using-graspologic">
<h4><span class="section-number">1.5.6. </span>Using Graspologic<a class="headerlink" href="#using-graspologic" title="Permalink to this headline">¶</a></h4>
<p>Of course, you don’t have to do all of this manually. Below we generate an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> from an SBM, as well as the adjacency vector for an out-of-sample node <span class="math notranslate nohighlight">\(a_1\)</span>. Once we fit an instance of the ASE class, the latent position for any new nodes can be predicted by simply calling <code class="docutils literal notranslate"><span class="pre">ase.transform</span></code> on the new adjacency vectors.</p>
<p>You can do the same thing with multiple out-of-sample nodes if you want by stacking their adjacency vectors on top of each other in a numpy array, then transforming the whole stack.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span> <span class="k">as</span> <span class="n">ASE</span>

<span class="c1"># Generate parameters</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>

<span class="c1"># Generate a network along with community memberships</span>
<span class="n">network</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">return_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Grab out-of-sample vertex</span>
<span class="n">oos_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">oos_label</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">oos_idx</span><span class="p">)</span>
<span class="n">A</span><span class="p">,</span> <span class="n">a_1</span> <span class="o">=</span> <span class="n">remove_vertices</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">oos_idx</span><span class="p">,</span> <span class="n">return_removed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Make an ASE model</span>
<span class="n">ase</span> <span class="o">=</span> <span class="n">ASE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># Predict out-of-sample latent positions by transforming</span>
<span class="n">v_1</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">a_1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="c1"># latent position plot</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent positions with out-of-sample estimate&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">v_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">v_1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Estimated latent position for&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot; the first adjacency vector: $X^+ a_0$&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">v_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+.</span><span class="mi">002</span><span class="p">,</span> <span class="n">v_1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+.</span><span class="mi">008</span><span class="p">),</span> 
            <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">v_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-.</span><span class="mi">1</span><span class="p">,</span> <span class="n">v_1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-.</span><span class="mi">3</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;k&quot;</span><span class="p">})</span>
<span class="n">sns</span><span class="o">.</span><span class="n">move_legend</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="s2">&quot;center right&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/out-of-sample_28_0.png" src="_images/out-of-sample_28_0.png" />
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-applications/ch9/ch9"></span><div class="section" id="applications-for-two-networks">
<h2><span class="section-number">2. </span>Applications for Two Networks<a class="headerlink" href="#applications-for-two-networks" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-applications/ch9/two-sample-hypothesis"></span><div class="section" id="two-sample-hypothesis-testing">
<h3><span class="section-number">2.1. </span>Two-Sample Hypothesis Testing<a class="headerlink" href="#two-sample-hypothesis-testing" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-applications/ch9/graph-matching-vertex"></span><div class="section" id="graph-matching">
<h3><span class="section-number">2.2. </span>Graph Matching<a class="headerlink" href="#graph-matching" title="Permalink to this headline">¶</a></h3>
<p>You work at Facebook and Twitter, but there’s been a terrible incident. All twitter users’ names and handles have been somehow been deleted! Your bosses are furious and have tasked you with somehow recovering the lost information. How might you go about doing this? Luckily, you’ve been working hard and have somehow earned yourself this dual Facebook/Twitter gig, so you have a great resource at your disposal: the Facebook social network. You know all facebook users and who they are friends with, and since you’ve only lost the twitter usernames, you can still figure out which ghost-users follow each other (make this sentence more specific, split it up into more sentences). You decide to use the Facebook network connectivity data to re-label the twitter social network. Alternatively, you can say the we are “aligning” Twitter based on Facebook.</p>
<p>In the example above, the social networks are represented with each user as a node, and an edge exists if two users are friends. We’ll define the facebook and twitter networks as <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(T\)</span> respectively, with associated adjacency matrices <span class="math notranslate nohighlight">\(A_F\)</span> and <span class="math notranslate nohighlight">\(A_T\)</span>. This method is known as <span class="math notranslate nohighlight">\(\textit{Graph Matching}\)</span>, because we are matching the node labels of one graph to another. This can also be thought of as a mapping; that is, based on the neighborhood structure of a node in the <span class="math notranslate nohighlight">\(F\)</span> network, we assign the same label to the node in <span class="math notranslate nohighlight">\(T\)</span> with the most similar structure. In other words, one of our twitter users will be assigned the user name of the Facebook user with the most followers in common. This is then done for the whole network, such that overall the structure is best preserved.</p>
<p>As you can imagine, there are a very large number of these possible mappings. In fact, for network pairs with <span class="math notranslate nohighlight">\(n\)</span> nodes, there are <span class="math notranslate nohighlight">\(n!\)</span> possible mappings. So how would we go about solving this(more specific) mathematically? First, we need a metric that tells us how similar two networks are to each other. For graph matching, this similarity metric is defined as <span class="math notranslate nohighlight">\(f(A, B) = ||A - B||_F^2\)</span> for unweighted adjacency matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times n}\)</span>. In other words, <span class="math notranslate nohighlight">\(f(A, B)\)</span> is the sum of the squared elementwise differences between <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. To understand this functionally, consider the best possible case where <span class="math notranslate nohighlight">\(A=B\)</span>, that is, the networks are identical. The difference will be a matrix of all zeros, and taking the squared norm will then yield <span class="math notranslate nohighlight">\(f(A,B) = 0\)</span>. If we remove one edge from <span class="math notranslate nohighlight">\(A\)</span>, then <span class="math notranslate nohighlight">\(f(A,B) = 1\)</span>. If we consider the worst possible case (every edge in <span class="math notranslate nohighlight">\(A\)</span> does not exist in <span class="math notranslate nohighlight">\(B\)</span>, and vice versa), then <span class="math notranslate nohighlight">\(f(A,B) = n^2\)</span>. This metric effectively counts the number of adjacnecy disagreements between <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. Thus, we want to find the mapping where <span class="math notranslate nohighlight">\(f(A, B)\)</span> is as small as possible.</p>
<div class="section" id="graph-matching-small-networks">
<h4><span class="section-number">2.2.1. </span>Graph Matching Small Networks<a class="headerlink" href="#graph-matching-small-networks" title="Permalink to this headline">¶</a></h4>
<p>Say we have the network pairs below, <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(F\)</span>. They are four nodes each, <span class="math notranslate nohighlight">\(\{1, 2, 3, 4\}\)</span> for <span class="math notranslate nohighlight">\(T\)</span>, and <span class="math notranslate nohighlight">\(\{a, b, c, d\}\)</span> for <span class="math notranslate nohighlight">\(F\)</span>. The two networks are clearly equal to each other.</p>
<p><img alt="gm_11" src="_images/gm_1.png" /></p>
<p>However, the spatial layout of a network’s nodes is arbirary, and in reality it can often be much harder to tell whether two networks are the same. For instance, we can swap the spatial location of nodes <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(d\)</span> in network <span class="math notranslate nohighlight">\(F\)</span>, as shown below. Even with such a small network, it’s hard to tell whether the networks are the same. Nonetheless, by looking at the adjacency matrices, we see that the networks are in fact the same, with <span class="math notranslate nohighlight">\(f(A_T, A_F) = 0\)</span></p>
<p><img alt="gm_22" src="_images/gm_2.png" /></p>
<div class="math">
\[
A_T = 
\begin{array}{cc} &
\begin{array}{cccc} 0 & 1 & 2 & 3 \end{array}
\\
\begin{array}{cccc}
0 \\
1 \\
2 \\
3 \end{array}
&
\left(
\begin{array}{cccc}
0 & 1 & 1 & 0\\
1 & 0 & 0 & 1\\
1 & 0 & 0 & 1\\
0 & 1 & 1 & 0\end{array}
\right)\end{array}
\quad \quad
A_F = 
\begin{array}{cc} &
\begin{array}{cccc} a & b & c & d \end{array}
\\
\begin{array}{ccc}
a \\
b \\
c \\
d \end{array}
&
\left(
\begin{array}{ccc}
0 & 1 & 1 & 0\\
1 & 0 & 0 & 1\\
1 & 0 & 0 & 1\\
0 & 1 & 1 & 0\end{array}
\right)\end{array}
\]
    </div>
<p>Next, we swap the actual the node labels of nodes 2 and 3 in network <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p><img alt="gm_33" src="_images/gm_3.png" /></p>
<div class="math">
\[
A_T = 
\begin{array}{cc} &
\begin{array}{cccc} 0 & 1 & 2 & 3 \end{array}
\\
\begin{array}{cccc}
0 \\
1 \\
2 \\
3 \end{array}
&
\left(
\begin{array}{cccc}
0 & 1 & 1 & 0\\
1 & 0 & 0 & 1\\
1 & 0 & 0 & 1\\
0 & 1 & 1 & 0\end{array}
\right)\end{array}
\quad \quad 
A_F = 
\begin{array}{cc} &
\begin{array}{cccc} a & b & c & d \end{array}
\\
\begin{array}{ccc}
a \\
b \\
c \\
d \end{array}
&
\left(
\begin{array}{ccc}
0 & 1 & 0 & 1\\
1 & 0 & 1 & 0\\
0 & 1 & 0 & 1\\
1 & 0 & 1 & 0\end{array}
\right)\end{array}
\]
    </div>
<p>As we see the networks are no longer the same, with <span class="math notranslate nohighlight">\(f(A_T,A_F) = 8\)</span>. This might seem a bit high, but note that due to the graph being undirected, adjecency disagreements are effectively counted twice, since all edges (in and out) appear twice in the adjacency matrix. After showing how networks with a low number of edge disagreements are considered to be better matches, we will now demonstrate how to manipulate our networks and adjacency matrices such that we can find alignments that match well.</p>
</div>
<div class="section" id="permutation-matrices">
<h4><span class="section-number">2.2.2. </span>Permutation Matrices<a class="headerlink" href="#permutation-matrices" title="Permalink to this headline">¶</a></h4>
<p>Mappings are represented via <span class="math notranslate nohighlight">\(\textit{Permutation Matrices}\)</span> when solving the graph matching problem. A permutation matrix is a matrix of all ones and zeros, where each row and column adds up to one. In other words, each row and column has exactly one entry equal to one, with the rest being zeros.</p>
<div class="section" id="pb-moves-the-rows-bp-t-moves-the-columns">
<h5><span class="section-number">2.2.2.1. </span><span class="math notranslate nohighlight">\(PB\)</span> moves the rows, <span class="math notranslate nohighlight">\(BP^T\)</span> moves the columns<a class="headerlink" href="#pb-moves-the-rows-bp-t-moves-the-columns" title="Permalink to this headline">¶</a></h5>
<p>Permutation matrices are commonly used as a method to move around the rows and columns of a square matrix. Consider the following example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;Original Matrix $B$&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="nd">@B</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;Row Permutation $PB$:&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">B</span><span class="nd">@P</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;Row Permutation $BP^T$:&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;Row Permutation $BP^T$:&#39;}&gt;
</pre></div>
</div>
<img alt="_images/graph-matching-vertex_7_1.png" src="_images/graph-matching-vertex_7_1.png" />
</div>
</div>
<p>The permutation matrix represents the following mapping:<br />
<span class="math notranslate nohighlight">\(0 \rightarrow 1\)</span><br />
<span class="math notranslate nohighlight">\(1 \rightarrow 0\)</span><br />
<span class="math notranslate nohighlight">\(2 \rightarrow 2\)</span><br />
<span class="math notranslate nohighlight">\(3 \rightarrow 3\)</span><br />
The matrix multiplication <span class="math notranslate nohighlight">\(PB\)</span> moves the rows based on the mapping, and <span class="math notranslate nohighlight">\(BP^T\)</span> moves the columns based on the mapping. In other words, in this case <span class="math notranslate nohighlight">\(PB\)</span> swaps rows 0 and 1 of <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(BP^T\)</span> swaps columns 0 and 1 of <span class="math notranslate nohighlight">\(B\)</span>. Therefore by combined these two operations <span class="math notranslate nohighlight">\(PBP^T\)</span>, we can move both the rows and columns based on a single bijection.</p>
</div>
<div class="section" id="permutation-matrices-to-match-graphs">
<h5><span class="section-number">2.2.2.2. </span>Permutation Matrices to Match Graphs<a class="headerlink" href="#permutation-matrices-to-match-graphs" title="Permalink to this headline">¶</a></h5>
<p>Next, we again consider the previous simple network pair example of swapping the node labels of 2 and 3 in network H:
<img alt="gm_3" src="_images/gm_3.png" /></p>
<div class="math">
\[
A_T = 
\begin{array}{cc} &
\begin{array}{cccc} 0 & 1 & 2 & 3 \end{array}
\\
\begin{array}{cccc}
0 \\
1 \\
2 \\
3 \end{array}
&
\left(
\begin{array}{cccc}
0 & 1 & 1 & 0\\
1 & 0 & 0 & 1\\
1 & 0 & 0 & 1\\
0 & 1 & 1 & 0\end{array}
\right)\end{array}
\quad \quad 
A_F = 
\begin{array}{cc} &
\begin{array}{cccc} a & b & c & d \end{array}
\\
\begin{array}{ccc}
a \\
b \\
c \\
d \end{array}
&
\left(
\begin{array}{ccc}
0 & 1 & 0 & 1\\
1 & 0 & 1 & 0\\
0 & 1 & 0 & 1\\
1 & 0 & 1 & 0\end{array}
\right)\end{array}
\]
    </div>
<p>This swap is represented by the following bijection to recover the node correspondence between <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(H\)</span><br />
<span class="math notranslate nohighlight">\(0 \rightarrow 0\)</span><br />
<span class="math notranslate nohighlight">\(1 \rightarrow 1\)</span><br />
<span class="math notranslate nohighlight">\(2 \rightarrow 3\)</span><br />
<span class="math notranslate nohighlight">\(3 \rightarrow 2\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$A_T$&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$A_F$&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="nd">@B@P</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$A_F$ with row and column permutation&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;$A_F$ with row and column permutation&#39;}&gt;
</pre></div>
</div>
<img alt="_images/graph-matching-vertex_10_1.png" src="_images/graph-matching-vertex_10_1.png" />
</div>
</div>
<p>As shown in the code block above, using the permutation matrix we are able to recover the correspondence between <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(F\)</span>.<br />
Thus, the graph matching formulation for any two adjacency matrices <span class="math notranslate nohighlight">\(A, B\)</span>, seeks to minimize <span class="math notranslate nohighlight">\(|| A - PBP^\intercal||_F^2\)</span> such that <span class="math notranslate nohighlight">\(P\)</span> is a permuation matrix. This means that you shuffle the rows and columns of <span class="math notranslate nohighlight">\(B\)</span>, such that it is as close as possible to <span class="math notranslate nohighlight">\(A\)</span>. In mathematics, the process of minimizing (or maximizing) a function based on some constraint is known as optimization.</p>
</div>
</div>
<div class="section" id="finding-an-good-permutation-with-gradient-descent">
<h4><span class="section-number">2.2.3. </span>Finding an Good Permutation with Gradient Descent<a class="headerlink" href="#finding-an-good-permutation-with-gradient-descent" title="Permalink to this headline">¶</a></h4>
<p>The algorithm used for solving graph matching is a variation of gradient descent.  The specifics of the algorithm are beyond the scope of this book, but for now you can simply imagine it as gradient descent. A gradient can be thought of as a vector valued slope; it is simply the slope of a function in all of it’s dimensions, at a single point in space. Gradient Descent is a very common optimization method using to find optimal solutions for a wide range of problems.</p>
<p>A simple way to think of the method is gravity.  Consider an inspector who might use a golf ball to find the lowest point when installing a drain. The ball rolls down hill until it comes to a stop; once stopped, we know we’ve found the lowest point. Gradient descent works in a similar way, taking steps in the direction of the local gradient with respect to some parameter. Once the gradient is zero, the minimum has been found.</p>
<p>The main steps of a gradient descent method are choosing a suitable initial position (can be chosen randomly), then gradually improving the cost function one step at a time, until the function is changing by a very small amount, converging to a minimum. The main issue with gradient descent is that it does not guarantee that you will find a global minimum, only that you will find the local minimum of your initial position.</p>
<p><img alt="grad_desc" src="_images/grad_desc.png" /></p>
<p>The image above is a simplification in two dimensions; the network functions we optimize over are n dimensional when matching networks with n nodes, making the problem incredibly difficult to solve. For this reason (among others outside of the scope), the state-of-the-art graph matching algorithm is an approximation algorithm.</p>
</div>
<div class="section" id="graph-matching-with-graspologic">
<h4><span class="section-number">2.2.4. </span>Graph Matching with graspologic<a class="headerlink" href="#graph-matching-with-graspologic" title="Permalink to this headline">¶</a></h4>
<p>For the example below, we will match two networks with a known to be have a node bijection that preserves a common network structure. To do this, we simulate a single Erdos-Reyni network, <span class="math notranslate nohighlight">\(A\)</span>, with six nodes and edge probability of 30. Then, we generate <span class="math notranslate nohighlight">\(B\)</span> by randomly permuting the node labels of <span class="math notranslate nohighlight">\(A\)</span>. Thus, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are said to be <span class="math notranslate nohighlight">\(\textit{isomorphic}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># np.random.seed(1)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
<span class="n">node_shuffle_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">node_shuffle_input</span><span class="p">,</span> <span class="n">node_shuffle_input</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of adjecnecy disagreements: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">B</span><span class="p">)))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;A [ER-NP(4, 0.3) Simulation]&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;B [A Randomly Shuffled]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of adjecnecy disagreements:  16.0
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;B [A Randomly Shuffled]&#39;}&gt;
</pre></div>
</div>
<img alt="_images/graph-matching-vertex_16_2.png" src="_images/graph-matching-vertex_16_2.png" />
</div>
</div>
<p>Below, we create a model to solve the Graph Matching Problem. The model is then fitted for the two graphs A and B.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.match</span> <span class="kn">import</span> <span class="n">GraphMatch</span>

<span class="n">gmp</span> <span class="o">=</span> <span class="n">GraphMatch</span><span class="p">()</span>
<span class="n">gmp</span> <span class="o">=</span> <span class="n">gmp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">gmp</span><span class="o">.</span><span class="n">perm_inds_</span><span class="p">,</span> <span class="n">gmp</span><span class="o">.</span><span class="n">perm_inds_</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of adjecnecy disagreements: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">B</span><span class="p">)))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;A [ER-NP(6, 0.3) Simulation]&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;B [Unshuffled]&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">((</span><span class="n">A</span><span class="o">-</span><span class="n">B</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;A-B [Unshuffled]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of adjecnecy disagreements:  8.0
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;A-B [Unshuffled]&#39;}&gt;
</pre></div>
</div>
<img alt="_images/graph-matching-vertex_18_2.png" src="_images/graph-matching-vertex_18_2.png" />
</div>
</div>
<p>The graph matching algorithm is able to successfully unshuffle <span class="math notranslate nohighlight">\(B\)</span>, with zero adjacency disagreements between <span class="math notranslate nohighlight">\(A\)</span> and the matched <span class="math notranslate nohighlight">\(B\)</span>.</p>
</div>
<div class="section" id="seeds">
<h4><span class="section-number">2.2.5. </span>Seeds<a class="headerlink" href="#seeds" title="Permalink to this headline">¶</a></h4>
<p>As mentioned previously, as network become larger, they quickly become more difficult to match. One method to mitigate this difficulty is to use <span class="math notranslate nohighlight">\(\textit{seeds}\)</span>. Seeds are a subset of matches that we already know before we perform the graph matching. For example, if we are given two graphs <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(F\)</span> with 300 nodes each, we might already know ten node matches between <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(F\)</span>. Having this prior information greatly improves our ability to match the networks.</p>
</div>
<div class="section" id="seeded-graph-matching-on-correlated-graph-pairs">
<h4><span class="section-number">2.2.6. </span>Seeded Graph Matching on Correlated Graph Pairs<a class="headerlink" href="#seeded-graph-matching-on-correlated-graph-pairs" title="Permalink to this headline">¶</a></h4>
<p>To demonstrate the effectiveness of Seeded Graph Matching (SGM), the algorithm will be applied on a pair of correlated SBM graphs (undirected, no self loops) <span class="math notranslate nohighlight">\(T, F \sim SBM\,(n, p, rho)\)</span>  with the following parameters:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
n &amp;= [100, 100, 100]\\
p &amp;= \begin{bmatrix} 
0.7 &amp; 0.3 &amp; 0.4\\
0.3 &amp; 0.7 &amp; 0.3\\
0.4 &amp; 0.3 &amp; 0.7
\end{bmatrix}\\
rho &amp;= 0.9
\end{align*}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_corr</span><span class="p">,</span> <span class="n">sbm</span><span class="p">,</span> <span class="n">sbm_corr</span>
<span class="n">directed</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">loops</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">n_per_block</span> <span class="o">=</span> <span class="mi">75</span>
<span class="n">n_blocks</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">block_members</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_blocks</span> <span class="o">*</span> <span class="p">[</span><span class="n">n_per_block</span><span class="p">])</span>
<span class="n">n_verts</span> <span class="o">=</span> <span class="n">block_members</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">block_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]])</span>

<span class="n">A1</span><span class="p">,</span> <span class="n">A2</span> <span class="o">=</span> <span class="n">sbm_corr</span><span class="p">(</span><span class="n">block_members</span><span class="p">,</span> <span class="n">block_probs</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="n">directed</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="n">loops</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 1&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 2&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span> <span class="o">-</span> <span class="n">A2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Diff (G1 - G2)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/graph-matching-vertex_23_0.png" src="_images/graph-matching-vertex_23_0.png" />
</div>
</div>
<p>To emphasize the effectiveness of SGM, as well as why having seeds is important, we will randomly shuffle the vertices of Graph 2. This random permutation is stored, and unshuffled, such that we have available the optimal permutation that returns the original graph 2.</p>
<p>Here we see that after shuffling graph 2, there are many more edge disagreements, as expected.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">node_shuffle_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n_verts</span><span class="p">)</span>
<span class="n">A2_shuffle</span> <span class="o">=</span> <span class="n">A2</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">node_shuffle_input</span><span class="p">,</span> <span class="n">node_shuffle_input</span><span class="p">)]</span>
<span class="n">node_unshuffle_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_verts</span><span class="p">))</span>
<span class="n">node_unshuffle_input</span><span class="p">[</span><span class="n">node_shuffle_input</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_verts</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 1&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A2_shuffle</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 2 shuffled&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span> <span class="o">-</span> <span class="n">A2_shuffle</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Diff (G1 - G2 shuffled)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/graph-matching-vertex_25_0.png" src="_images/graph-matching-vertex_25_0.png" />
</div>
</div>
</div>
<div class="section" id="unshuffling-graph-2-without-seeds">
<h4><span class="section-number">2.2.7. </span>Unshuffling graph 2 without seeds<a class="headerlink" href="#unshuffling-graph-2-without-seeds" title="Permalink to this headline">¶</a></h4>
<p>First, we will run SGM on graph 1 and the shuffled graph 2 with no seeds, and return the match ratio, that is the fraction of vertices that have been correctly matched.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sgm</span> <span class="o">=</span> <span class="n">GraphMatch</span><span class="p">()</span>
<span class="n">sgm</span> <span class="o">=</span> <span class="n">sgm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span><span class="n">A2_shuffle</span><span class="p">)</span>
<span class="n">A2_unshuffle</span> <span class="o">=</span> <span class="n">A2_shuffle</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">sgm</span><span class="o">.</span><span class="n">perm_inds_</span><span class="p">,</span> <span class="n">sgm</span><span class="o">.</span><span class="n">perm_inds_</span><span class="p">)]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 1&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A2_unshuffle</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 2 unshuffled&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span> <span class="o">-</span> <span class="n">A2_unshuffle</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Diff (G1 - G2 unshuffled)&quot;</span><span class="p">)</span>

<span class="n">match_ratio</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">sgm</span><span class="o">.</span><span class="n">perm_inds_</span><span class="o">-</span><span class="n">node_unshuffle_input</span><span class="p">))</span><span class="o">/</span><span class="n">n_verts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Match Ratio with no seeds: &quot;</span><span class="p">,</span> <span class="n">match_ratio</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Match Ratio with no seeds:  0.06222222222222218
</pre></div>
</div>
<img alt="_images/graph-matching-vertex_27_1.png" src="_images/graph-matching-vertex_27_1.png" />
</div>
</div>
<p>While the predicted permutation for graph 2 did recover the basic structure of the stochastic block model (i.e. graph 1 and graph 2 look qualitatively similar), we see that the number of edge disagreements between them is still quite high, and the match ratio quite low.</p>
</div>
<div class="section" id="unshuffling-graph-2-with-10-seeds">
<h4><span class="section-number">2.2.8. </span>Unshuffling graph 2 with 10 seeds<a class="headerlink" href="#unshuffling-graph-2-with-10-seeds" title="Permalink to this headline">¶</a></h4>
<p>Next, we will run SGM with 10 seeds randomly selected from the optimal permutation vector found ealier. Although 10 seeds is only about 4% of the 300 node graph, we will observe below how much more accurate the matching will be compared to having no seeds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">A1</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)[:</span><span class="mi">10</span><span class="p">])</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">node_unshuffle_input</span><span class="p">[</span><span class="n">W1</span><span class="p">])</span>
    
<span class="n">sgm</span> <span class="o">=</span> <span class="n">GraphMatch</span><span class="p">()</span>
<span class="n">sgm</span> <span class="o">=</span> <span class="n">sgm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span><span class="n">A2_shuffle</span><span class="p">,</span><span class="n">W1</span><span class="p">,</span><span class="n">W2</span><span class="p">)</span>
<span class="n">A2_unshuffle</span> <span class="o">=</span> <span class="n">A2_shuffle</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">sgm</span><span class="o">.</span><span class="n">perm_inds_</span><span class="p">,</span> <span class="n">sgm</span><span class="o">.</span><span class="n">perm_inds_</span><span class="p">)]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 1&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A2_unshuffle</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Graph 2 unshuffled&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A1</span> <span class="o">-</span> <span class="n">A2_unshuffle</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Diff (G1 - G2 unshuffled)&quot;</span><span class="p">)</span>

<span class="n">match_ratio</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">sgm</span><span class="o">.</span><span class="n">perm_inds_</span><span class="o">-</span><span class="n">node_unshuffle_input</span><span class="p">))</span><span class="o">/</span><span class="n">n_verts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Match Ratio with 10 seeds: &quot;</span><span class="p">,</span> <span class="n">match_ratio</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Match Ratio with 10 seeds:  1.0
</pre></div>
</div>
<img alt="_images/graph-matching-vertex_30_1.png" src="_images/graph-matching-vertex_30_1.png" />
</div>
</div>
<p>From the results above, we see that when running SGM on the same two graphs, with no seeds there is match ratio is quite low. However including 10 seeds increases the match ratio to 100% (meaning that the shuffled graph 2 was completely correctly unshuffled).</p>
</div>
</div>
<span id="document-applications/ch9/multiple-vertex-nomination"></span><div class="section" id="vertex-nomination-for-multiple-networks">
<h3><span class="section-number">2.3. </span>Vertex Nomination For Multiple Networks<a class="headerlink" href="#vertex-nomination-for-multiple-networks" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
<span id="document-applications/ch10/ch10"></span><div class="section" id="applications-for-many-networks">
<h2><span class="section-number">3. </span>Applications for Many Networks<a class="headerlink" href="#applications-for-many-networks" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-applications/ch10/anomaly-detection"></span><div class="section" id="anomaly-detection-for-timeseries-of-networks">
<h3><span class="section-number">3.1. </span>Anomaly Detection For Timeseries of Networks<a class="headerlink" href="#anomaly-detection-for-timeseries-of-networks" title="Permalink to this headline">¶</a></h3>
<p>There is a particular type of sea slug which has gills on the outside of its body. When you squirt water at these gills, they withdraw into the slug. The interesting thing about this type of slug is that the brain network involved in this gill withdrawal reflex is entirely mapped out, from the neurons which detect and transmit information about the water into the slug’s brain, to the neurons that leave the brain and fire at its muscles. (For those interested, this is a real thing - look up Eric Kandel’s research on Aplysia!)</p>
<p>Say you’re a researcher studying these sea slugs, and you have a bunch of brain networks of the same slug. We can define each node as a single neuron, and edges denote connections between neurons. Each of the brain networks that you have were taken at different time points: some before water started getting squirted at the slug’s gills, and some as the water was getting squirted. Your goal is to reconstruct when water started to get squirted, using only the networks themselves. You hypothesize that there should be some signal change in your networks which can tell you the particular time at which water started getting squirted. Given the network data you have, how do you figure out which timepoints these are?</p>
<p>The broader class of problems this question addresses is called <em>anomaly detection</em>. The idea, in general, is that you have a bunch of snapshots of the same network over time. Although the nodes are the same, the edges are changing at each time point. Your goal is to figure out which time points correspond to the most change, either in the entire network or in particular groups of nodes. You can think of a network as “anomalous” with respect to time if some potentially small group of nodes within the network concurrently changes behavior at some point in time compared to the recent past, while the remaining nodes continue with whatever noisy, normal behavior they had.</p>
<p>In particular, what we would really like to do is separate the signal from the noise. All of the nodes in the network are likely changing a bit over time, since there is some variability intrinsic in the system. Random noise might just dictate that some edges get randomly deleted and some get randomly created at each step. We want to figure out if there are timepoints where the change isn’t just random noise: we’re trying to figure out a point in time where the probability distribution that the network <em>itself</em> is generated from changes.</p>
<p>Let’s simulate some network timeseries data so that we can explore anomaly detection more thoroughly.</p>
<div class="section" id="simulating-network-timeseries-data">
<h4><span class="section-number">3.1.1. </span>Simulating Network Timeseries Data<a class="headerlink" href="#simulating-network-timeseries-data" title="Permalink to this headline">¶</a></h4>
<p>For this data generation, we’re going to assemble a set of 12 time-points for a network directly from its latent positions (we’ll assume that each time-point for the network is drawn from an RDPG). Ten of these time points will just have natural variability, and two will have a subset of nodes whose latent positions were perturbed a bit. These two will be the anomalies.</p>
<p>We’ll say that the latent positions for the network are one-dimensional, and that it has 100 nodes. There will be the same number of adjacency matrices as there are time points, since our network will be changing over time.</p>
<p>To make the ten non-anomalous time points, we’ll:</p>
<ol class="simple">
<li><p>Generate 100 latent positions. Each latent position will be a (uniformly) random number between 0.2 and 0.8.</p></li>
<li><p>Use graspologic’s rdpg function to sample an adjacency matrix using these latent positions. Do this ten times.</p></li>
</ol>
<p>And to make the two perturbed time points, we’ll do the following twice:</p>
<ol class="simple">
<li><p>Add a small amount of noise to the first 20 latent positions that we generated above.</p></li>
<li><p>Generate an adjacency matrix from this perturbed set of latent positions.</p></li>
</ol>
<p>Once we have this simulated data, we’ll move into some discussion about how we’ll approach detecting the anomalous time points.</p>
<p>Below is code for generating the data. We define a function to generate a particular time-point, with an argument which toggles whether we’ll perturb latent positions for that time point. Then, we just loop through our time-points to sample an adjacency matrix for each one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>


<span class="k">def</span> <span class="nf">gen_timepoint</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">perturbed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_perturbed</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">perturbed</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">baseline</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">baseline</span><span class="p">,</span> <span class="p">(</span><span class="n">n_perturbed</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> 
                                     <span class="n">n_perturbed</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> 
                                     <span class="n">nodes</span><span class="o">-</span><span class="n">n_perturbed</span><span class="p">))</span>
        <span class="n">X</span> <span class="o">+=</span> <span class="p">(</span><span class="n">delta</span> <span class="o">*</span> <span class="o">.</span><span class="mi">15</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">A</span>
    

<span class="n">time_points</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">nodes</span><span class="p">)</span>
<span class="n">networks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">time_points</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">gen_timepoint</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">networks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="k">for</span> <span class="n">perturbed_time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">gen_timepoint</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">perturbed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">networks</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">perturbed_time</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
    
<span class="n">networks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">networks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see the adjacency matrices we generated below. Note that you can’t really distinguish a difference between the ten normal time points and the two perturbed time points with the naked eye, even though the difference is there, so it would be pretty difficult to manually mark the time points - and if you have many time points, rather than just a few, you’d want to be able to automate the process.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span><span class="p">,</span> <span class="n">cmaps</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="k">def</span> <span class="nf">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span>

<span class="c1"># adjacency matrices</span>
<span class="n">perturbed_points</span> <span class="o">=</span> <span class="p">{</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">time_points</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">perturbed_points</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_axes</span><span class="p">([</span><span class="o">.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="o">+.</span><span class="mi">8</span><span class="p">,</span> <span class="o">-.</span><span class="mi">02</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">])</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">networks</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Ten Normal Time Points&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Two Perturbed Time Points&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">rm_ticks</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/anomaly-detection_6_0.png" src="_images/anomaly-detection_6_0.png" />
</div>
</div>
</div>
<div class="section" id="how-do-we-figure-out-which-time-points-are-anomalies">
<h4><span class="section-number">3.1.2. </span>How Do We Figure Out Which Time Points Are Anomalies?<a class="headerlink" href="#how-do-we-figure-out-which-time-points-are-anomalies" title="Permalink to this headline">¶</a></h4>
<p>It’s time to start thinking about how we’d approach figuring out which of the time points are anomalies.</p>
<p>One of the simplest approaches to this problem might just be to figure out which node has the highest count of edge changes across your timeseries. For each node across the timeseries, you’d count the number of new edges that appeared (compared to the previous point in time), and the number of existing edges that were deleted. Whichever count is highest could be your anomalous node.</p>
<p>This might give you a rough estimate – and you could even potentially find perturbed time points with this approach – but it’s not necessarily the best solution. Counting edges doesn’t account for other important pieces of information: for instance, you might be interested in which other nodes new edges were formed with. It seems like deleting or creating edges with more important nodes, for instance, should be weighted higher than deleting or creating edges with unimportant nodes.</p>
<p>So let’s try another method. You might actually be able to guess it! The idea will be to simply estimate each network’s latent positions, followed by a hypothesis testing approach. Here’s the idea.</p>
<p>Let’s call the latent positions for our network <span class="math notranslate nohighlight">\(X^{(t)}\)</span> for the snapshot of the network at time <span class="math notranslate nohighlight">\(t\)</span>. You’re trying to find specific time points, <span class="math notranslate nohighlight">\(X^{(i)}\)</span>, which are different from their previous time point <span class="math notranslate nohighlight">\(X^{(i-1)}\)</span> by a large margin. You can define “different” as “difference in matrix norm”. Remember that the matrix norm is just a number that generalizes the concept of vector magnitude to matrices. In other words, We’re trying to find a time point where the difference in norm between the latent positions at time <span class="math notranslate nohighlight">\(t\)</span> and the latent positions at time <span class="math notranslate nohighlight">\(t-1\)</span> is greater than some constant:  <span class="math notranslate nohighlight">\(||X^{(t)} - X^{(t-1)}|| &gt; c\)</span>. The idea is that non-anomalous time points will probably be a bit different, but that the difference will be within some reasonable range of variability.</p>
<p>There’s an alternate problem where you restrict your view to <em>nodes</em> rather than entire adjacency matrices. The idea is that you’d find time-points which are anomalous for particular nodes or groups of nodes, rather than the entire network. The general idea is the same: you find latent positions, then test for how big the difference is between time point <span class="math notranslate nohighlight">\(t\)</span> and time point <span class="math notranslate nohighlight">\(t-1\)</span>. This time, however, your test is for particular nodes. You want to figure out if <span class="math notranslate nohighlight">\(||X_i^{(t)} - X_i^{(t-1)}|| &gt; c\)</span>, where you’re looking at a particular latent position <span class="math notranslate nohighlight">\(X_i\)</span> rather than all of them at once. We’ll be focusing on the problem for whole networks, but you can take a look at the original paper if you’re curious about how to apply it to nodes [cite]</p>
</div>
<div class="section" id="detecting-if-the-first-time-point-is-an-anomaly">
<h4><span class="section-number">3.1.3. </span>Detecting if the First Time Point is an Anomaly<a class="headerlink" href="#detecting-if-the-first-time-point-is-an-anomaly" title="Permalink to this headline">¶</a></h4>
<p>We’ll start with the first time point, which (because we generated the data!) we know in advance is not an anomaly.</p>
<p>If we were to just estimate the latent positions for each timepoint separately with ASE or LSE, we’d run into the nonidentifiability problem that we’ve seen a few times over the course of this book: The latent positions would be rotated versions of each other, and we’d have to use something like Procrustes (which adds variance, since it’s just an estimate) to rotate them back into the same space.</p>
<p>However, since we have multiple time points, each of which is associated to an adjacency matrix, it’s natural to use models from the Multiple-Network Representation Learning section (You can go back and read chapter 6.7 if you’re fuzzy on the details here). In that section, we introduced the Omnibus Embedding as a way to estimate latent positions for multiple <em>networks</em> simultaneously, but all we really need for it is multiple <em>adjacency matrices</em>. These exist in our network in the form of its multiple time points; So, we’ll just embed multiple time points at once with the Omnibus Embedding, and then they’ll live in the same space.</p>
<p>We only really <em>need</em> to embed two time points at a time, since all we really care about is being able to directly compare a time point <span class="math notranslate nohighlight">\(X^{(t)}\)</span> and the point prior to it <span class="math notranslate nohighlight">\(X^{(t-1)} = Y\)</span> - but because of the way Omni works, we’ll get smaller-variance estimates if we embed all the time points at once. Embedding them all at once also to be more robust to embedding dimension in practice. If you wanted to save computational power - for instance, if you had a lot of time points - you could instead choose to embed subsets of them, or just the two you’ll actually be using.</p>
<p>So, here’s what’s going on in the code below:</p>
<ol class="simple">
<li><p>We embed the time points using OMNI and then get our estimates for the first two sets of latent positions <span class="math notranslate nohighlight">\(\hat{X} = \hat{X}^{(t)}\)</span> and <span class="math notranslate nohighlight">\(\hat{Y} = \hat{X}^{(t-1)}\)</span>.</p></li>
<li><p>Then, we get the norm of their difference <span class="math notranslate nohighlight">\(||\hat{X} - \hat{Y}||\)</span> with numpy.</p></li>
</ol>
<p>An important point to clarify is that there are a lot of different types of matrix norms: Frobenius norm, spectral norm, and so on. In our case, we’ll be using the <span class="math notranslate nohighlight">\(l_2\)</span> operator norm, which is simply the largest singular value of the matrix. The <code class="docutils literal notranslate"><span class="pre">ord</span></code> parameter argument in numpy determines which norm we use, and <code class="docutils literal notranslate"><span class="pre">ord=2</span></code> is the operator norm.</p>
<p>Again, this norm, intuitively, will tell us how different two matrices are. If the norm of <span class="math notranslate nohighlight">\(X - Y\)</span> is small, then <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are very similar matrices; whereas if the norm of <span class="math notranslate nohighlight">\(X - Y\)</span> is large, then <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are very different. The norm should be large for anomalies, and small for everything else.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">OmnibusEmbed</span> <span class="k">as</span> <span class="n">OMNI</span>

<span class="k">def</span> <span class="nf">get_statistic</span><span class="p">(</span><span class="n">adjacencies</span><span class="p">,</span> <span class="n">return_latents</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the operator norm of the difference of two matrices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">omni</span> <span class="o">=</span> <span class="n">OMNI</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">latents_est</span> <span class="o">=</span> <span class="n">omni</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">adjacencies</span><span class="p">)</span>
    <span class="n">Xhat</span><span class="p">,</span> <span class="n">Yhat</span> <span class="o">=</span> <span class="n">latents_est</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latents_est</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Xhat</span> <span class="o">-</span> <span class="n">Yhat</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_latents</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">Xhat</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span>

<span class="n">y</span><span class="p">,</span> <span class="n">Xhat</span> <span class="o">=</span> <span class="n">get_statistic</span><span class="p">(</span><span class="n">networks</span><span class="p">,</span> <span class="n">return_latents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">y</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.570
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="n">var</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">y</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
</div>
<div class="section" id="hypothesis-testing-with-our-test-statistic">
<h4><span class="section-number">3.1.4. </span>Hypothesis Testing With our Test Statistic<a class="headerlink" href="#hypothesis-testing-with-our-test-statistic" title="Permalink to this headline">¶</a></h4>
<p>We have our norm <span class="math notranslate nohighlight">\(y\)</span>, which will be our test statistic. It should be a small value if the first two adjacency matrices in the timeseries are distributed the same, and large if they’re distributed differently. Remember that we’re fundamentally trying to figure out whether <span class="math notranslate nohighlight">\(X = X^{(t)}\)</span>, our latent positions at time <span class="math notranslate nohighlight">\(t\)</span>, is the same as <span class="math notranslate nohighlight">\(Y = X^{(t-1)}\)</span>, our latent positions at time <span class="math notranslate nohighlight">\(t-1\)</span>. This is also known as a <em>hypothesis test</em>: we’re testing the the null hypothesis that <span class="math notranslate nohighlight">\(X = Y\)</span> against the alternative hypothesis that <span class="math notranslate nohighlight">\(X \neq Y\)</span>.</p>
<p>The value of our test statistic is <span class="pasted-inline"><code class="output text_plain docutils literal notranslate"><span class="pre">'0.570'</span></code></span>. The problem is that we don’t know how big this is, relatively. Is <span class="pasted-inline"><code class="output text_plain docutils literal notranslate"><span class="pre">'0.570'</span></code></span> relatively large? small? how should we determine whether it’s small enough to say that X and Y probably come from the same distribution, and aren’t anomaly time points?</p>
<p>Well, what if we could use our estimated latent positions <span class="math notranslate nohighlight">\(\hat{X}\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> to generate a bunch of networks, then make test statistics from those new networks? We’d know for a fact that any pair of those networks are drawn from the same set of latent positions, and we could get a sense for what our test statistic should look like if the latent positions actually were the same. This technique is called <em>bootstrapping</em>, since you’re using estimated parameters to “pull yourself up by your own bootstraps” and generate a bunch of artificial data. Bootstrapping pops up all over the place in machine learning and statistics contexts.</p>
<div class="section" id="using-bootstrapping-to-figure-out-the-distribution-of-the-test-statistic">
<h5><span class="section-number">3.1.4.1. </span>Using Bootstrapping to Figure out the Distribution of the Test Statistic<a class="headerlink" href="#using-bootstrapping-to-figure-out-the-distribution-of-the-test-statistic" title="Permalink to this headline">¶</a></h5>
<p>We don’t have the true latent positions for a given time point, but we do have the estimated latent positions (we just used OMNI embedding to find them!)</p>
<p>So what we can do is the following:</p>
<ol class="simple">
<li><p>Using a set of the latent positions we just estimated, generate two new adjacency matrices.</p></li>
<li><p>Get the test statistic for these two adjacency matrices.</p></li>
<li><p>Repeat 1) and 2) a bunch of times, getting new test statistics each time</p></li>
<li><p>Look at the distribution of these test statistics, and determine whether {glue:}y is an outlier or not with respect to this distribution.</p></li>
</ol>
<p>So we’re artificially generating data that we <em>know for a fact</em> is distributed in exactly the same way, and then looking at how our test statistic is distributed under those assumptions. This artificial data will necessarily be a bit biased, since the latent positions you’re using to generate it are themselves only estimates, but it should be close enough to the real thing to be useful.</p>
<p>Below is some code. We generate 1000 pairs of adjacency matrices from our estimated latent positions for the first time point <span class="math notranslate nohighlight">\(\hat{X}^{(t)}\)</span>, and get the test statistic for each pair. Underneath this looping code, you can see the distribution of these bootstrapped test statistics in the form of a histogram. They look roughly normally distributed, and hover around 0.60. The red line shows where our actual test statistic lies, where we compare <span class="math notranslate nohighlight">\(\hat{X}^{(t)}\)</span> to <span class="math notranslate nohighlight">\(\hat{X}^{(t-1)}\)</span>.</p>
<p>If the red line is super far away from the bulk of the mass in the test statistic distribution, then it would be fairly unlikely to be drawn from the same set of latent positions as the bootstrapped test statistics, and we’d reject the hypothesis that it is. If it’s well within the range of values we’d reasonably expect, then we wouldn’t reject this possibility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># null hypothesis that X = Y. Bootstrap X.</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">bootstraps</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">A_est</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">Xhat</span><span class="p">)</span>
    <span class="n">B_est</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">Xhat</span><span class="p">)</span>
    <span class="n">bootstrapped_y</span> <span class="o">=</span> <span class="n">get_statistic</span><span class="p">([</span><span class="n">A_est</span><span class="p">,</span> <span class="n">B_est</span><span class="p">])</span>
    <span class="n">bootstraps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bootstrapped_y</span><span class="p">)</span>
<span class="n">bootstraps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bootstraps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">bootstraps</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Distribution of test statistics with the same latent positions&quot;</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Test statistic value&quot;</span><span class="p">);</span>

<span class="n">plot</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;y = difference in norm </span><span class="se">\n</span><span class="s2">between $\hat</span><span class="si">{X}</span><span class="s2">$ and $\hat</span><span class="si">{Y}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span><span class="o">+.</span><span class="mi">005</span><span class="p">,</span> <span class="mi">145</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">50</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">&quot;Bootstrapped Distribution&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 20, &#39;Bootstrapped Distribution&#39;)
</pre></div>
</div>
<img alt="_images/anomaly-detection_19_1.png" src="_images/anomaly-detection_19_1.png" />
</div>
</div>
<p>Fortunately, <span class="pasted-inline"><code class="output text_plain docutils literal notranslate"><span class="pre">'0.570'</span></code></span> is well within a reasonable range under the assumption that the time-points share latent positions. However, we can’t always eyeball stuff, and we need a way to formalize what it means for a test statistic to be “within a reasonable range”. Our test statistic is  <span class="math notranslate nohighlight">\(y = ||X^{(t)} - X^{(t-1)}||\)</span>, we’re trying to figure out if <span class="math notranslate nohighlight">\(X^{(t)} = X^{(t-1)}\)</span>, and we have a bunch of bootstrapped test statistics that we know were drawn from the same distribution (and are thus examples of the case where the null hypothesis is true).</p>
</div>
</div>
<div class="section" id="using-our-test-statistic-to-find-p-values">
<h4><span class="section-number">3.1.5. </span>Using our test statistic to find p-values<a class="headerlink" href="#using-our-test-statistic-to-find-p-values" title="Permalink to this headline">¶</a></h4>
<p>Since we have a range of examples of <span class="math notranslate nohighlight">\(y\)</span> values in which the null hypothesis is true, we have an estimate for the distribution of the null hypothesis. So, to find the probability that any new value drawn from this bootstrapped distribution is greater than a particular value <span class="math notranslate nohighlight">\(c\)</span>, we can just find the proportion of our bootstrapped values that are greater than <span class="math notranslate nohighlight">\(c\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p &amp;= \frac{\textrm{number of bootstrapped values greater than $c$}}{\textrm{total number of bootstrapped values}}
\end{align*}\]</div>
<p>When we let <span class="math notranslate nohighlight">\(c\)</span> be equal to our test statistic, <span class="math notranslate nohighlight">\(y\)</span>, we find the probability that any new bootstrapped value will be greater than <span class="math notranslate nohighlight">\(y\)</span> (assuming that <span class="math notranslate nohighlight">\(y\)</span> is drawn from the null distribution). Here we have our formalization.</p>
<p>Below is some simple numpy code that performs this estimation. We just count the number of bootstrapped statistics that are greater than our <span class="math notranslate nohighlight">\(y\)</span> value, and then divide by the number of bootstrapped test statistics. If the resulting <span class="math notranslate nohighlight">\(p\)</span>-value is less than some pre-determined probability (say, for instance, <span class="math notranslate nohighlight">\(0.05\)</span>), then we reject the null hypothesis and say that <span class="math notranslate nohighlight">\(y\)</span> probably comes from a different distribution than the bootstrapped statistics. This, in turn, implies that <span class="math notranslate nohighlight">\(X^{(t)} \neq X^{(t-1)}\)</span>, and we’ve found an anomaly time point.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="n">bootstraps</span> <span class="o">&gt;</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">N</span>

<span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7
</pre></div>
</div>
</div>
</div>
<p>Our <span class="math notranslate nohighlight">\(p\)</span> value is much larger than 0.05, so we don’t reject the null hypothesis, and we can conclude that we haven’t found an anomaly time. Since this is all synthetic data, we know how the data generation process worked, so we actually know for a fact that this is the right result – the adjacency matrix at time <span class="math notranslate nohighlight">\(t\)</span> actually <em>was</em> drawn from the same distribution as the adjacency matrix at time <span class="math notranslate nohighlight">\(t-1\)</span>.</p>
</div>
<div class="section" id="testing-the-rest-of-the-time-points-for-anomalies">
<h4><span class="section-number">3.1.6. </span>Testing the Rest of the Time Points For Anomalies<a class="headerlink" href="#testing-the-rest-of-the-time-points-for-anomalies" title="Permalink to this headline">¶</a></h4>
<p>Now that we’ve gone through this for one time point, we can do it for the rest. The process is exactly the same, except that you’re comparing different pairs of timepoints and you’re generating the bootstrapped test statistics with different estimated latent positions.</p>
<p>Below we get our test statistic for every pair of time points. Our two anomaly time points are drawn from the same distribution, by design, so we shouldn’t catch an anomaly when we test them against each other; however, we should catch anomalies when we test them against other, non-anomaly time points, and that’s exactly what we see.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ys_true</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">adjacency</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">networks</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">get_statistic</span><span class="p">([</span><span class="n">adjacency</span><span class="p">,</span> <span class="n">networks</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">ys_true</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">y</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">(</span><span class="n">ys_true</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">],</span> 
                   <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Test Statistics for Each Timeseries&quot;</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([]);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ys_true</span><span class="o">.</span><span class="n">keys</span><span class="p">()));</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Timeseries Pairs&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/anomaly-detection_28_0.png" src="_images/anomaly-detection_28_0.png" />
</div>
</div>
<p>If we were to plot a distribution of bootstrapped test statistics with each of our estimated y-values, it would look like the histogram below. Notice that two test statistics are clearly anomalous: the one comparing times five and six, and the one comparing times seven and eight. We know by design that networks six and seven actually are anomolous, and so we can see that our test managed to correctly determine the anomaly times.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">bootstraps</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Distribution of test statistics with the same latent positions&quot;</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Test statistic value&quot;</span><span class="p">);</span>

<span class="k">for</span> <span class="n">y_val</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">plot</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">);</span>
    
<span class="n">sorted_ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">(</span><span class="n">ys_true</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">))</span>
<span class="n">x_min</span> <span class="o">=</span> <span class="n">sorted_ys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="n">sorted_ys</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Non-Anomalous </span><span class="se">\n</span><span class="s2">Timeseries&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="mi">140</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x_min</span><span class="o">-.</span><span class="mi">12</span><span class="p">,</span> <span class="mi">140</span><span class="p">),</span> 
              <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span><span class="s2">&quot;k&quot;</span><span class="p">})</span>

<span class="n">plot</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Anomalous </span><span class="se">\n</span><span class="s2">Timeseries&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x_max</span><span class="p">,</span> <span class="mi">140</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x_max</span><span class="o">-.</span><span class="mi">12</span><span class="p">,</span> <span class="mi">140</span><span class="p">),</span> 
              <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arrowstyle&quot;</span><span class="p">:</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span><span class="s2">&quot;k&quot;</span><span class="p">});</span>

<span class="n">plot</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">.</span><span class="mi">49</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">&quot;Bootstrapped Distribution&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">70</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/anomaly-detection_30_0.png" src="_images/anomaly-detection_30_0.png" />
</div>
</div>
</div>
<div class="section" id="the-distribution-of-the-bootstrapped-test-statistic">
<h4><span class="section-number">3.1.7. </span>The Distribution of the Bootstrapped Test Statistic<a class="headerlink" href="#the-distribution-of-the-bootstrapped-test-statistic" title="Permalink to this headline">¶</a></h4>
<p>One issue that could pop up is that the bootstrapped test statistic is slightly biased. Since we’re generating it from an estimate <span class="math notranslate nohighlight">\(\hat{X}\)</span> of the true latent positions <span class="math notranslate nohighlight">\(X\)</span>, we’ll have a bias of <span class="math notranslate nohighlight">\(|\hat{X} - X|\)</span>. It’s worth comparing the two distributions to determine if that bias is a big deal in practice.</p>
<p>Below you can see the true distribution of the test statistic for the real, unperturbed set of latent positions <span class="math notranslate nohighlight">\(X\)</span> we generated the data from (that’s the blue distribution). You can also see a distribution of test statistics bootstrapped from a <span class="math notranslate nohighlight">\(\hat{X}\)</span>. You can see that in this case, they’re fairly close.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">OmnibusEmbed</span> <span class="k">as</span> <span class="n">OMNI</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">networks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">networks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">networks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">get_statistic</span><span class="p">(</span><span class="n">adjacencies</span><span class="p">,</span> <span class="n">return_latents</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the operator norm of the difference of two matrices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">omni</span> <span class="o">=</span> <span class="n">OMNI</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">latents_est</span> <span class="o">=</span> <span class="n">omni</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">adjacencies</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">latents_est</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latents_est</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">Y</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_latents</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span>

<span class="n">omni</span> <span class="o">=</span> <span class="n">OMNI</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">latents</span> <span class="o">=</span> <span class="n">omni</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">networks</span><span class="p">)</span>

<span class="n">ys_bootstrap</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">A_</span><span class="p">,</span> <span class="n">B_</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">latents</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">latents</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y_</span> <span class="o">=</span> <span class="n">get_statistic</span><span class="p">([</span><span class="n">A_</span><span class="p">,</span> <span class="n">B_</span><span class="p">])</span>
    <span class="n">ys_bootstrap</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>
    
<span class="n">ys_true</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">A_</span><span class="p">,</span> <span class="n">B_</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_</span> <span class="o">=</span> <span class="n">get_statistic</span><span class="p">([</span><span class="n">A_</span><span class="p">,</span> <span class="n">B_</span><span class="p">])</span>
    <span class="n">ys_true</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>
    
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">ys_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true distribution of y&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">ys_bootstrap</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;distribution of bootstrapped y values&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x16f70db80&gt;
</pre></div>
</div>
<img alt="_images/anomaly-detection_33_1.png" src="_images/anomaly-detection_33_1.png" />
</div>
</div>
</div>
<div class="section" id="id1">
<h4><span class="section-number">3.1.8. </span><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="references">
<h4><span class="section-number">3.1.9. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>j1’s paper – heritability</p></li>
<li><p>vivek’s paper – mcc</p></li>
</ul>
</div>
<div class="section" id="notes">
<h4><span class="section-number">3.1.10. </span>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h4>
<p>guodong’s stuff: uses MASE and OMNI combined with DCORR to do hypothesis testing</p>
<ul class="simple">
<li><p>vivek did something similar for MCC</p></li>
</ul>
</div>
</div>
<span id="document-applications/ch10/significant-edges"></span><div class="section" id="testing-for-significant-edges">
<h3><span class="section-number">3.2. </span>Testing for Significant Edges<a class="headerlink" href="#testing-for-significant-edges" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-applications/ch10/significant-vertices"></span><div class="section" id="testing-for-significant-vertices">
<h3><span class="section-number">3.3. </span>Testing for Significant Vertices<a class="headerlink" href="#testing-for-significant-vertices" title="Permalink to this headline">¶</a></h3>
</div>
<span id="document-applications/ch10/significant-communities"></span><div class="section" id="testing-for-significant-communities">
<h3><span class="section-number">3.4. </span>Testing for Significant Communities<a class="headerlink" href="#testing-for-significant-communities" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Joshua Vogelstein, Alex Loftus, and Eric Bridgeford<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>