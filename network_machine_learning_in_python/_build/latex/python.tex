%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=0,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Introduction}}

\usepackage{sphinxmessages}



         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        

\title{Hands-on Network Machine Learning with Scikit-Learn and Graspologic}
\date{Dec 03, 2021}
\release{}
\author{Joshua Vogelstein, Alex Loftus, and Eric Bridgeford}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{coverpage::doc}}


\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=700\sphinxpxdimen]{{umap_pedigo_small}.jpg}
\caption{2D representation of a \sphinxstyleemphasis{Drosophila} larva brain connectome network. Credit to Ben Pedigo, PhD student at Johns Hopkins University.}\label{\detokenize{coverpage:maggot-connectome}}\end{figure}


\part{Introduction}


\chapter{Preface}
\label{\detokenize{introduction/preface:preface}}\label{\detokenize{introduction/preface::doc}}

\section{Network Machine Learning and You}
\label{\detokenize{introduction/preface:network-machine-learning-and-you}}
\sphinxAtStartPar
This book is about networks, and how you can use tools from machine learning to understand and explain them more deeply. Why is this an interesting thing to learn about, and why should you care?

\sphinxAtStartPar
Well, at some level, every aspect of reality seems to be made of interconnected parts. Atoms and molecules are connected to each other with chemical bonds. Your neurons connect to each other through synapses, and the different parts of your brain connect to each other through groups of neurons interacting with each other. At a larger level, you are interconnected with other humans through social networks, and our economy is a global, interconnected trade network. The Earth’s food chain is an ecological network, and larger still, every object with mass in the universe is connected to every other object through a gravitational network.

\sphinxAtStartPar
So if you can understand networks, you can understand a little something about everything!


\section{Network Machine Learning in Your Projects}
\label{\detokenize{introduction/preface:network-machine-learning-in-your-projects}}
\sphinxAtStartPar
So, naturally you are excited about network machine learning and you would love to join the party!

\sphinxAtStartPar
Perhaps you’re a researcher and you want to expose shadowy financial networks and corporate fraud? Or create a network framework for measuring teamwork in healthcare? Maybe you’re interested in evolutionary releationships between different animals, or maybe you want to model communities of neurons in the brain?

\sphinxAtStartPar
Or maybe you’re a data scientist and your company has tons of data (user logs, financial data, production data, machine sensor data, hotline stats, HR reports, etc.), and more than likely you could
view the data as a network and unearth some hidden gems of knowledge if you just knew where to look? For example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Explore purchasing networks and isolate the most active customers

\item {} 
\sphinxAtStartPar
Explore patterns of collaboration in your company’s network of employees

\item {} 
\sphinxAtStartPar
Detect which transactions are likely to be fraudulent

\item {} 
\sphinxAtStartPar
Isolate groups in your company which are overperforming or underperforming

\item {} 
\sphinxAtStartPar
Model the transportation chain necessary to produce and disseminate your product

\item {} 
\sphinxAtStartPar
And more

\end{itemize}

\sphinxAtStartPar
Whatever the reason, you have decided to learn about networks and implement their analysis in your projects. Great idea!


\section{Objective and Approach}
\label{\detokenize{introduction/preface:objective-and-approach}}
\sphinxAtStartPar
This book assumes you know next to nothing about how networks can be viewed as a statistical object. Its goal is to give you the concepts, the intuitions, and the tools you need to actually implement programs capable of learning from network data.

\sphinxAtStartPar
The book is intended to give you the best introduction you can possibly get to explore and exploit network data. You might be a graduate student, doing research on biochemical networks or trade networks in ancient Mesopotamia. Or you might be a professional interested in an introduction to the field of network data science, because you think it might be useful for your company. Whoever you are, we think you’ll find a lot of things that are useful and interesting in this book!

\sphinxAtStartPar
We’ll cover the fundamentals of network data science, focusing on developing intuition on networks as statistical objects, doing so while paired with relevant Python tutorials. By the end of this book, you will be able to utilize efficient and easy to use tools available for performing analyses on networks. You will also have a whole new range of statistical techniques in your toolbox, such as representations, theory, and algorithms for networks.

\sphinxAtStartPar
We’ll spend this book learning about network algorithms by showing how they’re implemented in production\sphinxhyphen{}ready Python frameworks:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Numpy and Scipy are used for scientific programming. They give you access to array objects, which are the main way we’ll represent networks computationally.

\item {} 
\sphinxAtStartPar
Scikit\sphinxhyphen{}Learn is very easy to use, yet it implements many Machine Learning algorithms efficiently, so it makes a great entry point for downstream analysis of networks.

\item {} 
\sphinxAtStartPar
Graspologic is an open\sphinxhyphen{}source Python package developed by Microsoft and the NeuroData lab at Johns Hopkins University which gives you utilities and algorithms for doing statistical analyses on network\sphinxhyphen{}valued data.

\end{itemize}

\sphinxAtStartPar
The book favors a hands\sphinxhyphen{}on approach, growing an intuitive understanding of
networks through concrete working examples and a bit of theory.
While you can read this book without picking up your laptop, we highly recommend
you experiment with the code examples available online as Jupyter notebooks at \sphinxurl{http://docs.neurodata.io/graph-stats-book/index.html}.


\section{Prerequisites}
\label{\detokenize{introduction/preface:prerequisites}}
\sphinxAtStartPar
We assume you have a basic knowledge of mathematics. Because network science uses a lot of linear algebra, requiring a bit of linear algebra knowledge is unfortunately unavoidable. (You should know what an eigenvalue is!)

\sphinxAtStartPar
If you care about what’s under the hood mathematically, we have certain sections marked as “advanced material” \sphinxhyphen{} you should have a reasonable understanding of college\sphinxhyphen{}level math, such as calculus, linear algebra, probability, and statistics for these sections.

\sphinxAtStartPar
You should also probably have some background in programming \sphinxhyphen{} we’ll mainly be using Python to build and explore our networks. If you don’t have too much of a Python or math background, don’t worry \sphinxhyphen{} we’ll link some resources to give you a head start.

\sphinxAtStartPar
If you’ve never used Jupyter, don’t worry about it. It is a great tool to have in your toolbox and it’s easy to learn. We’ll also link some resources for you if you are not familiar with Python’s scientific libraries, like numpy, scipy, networkx, and scikit\sphinxhyphen{}learn.


\section{Roadmap}
\label{\detokenize{introduction/preface:roadmap}}
\sphinxAtStartPar
This book is organized into three parts.

\sphinxAtStartPar
Part I, Foundations, gives you a brief overview of the kinds of things you’ll be doing in this book, and shows you how to solve a network data science problem from start to finish. It covers the following topics:
\begin{itemize}
\item {} 
\sphinxAtStartPar
What a network is and where you can find networks in the wild

\item {} 
\sphinxAtStartPar
All the reasons why you should care about studying networks

\item {} 
\sphinxAtStartPar
Examples of ways you could apply network data science to your own projects

\item {} 
\sphinxAtStartPar
An overview of the types of problems Network Machine Learning is good at dealing with

\item {} 
\sphinxAtStartPar
The main challenges you’d encounter if you explored Network Learning more deeply

\item {} 
\sphinxAtStartPar
Exploring a real network data science dataset, to get a broad understanding of what you might be able to learn.

\end{itemize}

\sphinxAtStartPar
Part II, Representations, is all about how we can represent networks statistically, and what we can do with those representations. It covers the following topics:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ways you can represent individual networks

\item {} 
\sphinxAtStartPar
Ways you can represent groups of networks

\item {} 
\sphinxAtStartPar
The various useful properties different types of networks have

\item {} 
\sphinxAtStartPar
Types of network representations and why they’re useful

\item {} 
\sphinxAtStartPar
How to represent networks as a bunch of points in space

\item {} 
\sphinxAtStartPar
How to represent multiple networks

\item {} 
\sphinxAtStartPar
How to represent networks when you have extra information about your nodes

\end{itemize}

\sphinxAtStartPar
Part III, Applications, is about using the representations from Part II to explore and exploit your networks. It covers the following topics:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Figuring out if communities in your networks are different from each other

\item {} 
\sphinxAtStartPar
Selecting a reasonable model to represent your data

\item {} 
\sphinxAtStartPar
Finding nodes, edges, or communities in your networks that are interesting

\item {} 
\sphinxAtStartPar
Finding time points which are anomalies in a network which is evolving over time

\item {} 
\sphinxAtStartPar
What to do when you have new data after you’ve already trained a network model

\item {} 
\sphinxAtStartPar
How hypothesis testing works on networks

\item {} 
\sphinxAtStartPar
Figuring out which nodes are the most similar in a pair of networks

\end{itemize}


\section{Conventions Used In This Book}
\label{\detokenize{introduction/preface:conventions-used-in-this-book}}

\section{Using Code Examples}
\label{\detokenize{introduction/preface:using-code-examples}}

\section{About the Authors}
\label{\detokenize{introduction/preface:about-the-authors}}
\sphinxAtStartPar
\sphinxstylestrong{Dr. Joshua Vogelstein} is an Assistant Professor in the Department of Biomedical Engineering at Johns Hopkins University, with joint appointments in Applied Mathematics and Statistics, Computer Science, Electrical and Computer Engineering, Neuroscience, and Biostatistics. His research focuses on the statistics of networks in brain science (connectomes). His lab and collaborators have developed the leading computational algorithms and libraries to perform statistical analysis on networks.

\sphinxAtStartPar
\sphinxstylestrong{Alex Loftus} is a master’s student at Johns Hopkins University in the Department of Biomedical Engineering, with an undergraduate degree in neuroscience. He has worked on implementing network spectral embedding and clustering algorithms in Python, and helped develop an MRI pipeline to produce brain networks from diffusion MRI data.

\sphinxAtStartPar
\sphinxstylestrong{Eric Bridgeford} is a PhD student in the Department of Biostatistics at Johns Hopkins University. Eric’s background includes Computer Science and Biomedical Engineering, and he is an avid contributor of packages to CRAN and PyPi for nonparametric hypothesis testing. Eric studies general approaches for statistical inference in network data, with applications to problems with network estimation in MRI connectomics data, including replicability and batch effects.

\sphinxAtStartPar
\sphinxstylestrong{Dr. Carey E. Priebe} is Professor of Applied Mathematics and Statistics, and a founding member of the Center for Imaging Science (CIS) and the Mathematical Institute for Data Science (MINDS) at Johns Hopkins University. He is a leading researcher in theoretical, methodological, and applied statistics / data science; much of his recent work focuses on spectral network analysis and subsequent statistical inference. Professor Priebe is Senior Member of the IEEE, Elected Member of the International Statistical Institute, Fellow of the Institute of Mathematical Statistics, and Fellow of the American Statistical Association.

\sphinxAtStartPar
\sphinxstylestrong{Dr. Christopher M. White} is Managing Director, Microsoft Research Special Projects. He leads mission\sphinxhyphen{}oriented research and software development teams focusing on high risk problems. Prior to joining Microsoft, he was a Fellow at Harvard for network statistics and machine learning. Chris’s work has been featured in media outlets including Popular Science, CBS’s 60 Minutes, CNN, the Wall Street Journal, Rolling Stone Magazine, TEDx, and Google’s Solve for X. Chris was profiled in a cover feature for the Sept/Oct 2016 issue of Popular Science.

\sphinxAtStartPar
\sphinxstylestrong{Weiwei Yang} is a Principal Development Manager at Microsoft Research. Her interests are in resource efficient alt\sphinxhyphen{}SGD ML methods inspired by biological learning. The applied research group she leads aims to democratize AI by addressing issues of sustainability, robustness, scalability, and efficiency in ML. Her group has applied ML to address social issues such as countering human trafficking and to energy grid stabilizations.


\section{Acknowledgements}
\label{\detokenize{introduction/preface:acknowledgements}}
\sphinxAtStartPar
First of all, big thanks to everybody who has been reading the book as we write and giving feedback. So far, this list includes Dax Pryce, Ross Lawrence, Geoff Loftus, Alexandra McCoy, Olivia Taylor, and Peter Brown.


\section{Finished Sections}
\label{\detokenize{introduction/preface:finished-sections}}
\sphinxAtStartPar
(lots more in progress…)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Preface: {\hyperref[\detokenize{introduction/preface::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Preface}}}}

\item {} 
\sphinxAtStartPar
Why Use Statistical Models: {\hyperref[\detokenize{representations/ch5/why-use-models::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Why Use Statistical Models?}}}}

\item {} 
\sphinxAtStartPar
Single\sphinxhyphen{}Network Models: \DUrole{xref,myst}{}

\item {} 
\sphinxAtStartPar
Multi\sphinxhyphen{}Network Representation Learning: {\hyperref[\detokenize{representations/ch6/multigraph-representation-learning::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Multiple\sphinxhyphen{}Network Representation Learning}}}}

\item {} 
\sphinxAtStartPar
Joint Representation Learning: {\hyperref[\detokenize{representations/ch6/joint-representation-learning::doc}]{\sphinxcrossref{\DUrole{doc,std,std-doc}{Joint Representation Learning}}}}

\end{enumerate}


\chapter{Terminology and Math Refresher}
\label{\detokenize{introduction/terminology:terminology-and-math-refresher}}\label{\detokenize{introduction/terminology::doc}}
\sphinxAtStartPar
In this section, we outline some background terminology which will come up repeatedly throughout the book. This section attempts to standardize some background material that we think is useful going in. It is important to realize that many of the concepts discussed below are only crucial for understanding the advanced, starred sections. If you aren’t familiar with some (or any!) of the below concepts, we don’t think this would detract from your understanding of the broader content.


\section{Vectors, Matrices, and Numerical Spaces}
\label{\detokenize{introduction/terminology:vectors-matrices-and-numerical-spaces}}
\sphinxAtStartPar
Throughout this book, we will need some level of familiarity with numerical spaces, and the grammar that we use to describe them. Taking the time to understand this notation will better help you understand many of the concepts in the rest of the book.


\subsection{Numerical Spaces}
\label{\detokenize{introduction/terminology:numerical-spaces}}
\sphinxAtStartPar
Numerical spaces are everywhere. If you have taken any calculus or algebra courses, you are likely familiar with the natural numbers \sphinxhyphen{} these are just your basic one, two, three, and so on. This constitutes the most basic numerical space and is denoted by the symbol \(\mathbb N\). Formally, the natural numbers describes the set:
\begin{align*}
    \mathbb N &= \{1, 2, 3, ...\}
\end{align*}
\sphinxAtStartPar
and continues infinitely (notice that neither negative numbers nor numbers with decimal points appear in \(\mathbb N\)). On a similar note, we will frequently resort to short hand to describe subsets of the natural numbers. We will use the symbol \(\in\) (read, “in”) to denote that one quantity is found within a particular set. For example, since \(5\) is a natural number, we would say that \(5 \in \mathbb N\), which can be thought of as “\(5\) is in the set of natural numbers”. To describe a subset of the first \(5\) natural numbers, we would use the notation \([5]\), which denotes the set:
\begin{align*}
    [5] &= \{1,2,3,4,5\}
\end{align*}
\sphinxAtStartPar
In the more general case where we have some variable \(n\) where \(n \in \mathbb N\) (again, \(n\) is some arbitrary natural number), then:
\begin{align*}
    [n] &= \{1,2,...,n\}
\end{align*}
\sphinxAtStartPar
The next most basic numerical space is known as the integers, which is just the natural numbers combined with the negative numbers and zero. Specifically:
\begin{align*}
    \mathbb Z &= \{..., -2, -1, 0, 1, 2, ...\}
\end{align*}
\sphinxAtStartPar
From \(-\infty\) up to \(+\infty\).

\sphinxAtStartPar
There are many more numerical spaces, but in this book we’ll focus on one in particular: real numbers, denoted \(\mathbb R\). The real numbers can be thought of as all the numbers that can be represented by a finite or infinite number of decimal places in between (and including) the integers. We won’t go into too many details; if you want more details on the real numbers, a good place to start would be coursework in \sphinxstylestrong{real analysis}. Particularly, the real numbers include any natural number, and integer, any decimal, or any irrational number (such as \(\pi\) or \(\sqrt{2}\)). The main thing that is interesting about the real numbers that we will \sphinxstyleemphasis{indirectly} use throughout the book is that if we have any two real numbers \(x\) and \(y\) (remember, this would be written \(x, y \in \mathbb R\)), then the products, ratios, or sums of them are also real numbers:
\begin{align*}
    x \cdot y, \frac{x}{y}, x + y \in \mathbb R
\end{align*}
\sphinxAtStartPar
Throughout the book, we will build upon some of these numerical spaces and introduce several new ones along the way that are interesing for network machine learning. We will do this by attempting to relate them back to the basic numerical spaces we have introduced here.


\subsection{One\sphinxhyphen{}Dimensional Quantities}
\label{\detokenize{introduction/terminology:one-dimensional-quantities}}
\sphinxAtStartPar
We will frequently see the term “dimensional” come up in this book, and we will attempt to give some insight into what this means here. If we were to say that \(x \in \mathbb R\), we know from the above description that this means that \(x\) is a real number, and is therefore “in” the set of real numbers. A one\sphinxhyphen{}dimensional quantity is a quantity which is described by a single element from one numerical space. In this instance, \(x\) is described by one real number, and is therefore one\sphinxhyphen{}dimensional. We will use a lowercase letter (for instance, \(x, a, b, \alpha, \beta\); the letters may be Roman or Greek) to denote that a quantity is one\sphinxhyphen{}dimensional.


\subsection{Vectors}
\label{\detokenize{introduction/terminology:vectors}}
\sphinxAtStartPar
Building off the concept of one\sphinxhyphen{}dimensional variables, what if we had some variable that existed in two dimensions? For instance, consider the following:
\begin{align*}
    \vec x = \begin{bmatrix}1.5 \\ 2\end{bmatrix}
\end{align*}
\sphinxAtStartPar
As we can see here, \(\vec x\) is now described by two real numbers (namely, \(1.5\) and \(2\)). This means that \(\vec x\) is now a two\sphinxhyphen{}dimensional quantity, since we have two separate values needed to describe \(\vec x\). In this case, \(\vec x\) no longer is “in” the real numbers, it is instead in the two\sphinxhyphen{}dimensional real vectors, or \(\mathbb R^2\). Here, \(\vec x\) is called a \sphinxstylestrong{vector}, and each of its dimensions are defined using the notation \(x_1 = 1.5\) and \(x_2 = 2\). The subscript \(x_j\) just means the \(j^{th}\) element of \(\vec x\), which is numbered by counting downwards from the first row (\(j = 1\)) to however many rows \(\vec x\) has in total. Since \(\vec x\) is two\sphinxhyphen{}dimensional, we would say that \(j \in [2]\), which means \(j\) can be either \(1\) or \(2\). In general, we will assume that all vectors are \sphinxstylestrong{column vectors} unless otherwise stated, which means that \(\vec x\) will be assumed to be vertically aligned. This will not make much of a conceptual difference, but it will play a role when we define operations between vectors and matrices later on. On the other hand, a \sphinxstylestrong{row vector} will typically be denoted by using the \sphinxstylestrong{transpose} symbol, which we will learn about later on in the section on operators. Unlike a column vector, a row vector is aligned horizontally. For example, a row vector with entries identical to \(\vec x\) will be denoted:
\begin{align*}
    \vec x^\top = \begin{bmatrix}1.5 & 2\end{bmatrix}
\end{align*}
\sphinxAtStartPar
In the general case, for any set \(\mathcal S\), we would say that \(\vec s \in \mathcal S^d\) if (think through this notation!) for any \(j \in [d]\), \(s_j \in \mathcal S\). The key aspects are that the symbol for the vector will be a lower case letter (in this example, \(s\)) like the one\sphinxhyphen{}dimensional quantity, but will add the \(\vec{}\) symbol to denote that it is a vector with more than one dimension. The quantity \(d\) that you see in the superscript is referred to as the dimensionality. In this example, we would say that \(\vec s\) is a \(d\)\sphinxhyphen{}dimensional \(\mathcal S\)\sphinxhyphen{}vector.


\subsection{Matrices}
\label{\detokenize{introduction/terminology:matrices}}
\sphinxAtStartPar
Matrices come up a lot in network science because we often represent networks as matrices: the adjacency matrix, for instance, is a way to represent a network in terms of its edge connections. Because networks can be represented as matrices, we’ll sometimes just talk about matrices directly.

\sphinxAtStartPar
We will see a variety of different types of matrices throughout this book, so let’s start with a simple example. Consider the following marix:
\begin{align*}
    X = \begin{bmatrix}
    1.5 & 1.7 \\
    2 & 1.8
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Here, we can see that \(X\) is described by four real numbers, with a particular arrangement. This time, we say that \(X\) is an element of the set of all possible \(2 \times 2\) (2 rows, and 2 columns) matrices with real entries. In symbols, we would describe this as \(\mathbb R^{2 \times 2}\), where \(\mathbb R\) says that the elements of the matrix are real numbers, and \(2 \times 2\) means that the matrix has two rows and two columns. We can describe the entries of a matrix using indexing, very similar to what we did for vectors. In matrices, the rows and columns matter. In this case, the rows go from left to right horizontally, and the columns go from top to bottom vertically. The rows will be numbered from the top of the matrix to the bottom, and the columns will be numbered from the left\sphinxhyphen{}most column to the right\sphinxhyphen{}most column. For instance, the first row of the matrix \(X\) is the row\sphinxhyphen{}vector \(\begin{bmatrix}1.5 & 1.7\end{bmatrix}\), and the second column of the matrix \(X\) is the column\sphinxhyphen{}vector \(\begin{bmatrix}1.7 \\ 1.8\end{bmatrix}\). This subscripts \(x_{ij}\) means the entry of the matrix \(X\) in the \(i^{th}\) row aand the \(j^{th}\) column. In this instance, we would describe that \(x_{11} = 1.5\), \(x_{12} = 1.7\), \(x_{21}=2\), and \(x_{22} = 1.8\).

\sphinxAtStartPar
In the general case, for a set \(\mathcal S\), we would say that \(S \in \mathcal S^{r \times c}\) if (think this through!) for any \(i \in [r]\) and any \(j \in [c]\), \(s_{ij} \in \mathcal S\). Like before, the key aspects are that the symbol for a matrix will be a capital letter (in this example, \(S\)) to denote that it is a matrix, and its entries \(s_{ij}\) will be denoted using a lowercase letter. The quantity \(r\) is known as the row count and the quantity \(c\) is known as the column count of the matrix \(S\). In this example, we would say that \(S\) is a \(\mathcal S\)\sphinxhyphen{}matrix with \(r\) rows and \(c\) columns.

\sphinxAtStartPar
Another thing we will see arise periodically is that vectors can be denoted as matrices with a single column. For example, in our example above in the vector section, we might equivalently write that \(\vec s \in \mathcal S^{d \times 1}\). The “1” for the columns just denotes that \(\vec s\) is a column vector with \(d\) rows in total. This will be useful when we define functions for matrices, and use the same notation for functions on vectors.


\section{Useful Functions}
\label{\detokenize{introduction/terminology:useful-functions}}
\sphinxAtStartPar
Throughout the book, we will deal with many types of functions which take mathematical objects (potentially multiple) that exist in one numerical space and produce a mathematical object (potentially in a different) numerical space. You are probably familiar with several of these, such as the addition or multiplication operators on one\sphinxhyphen{}dimensional quantities. We will touch on some of the more fancy ones that we will see arise throughout the book.

\sphinxAtStartPar
The \sphinxstylestrong{sum}, denoted by a fancy capital epsilon \(\sum\), denotes that we are summing a bunch of items which can be easily indexed. For instance, consider if we have a vector \(\vec x \in \mathbb R^d\), so \(\vec x\) is a \(d\)\sphinxhyphen{}dimensional vector. If we wanted to take the sum of all of the elements of \(\vec x\), we would write:
\begin{align*}
    \sum_{i = 1}^d x_i = x_1 + x_2 + ... + x_d
\end{align*}
\sphinxAtStartPar
The \sphinxstyleemphasis{summand} of the sum, the \(x_i\)s next to the \(\sum\) symbol, are the terms that will be summed up. Further, note that the \(\sum\) symbol also indicates the indices of \(\vec x\) that will be summed. Note that on the bottom, we see that the sum says from \(i = 1\) and above it says \(d\). This means that we sum all the elements of \(x_i\) starting from below at \(1\) and going up until \(d\). We could say the exact same thing using our shorthand for this set, which we described in the section on natural numbers, \([d]\):
\begin{align*}
\sum_{i \in [d]} x_i = \sum_{i = 1}^d x_i = x_1 + x_2 + ... + x_d
\end{align*}
\sphinxAtStartPar
We could similarly define \sphinxstylestrong{any} indexing set, such as \(\mathcal I = \{1,3\}\), and write:
\begin{align*}
    \sum_{i \in \mathcal I} x_i = x_1 + x_3
\end{align*}
\sphinxAtStartPar
The key is that the notation above or below the summand just tells us which elements we are applying the sum over. For instance, if \(\vec x\) was a \(3\)\sphinxhyphen{}dimensional vector:
\begin{align*}
   \vec x = \begin{bmatrix}
      1.7 \\ 1.8 \\ 2
   \end{bmatrix}
\end{align*}
\sphinxAtStartPar
We would have that:
\begin{align*}
   \sum_{i = 1}^3 x_i = 5.5
\end{align*}
\sphinxAtStartPar
if we were to use \(\mathcal I = \{1,3\}\), then:
\begin{align*}
    \sum_{i \in \mathcal I}x_i = 3.7
\end{align*}
\sphinxAtStartPar
the \sphinxstylestrong{product}, denoted by a capital pi \(\prod\), behaves extremely similarly to the sum, except insted of applying sums, it applies multiplication. For instance, if we instead wanted to multiply all the elements of \(\vec x\), we would write:
\begin{align*}
    \prod_{i = 1}^d x_i = x_1 \times x_2 \times ... \times x_d
\end{align*}
\sphinxAtStartPar
Where \(\times\) is just multiplication like you are probably used to. Again, we have the exact same indexing conventions, where:
\begin{align*}
    \prod_{i \in [d]} x_i=
    \prod_{i = 1}^d x_i = x_1 \times x_2 \times ... \times x_d
\end{align*}
\sphinxAtStartPar
We can again just use indexing sets, too:
\begin{align*}
    \prod_{i \in \mathcal I}x_i = x_1 \times x_3
\end{align*}
\sphinxAtStartPar
With \(\vec x\) defined as above in the sum example, we would have that:
\begin{align*}
   \prod_{i = 1}^3 x_i = 6.12
\end{align*}
\sphinxAtStartPar
if we were to use \(\mathcal I = \{1,3\}\), then:
\begin{align*}
    \prod_{i \in \mathcal I}x_i = 3.4
\end{align*}
\sphinxAtStartPar
The \sphinxstylestrong{Euclidean inner product}, or the \sphinxstyleemphasis{inner product} we will refer to in our book, is obtained by multiplying two vectors element\sphinxhyphen{}wise, and summing the result. Suppose we have two vectors \(\vec x\) and \(\vec y\), which are each \(d\)\sphinxhyphen{}dimensional real vectors (both \(x\) and \(y\) must have the same number of elements). The inner product is the quantity:
\begin{align*}
    \langle \vec x, \vec y\rangle &= \sum_{i = 1}^d x_i y_i
\end{align*}
\sphinxAtStartPar
as we will see in a second, in matrix notation, this is exactly equivalent to writing:
\begin{align*}
    \langle \vec x, \vec y\rangle &= \vec x^T \vec y
\end{align*}
\sphinxAtStartPar
\sphinxstylestrong{Matrix multiplication}, denoted by a circle \(\cdot\) (or in most cases, just two matrices side by side, with no separation), is an operation which takes a matrix which has \(r\) rows and \(c\) columns and another matrix which has \(c\) rows and \(l\) columns, and produces a matrix with \(r\) rows and \(l\) columns. Suppose we have a matrix \(A \in \mathbb R^{r \times c}\), and \(B \in \mathbb R^{c \times l}\). Here, \(r\), \(c\), and \(l\) could be \sphinxstyleemphasis{any} natural numbers. A matrix multiplication produces a matrix \(D \in \mathbb R^{r \times l}\), where:
\begin{align*}
    d_{ij} = \sum_{k = 1}^c a_{ik}b_{kj}
\end{align*}
\sphinxAtStartPar
What does this mean intuitively? Well, let’s think about it. Let’s imagine that the \sphinxstyleemphasis{rows} of \(A\) are indexed from \(1\) to \(r\), like this:
\begin{align*}
    A &= \begin{bmatrix}
        \vec a_1^T \\
        \vec a_2^T \\
        \vdots \\
        \vec a_r^T
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Note that the vectors \(\vec a_i\) are transposed when oriented in the matrix \(A\), because they are each \(c\)\sphinxhyphen{}dimensional vectors (and by convention in our book, all vectors will be \sphinxstyleemphasis{column} vecors. So to comprise the rows of \(A\), they must be “flipped”). Similarly, let’s imagine that the columns of \(B\) are indexed from \(1\) to \(l\), like this:
\begin{align*}
    B &= \begin{bmatrix}
        \vec b_1 & \vec b_2 & ... & \vec b_l
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
So what is the marix \(D\)? Note that each entry, \(d_{ij} = \langle \vec a_i, \vec b_j\rangle = \vec a_i^T \vec b_j\). So the matrix \(D\) is the matrix whose entries are the \sphinxstyleemphasis{inner products of the rows of \(A\) with the columns of \(B\)}. In a diagram, \(D\) is like this:
\begin{align*}
    D &= \begin{bmatrix}
        \vec a_1^T\vec b_1 & ... & \vec a_1^T \vec b_l \\
        \vdots & \ddots & \vdots \\
        \vec a_r^T \vec b_1 & ... & \vec a_r^T \vec b_l
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
As a matter of notation, we might often have the case where we want to discuss or interpret a single element which is a product of two matrices. For instance, suppose we care about the entry \((i, j)\) of \(AB\). We might also describe the resulting quantity \(d_{ij}\) using the notation \((AB)_{ij}\). The reason we adopt this notation is that we want to emphasize that the matrix multiplication operation is performed first (it is in \sphinxstyleemphasis{parentheses}), and then we look at the \((i,j)\) entry of the resulting matrix.

\sphinxAtStartPar
The \sphinxstylestrong{Euclidean distance} is the most common distance between vectors we will see in this book. The Euclidean distance effectively tells us how far apart two points in \(d\)\sphinxhyphen{}dimensional space are. Given \(\vec x, \vec y \in \mathbb R^d\) (\(\vec x\) and \(\vec y\) are \(d\)\sphinxhyphen{}dimensional real vectors), the Euclidean distance is the quantity:
\begin{align*}
    \delta(\vec x, \vec y) &= \langle \vec x - \vec y, \vec x - \vec y\rangle = \sum_{i = 1}^d (x_i - y_i)^2
\end{align*}
\sphinxAtStartPar
In particular, if we check the distance between a vector and the origin (the \sphinxstylestrong{zero\sphinxhyphen{}vector}, denoted \(0_d\), which is a \(d\)\sphinxhyphen{}dimensional vector where all entries are \(0\)), we end up with a very useful quantity, called the squared Euclidean norm. We will use a special notation for the Euclidean norm, which is:
\begin{align*}
    ||\vec x||_2^2 &= \delta(\vec x, 0_d) = \sum_{i = 1}^dx_i^2
\end{align*}
\sphinxAtStartPar
The subscript \(_2\) just means that this is the “2”\sphinxhyphen{}norm, which is a concept outside of the scope of this book. The superscript \(^2\) means that this is the squared Euclidean norm. Therefore, the Euclidean norm itself is:
\begin{align*}
||\vec x||_2 &= \sqrt{\delta(\vec x, 0_d)} = \sqrt{\sum_{i = 1}^d x_i^2}
\end{align*}
\sphinxAtStartPar
What does this mean interpretation wise? The “square” operation basically means, if there are dimensions of \(\vec x\) that are big, the norm will end up being big. If the dimensions of \(\vec x\) are small, they will not contribute very much to the norm.

\sphinxAtStartPar
Based on the equation we saw above for the Euclidean distance, we could also understand the Euclidean distance to be the squared Euclidean norm of the vector which is the difference between \(\vec x\) and \(\vec y\). Using this convention:
\begin{align*}
    \delta(\vec x, \vec y) &= ||\vec x - \vec y||_2^2
\end{align*}
\sphinxAtStartPar
In this sense, we can see that the Euclidean distance and the Euclidean norms are attributing a concept of “length” and “how far” a vector is from another (whether that is the origin or an arbitrary real vector). Next, we will see a related concept for matrices. The \sphinxstylestrong{squared Frobenius norm} is the quantity, given a matrix \(A \in \mathbb R^{r \times c}\):
\begin{align*}
    ||A||_F^2&= \sum_{i = 1}^r \sum_{i = 1}^c a_{ij}^2
\end{align*}
\sphinxAtStartPar
Note that this is very similar to the squared Euclidean norm of a vector, except it is applied to both the rows \sphinxstyleemphasis{and} the columns of \(A\). Again, we have a similar interpretation to the Euclidean norm. If an entry of \(A\) is big, it will contribute much to the Frobenius norm due to the squared \(a_{ij}\) term. If an entry is smaller, it will not contribute as much. The Frobenius norm itself is just the square root of this:
\begin{align*}
    ||A||_F &= \sqrt{\sum_{i = 1}^r \sum_{i = 1}^c a_{ij}^2}
\end{align*}

\section{Probability}
\label{\detokenize{introduction/terminology:probability}}
\sphinxAtStartPar
Throughout this book, we will be very concerned with probabilities and probability distributions. For this reason, we will introduce some basic notation that we will be concerned with. In probability analyses, we are concerned with describing things that occur in the real world with some level of uncertainty. We capture this uncertainty using probability, which in essence, describes how likely (or unlikely) a particular outcome is compared to all of the possible outcomes that could be realized. In general, we will call the most basic objects which occur with some uncertainty \sphinxstylestrong{random variables}, which is a variable whose values that we get to see in the real world (the \sphinxstyleemphasis{realizations} of the random variable) depend on some random phenomenon. We will denote a random variable using a similar notation to a one\sphinxhyphen{}dimensional variable, with the exception that we will \sphinxstyleemphasis{bold face} the variable to make clear that it is random. For instance, for a one\sphinxhyphen{}dimensional random variable, we will use notation like \(\mathbf x\).

\sphinxAtStartPar
Like before, we can also have random vectors and random matrices. Like for the random variable, we will denote these with bold faces too. A random vector will be denoted using a bold faced variable with the vector symbol; for example, \(\vec{\mathbf x}\). Likewise, a random matrix will be denoted using a bold faced upper case letter; for example, \(\mathbf X\). Similar to how we indexed vectors and matrices, the index positions of random vectors and random matrices are random variables, too. That is, \(\vec{\mathbf x}\) is a \(d\)\sphinxhyphen{}dimensional random vector whose entries are the random variables \(\mathbf x_i\) for all \(i\) from \(1\) to \(d\):
\begin{align*}
    \vec{\mathbf x} &= \begin{bmatrix}
        \mathbf x_1 \\
        \vdots \\
        \mathbf x_d
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
And \(\mathbf X\) is a \((r \times c)\) random matrix whose entries are the random varaiables \(\mathbf x_{ij}\) for all \(i\) from \(1\) to \(r\) and \(j\) from \(1\) to \(c\):
\begin{align*}
    \mathbf X &= \begin{bmatrix}
        \mathbf x_{11} & ... & \mathbf x_{1c} \\
        \vdots & \ddots & \vdots \\
        \mathbf x_{r1} & ... & \mathbf x_{rc}
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
A probability distribution, denoted by \(\mathbb P\), is a function which gives the probability of a particular value being attained by a random quantity. To state this another way, the probability distribution is concerned with fixing probabilities to realizations of random quantities. to make this a little more concrete, we will give an example with the simplest possible probability distribution, the Bernoulli distribution, denoted \(Bernoulli(p)\). For the sake of this example, we will say that \(\mathbf x\) is a random variable which is \(Bernoulli(p)\) distributed, which we denote by \(\mathbf x \sim Bernoulli(p)\). The Bernoulli distribution describes that the probability of the random variable \(\mathbf x\) taking a realization of \(1\) is \(p\), whereas the probability of the random variable \(\mathbf x\) taking a realization of \(0\) is \(1 - p\). Using the probability distribution, we would say that:
\begin{align*}
    \mathbb P(\mathbf x = 0) &= 1 - p \\
    \mathbb P(\mathbf x = 1) &= p
\end{align*}

\section{Advanced Probability*}
\label{\detokenize{introduction/terminology:advanced-probability}}
\sphinxAtStartPar
the probability distribution for a random vector or a random matrix is described very similarly. The caveat is that with a random vector/matrix, we affix a probability of \sphinxstyleemphasis{every element} of the random vector/matrix equaling the realized vector/matrix. For instance, if \(\vec{\mathbf x}\) is a random vector taking realizations which are \(d\)\sphinxhyphen{}dimensional vectors, and \(\vec x\) is one such \(d\)\sphinxhyphen{}dimensional vector, then:
\begin{align*}
    \mathbb P(\vec{\mathbf x} = \vec x) = \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_d = x_d)
\end{align*}
\sphinxAtStartPar
and likewise, if \(\mathbf X\) is a random matrix taking realizations which are \(r \times c\) matrices, and \(X\) is one such \(r \times c\) matrix, then:
\begin{align*}
    \mathbb P(\mathbf X = X) &= \mathbb P(\mathbf x_{11} = x_{11}, ..., \mathbf x_{rc} = x_{rc}) \\
    &= \mathbb P(\mathbf x_{ij} = x_{ij} \text{ for any }i\text{ and }j)
\end{align*}
\sphinxAtStartPar
A probability concept we will see arise frequently in the advanced sections of the book is one called independence. A pair of random variables are independent if for any \(x\) which is a possible realization of \(\mathbf x\) and \(y\) is a possible realization of \(\mathbf y\), then:
\begin{align*}
    \mathbb P(\mathbf x = x, \mathbf y = y) = \mathbb P(\mathbf x = x) \mathbb P(\mathbf y = y)
\end{align*}
\sphinxAtStartPar
A related concept that will be very important in our study of random matrices is the idea of mutual independence. If we have a set of \(n\) random variables \(\mathbf x_i\) for all \(i = 1,..., n\), this set of random variables is said to be mutually independent if for any \(x_1\) which is a possible realization of \(\mathbf x_1\), any \(x_2\) which is a possible realization of \(\mathbf x_2\), and so on up to \(\mathbf x_n\), then:
\begin{align*}
    \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n) = \prod_{i = 1}^n \mathbb P(\mathbf x_i = x_i)
\end{align*}
\sphinxAtStartPar
The ways in which this is useful will become more obvious through some of the advanced material of later chapters.

\sphinxAtStartPar
Another important concept we will see arise in some of the advanced material is the idea of conditional distributions. Given \(x\) which is a possible realization of \(\mathbf x\) and \(y\) is a possible realization of \(\mathbf y\), then the conditional distribution of \(\mathbf x\) on \(\mathbf y\) is the quantity:
\begin{align*}
\mathbb P(\mathbf x = x | \mathbf y = y) &= \frac{\mathbb P(\mathbf x = x, \mathbf y = y)}{\mathbb P(\mathbf y = y)}
\end{align*}
\sphinxAtStartPar
While outside the scope of this book, it can be shown that this is a proper probability distribution function, but we mainly are concerned with the fact that this is simply a useful notation for an intuitive idea. What this allows us to capture is the idea of attributing a probability for a random variable \(\mathbf x\) obtaining the value \(x\), given that we already know that \(\mathbf y\) obtains the value \(y\). A related concept, Baye’s Rule, uses a simple consequence of this theorem. Note that we could flip the probability statement above, and would obtain that:
\begin{align*}
\mathbb P(\mathbf y = y | \mathbf x = x) &= \frac{\mathbb P(\mathbf x = x, \mathbf y = y)}{\mathbb P(\mathbf x = x)}
\end{align*}
\sphinxAtStartPar
a simple rearrangement of terms by multiplying both sides by \(\mathbb P(\mathbf x = x)\) gives us that:
\begin{align*}
\mathbb P(\mathbf x, \mathbf y)&= 
\mathbb P(\mathbf y = y | \mathbf x = x)\mathbb P(\mathbf x = x)
\end{align*}
\sphinxAtStartPar
Substituting this in to our first definition for a conditional distribution of \(\mathbf x\) on \(\mathbf y\) gives:
\begin{align*}
\mathbb P(\mathbf x = x | \mathbf y = y) &= \frac{\mathbb P(\mathbf y = y | \mathbf x = x)\mathbb P(\mathbf x = x)}{\mathbb P(\mathbf y = y)}
\end{align*}
\sphinxAtStartPar
which is Baye’s Rule.


\part{Foundations}


\chapter{The Network Machine Learning Landscape}
\label{\detokenize{foundations/ch1/ch1:the-network-machine-learning-landscape}}\label{\detokenize{foundations/ch1/ch1::doc}}

\section{What Is A Network?}
\label{\detokenize{foundations/ch1/what-is-a-network:what-is-a-network}}\label{\detokenize{foundations/ch1/what-is-a-network::doc}}
\sphinxAtStartPar
I would say start with a task: I have a bunch of edges.
how can I find the most similar nodes to a given node, ranked?
if these aren’t connected, should they be?  did we just link predict? (actual question I feel like link prediction is handwavy af)


\subsection{links for inspiration}
\label{\detokenize{foundations/ch1/what-is-a-network:links-for-inspiration}}\begin{itemize}
\item {} 
\sphinxAtStartPar
ez intro on graps for ML https://towardsdatascience.com/graph\sphinxhyphen{}theory\sphinxhyphen{}and\sphinxhyphen{}deep\sphinxhyphen{}learning\sphinxhyphen{}know\sphinxhyphen{}hows\sphinxhyphen{}6556b0e9891b

\end{itemize}


\section{Why Study Networks?}
\label{\detokenize{foundations/ch1/why-study-networks:why-study-networks}}\label{\detokenize{foundations/ch1/why-study-networks::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
motivation for network analysis: https://arxiv.org/pdf/0912.5410.pdf

\end{itemize}


\section{Examples of applications}
\label{\detokenize{foundations/ch1/examples-of-applications:examples-of-applications}}\label{\detokenize{foundations/ch1/examples-of-applications::doc}}

\section{Types of Networks}
\label{\detokenize{foundations/ch1/types-of-networks:types-of-networks}}\label{\detokenize{foundations/ch1/types-of-networks::doc}}

\section{Types of Network Learning Problems}
\label{\detokenize{foundations/ch1/types-of-learning-probs:types-of-network-learning-problems}}\label{\detokenize{foundations/ch1/types-of-learning-probs::doc}}

\section{Main Challenges of Network Learning}
\label{\detokenize{foundations/ch1/main-challenges:main-challenges-of-network-learning}}\label{\detokenize{foundations/ch1/main-challenges::doc}}

\section{Exercises}
\label{\detokenize{foundations/ch1/exercises:exercises}}\label{\detokenize{foundations/ch1/exercises::doc}}

\chapter{End\sphinxhyphen{}to\sphinxhyphen{}end Biology Network Machine Learning Project}
\label{\detokenize{foundations/ch2/ch2:end-to-end-biology-network-machine-learning-project}}\label{\detokenize{foundations/ch2/ch2::doc}}

\section{Look at the big picture}
\label{\detokenize{foundations/ch2/big-picture:look-at-the-big-picture}}\label{\detokenize{foundations/ch2/big-picture::doc}}

\section{Get the Data}
\label{\detokenize{foundations/ch2/get-the-data:get-the-data}}\label{\detokenize{foundations/ch2/get-the-data::doc}}

\section{Prepare the Data for Network Algorithms}
\label{\detokenize{foundations/ch2/prepare-the-data:prepare-the-data-for-network-algorithms}}\label{\detokenize{foundations/ch2/prepare-the-data::doc}}

\section{Transformation Techniques}
\label{\detokenize{foundations/ch2/transformation-techniques:transformation-techniques}}\label{\detokenize{foundations/ch2/transformation-techniques::doc}}

\section{Select and Train a Model}
\label{\detokenize{foundations/ch2/select-and-train:select-and-train-a-model}}\label{\detokenize{foundations/ch2/select-and-train::doc}}

\section{Fine\sphinxhyphen{}Tune your Model}
\label{\detokenize{foundations/ch2/fine-tune:fine-tune-your-model}}\label{\detokenize{foundations/ch2/fine-tune::doc}}

\chapter{End\sphinxhyphen{}to\sphinxhyphen{}end Business Network Machine Learning Project}
\label{\detokenize{foundations/ch3/ch3:end-to-end-business-network-machine-learning-project}}\label{\detokenize{foundations/ch3/ch3::doc}}

\section{Look at the Big Picture}
\label{\detokenize{foundations/ch3/big-picture:look-at-the-big-picture}}\label{\detokenize{foundations/ch3/big-picture::doc}}

\section{Get the Data}
\label{\detokenize{foundations/ch3/get-the-data:get-the-data}}\label{\detokenize{foundations/ch3/get-the-data::doc}}

\section{Discover and Visualize the Data to Gain Insights}
\label{\detokenize{foundations/ch3/discover-and-visualize:discover-and-visualize-the-data-to-gain-insights}}\label{\detokenize{foundations/ch3/discover-and-visualize::doc}}

\section{Prepare the Data for Network Algorithms}
\label{\detokenize{foundations/ch3/prepare-the-data:prepare-the-data-for-network-algorithms}}\label{\detokenize{foundations/ch3/prepare-the-data::doc}}

\part{Representations}


\chapter{Properties of Networks as a Statistical Object}
\label{\detokenize{representations/ch4/ch4:properties-of-networks-as-a-statistical-object}}\label{\detokenize{representations/ch4/ch4::doc}}

\section{Matrix Representations Of Networks}
\label{\detokenize{representations/ch4/matrix-representations:matrix-representations-of-networks}}\label{\detokenize{representations/ch4/matrix-representations::doc}}
\sphinxAtStartPar
When we work with networks, we need a way to represent them mathematically and in our code. A network itself lives in network space, which is just the set of all possible networks. Network space is kind of abstract and inconvenient if we want to use traditional mathematics, so we’d generally like to represent networks with groups of numbers to make everything more concrete.

\sphinxAtStartPar
More specifically, we would often like to represent networks with \sphinxstyleemphasis{matrices}. In addition to being computationally convenient, using matrices to represent networks lets us bring in a surprising amount of tools from linear algebra and statistics. Programmatically, using matrices also lets us use common Python tools for array manipulation like numpy.

\sphinxAtStartPar
The most common matrix representation of a network is called the Adjacency Matrix, and we’ll learn about that first.


\subsection{The Adjacency Matrix}
\label{\detokenize{representations/ch4/matrix-representations:the-adjacency-matrix}}
\sphinxAtStartPar
The beating heart of matrix representations for networks throughout this book is the adjacency matrix. The idea is pretty straightforward: Let’s say you have a network with \(n\) nodes. You give each node an index – usually some value between 0 and n – and then you create an \(n \times n\) matrix. If there is an edge between node \(i\) and node \(j\), you fill the \((i, j)_{th}\) value of the matrix with an entry, usually \(1\) if your network has unweighted edges. In the case of undirected networks, you end up with a symmetric matrix with full of 1’s and 0’s, which completely represents the topology of your network.

\sphinxAtStartPar
Let’s see this in action. We’ll make a network with only three nodes, since that’s small and easy to understand, and then we’ll show what it looks like as an adjacency matrix.

\noindent\sphinxincludegraphics{{matrix-representations_4_0}.png}

\sphinxAtStartPar
Our network has three nodes, labeled \(1\), \(2\), and \(3\). Each of these three nodes is either connected or not connected to each of the two other nodes. We’ll make a square matrix \(A\), with 3 rows and 3 columns, so that each node has its own row and column associated to it.

\sphinxAtStartPar
So, let’s fill out the matrix. We start with the first row, which corresponds to the first node, and move along the columns. If there is an edge between the first node and the node whose index matches the current column, put a 1 in the current location. If the two nodes aren’t connected, add a 0. When you’re done with the first row, move on to the second. Keep going until the whole matrix is filled with 0’s and 1’s.

\sphinxAtStartPar
The end result looks like the matrix below. Since the second and third nodes aren’t connected, there is a \(0\) in locations \(A_{2, 1}\) and \(A_{1, 2}\). There are also zeroes along the diagonals, since nodes don’t have edges with themselves.

\noindent\sphinxincludegraphics{{matrix-representations_6_0}.png}

\sphinxAtStartPar
Although the adjacency matrix is straightforward and easy to understand, it isn’t the only way to represent networks.


\subsection{The Incidence Matrix}
\label{\detokenize{representations/ch4/matrix-representations:the-incidence-matrix}}
\sphinxAtStartPar
Instead of having values in a symmetric matrix represent possible edges, like with the Adjacency Matrix, we could have rows represent nodes and columns represent edges. This is called the \sphinxstyleemphasis{Incidence Matrix}, and it’s useful to know about – although it won’t appear too much in this book. If there are \(n\) nodes and \(m\) edges, you make an \(n \times m\) matrix. Then, to determine whether a node is a member of a given edge, you’d go to that node’s row and the edge’s column. If the entry is nonzero (\(1\) if the network is unweighted), then the node is a member of that edge, and if there’s a \(0\), the node is not a member of that edge.

\sphinxAtStartPar
You can see the incidence matrix for our network below. Notice that with incidence plots, edges are (generally arbitrarily) assigned indices as well as nodes.

\noindent\sphinxincludegraphics{{matrix-representations_10_0}.png}

\sphinxAtStartPar
When networks are large, incidence matrices tend to be extremely sparse – meaning, their values are mostly 0’s. This is because each column must have exactly two nonzero values along its rows: one value for the first node its edge is connected to, and another for the second. Because of this, incidence matrices are usually represented in Python computationally as scipy’s \sphinxstyleemphasis{sparse matrices} rather than as numpy arrays, since this data type is much better\sphinxhyphen{}suited for matrices which contain mostly zeroes.

\sphinxAtStartPar
You can also add orientation to incidence matrices, even in undirected networks, which we’ll discuss next.


\subsection{The Oriented Incidence Matrix}
\label{\detokenize{representations/ch4/matrix-representations:the-oriented-incidence-matrix}}
\sphinxAtStartPar
The oriented incidence matrix is extremely similar to the normal incidence matrix, except that you assign a direction or orientation to each edge: you define one of its nodes as being the head node, and the other as being the tail. For undirected networks, you can assign directionality arbitrarily. Then, for the column in the incidence matrix corresponding to a given edge, the tail node has a value of \(-1\), and the head node has a value of \(0\). Nodes who aren’t a member of a particular edge are still assigned values of \(0\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.9, \PYGZhy{}0.05, \PYGZsq{}Head Node\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{matrix-representations_14_1}.png}

\sphinxAtStartPar
Although we won’t use incidence matrices, oriented or otherwise, in this book too much, we introduced them because there’s a deep connection between incidence matrices, adjacency matrices, and a matrix representation that we haven’t introduced yet called the Laplacian. Before we can explore that connection, we’ll discuss one more representation: the degree matrix.


\subsection{The Degree Matrix}
\label{\detokenize{representations/ch4/matrix-representations:the-degree-matrix}}
\sphinxAtStartPar
The degree matrix isn’t a full representation of our network, because you wouldn’t be able to reconstruct an entire network from a degree matrix.


\subsection{The Laplacian Matrix}
\label{\detokenize{representations/ch4/matrix-representations:the-laplacian-matrix}}

\subsubsection{The Symmetric Laplacian}
\label{\detokenize{representations/ch4/matrix-representations:the-symmetric-laplacian}}

\subsubsection{The Random\sphinxhyphen{}Walk Laplacian}
\label{\detokenize{representations/ch4/matrix-representations:the-random-walk-laplacian}}

\section{Representations of Networks}
\label{\detokenize{representations/ch4/network-representations:representations-of-networks}}\label{\detokenize{representations/ch4/network-representations::doc}}

\section{Properties of Networks}
\label{\detokenize{representations/ch4/properties-of-networks:properties-of-networks}}\label{\detokenize{representations/ch4/properties-of-networks::doc}}

\subsection{Descriptive Properties of Networks}
\label{\detokenize{representations/ch4/properties-of-networks:descriptive-properties-of-networks}}
\sphinxAtStartPar
Remember that a network topology, a collection of nodes \(\mathcal V\), edges \(\mathcal E\), can be represented as an \(n \times n\) adjacency matrix, where \(n\) is the total number of nodes. The adjacency matrix looks like this:
\begin{align*}
    A &= \begin{bmatrix}
        a_{11} & ... & a_{1n} \\
        \vdots & \ddots & \vdots \\
        a_{n1} & ... & a_{nn}
    \end{bmatrix},
\end{align*}
\sphinxAtStartPar
Let’s say we have a network representing the five boroughs of New York (Staten Island SI, Brooklyn BK, Queens Q, the Bronx BX, and Manhattan MH). The nodes in our network are the five boroughs. The edges \((i,j)\) of our network exist if one can travel from borough \(i\) to borough \(j\) along a bridge.

\sphinxAtStartPar
Below, we will look at a map of New York City, with the bridges connecting the different boroughs. In the middle, we look at this map as a network layout plot. The arrows indicate the direction of travel. On the right, we look at this map as an adjacency matrix:

\noindent\sphinxincludegraphics{{properties-of-networks_1_0}.png}


\subsubsection{The edges of undirected networks are bi\sphinxhyphen{}directional}
\label{\detokenize{representations/ch4/properties-of-networks:the-edges-of-undirected-networks-are-bi-directional}}
\sphinxAtStartPar
When you decide to travel from borough \(i\) to borough \(j\), you care about whether you can \sphinxstyleemphasis{actually drive} on that bridge! In a similar way, the concept of directedness describes whether we need to worry about one\sphinxhyphen{}way bridges and bridge closures. If there are one\sphinxhyphen{}way bridges in our network, then a bridge from borough \(i\) to borough \(j\) doesn’t \sphinxstyleemphasis{necessarily} imply that a bridge from borough \(j\) to borough \(i\) exists (just ask New York drivers). If, for instance, the Brooklyn bridge was closed from Manhattan to Brooklyn, our network might change like this. Note that the red arrow going from Manhattan (MH) to Brooklyn (BK) is no longer present:

\noindent\sphinxincludegraphics{{properties-of-networks_3_0}.png}

\sphinxAtStartPar
Fortunately, in the context of this book, we will usually only worry about the undirected case, or when the presence of an arrow implies that the other direction exists, too. A network is \sphinxstylestrong{undirected} if a connection between node \(i\) and node \(j\) implies that node \(j\) is also connected to node \(i\). For this reason, we will usually omit the arrows entirely, like we show below:

\noindent\sphinxincludegraphics{{properties-of-networks_5_0}.png}

\sphinxAtStartPar
For the adjacency matrix \(A\), remember a connection between nodes \(i\) and \(j\) is represented by the adjacency \(a_{ij}\). This means that if the network is undirected, \(a_{ij} = a_{ji}\), for all pairs of nodes \(i\) and \(j\). By definition, this tells us that the adjacency matrix \(A\) is symmetric, so \(A = A^\top\).


\subsubsection{Loopless networks do not have self\sphinxhyphen{}loops}
\label{\detokenize{representations/ch4/properties-of-networks:loopless-networks-do-not-have-self-loops}}
\sphinxAtStartPar
If we are already in a borough, why would we want to take a bridge to that same borough? This logic relates to the concept of \sphinxstyleemphasis{self\sphinxhyphen{}loops} in a network. A \sphinxstylestrong{self\sphinxhyphen{}loop} in a network describes whether nodes can connect back to themselves. For instance, consider the following loop from Staten Island back to itself. This would have the interpretation of a bridge which connects Staten Island back to itself:

\noindent\sphinxincludegraphics{{properties-of-networks_7_0}.png}

\sphinxAtStartPar
In this example, the concept of self\sphinxhyphen{}loops is a little trite, but it is worth mentioning as you might see it arise elsewhere. A network is \sphinxstylestrong{loopless} if self\sphinxhyphen{}loops are not possible. For the adjacency matrix \(A\), a self\sphinxhyphen{}loop would be represented by the adjacencies \(a_{ii}\) for all nodes \(i\). Note that these entries \(a_{ii}\) are all of the \sphinxstyleemphasis{diagonal} entries of \(A\). Therefore, for a network which is loopless, all adjacencies \(a_{ii}\) on the diagonal are \(0\). You might also see this property abbreviated by stating that the diagonal of the adjacency matrix is \(0\), or \(diag(A) = 0\).


\subsubsection{Unweighted networks either have an edge, or they don’t}
\label{\detokenize{representations/ch4/properties-of-networks:unweighted-networks-either-have-an-edge-or-they-don-t}}
\sphinxAtStartPar
Do we need to convey information about how long it takes to get from borough \(i\) to borough \(j\) with our network? This fundamental question underlies the concept of \sphinxstyleemphasis{weightedness} in networks. We could use things called \sphinxstyleemphasis{edge\sphinxhyphen{}weights} \(w(i, j)\) could be used to describe the amount of time it takes to get from borough \(i\) to borough \(j\). An \sphinxstylestrong{edge\sphinxhyphen{}weight} \(w(i,j)\) assigns a weight to an edge between nodes \(i\) and \(j\) if that edge exists. If we care about weightedness in the network, the network is called \sphinxstyleemphasis{weighted}. The adjacencies \(a_{ij}\) of \(A\) for a weighted network take the value of the edge\sphinxhyphen{}weight; that is, \(a_{ij} = w_{ij}\) for any edge which exists between nodes \(i\) and \(j\). In the below plot, edge\sphinxhyphen{}weight indicates the approximate time to travel from one borough to the other. The network is undirected, so we don’t have to worry about directionality differences. The edge\sphinxhyphen{}weight is indicated by the number along the corresponding edge. We can also visualize edge\sphinxhyphen{}weights in terms of the adjacency matrix, which we show on the right:

\noindent\sphinxincludegraphics{{properties-of-networks_9_0}.png}

\sphinxAtStartPar
For most examples in this book, we will usually discuss \sphinxstyleemphasis{unweighted} or \sphinxstyleemphasis{binary} networks. A network is \sphinxstylestrong{unweighted} or \sphinxstylestrong{binary} if we only care about whether edges are \sphinxstyleemphasis{present} or \sphinxstyleemphasis{absent}. In a network which is unweighted, an adjacency \(a_{ij}\) takes the value \(1\) if there is an edge from node \(i\) to node \(j\), and takes the value \(0\) if there is \sphinxstyleemphasis{not} an edge from node \(i\) to node \(j\).

\begin{sphinxadmonition}{note}{This book considers \sphinxstyleemphasis{simple networks}}

\sphinxAtStartPar
This point is a \sphinxstyleemphasis{really} big deal conceptually for our study of network machine learning. A \sphinxstylestrong{simple network} is loopless, undirected, and unweighted. Most of the examples and techniques we look at in this book are developed in the context of simple networks. Fortunately, this note is largely conceptual, and doesn’t really impact much from an implementation perspective. All the techniques and packages we use will make sensible choices, or will directly extend, to cases that fall outside of this particular setup. If your networks don’t satisfy one or any of these properties, most of the approaches discussed herein will still work. If the technique will not work for the network you have provided, the software package used, \sphinxcode{\sphinxupquote{graspologic}}, will either give you a warning or an explicit error if there is a substantial issue with the network you have provided.
\end{sphinxadmonition}


\subsection{Descriptive Properties of Nodes}
\label{\detokenize{representations/ch4/properties-of-networks:descriptive-properties-of-nodes}}
\sphinxAtStartPar
Just like we have many words and properties which describe the network itself, we also have special vocabulary in network machine learning to describe properties about the individual nodes in the network. Remember that the nodes of the network are the \(n\)\sphinxhyphen{}element set \(\mathcal V\), which is just the collection \(\left\{v_1, ..., v_n\right\}\), where \(v_1\) is node \(1\), \(v_2\) is node \(2\), so on and so forth. We will tend to use the short\sphinxhyphen{}hand \(v_i\) to describe the node \(i\), for all nodes from \(1\) to \(n\).


\subsubsection{Node adjacencies and incidences}
\label{\detokenize{representations/ch4/properties-of-networks:node-adjacencies-and-incidences}}
\sphinxAtStartPar
We begin by descrcibing properties of single nodes in a simple network. The simplest property of a network is \sphinxstyleemphasis{adjacency}. A pair of nodes \(i\) and \(j\) in an undirected network are \sphinxstylestrong{adjacent} or are \sphinxstylestrong{neighbors} if an edge exists between them. In terms of the adjacency matrix, two nodes \(i\) and \(j\) are adjacent/neighbors if the element \(a_{ij}\) has a value of one. For instance, in the New York City example, the nodes SI and BK are adjacent/neighbors due to the presence of the green edge, shown in the figure. A related property is known as \sphinxstyleemphasis{incidence}. A node \(i\) is \sphinxstylestrong{incident} an edge \((i, j)\) or an edge \((j,i)\) if it is one of the two nodes which the edge connects. The adjacencies corresponding to this edge, \(a_{ij}\) and \(a_{ji}\), will both take a value of one. For instance, the nodes SI and BK are incident the green edge shown in the figure, as this edge connects SI to BK:

\noindent\sphinxincludegraphics{{properties-of-networks_11_0}.png}

\sphinxAtStartPar
These two nodes are \sphinxstyleemphasis{adjacent} one another due to the fact that an edge exists between them.


\subsubsection{Node degree quantifies the number of incidences}
\label{\detokenize{representations/ch4/properties-of-networks:node-degree-quantifies-the-number-of-incidences}}
\sphinxAtStartPar
The simplest summary statistic for a node is known as the \sphinxstyleemphasis{node degree}. The \sphinxstylestrong{node degree} of a node \(i\) in a simple network is the number of edges incident to it. Since every edge incident \((i, j)\) which is incident node \(i\) takes the value of \(1\), we can count the adjacencies that correspond to the edges incident node \(i\). If an edge does not exist, the adjacency corresponding to this \sphinxstyleemphasis{potential} edge takes a value of zero. Therefore, we can just sum along the \(i^{th}\) row or the \(i^{th}\) column of the adjacency matrix, since the row (column) correspond to the edges incident node \(i\):
\begin{align*}
    degree(v_i) &= \sum_{j = 1}^n a_{ij} = \sum_{j = 1}^n a_{ji}
\end{align*}
\sphinxAtStartPar
This means we will sum all of the potential edges which do \sphinxstyleemphasis{not} exist (any of the \(a_{ij}\)s which take a value of zero, and therefore no edge exists between nodes \(i\) and \(j\)) with all of the edges which \sphinxstyleemphasis{do} exist and are incident node \(i\) (since these \(a_{ij}\)s will take a value of one). For instance, if we consider the node BK in our example, we have two incident edges, indicated in green, so \(degree(v_{BK}) = 2\). When we look at the corresponding adjacency matrix, if we sum the adjacencies for node \(v_{BK}\), we also get two. The adjacencies which would be summed \(\sum_{i = 1}^n a_{ji}\) are shown in blue, and the adjacencies which would be summed \(\sum_{j = 1}^n a_{ij}\) are shown in green:

\noindent\sphinxincludegraphics{{properties-of-networks_13_0}.png}


\subsubsection{The degree matrix indicates the degrees of each node}
\label{\detokenize{representations/ch4/properties-of-networks:the-degree-matrix-indicates-the-degrees-of-each-node}}
\sphinxAtStartPar
A useful quantity which we will come across in many of the later chapters of this book is called the \sphinxstyleemphasis{degree matrix} of the network. The degree matrix is the \sphinxstyleemphasis{diagonal} matrix:
\begin{align*}
    D &= \begin{bmatrix}
        d_1 & 0 & ... & 0 \\
        0 & \ddots & \ddots& \vdots \\
        \vdots & \ddots & \ddots & 0 \\
        0 & ... & 0 & d_n
    \end{bmatrix}, \;\;\; d_i = degree(v_i)
\end{align*}
\sphinxAtStartPar
This matrix \(D\) is called \sphinxstylestrong{diagonal} because all of the entries \(d_{ij} = 0\) unless \(i = j\). The diagonal entries \(d_{ii}\) of the degree matrix are simply the node degrees \(degree(v_i)\) for each node \(i\). Using the counting procedure we described above, we can see that the node SI has degree one, the node BK has degree two, the node MH has degree three, the node Q has degree two, and the node BX has degree two. Therefore, the degree matrix is:

\noindent\sphinxincludegraphics{{properties-of-networks_15_0}.png}


\subsection{Network summary statistics tell us useful attributes about networks}
\label{\detokenize{representations/ch4/properties-of-networks:network-summary-statistics-tell-us-useful-attributes-about-networks}}
\sphinxAtStartPar
When we learn about networks, it is often valuable to compute properties of the network so that we can get a better understanding of the relationships within it. We will caall these properties \sphinxstyleemphasis{network summary statistics}. Although this book will focus more on finding and using \sphinxstyleemphasis{representations} of networks than using summary statistics, they’re useful to know about. We will introduce two network summary statistics, the network density and the clustering coefficient, and then show an example as to why we do not find summary statistics all that useful for network machine learning.


\subsubsection{The network density indicates the fraction of possible edges which exist}
\label{\detokenize{representations/ch4/properties-of-networks:the-network-density-indicates-the-fraction-of-possible-edges-which-exist}}
\sphinxAtStartPar
Given athe adjacency matrix \(A\) of a simple network, what fraction of the possible edges \sphinxstyleemphasis{actually} exist?

\sphinxAtStartPar
To understand this quantity, first we need to understand how many edges are possible in a network. We have \(n\) total nodes in the network, so \(A\) is an \(n \times n\) matrix. Therefore, \(A\) has \(n^2\) total entries. However, it turns out that over \sphinxstyleemphasis{half} of these entries are redundant. Since we said that the network was loopless, this means that every entry is \sphinxstyleemphasis{by default} \(0\) along the diagonal. Since each node \(i\) has a corresponding diagonal entry \(a_{ii}\), this comes to \(n\) entries in total that we do not need to count. This leaves our total possible number of edges at \(n^2\) (the total number of entries in the matrix \(A\)) minus \(n\) (the total number of entries which are automatically \(0\)), or \(n^2 - n = n(n - 1)\). This quantity represents the total number of possible edges which are \sphinxstyleemphasis{not} in the diagonal.

\sphinxAtStartPar
What else are we overcounting? Well, as it turns out, since the network is also \sphinxstyleemphasis{undirected}, every node that is \sphinxstyleemphasis{not} in the diagonal is also being double counted. Why is this? Remember that an undirected network has an adjacency matrix where for every pair of nodes \(i\) and \(j\), \(a_{ij} = a_{ji}\). This means that we overcount the number of possible edges not in the diagonal by a factor of \sphinxstyleemphasis{two}, since each off\sphinxhyphen{}diagonal entry \(a_{ij}\) has a corresponding entry \(a_{ji}\). This leaves the total number of possible edges in the network as \(\frac{1}{2}n(n - 1)\), or the total number of possible edges not in the diagonal reduced by a factor of two. This quantity is equivalent to the notation \(\binom n 2\), which is read as “\(n\) \sphinxstyleemphasis{choose} \(2\)”. You might see this notation arise in the study of \sphinxstyleemphasis{combinatorics}, where it is used to answer the question of, “In how many ways can we \sphinxstyleemphasis{choose} two items from \(n\) items?” In the network below, we see all of the \sphinxstyleemphasis{possible} edges indicated in red. If you count them up, there are \(\frac{1}{2}\cdot 5 \cdot (5 - 1) = 10\) red edges, in total:

\noindent\sphinxincludegraphics{{properties-of-networks_17_0}.png}

\sphinxAtStartPar
Now, how many edges \sphinxstyleemphasis{actually} exist in our network? The sum of all of the entries of \(A\) can be represented by the quantity \(\sum_{i = 1}^n \sum_{i = 1}^n a_{ij}\), however, there are some redundancies. Remember that \(A\) is loopless, so we don’t need to count the diagonal entries at all. This brings our quantity to \(\sum_{i = 1}^n \sum_{i \neq j}a_{ij}\), since we don’t need to count any edges along the diagonal of \(A\). Next, remember that if an edge in \(A\) exists between nodes \(i\) and \(j\), that \sphinxstyleemphasis{both} \(a_{ij}\) and \(a_{ji}\) take the value of \(1\), due to the undirected property. This means that to obtain the edge count of \(A\), that we only need to count \sphinxstyleemphasis{either} \(a_{ij}\) \sphinxstyleemphasis{or} \(a_{ji}\). Somewhat arbitrarily in this book, we will always count the adjacencies \(a_{ij}\) in the upper triangle of \(A\), which are the entries where \(j > i\). This brings our quantity to \(\sum_{i = 1}^n \sum_{j > i} a_{ij}\), which we can write \(\sum_{j > i}a_{ij}\) for short. The edges which exist in our network will be indicated with green, in the following figure, of which there are \(6\) total. Remember that the red edges were the \sphinxstyleemphasis{possible} edges:

\noindent\sphinxincludegraphics{{properties-of-networks_19_0}.png}

\sphinxAtStartPar
To put it lal together, the \sphinxstylestrong{network density} is a summary statistic which indicates the \sphinxstyleemphasis{density of edges} which are present in the network. For a simple network, the network density can be defined as the ratio between the total number of edges in \(A\) and the total number of edges possible in \(A\):
\begin{align*}
    density(A) &= \frac{\sum_{j > i}a_{ij}}{\frac{n(n - 1)}{2}} = \frac{2\sum_{j > i}a_{ij}}{n(n - 1)}
\end{align*}
\sphinxAtStartPar
In our example, this is simply the ratio of green edges which \sphinxstyleemphasis{actually} exist to red edges which could \sphinxstyleemphasis{possibly} exist, which is \(\frac{5}{10} = 0.5\).


\subsubsection{The clustering coefficient indicates how much nodes tend to cluster together}
\label{\detokenize{representations/ch4/properties-of-networks:the-clustering-coefficient-indicates-how-much-nodes-tend-to-cluster-together}}
\sphinxAtStartPar
The clustering coefficient inddicates the fraction of triplets of nodes which are closed. What the heck is that? Let’s look at only Brooklyn, Manhattan, Queens, and the Bronx:

\noindent\sphinxincludegraphics{{properties-of-networks_21_0}.png}

\sphinxAtStartPar
To begin to define the clustering coefficient, we first must understand what a \sphinxstyleemphasis{triplet} is. A \sphinxstylestrong{triplet} is an ordered tuple of three nodes which are connected by two or three edges. For instance, in the above network, we have the following triplets of nodes:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
(BX, MH, BK), (BX, BK, MH), (MH, BX, BK), (MH, BK, BX), (BK, BX, MH), (BK, MH, BX): two edges,

\item {} 
\sphinxAtStartPar
(MH, BK, Q), (MH, Q, BK), (BK, MH, Q), (BK, Q, MH), (Q, MH, BK), (Q, BK, MH): two edges,

\item {} 
\sphinxAtStartPar
(BX, MH, Q), (BX, Q, MH), (MH, BX, Q), (MH, Q, BX), (Q, BX, MH), (Q, MH, BX): three edges,

\end{enumerate}

\sphinxAtStartPar
and one three\sphinxhyphen{}node sets which has no triplets between \{BK, BX, Q\}, which has no triplets because there is only a single edge between BX andd Q amongst the three nodes. A triplet is \sphinxstyleemphasis{closed} if there are three edges, and is open if there are only two edges. In our example, there are six closed triplets amongst the nodes \{BX, MH, Q\}, and there are twele open triplets across \{BK, MH, Q\} and \{BK, MH, BX\}. The global clustering coefficient is defined as:
\begin{align*}
    C &= \frac{\text{number of closed triplets}}{\text{number of closed triplets} + \text{number of open triplets}}
\end{align*}
\sphinxAtStartPar
In our example, this comes to \(C = \frac{6}{6 + 12} = \frac{1}{3}\). This equation can also be understood in terms of the adjacency matrix. Note that if a triplet between nodes \(i\), \(j\), and \(k\) is closed, then all three of the adjacenies \(a_{ij}\), \(a_{jk}\), and \(a_{ki}\) have a value of \(1\). Therefore, if we could the number of times that \(a_{ij}a_{jk}a_{ki} = 1\), we also count the number of closed triplets! This means that the number of closed triplets can be expressed as \(\sum_{i,j,k}a_{ij}a_{jk}a_{ki}\).

\sphinxAtStartPar
Further, note that for a given node \(i\), that we can find an arbitrary triplet (either open or closed) through the following procedure.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Pick a single neighbor \(j\) for node \(i\). Note that the node \(i\) has a number of neighbors equal to \(degree(v_i) = d_i\), so there are \(d_i\) possible neighbors to choose from.

\item {} 
\sphinxAtStartPar
Pick a different neighbor \(k\) for node \(i\). Note that since node \(i\) had \(d_i\) neighbors, it has \(d_i - 1\) neighbors that are not node \(j\).

\item {} 
\sphinxAtStartPar
Since we know that nodes \(j\) and \(k\) are both neighbors of node \(i\), we know that \(a_{ij}\) and \(a_{ik}\) both have values of one, and therefore the edges \((i, j)\) and \((i, k)\) exist. Therefore, the tuple of nodes \((i, j, k)\) is a triplet, because \sphinxstyleemphasis{at least} two edges exist amongst the three nodes. This tuple is closed if the edge \((j, k)\) exists, and open if the edge \((j, k)\) does not exist.

\item {} 
\sphinxAtStartPar
Therefore, there are \(d_i (d_i - 1)\) triplets in which node \(i\) is the leading node of the triplet.

\end{enumerate}

\sphinxAtStartPar
As it turns out, since triplets are \sphinxstyleemphasis{ordered tuples}, we can repeat this procedure for all nodes, and if we count how many triplets we get in total, we get the \sphinxstyleemphasis{total number of triplets} for the entire network. Therefore, the number of open and closed triplets in the network is the quantity \(\sum_i d_i (d_i - 1)\).  Then we could express the clustering coefficient in terms of the adjacency matrix as:
\begin{align*}
    C &= \frac{\sum_{i,j,k}a_{ij}a_{jk}a_{ki}}{\sum_i d_i (d_i - 1)}, \;\;\; d_i = degree(v_i)
\end{align*}
\sphinxAtStartPar
which is a bit easier to implement programmatically.


\subsubsection{The path length describes how far two nodes are}
\label{\detokenize{representations/ch4/properties-of-networks:the-path-length-describes-how-far-two-nodes-are}}
\sphinxAtStartPar
How many bridges would we need to cross to get from Staten Island to Bronx? This concept relates directly to the concept of the \sphinxstyleemphasis{path length} in a network. A \sphinxstylestrong{path} between two nodes \(i\) and \(j\) is a sequence of edges which starts at node \(i\), and traverses through other nodes in the network until reaching node \(j\). Two nodes are described as \sphinxstylestrong{connected} if a path exists between them. The \sphinxstylestrong{path length} is the number of edges in the path. For instance, if we remember our network from the New York example, we could get from Staten Island to Bronx in two possible ways, indicated in green and blue in the following example:

\noindent\sphinxincludegraphics{{properties-of-networks_23_0}.png}

\sphinxAtStartPar
In this case, there are only two paths from SI to BX which do not visit the same node more than once, but in a larger network, there may be \sphinxstyleemphasis{many} possible paths from one node to another. For this reason, we will usually be interested in one particular path, the \sphinxstyleemphasis{shortest path}. The \sphinxstylestrong{shortest path} or \sphinxstylestrong{distance} between nodes \(i\) and \(j\) is the path with the smallest path length that connects nodes \(i\) and \(j\). In our example, the shortest path is indicated by the green edges, and the shortest path length is therefore three.  If it is not possible to get from node \(i\) to node \(j\) using edges of the network, the shortest path length is defined to be infinite. The shortest path between nodes \(i\) and \(j\) will often be abbreviated using the notation \(l_{ij}\).

\sphinxAtStartPar
A common summary statistic is to view the \sphinxstyleemphasis{distance matrix} \(L\), which is the \(n \times n\) matrix whose entries \(l_{ij}\) are the shortest path lengths between all pairs of nodes in the network. For our New York example, the distance matrix is:

\noindent\sphinxincludegraphics{{properties-of-networks_25_0}.png}

\sphinxAtStartPar
A common network statistic we can compute using the distance matrix is the \sphinxstyleemphasis{average shortest path length}. The average shortest path length \(l\) of a simple network is simply the average of all of the shortest paths between two distinct nodes \(i\) and \(j\) of the distance matrix:
\begin{align*}
    l &= \frac{1}{n(n - 1)}\sum_{i \neq j} l_{ij}
\end{align*}

\subsubsection{Network summary statistics can be misleading when comparing networks}
\label{\detokenize{representations/ch4/properties-of-networks:network-summary-statistics-can-be-misleading-when-comparing-networks}}
\sphinxAtStartPar
When we perform network machine learning, we want the data we are analyzing to be \sphinxstyleemphasis{sensitive} in the sense that, if two networks are \sphinxstyleemphasis{different} (we use the term \sphinxstyleemphasis{different} a little loosely here, but we will be more specific in a second!) we want the data to reflect that. Let’s say we had the following four networks:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.5222222222222223
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{properties-of-networks_27_1}.png}

\sphinxAtStartPar
As it turns out, all of these networks share the same number of nodes, the same network density, and the same clustering coefficient:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Network
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Network Density
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Clustering Coefficient
\\
\hline
\sphinxAtStartPar
Network 1
&
\sphinxAtStartPar
\(\frac{1}{3}\)
&
\sphinxAtStartPar
\(0.6\)
\\
\hline
\sphinxAtStartPar
Network 2
&
\sphinxAtStartPar
\(\frac{1}{3}\)
&
\sphinxAtStartPar
\(0.6\)
\\
\hline
\sphinxAtStartPar
Network 3
&
\sphinxAtStartPar
\(\frac{1}{3}\)
&
\sphinxAtStartPar
\(0.6\)
\\
\hline
\sphinxAtStartPar
Network 4
&
\sphinxAtStartPar
\(\frac{1}{3}\)
&
\sphinxAtStartPar
\(0.6\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
To conclude our discussion on network properties, we will turn to a final property of a network, known as a subnetwork, which will be useful as a pre\sphinxhyphen{}processing step for networks. The concept of a subnetwork will introduce the idea of a \sphinxstyleemphasis{connected network}, which is a network in which a path exists between all pairs of nodes in the network. Network machine learning methods may exhibit unexpected behavior when the network is not connected, so reducing the network to a connected component is often aa useful pre\sphinxhyphen{}processing step to prepare data.


\subsection{Subnetworks are subsets of larger networks}
\label{\detokenize{representations/ch4/properties-of-networks:subnetworks-are-subsets-of-larger-networks}}
\sphinxAtStartPar
When we think of an entire network, it is often useful to consider it in smaller bits. For instance, when we were looking at the clustering coefficient, we found it useful to break out the nodes \{BK, Q, BX, MH\} so we could count triplets:

\noindent\sphinxincludegraphics{{properties-of-networks_29_0}.png}

\sphinxAtStartPar
This portion of the network is called a \sphinxstyleemphasis{subnetwork}. A \sphinxstylestrong{subnetwork} is a network topology whose nodes and edges are \sphinxstyleemphasis{subsets} of the nodes and edges for another network topology. In this case, the  network toplogy of the New York example is \((\mathcal V, \mathcal E)\) defined by the sets:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The nodes \(\mathcal V\): \(\{SI, BK, Q, MH, BX\}\),

\item {} 
\sphinxAtStartPar
The edges \(\mathcal E\): \(\left\{(SI, BK), (BK, MH), (MH, Q), (MH, BX), (Q, BX)\right\}\).

\end{enumerate}

\sphinxAtStartPar
and the subnetwork is the network:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The nodes \(\mathcal V_s\): \(\{BK, Q, MH, BX\}\),

\item {} 
\sphinxAtStartPar
The edges \(\mathcal E_s\): \(\left\{(BK, MH), (MH, Q), (MH, BX), (Q, BX)\right\}\).

\end{enumerate}

\sphinxAtStartPar
As we can see, the subnetwork topology \((\mathcal V_s, \mathcal E_s)\) is such that every element in \(\mathcal V_s\) is an element of \(\mathcal V\), and therefore the nodes of the subnetwork are a subset of the nodes of the complete network. Further, every element in \(\mathcal E_s\) is an element of \(\mathcal E\), and therefore the edges of the subnetwork are a subset of the edges of the complete network. So the subnetwork toplogy \((\mathcal V_s, \mathcal E_s)\) is a subnetwork of the network topology \((\mathcal V, \mathcal E)\). This particular subnetwork can be described further as an \sphinxstylestrong{induced} subnetwork. A subnetwork of a network is \sphinxstylestrong{induced} by a set of vertices as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The nodes \(\mathcal V_s\) are a subset of the nodes of the network \(\mathcal V\),

\item {} 
\sphinxAtStartPar
The edges \(\mathcal E_s\) consist of \sphinxstyleemphasis{all} of the edges from the original network which are incident pairs of node in \(\mathcal V_s\).

\end{enumerate}

\sphinxAtStartPar
To see an example of a subnetwork which is \sphinxstyleemphasis{not} an induced subnetwork, we can consider a subnetwork which removes one of the edges that exist in the original network:

\noindent\sphinxincludegraphics{{properties-of-networks_31_0}.png}

\sphinxAtStartPar
A particular induced subnetwork that we will often be concerned with is known as the largest connected component (LCC).


\subsubsection{The largest connected component (LCC) is the largest subnetwork of connected nodes}
\label{\detokenize{representations/ch4/properties-of-networks:the-largest-connected-component-lcc-is-the-largest-subnetwork-of-connected-nodes}}
\sphinxAtStartPar
To define the largest connected component, we’ll modify our example slightly. Let’s say our network also includes the Boston area, and we have two new nodes, Boston (BO) and Cambridge (CA). Boston and Cambridge are incident several bridges between one another, so an edge exists between them. However, there are no bridges between boroughs of New York and the Boston area, so there are no edges from nodes in the Boston area to nodes in the New York area:

\noindent\sphinxincludegraphics{{properties-of-networks_33_0}.png}

\sphinxAtStartPar
The entire network topology can be described by the sets:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathcal V = \{SI, MH, BK, BX, Q, CA, BO\}\),

\item {} 
\sphinxAtStartPar
\(\mathcal E = \{(SI, BK), (MH, BK), (MH, Q), (MH, BX), (MX, Q), (CA, BO)\}\).

\end{enumerate}

\sphinxAtStartPar
Notice that we have two distinct sets of nodes, those of New York and those of Boston, which are \sphinxstyleemphasis{only} connected amongst one another. Formally, these two sets of nodes can be described as inducing \sphinxstyleemphasis{connected components} of the network topology \((\mathcal V, \mathcal E)\). A \sphinxstylestrong{connected component} is an induced subnetwork in which any two nodes are connected to each other by a path through the network. The two connected components are the New York induced subnetwork:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The nodes \(\mathcal V_N\): \(\{SI, BK, Q, MH, BX\}\),

\item {} 
\sphinxAtStartPar
The edges \(\mathcal E_N\): \(\left\{(SI, BK), (BK, MH), (MH, Q), (MH, BX), (Q, BX)\right\}\).

\end{enumerate}

\sphinxAtStartPar
and the Boston induced subnetwork:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The nodes \(\mathcal V_B\): \(\{CA, BO\}\),

\item {} 
\sphinxAtStartPar
The edges \(\mathcal E_B\): \(\left\{(CA, BO)\right\}\).

\end{enumerate}

\sphinxAtStartPar
which we can represent visually here:

\noindent\sphinxincludegraphics{{properties-of-networks_35_0}.png}

\sphinxAtStartPar
The \sphinxstylestrong{largest connected component} (LCC) of a network is the connected component with the most nodes. In our example, the New York connected component has five nodes, whereas the Bosston connected component has two nodes. Therefore, the New York connected component is the LCC.


\section{Regularization}
\label{\detokenize{representations/ch4/regularization:regularization}}\label{\detokenize{representations/ch4/regularization::doc}}
\sphinxAtStartPar
In practice, many networks we will encounter in network machine learning will \sphinxstyleemphasis{not} be simple networks. As we discussed in the preceding discussion, many of the techniques we discuss will be just fine to use with weighted networks. Unfortunately, real world networks are often extremely noisy, and so the analysis of one real world network might not generalize very well to a similar real world network. For this reason, we turn to \sphinxstyleemphasis{regularization}. \sphinxstylestrong{Regularization} is defined as, “the process of adding information in order to solve an ill\sphinxhyphen{}posed problem or to prevent overfitting.” In network machine learning, what this usually will entail is modifying the network (or networks) themselves to allow better generalization of our statistical inference to new datasets. For each section, we’ll pose an example, a simulation, and code for how to implement the desired regularization approach. It is important to realize that you might use several of these techniques simultaneously in practice, or you might have a reason to use these techniques that go outside of our working examples.

\sphinxAtStartPar
To start this section off, we’re going to introduce an example that’s going to be fundamental in many future sections we see in this book. We have a group of \(50\) local students who attend a school in our area. The first \(25\) of the students polled are athletes, and thhhe second \(25\) of the students polled are in marching band. We want to analyze how good of friends the students are, and to do so, we will use network machine learning. The nodes of the network will be the students. Next, we will describe how the two networks are collected:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Activity/Hobby Network: To collect the first network, we ask each student to select from a list of \(50\) school activities and outside hobbies that they enjoy. For a pair of students \(i\) and \(j\), the weight of their interest alignment will be a score between \(0\) and \(50\) indicating how many activities or hobbies that they have in common. We will refer to this network as the common interests network. This network is obviously undirected, since if student \(i\) shares \(x\) activities or hobbies with student \(j\), then student \(j\) also shares \(x\) activities or hobbies with student \(i\). This network is weighted, since the score is between \(0\) and \(50\). Finally, this network is loopless, because it would not make sense to look at the activity/hobby alignment of a student with themself, since this number would be largely uninformative as every student would have perfect alignment of activities and hobbies with him or herself.

\item {} 
\sphinxAtStartPar
Friendship Network: To collect the second network, we ask each student to rate how good of friends they are with other students, on a scale from \(0\) to \(1\). A score of \(0\) means they are not friends with the student or do not know the student, and a score of \(1\) means the student is their best friend. We will refer to this network as the friendship network. This nework is clearly directed, since two students may differ on their understanding of how good of friends they are. This network is weighted, since the score is between \(0\) and \(1\). Finally, this network is also loopless, because it would not make sense to ask somebody how good of friends they are with themself.

\end{enumerate}

\sphinxAtStartPar
Our scientific question of interest is how well activities and hobbies align with perceived notions of friendship. We want to use the preceding networks to learn about a hypothetical third network, a network whose nodes are identical to the two networks above, but whose edges are whether the two individuals are friends (or not) on facebook. To answer this question, we have quite the job to do to make our networks better suited to the task! We begin by simulating some example data, shown below as adjacency matrix heatmaps:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{wtargsa} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{09}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{02}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}
          \PYG{p}{[}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{02}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{06}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{wtargsf} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{b}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{b}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}
          \PYG{p}{[}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{b}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{b}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} human brain network}
\PYG{n}{A\PYGZus{}activity} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{l+m+mi}{25}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{wt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{binomial}\PYG{p}{,} \PYG{n}{wtargs}\PYG{o}{=}\PYG{n}{wtargsa}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} alien brain network}
\PYG{n}{A\PYGZus{}friend} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{l+m+mi}{25}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{wt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{beta}\PYG{p}{,} \PYG{n}{wtargs}\PYG{o}{=}\PYG{n}{wtargsf}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{regularization_2_0}.png}


\subsection{Regularization of the Nodes}
\label{\detokenize{representations/ch4/regularization:regularization-of-the-nodes}}

\subsubsection{The Largest Connected Component is the largest subnetwork of connected nodes}
\label{\detokenize{representations/ch4/regularization:the-largest-connected-component-is-the-largest-subnetwork-of-connected-nodes}}
\sphinxAtStartPar
We have already learned about the LCC in the preceding section, so we won’t cover the in\sphinxhyphen{}depth, but it is important to realize that this is a node regularization technique.


\subsubsection{Degree trimming removes nodes with low degree}
\label{\detokenize{representations/ch4/regularization:degree-trimming-removes-nodes-with-low-degree}}
\sphinxAtStartPar
Let’s imagine that in our friendship network, there were an additional three athlete students from a nearby school. Perhaps one of these students had a friend in the first school he met at a sports camp, so these students are not a separate component of the network entirely. Even though these students are not \sphinxstyleemphasis{totally} disconnected from the rest of the network entirely, and therefore would not be removed by computing the LCC, their presence in our analysis might still lead to stability issues in future network machine learning tasks. For this reason, it may be advantageous to remove nodes whose degrees are much different from the other nodes in the network.


\subsection{Regularizing the Edges}
\label{\detokenize{representations/ch4/regularization:regularizing-the-edges}}

\subsubsection{Symmetrizing the network gives us undirectedness}
\label{\detokenize{representations/ch4/regularization:symmetrizing-the-network-gives-us-undirectedness}}
\sphinxAtStartPar
If we wanted to learn from the friendship network about whether two people were friends on facebook, a reasonable first place to start might be to \sphinxstyleemphasis{symmetrize} the friendship network. The facebook network is \sphinxstyleemphasis{undirected}, which means that if a student \(i\) is friends on facebook with student \(j\), then student \(j\) is also friends with student \(i\). On the other hand, as we learned above, the friendship network was directed. Since our question of interest is about an undirected network but the network we have is directed, it might be useful if we could take the directed friendship network and learn an undirected network from it. This relates directly to the concept of \sphinxstyleemphasis{interpretability}, in that we need to represent our friendship network in a form that will produce an answer or us about our facebook network which we can understand.

\sphinxAtStartPar
Another reason we might seek to symmetrize the friendship network is that we might think that asymmetries that exist in the network are just \sphinxstyleemphasis{noise}. We might assume that the adjacency entries \(a_{ij}\) and \(a_{ji}\) relate to one another, so together they might be able to produce a single summary number that better summarizes their relationship all together.

\sphinxAtStartPar
Remember that in a symmetric network, \(a_{ij} = a_{ji}\), so in an \sphinxstyleemphasis{asymmetric} network, \(a_{ij} \neq a_{ji}\). To symmetrize the friendship network, what we want is a \sphinxstyleemphasis{new} adjacency value, which we will call \(w_{ij}\), which will be a function of \(a_{ij}\) and \(a_{ji}\). Then, we will construct a new adjacency matrix \(A'\), where each entry \(a_{ij}'\) \sphinxstyleemphasis{and} \(a_{ji}'\) are set equal to \(w_{ij}\).  The little apostrophe just signifies that this is a potentially different value than either \(a_{ij}\) or \(a_{ji}\). Note that by construction, \(A'\) is in fact symmetric, because \(a_{ij}' = a_{ji}'\) due to how we built \(A'\).


\paragraph{Ignoring a “triangle” of the adjacency matrix}
\label{\detokenize{representations/ch4/regularization:ignoring-a-triangle-of-the-adjacency-matrix}}
\sphinxAtStartPar
The easiest way to symmetrize a network \(A\) is to just ignore part of it entirely. In the adjacency matrix \(A\), you will remember that we have an upper and a lower triangular part of the matrix:
\begin{align*}
    A &= \begin{bmatrix}
        a_{11} & \color{red}{a_{12}} & \color{red}{...} & \color{red}{a_{1n}} \\
        \color{blue}{a_{21}} & \ddots & \color{red}{\ddots} & \color{red}{\vdots} \\
        \color{blue}{\vdots} &\color{blue}{\ddots} &\ddots & \color{red}{a_{n-1, n}}\\
        \color{blue}{a_{n1}} & \color{blue}{...} & \color{blue}{a_{n,n-1}} & a_{nn}
    \end{bmatrix},
\end{align*}
\sphinxAtStartPar
The entries which are listed in red are called the \sphinxstylestrong{upper right triangle of the adjacency matrix above the diagonal}. You will notice that for the entries in the upper right triangle of the adjacency matrix, \(a_{ij}\) is such that \(j\) is \sphinxstyleemphasis{always} greater than \(i\). Similarly, the entries which are listed in blue are called the \sphinxstylestrong{lower left triangle of the adjacency matrix below the diagonal}. In the lower left triangle, \(i\) is \sphinxstyleemphasis{always} greater than \(j\). These are called \sphinxstyleemphasis{triangles} because of the shape they make when you look at them in matrix form: notice, for instance, that in the upper right triangle, we have a triangle with three corners of values: \(a_{12}\), \(a_{1n}\), and \(a_{n-1, n}\).

\sphinxAtStartPar
So, how do we ignore a triangle all\sphinxhyphen{}together? Well, it’s really quite simple! We will visually show how to ignore the lower left triangle of the adjacency matrix. We start by forming a triangle matrix, \(\Delta\), as follows:
\begin{align*}
    \Delta &= \begin{bmatrix}
        0 & \color{red}{a_{12}} & \color{red}{...} & \color{red}{a_{1n}} \\
        \color{blue}{0} & \ddots & \color{red}{\ddots} & \color{red}{\vdots} \\
        \color{blue}{\vdots} &\color{blue}{\ddots} &\ddots & \color{red}{a_{n-1, n}}\\
        \color{blue}{0} & \color{blue}{...} & \color{blue}{0} & 0
    \end{bmatrix},
\end{align*}
\sphinxAtStartPar
Notice that this matrix \sphinxstyleemphasis{keeps} all of the upper right triangle of the adjacency matrix above the diagonal the same as in the matrix \(A\), but replaces the lower left triangle of the adjacency matrix below the diagonal and the diagonal with \(0\)s. Notice that the transpose of \(\Delta\) is the matrix:
\begin{align*}
    \Delta^\top &= \begin{bmatrix}
        0 & \color{blue}{0} & \color{blue}{...} &\color{blue}{0}\\
        \color{red}{a_{12}}& \ddots & \color{blue}{\ddots} & \color{blue}{\vdots} \\
        \color{red}{\vdots}&\color{red}{\ddots} & \ddots & \color{blue}{0} \\
        \color{red}{a_{1n}}&\color{red}{...} &\color{red}{a_{n-1,n}} & 0
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
So when we add the two together, we get this:
\begin{align*}
    \Delta + \Delta^\top &= \begin{bmatrix}
        0 & \color{red}{a_{12}} & \color{red}{...} & \color{red}{a_{1n}} \\
        \color{red}{a_{12}} & \ddots & \color{red}{\ddots} & \color{red}{\vdots} \\
        \color{red}{\vdots}&\color{red}{\ddots} &\ddots & \color{red}{a_{n-1, n}}\\
        \color{red}{a_{1n}}&\color{red}{...} &\color{red}{a_{n-1,n}} & 0
    \end{bmatrix},
\end{align*}
\sphinxAtStartPar
We’re almost there! We just need to add back the diagonal of \(A\), which we will do using the matrix \(diag(A)\) which has values \(diag(A)_{ii} = a_{ii}\), and \(diag(A)_{ij} = 0\) for any \(i \neq j\):
\begin{align*}
    A' &= \Delta + \Delta^\top + diag(A) = \begin{bmatrix}
        a_{11} & \color{red}{a_{12}} & \color{red}{...} & \color{red}{a_{1n}} \\
        \color{red}{a_{12}} & \ddots & \color{red}{\ddots} & \color{red}{\vdots} \\
        \color{red}{\vdots}&\color{red}{\ddots} &\ddots & \color{red}{a_{n-1, n}}\\
        \color{red}{a_{1n}}&\color{red}{...} &\color{red}{a_{n-1,n}} & a_{nn}
    \end{bmatrix},
\end{align*}
\sphinxAtStartPar
Which leaves \(A'\) to be a matrix consisting \sphinxstyleemphasis{only} of entries which were in the upper right triangle of \(A\). \(A'\) is obviously symmetric, because \(a_{ij}' = a_{ji}'\) for all \(i\) and \(j\). Since the adjacency matrix is symmetric, the network \(A'\) represents is undirected.

\sphinxAtStartPar
So what does this mean in terms of the network itself? What this means is that the network originally had edge weights \(a_{ij}\), where \(a_{ij}\) might not be equal to \(a_{ji}\). This means student \(i\) might perceive their friendship with student \(j\) as being stronger or weaker than student \(j\) perceived about student \(i\). What we did here was we basically just ignored any perceived friendships \(a_{ji}\) when \(j\) exceeded \(i\) (the lower left triangle), and simply “replaced” that perceived friendship with the corresponding entry \(a_{ij}\) in the upper right triangle of the adjacency matrix. This produced for us a single friendship strength \(a_{ij}'\) where \(a_{ij}' = a_{ji}'\).

\sphinxAtStartPar
In graspologic, we can implement this as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{symmetrize}

\PYG{c+c1}{\PYGZsh{} symmetrize with upper right triangle}
\PYG{n}{A\PYGZus{}friend\PYGZus{}upright\PYGZus{}sym} \PYG{o}{=} \PYG{n}{symmetrize}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{triu}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{regularization_8_0}.png}

\sphinxAtStartPar
Likewise, we can lower\sphinxhyphen{}left symmetrize as well:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} symmetrize with lower left triangle}
\PYG{n}{A\PYGZus{}friend\PYGZus{}lowleft\PYGZus{}sym} \PYG{o}{=} \PYG{n}{symmetrize}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tril}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{regularization_11_0}.png}


\paragraph{Taking a function of the two values}
\label{\detokenize{representations/ch4/regularization:taking-a-function-of-the-two-values}}
\sphinxAtStartPar
There are many other ways we can also take a function of \(a_{ij}\) and \(a_{ji}\) to get a symmetric matrix. One is to just average the two. That is, we can let the matrix \(A'\) be the matrix with entries \(a'_{ij} = \frac{a_{ij} + a_{ji}}{2}\) for all \(i\) and \(j\). In matrix form, this operation looks like this:
\begin{align*}
    A' &= \frac{1}{2} (A + A^\top) \\
    &= \frac{1}{2}\left(\begin{bmatrix}
        a_{11} & ... & a_{1n} \\
        \vdots & \ddots & \vdots \\
        a_{n1} & ... & a_{nn}
    \end{bmatrix} + \begin{bmatrix}
        a_{11} & ... & a_{n1} \\
        \vdots & \ddots & \vdots \\
        a_{1n} & ... & a_{nn}
    \end{bmatrix}\right)\\
    &= \begin{bmatrix}
        \frac{1}{2}(a_{11} + a_{11}) & ... & \frac{1}{2}(a_{1n} + a_{n1}) \\
        \vdots & \ddots & \vdots \\
        \frac{1}{2} (a_{n1} + a_{1n}) & ... & \frac{1}{2}(a_{nn} + a_{nn})
    \end{bmatrix} \\
    &= \begin{bmatrix}
        a_{11} & ... & \frac{1}{2}(a_{1n} + a_{n1}) \\
        \vdots & \ddots & \vdots \\
        \frac{1}{2} (a_{n1} + a_{1n}) & ... & a_{nn}
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
As we can see, for all of the entries, \(a'_{ij} = \frac{1}{2} (a_{ij} + a_{ji})\), and also \(a_{ji}' = \frac{1}{2}(a_{ji} + a_{ij})\). These quantities are the same, so \(a_{ij}' = a_{ji}'\), and \(A'\) is symmetric. As the adjacency matrix is symmetric, the network that \(A'\) represents is undirected.

\sphinxAtStartPar
Remember that the asymmetry in the friendship network means student \(i\) might perceive their friendship with student \(j\) as being stronger or weaker than student \(j\) perceived about student \(i\). What we did here was instead of just arbitrarily throwing one of those values away, we said that their friendship might be better indicated by averaging the two values. This produced for us a single friendship strength \(a_{ij}'\) where \(a_{ij}' = a_{ji}'\).

\sphinxAtStartPar
We can implement this in graspologic as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} symmetrize with averaging}
\PYG{n}{A\PYGZus{}friend\PYGZus{}avg\PYGZus{}sym} \PYG{o}{=} \PYG{n}{symmetrize}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{avg}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{regularization_14_0}.png}

\sphinxAtStartPar
We will use the friendship network symmetrized by averaging in several of the below examples, which we will call the “undirected friendship network”.


\subsubsection{Diagonal augmentation}
\label{\detokenize{representations/ch4/regularization:diagonal-augmentation}}
\sphinxAtStartPar
In our future works with network machine learning, we will come across numerous techniques which operate on adjacency matrices which are \sphinxstyleemphasis{positive semi\sphinxhyphen{}definite}. This word doesn’t mean a whole lot to us for network machine learning, but it has a big implication when we try to use algorithms on many of our networks. Remember that when we have a loopless network, a common practice in network science is to set the diagonal to zero. What this does is it leads to our adjacency matrices being \sphinxstyleemphasis{indefinite} (which means, \sphinxstyleemphasis{not} positive semi\sphinxhyphen{}definite). For us, this means that many network machine learning techniques simply cannot operate on these adjacency matrices. However, as we mentioned before, these entries are not actually zero, but simply \sphinxstyleemphasis{do not exist} and we just didn’t have a better way to represent them. Or do we?

\sphinxAtStartPar
\sphinxstyleemphasis{Diagonal augmentation} is a procedure for imputing the diagonals of adjacency matrices for loopless networks. This gives us “placeholder” values that do not cause this issue of indefiniteness, and allow our network machine learning techniques to still work. Remember that for a simple network, the adjacency matrix will look like this:
\begin{align*}
    A &= \begin{bmatrix}
        0 & a_{12} & ... & a_{1n} \\
        a_{21}& \ddots & & \vdots \\
        \vdots & & \ddots & a_{n-1, n} \\
        a_{n1} &...& a_{n, n-1} & 0
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
What we do is impute the diagonal entries using the \sphinxstyleemphasis{fraction of possible edges which exist} for each node. This quantity is simply the node degree \(d_i\) (the number of edges which exist for node \(i\)) divided by the number of possible edges node \(i\) could have (which would be node \(i\) connected to each of the other \(n-1\) nodes). Remembering that the degree matrix \(D\) is the matrix whose diagonal entries are the degrees of each node, the diagonal\sphinxhyphen{}augmented adjacency matrix is given by:
\begin{align*}
    A' &= A + \frac{1}{n-1}D = \begin{bmatrix}
        \frac{d_1}{n-1} & a_{12} & ... & a_{1n} \\
        a_{21}& \ddots & & \vdots \\
        \vdots & & \ddots & a_{n-1, n} \\
        a_{n1} &...& a_{n, n-1} & \frac{d_n}{n-1}
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
When the matrices are directed or weighted, the computation is a little different, but fortunately \sphinxcode{\sphinxupquote{graspologic}} will handle this for us. Let’s see how we would apply this to the directed friendship network:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{augment\PYGZus{}diagonal}

\PYG{n}{A\PYGZus{}friend\PYGZus{}aug} \PYG{o}{=} \PYG{n}{augment\PYGZus{}diagonal}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{regularization_18_0}.png}

\sphinxAtStartPar
As we can see, the diagonal\sphinxhyphen{}augmented friendship network and the original directed friendship network differ only in that the diagonals of the diagonal\sphinxhyphen{}augmented friendship network are non\sphinxhyphen{}zero.


\subsubsection{Lowering edge bias}
\label{\detokenize{representations/ch4/regularization:lowering-edge-bias}}
\sphinxAtStartPar
As you are probably aware, in all of machine learning, we are always concerned with the \sphinxstyleemphasis{bias/variance tradeoff}. The \sphinxhref{https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\#:~:text=Bias\%20is\%20the\%20simplifying\%20assumptions,change\%20given\%20different\%20training\%20data.}{\sphinxstylestrong{bias/variance tradeoff}} is an unfortunate side\sphinxhyphen{}effect that concerns how well a learning technique will generalize to new datasets.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Bias} is a simplifying assumption of a model that makes the task easier to estimate. For instance, if we have a friendship network, we might make simplifying assumptions, such as an assumption that two athletes frorm different sports have an equally likely chance of being friends with a member of the band.

\item {} 
\sphinxAtStartPar
On the other hand, the \sphinxstylestrong{variance} is the degree to which the an estimate of a task will change when given new data. An assumption that if a player is a football player he has a higher chance of being friends with a band member might make sense given that the band performs at football games.

\end{enumerate}

\sphinxAtStartPar
The “trade\sphinxhyphen{}off” is that these two factors tend to be somewhat at odds, in that raising the bias tends to lower the variance, and vice\sphinxhyphen{}versa:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High bias, but low variance}: Whereas a lower variance model might be better suited to the situation when the data we expect to see is noisy, it might not as faithfully represent the underlying dynamics we think the network possesses. A low variance model might ignore that athletes might have a different chance of being friends with a band member based on their sport all together. This means that while we won’t get the student relationships \sphinxstyleemphasis{correct}, we might still be able to get a reasonable estimate that we think is not due to overfitting.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Low bias, but high variance}: Whereas a low bias model might more faithfully model true relationships in our training data, it might fit our training data a little \sphinxstyleemphasis{too} well. Fitting the training data too well is a problem known as \sphinxstylestrong{overfitting}. If we only had three football team members and tried to assume that football players were better friends with band members, we might not be able to well approximate this relationship because of how few individuals we have who reflect this situation.

\end{enumerate}

\sphinxAtStartPar
Here, we show several strategies to reduce the bias due to edge weight noise in network machine learning.


\paragraph{Thresholding converts weighted networks to binary networks}
\label{\detokenize{representations/ch4/regularization:thresholding-converts-weighted-networks-to-binary-networks}}
\sphinxAtStartPar
The simplest way to reduce edge bias is the process of \sphinxstyleemphasis{thresholding}. Through thresholding, we choose a threshold value, \(t\). Next, we simply set all of the entries of the adjacency matrix less than or equal to \(t\) to zero, and the entries of the adjacency matrix above \(t\) to one.

\sphinxAtStartPar
Some of the most common approaches to choosing this threshold are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Set the threshhold to zero: set all non\sphinxhyphen{}zero weighted entries to one, and all zero\sphinxhyphen{}weight entries to zero. This is most commonly used when we see zero\sphinxhyphen{}inflated networks, or networks where the adjacency matrix takes values that are either zero or some quantity different from one,

\item {} 
\sphinxAtStartPar
Set te threshold to be the mean: set all values below the mean edge\sphinxhyphen{}weight to zero, and all values above the mean edge\sphinxhyphen{}weight to one,

\item {} 
\sphinxAtStartPar
Use a quantile: A quantile is a percentile divided by \(100\). In this strategy, we identify a target quantile of the edge\sphinxhyphen{}weight distribution. What this means is that we are selecting the lowest \sphinxstyleemphasis{fraction} of the edge\sphinxhyphen{}weights (where that fraction is the quantile that we choose) and setting these edges to \(0\), and selecting the remaining edges to \(1\). If we select a quantile of \(0.5\), this means that we take the smallest \(50\%\) of edges and set them to zero, and the largest \(50\%\) of edges and set them to \(1\).

\end{enumerate}

\sphinxAtStartPar
We will show how to use the percentile approach to binarization, with both our activity/hobby and friendship networks. We will threshold using the edge\sphinxhyphen{}weight in the \(50^{th}\) percentile. Our example networks of activity/hobby and friendship were loopless, as you could see above. Remember as we learned in the preceding section, that if the network itself is loopless, the diagonal entries simply \sphinxstyleemphasis{do not exist}; \(0\) is simply a commonly used placeholder. For this reason, when we compute percentiles of edge\sphinxhyphen{}weights, we need to \sphinxstyleemphasis{exclude the diagonal}. Further, since this network is undirected, we also need to restrict our attention to one triangle of the corresponding adjacency matrix. We choose the upper\sphinxhyphen{}right triangle arbitrarily, as the adjacency matrix’s symmetry means the upper\sphinxhyphen{}right triangle and lower\sphinxhyphen{}right triangle have identical edge\sphinxhyphen{}weight distributions. We begin by using this procedure on the friendship network. To complete this processs, we first look at the edge\sphinxhyphen{}weight distribution for the friendship network, which is shown below, and identified the edge\sphinxhyphen{}weight at the \(0.5\) quartile:

\noindent\sphinxincludegraphics{{regularization_23_0}.png}

\sphinxAtStartPar
The \(0.5\) quantile, it turns out, is about \(0.3\). This is because about \(50\%\) of the edges are less than this threshold, and about \(50\%\) of the edges are greater than this threshold. There is exactly one more edge in less than or equal to \(t\), because this edge is exactly the median (an alternative name for the \(0.5\) quartile) value:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of edges less than or equal to t: 613
Number of edges greater than or equal to t: 612
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we will assign the edges less than or equal to \(t\) to zero, and the edges greater than or equal to \(t\) to one:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{NameError}\PYG{g+gWhitespace}{                                 }Traceback (most recent call last)
\PYG{o}{\PYGZlt{}}\PYG{n}{ipython}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{input}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14}\PYG{o}{\PYGZhy{}}\PYG{n}{fb4cba753867}\PYG{o}{\PYGZgt{}} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{A\PYGZus{}friend\PYGZus{}thresh} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend\PYGZus{}avg\PYGZus{}sym}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} copy the network over}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} 
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3} \PYG{c+c1}{\PYGZsh{} threshold using t}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{n}{A\PYGZus{}friend\PYGZus{}thresh}\PYG{p}{[}\PYG{n}{A\PYGZus{}friend\PYGZus{}thresh} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{t}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5} \PYG{n}{A\PYGZus{}friend\PYGZus{}thresh}\PYG{p}{[}\PYG{n}{A\PYGZus{}friend\PYGZus{}thresh} \PYG{o}{\PYGZgt{}} \PYG{n}{t}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n+ne}{NameError}: name \PYGZsq{}copy\PYGZsq{} is not defined
\end{sphinxVerbatim}

\sphinxAtStartPar
Since the friendship network is now undirected (we made the adjacency matrix symmetric through averaging), loopless (because we defined it that way), and binary (because we thresholded the edges), we have now turned it into a \sphinxstyleemphasis{simple} network! Great job. Next, we will discuss an important property as to \sphinxstyleemphasis{why} thresholding using a quantile tends to be a very common tactic to obtaining simple networks from networks which are undirected and loopless. Remember that in the last section, we defined the network density for a simple network as:
\begin{align*}
    density(A) &= \frac{2\sum_{j > i}a_{ij}}{n(n - 1)}.
\end{align*}
\sphinxAtStartPar
Since we have thresholded at the \(50^{th}\) percentile for the symmetric friendship network, this means that about \(50\) percent of the possible edges will exist (the \sphinxstyleemphasis{largest} \(50\) percent of edges), and \(50\) percent of the possible edges will not exist (the \sphinxstyleemphasis{smallest} \(50\) percent of edges). Remembering that the number of possible edges was \(\frac{1}{2}n(n - 1)\) for an undirected network, this means that \(\sum_{j > i}a_{ij}\) must be half of \(\frac{1}{2}n(n - 1)\), or \(\frac{1}{4}n(n - 1)\). Therefore:
\begin{align*}
    density(A) &= \frac{2\sum_{j > i}a_{ij}}{n(n - 1)}, \\
    &= \frac{2\cdot \frac{1}{4}n(n - 1)}{n(n - 1)},\;\;\;\sum_{j > i}a_{ij} = \frac{1}{4}n(n - 1) \\
    &= 0.5.
\end{align*}
\sphinxAtStartPar
So when we threshold the network at a quantile \(t\), we end up with a network of density also equal to \(t\)! Let’s confirm that this is the case for our symmetric friendship network:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{is\PYGZus{}unweighted}\PYG{p}{,} \PYG{n}{is\PYGZus{}loopless}\PYG{p}{,} \PYG{n}{is\PYGZus{}symmetric}

\PYG{k}{def} \PYG{n+nf}{simple\PYGZus{}network\PYGZus{}dens}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} make sure the network is simple}
    \PYG{k}{if} \PYG{p}{(}\PYG{o+ow}{not} \PYG{n}{is\PYGZus{}unweighted}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)} \PYG{o+ow}{or} \PYG{p}{(}\PYG{o+ow}{not} \PYG{n}{is\PYGZus{}loopless}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)} \PYG{o+ow}{or} \PYG{p}{(}\PYG{o+ow}{not} \PYG{n}{is\PYGZus{}symmetric}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{TypeError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Network is not simple!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} count the non\PYGZhy{}zero entries in the upper\PYGZhy{}right triangle}
    \PYG{c+c1}{\PYGZsh{} for a simple network X}
    \PYG{n}{nnz} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{triu}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} number of nodes}
    \PYG{n}{n} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{c+c1}{\PYGZsh{} number of possible edges is 1/2 * n * (n\PYGZhy{}1)}
    \PYG{n}{poss\PYGZus{}edges} \PYG{o}{=} \PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{n}{n}\PYG{o}{*}\PYG{p}{(}\PYG{n}{n}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{nnz}\PYG{o}{/}\PYG{n}{poss\PYGZus{}edges}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Network Density: }\PYG{l+s+si}{\PYGZob{}:.3f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{simple\PYGZus{}network\PYGZus{}dens}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend\PYGZus{}thresh}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This is desirable for network machine learning because many network properties (such as the summary statistics we have discussed so far, and numerous other properties we will discuss in later chapters) can vary when the network density changes. This means that a network of a different density might have a higher clustering coefficient than a network of a lower density simply due to the fact that its density is higher (and therefore, there are more opportunities for closed triangles because each node has more connections). This means that when we threshold groups of networks and compare them, thresholding using a quantile will be very valuable.

\sphinxAtStartPar
Note that a common pitfall you might run into with thresholding (and the broader class of techniques known as \sphinxstyleemphasis{sparsification} approaches) that rely on quantiles occurs when a weighted network can only take non\sphinxhyphen{}negative edge\sphinxhyphen{}weights. This corresponds to a network with an adjacency matrix \(A\) where every \(a_{ij}\) is greater than or equal to \(0\). In this case, one must be careful to choose a threshold which is not zero. Let’s consider a network were \(60\%\) of the entries are zeros, and \(40\%\) of the entries take a random value between \(5\) and \(10\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{er\PYGZus{}nm}

\PYG{c+c1}{\PYGZsh{} 10 nodes}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{c+c1}{\PYGZsh{} total number of edges is 40\PYGZpc{} of the number of possible edges}
\PYG{c+c1}{\PYGZsh{} 1/2 * n * (n\PYGZhy{}1)}
\PYG{n}{m} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{n}{n}\PYG{o}{*}\PYG{p}{(}\PYG{n}{n}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{er\PYGZus{}nm}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{wt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{,} \PYG{n}{wtargs}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{low}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{high}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If we threshold at the \(50^{th}\) percentile, since \(60\) percent of the edges do not exist, then the \(50^{th}\) percentile is still just zero:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} use the quantile function to obtain the threshold}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{quantile}\PYG{p}{(}\PYG{n}{A}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{triu}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,} \PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{q}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} quantile = percentile / 100}
\PYG{n}{A\PYGZus{}thresh} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} copy the network over}

\PYG{c+c1}{\PYGZsh{} threshold using t}
\PYG{n}{A\PYGZus{}thresh}\PYG{p}{[}\PYG{n}{A\PYGZus{}thresh} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{t}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{A\PYGZus{}thresh}\PYG{p}{[}\PYG{n}{A\PYGZus{}thresh} \PYG{o}{\PYGZgt{}} \PYG{n}{t}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Threshold for 50th percentile: }\PYG{l+s+si}{\PYGZob{}:d\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
And we don’t actually end up with a network having a density of \(0.5\), but rather, the same as the fraction of non\sphinxhyphen{}zero edges in the original network (which was \(40\%\), or \(0.4\)):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dens} \PYG{o}{=} \PYG{n}{simple\PYGZus{}network\PYGZus{}dens}\PYG{p}{(}\PYG{n}{A\PYGZus{}thresh}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
So the take\sphinxhyphen{}home message is that we need to be careful that if we want to conclude that two percentile\sphinxhyphen{}thresholded networks have the same network density (equal to the percentile we thresholded at), that we have enough non\sphinxhyphen{}zero entries to threshold with across both (or all) of the networks.


\paragraph{Sparsification removes potentially spurious low\sphinxhyphen{}weight edges}
\label{\detokenize{representations/ch4/regularization:sparsification-removes-potentially-spurious-low-weight-edges}}
\sphinxAtStartPar
The next simplest edge\sphinxhyphen{}weight regularization technique is called \sphinxstyleemphasis{sparsificiation}. Remember that our undirected friendship network looked like this:

\sphinxAtStartPar
Notice that for a \sphinxstyleemphasis{lot} of the off\sphinxhyphen{}diagonal entries, many of the values are really tiny compared to the maximum value in the network which is almost \(1\). What if the way we measured these edges was very sensitive to high values, but had trouble discerning whether a value was actually zero, or was just really small?

\sphinxAtStartPar
For this particular situation, we turn to \sphinxstyleemphasis{sparsification}. Through sparsification, we proceed very similar to thresholding like we did above. Remember that we chose a threshold, \(t\), and first set all adjacency values less than or equal to \(t\) to zero. Now, we’re done! We simply skip the step of setting values greater than \(t\) to one. Let’s try an example where we take the friendship network, and sparsify the network using the \(0.7\) quantile. Note that this will lead to the smallest \(70\) percent of edges to take the value of zero, and the largest \(30\) percent of edges will keep their original edge\sphinxhyphen{}weights:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{q}\PYG{o}{=}\PYG{l+m+mf}{0.7}  \PYG{c+c1}{\PYGZsh{} the quantile to sparsify with}

\PYG{c+c1}{\PYGZsh{} use the quantile function to obtain the threshold}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{quantile}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend\PYGZus{}avg\PYGZus{}sym}\PYG{p}{[}\PYG{n}{upper\PYGZus{}tri\PYGZus{}non\PYGZus{}diag\PYGZus{}idx}\PYG{p}{]}\PYG{p}{,} \PYG{n}{q}\PYG{o}{=}\PYG{n}{q}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} quantile = percentile / 100}
\PYG{n}{A\PYGZus{}friend\PYGZus{}sparse} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend\PYGZus{}avg\PYGZus{}sym}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} copy the network over}

\PYG{c+c1}{\PYGZsh{} sparsify using t}
\PYG{n}{A\PYGZus{}friend\PYGZus{}sparse}\PYG{p}{[}\PYG{n}{A\PYGZus{}friend\PYGZus{}sparse} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{t}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}

\sphinxAtStartPar
Notice that many of the small entries in the off\sphinxhyphen{}diagonal areas now have a value of zero. Again, we have the same pitfalls for sparsification as we did with thresholding, where if the network takes only non\sphinxhyphen{}negative edge weights and the percentile we choose corresponds to a threshold of zero, we might not actually end up changing anything.


\paragraph{Edge\sphinxhyphen{}weight normalization}
\label{\detokenize{representations/ch4/regularization:edge-weight-normalization}}
\sphinxAtStartPar
With weighted networks, it is often the case that we might want to reshape the distributions of edge\sphinxhyphen{}weights in our networks to highlight particular properties. Notice that the edge\sphinxhyphen{}weights for our human networks take values between \(0\) and \(1\), but for our alien network take values between \(0\) and almost \(40\). How can we possibly compare between these two networks when the edge\sphinxhyphen{}weights take such different ranges of values? We turn to standardization, which allows us to place values from different networks on the same scale.


\subparagraph{\protect\(z\protect\)\sphinxhyphen{}scoring standardizes edge weights using the normal distribution}
\label{\detokenize{representations/ch4/regularization:z-scoring-standardizes-edge-weights-using-the-normal-distribution}}
\sphinxAtStartPar
The first approach to edge\sphinxhyphen{}weight standardization is known commonly as \(z\)\sphinxhyphen{}scoring. Suppose that \(A\) is the adjacency matrix, with entries \(a_{ij}\). With a \(z\)\sphinxhyphen{}score, we will rescale the weights of the adjacency matrix, such that the new edge\sphinxhyphen{}weights (called \(z\)\sphinxhyphen{}scores) are approximately normally distributed. The reason this can be useful is that the normal distribution is pretty ubiquitous across many branches of science, and therefore, a \(z\)\sphinxhyphen{}score is relatively easy to communicate with other scientists. Further, many things that exist in nature can be well\sphinxhyphen{}approximated by a normal distribution, so it seems like a reasonable place to start to use a \(z\)\sphinxhyphen{}score for edge\sphinxhyphen{}weights, too! The \(z\)\sphinxhyphen{}score is defined as follows. We will construct the \(z\)\sphinxhyphen{}scored adjacency matrix \(Z\), whose entries \(z_{ij}\) are the corresponding \(z\)\sphinxhyphen{}scores of the adjacency matrix’s entries \(a_{ij}\). For a weighted, loopless network, we use an estimate of the \sphinxstyleemphasis{mean}, \(\hat \mu\), and the \sphinxstyleemphasis{unbiased} estimate of the \sphinxstyleemphasis{variance}, \(\hat \sigma^2\)), which can be computed as follows:
\begin{align*}
    \hat\mu &= \frac{1}{n}\sum_{i \neq j}a_{ij},\\
    \hat\sigma^2 &= \frac{1}{n - 1}\sum_{i \neq j} (a_{ij} - \hat\mu)^2.
\end{align*}
\sphinxAtStartPar
The \(z\)\sphinxhyphen{}score for the \((i,j)\) entry is simply the quantity:
\begin{align*}
    z_{ij} &= \frac{a_{ij} - \hat\mu}{\hat\sigma}
\end{align*}
\sphinxAtStartPar
Since our network is loopless, notice that these sums are for all \sphinxstyleemphasis{non\sphinxhyphen{}diagonal} entries where \(i \neq j\). If the network were not loopless, we would include diagonal entries in the calculation, and instead would sum over all possible combinations of \(i\) and \(j\). the interpretation of the \(z\)\sphinxhyphen{}score \(z_{ij}\) is the \sphinxstyleemphasis{number of stadard deviations} that the entry \(a_{ij}\) is from the mean, \(\hat \mu\).

\sphinxAtStartPar
We will demonstrate on the directed friendship network. We can implement \(z\)\sphinxhyphen{}scoring as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{zscore}

\PYG{k}{def} \PYG{n+nf}{z\PYGZus{}score\PYGZus{}loopless}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{is\PYGZus{}loopless}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{TypeError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The network has loops!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} the entries of the adjacency matrix that are not on the diagonal}
    \PYG{n}{non\PYGZus{}diag\PYGZus{}idx} \PYG{o}{=} \PYG{n}{where}\PYG{p}{(}\PYG{o}{\PYGZti{}}\PYG{n}{eye}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{bool}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{Z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
    \PYG{n}{Z}\PYG{p}{[}\PYG{n}{non\PYGZus{}diag\PYGZus{}idx}\PYG{p}{]} \PYG{o}{=} \PYG{n}{zscore}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{non\PYGZus{}diag\PYGZus{}idx}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Z}

\PYG{n}{ZA\PYGZus{}friend} \PYG{o}{=} \PYG{n}{z\PYGZus{}score\PYGZus{}loopless}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we will look at the edge\sphinxhyphen{}weight histogram for the directed friendship network before and after \(z\)\sphinxhyphen{}scoring. Remember that the network is loopless, so again we exclude the diagonal entries:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{seaborn} \PYG{k+kn}{import} \PYG{n}{histplot}

\PYG{n}{non\PYGZus{}diag\PYGZus{}idx} \PYG{o}{=} \PYG{n}{where}\PYG{p}{(}\PYG{o}{\PYGZti{}}\PYG{n}{eye}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{bool}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{histplot}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{[}\PYG{n}{non\PYGZus{}diag\PYGZus{}idx}\PYG{p}{]}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Edge Weight}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of Edges}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Directed Friendship Network, Before Z\PYGZhy{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{histplot}\PYG{p}{(}\PYG{n}{ZA\PYGZus{}friend}\PYG{p}{[}\PYG{n}{non\PYGZus{}diag\PYGZus{}idx}\PYG{p}{]}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Z\PYGZhy{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of Edges}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Directed Friendship Network, After Z\PYGZhy{}score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
The theory for when, and why, to use \(z\)\sphinxhyphen{}scoring for network machine learning tends to go something like this: many things tend to be normally distributed with the same mean and variance, so perhaps that is a reasonable expectation for our network, too. Unfortunately, we find this often to \sphinxstyleemphasis{not} be the case. In fact, we often find that the specific distribution of edge weights itself often might be lamost infeasible to identify in a population of networks, and therefore \sphinxstyleemphasis{almost} irrelevant all\sphinxhyphen{}together. To this end, we turn to instead \sphinxstyleemphasis{ranking} the edges.


\subparagraph{Ranking edges preserves ordinal relationships}
\label{\detokenize{representations/ch4/regularization:ranking-edges-preserves-ordinal-relationships}}
\sphinxAtStartPar
The idea behind ranking is as follows. We don’t really know much useful information as to how the distribution of edge weights varies between a given pair of networks. For this reason, we want to virtually eliminate the impact of that distribution \sphinxstyleemphasis{almost} entirely. However, we know that if one edge\sphinxhyphen{}weight is larger than another edge\sphinxhyphen{}weight, that we do in fact trust that relationship. What this means is that we want something which preserves \sphinxstyleemphasis{ordinal} relationships in our edge\sphinxhyphen{}weights, but ignores other properties of the edge\sphinxhyphen{}weights. An ordinal relationship just means that we have a natural ordering to the edge\sphinxhyphen{}weights. This means that we can identify a largest edge\sphinxhyphen{}weight, a smallest edge\sphinxhyphen{}weight, and every position in between. When we want to preserve ordinal relationships in our network, we do something called \sphinxstyleemphasis{passing the non\sphinxhyphen{}zero edge\sphinxhyphen{}weights to ranks}. We will often use the abbreviation \sphinxcode{\sphinxupquote{ptr}} to define this function because it is so useful for weighted networks. We pass non\sphinxhyphen{}zero edge\sphinxhyphen{}weights to ranks as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Identify all of the non\sphinxhyphen{}zero entries of the adjacency matrix \(A\).

\item {} 
\sphinxAtStartPar
Count the number of non\sphinxhyphen{}zero entries of the adjacency matrix \(A\), \(n_{nz}\).

\item {} 
\sphinxAtStartPar
Rank all of the non\sphinxhyphen{}zero edges in the adjacency matrix \(A\), where for a non\sphinxhyphen{}zero entry \(a_{ij}\), \(rank(a_{ij}) = 1\) if \(a_{ij}\) is the smallest non\sphinxhyphen{}zero edge\sphinxhyphen{}weight, and \(rank(a_{ij}) = n_{nz}\) if \(a_{ij}\) is the largest edge\sphinxhyphen{}weight. Ties are settled by using the average rank of the two entries.

\item {} 
\sphinxAtStartPar
Report the weight of each non\sphinxhyphen{}zero entry \((i,j)\) as \(r_{ij} = \frac{rank(a_{ij})}{n_{nz} + 1}\), and for eachh zero entry as \(r_{ij} = 0\).

\end{enumerate}

\sphinxAtStartPar
Below, we pass\sphinxhyphen{}to\sphinxhyphen{}ranks the directed friendship network using \sphinxcode{\sphinxupquote{graspologic}}, showing both the adjacency matrix and the edge\sphinxhyphen{}weight distribution before and after \sphinxcode{\sphinxupquote{ptr}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{pass\PYGZus{}to\PYGZus{}ranks}

\PYG{n}{RA\PYGZus{}friend} \PYG{o}{=} \PYG{n}{pass\PYGZus{}to\PYGZus{}ranks}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The edge\sphinxhyphen{}weights for the adjacency matrix \(R\) after \sphinxcode{\sphinxupquote{ptr}} has the interpretation that each entry \(r_{ij}\) which is non\sphinxhyphen{}zero is the \sphinxstyleemphasis{quantile} of that entry amongst \sphinxstyleemphasis{the other non\sphinxhyphen{}zero entries}. This is unique in that it is completely \sphinxstyleemphasis{distribution\sphinxhyphen{}free}, which means that we don’t need to assume anything about the distribution of the edge\sphinxhyphen{}weights to have a reasonably interpretable quantity. On the other hand, the \(z\)\sphinxhyphen{}score had the interpretation of the number of standard deviations from the mean, which is only a sensible quantity to compare if we assume the population of edge\sphinxhyphen{}weights are normally distributed.

\sphinxAtStartPar
Another useful quantity related to pass\sphinxhyphen{}to\sphinxhyphen{}ranks is known as the zero\sphinxhyphen{}boosted pass\sphinxhyphen{}to\sphinxhyphen{}ranks. Zero\sphinxhyphen{}boosted pass\sphinxhyphen{}to\sphinxhyphen{}ranks is conducted as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Identify all of the non\sphinxhyphen{}zero entries of the adjacency matrix \(A\).

\item {} 
\sphinxAtStartPar
Count the number of non\sphinxhyphen{}zero entries of the adjacency matrix \(A\), \(n_{nz}\), \sphinxstyleemphasis{and} the number of zero\sphinxhyphen{}entries of the adjacency matrix \(A\), \(n_z\). Note that since the values of the adjacency matrix are either zero or non\sphinxhyphen{}zero, that \(n_{nz} + n_z = n^2\), as \(A\) is an \(n \times n\) matrix and therefore has \(n^2\) total entries.

\item {} 
\sphinxAtStartPar
Rank all of the non\sphinxhyphen{}zero edges in the adjacency matrix \(A\), where for a non\sphinxhyphen{}zero entry \(a_{ij}\), \(rank(a_{ij}) = 1\) if \(a_{ij}\) is the smallest non\sphinxhyphen{}zero edge\sphinxhyphen{}weight, and \(rank(a_{ij}) = n_{nz}\) if \(a_{ij}\) is the largest edge\sphinxhyphen{}weight. Ties are settled by using the average rank of the two entries.

\item {} 
\sphinxAtStartPar
Report the weight of each non\sphinxhyphen{}zero entry \((i,j)\) as \(r_{ij}' = \frac{n_z + rank(a_{ij})}{n^2 + 1}\), and for each zero entry as \(r_{ij}' = 0\).

\end{enumerate}

\sphinxAtStartPar
The edge\sphinxhyphen{}weights for the adjacency matrix \(R'\) after zero\sphinxhyphen{}boosted \sphinxcode{\sphinxupquote{ptr}} have the interpretation that each entry \(r_{ij}'\) is the quantile of that entry amongst \sphinxstyleemphasis{all} of the entries. Let’s instead use zero\sphinxhyphen{}boosted \sphinxcode{\sphinxupquote{ptr}} on our network:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{RA\PYGZus{}friend\PYGZus{}zb} \PYG{o}{=} \PYG{n}{pass\PYGZus{}to\PYGZus{}ranks}\PYG{p}{(}\PYG{n}{A\PYGZus{}friend}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{zero\PYGZhy{}boost}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subparagraph{Logging reduces magnitudinal differences between edges}
\label{\detokenize{representations/ch4/regularization:logging-reduces-magnitudinal-differences-between-edges}}
\sphinxAtStartPar
When we look at the distribution of non\sphinxhyphen{}zero edge\sphinxhyphen{}weights for the activity/hobby network or the friendship network, we notice a strange pattern, known as a \sphinxstyleemphasis{right\sphinxhyphen{}skew}:

\sphinxAtStartPar
Notice that \sphinxstyleemphasis{most} of the edges have weights which are comparatively small, between \(0\) and \(34\), but some of the edges have weights which are much (much) larger. A \sphinxstylestrong{right\sphinxhyphen{}skew} exists when the majority of edge\sphinxhyphen{}weights are small, but some of the edge\sphinxhyphen{}weights take values which are much larger.

\sphinxAtStartPar
What if we want to make these large values more similar in relation to the smaller values, but we simultaneously want to preserve properties of the underlying distribution of the edge\sphinxhyphen{}weights? Well, we can’t use \sphinxcode{\sphinxupquote{ptr}}, because \sphinxcode{\sphinxupquote{ptr}} will throw away all of the information about the edge\sphinxhyphen{}weight distribution other than the ordinal relationship between pairs of edges. To interpret what this means, we might think that there is a big difference between sharing no interests compared to three interests in common, but there is not as much of a difference in sharing ten interests compared to thirteen interests in common.

\sphinxAtStartPar
To do this, we instead turn to the logarithm function. The logarithm function \(log_{10}(x)\) is defined for positive values \(x\) as the value \(c_x\) where \(x = 10^{c_x}\). In this sense, it is the “number of powers of ten” to obtain the value \(x\). You will notice that the logarithm function looks like this:

\sphinxAtStartPar
What is key to noice about this function is that, as \(x\) increases, the log of \(x\) increases by a \sphinxstyleemphasis{decreasing} amount. Let’s imagine we have three values, \(x = .001\), \(y = .1\), and \(z = 10\). A calculator will give you that \(log_{10}(x) = -3, log_{10}(y) = -1\), and \(log_{10}(z) = 1\). Even though \(y\) is only \(.099\) units bigger than \(x\), its logarithm \(log_{10}(y)\) exceeds \(log_{10}(x)\) by two units. on the other hand, \(z\) is \(9.9\) units bigger than \(y\), but yet its logarithm \(log_{10}(z)\) is still the same two units bigger than \(log_{10}(y)\). This is because thhe logarithm is instead looking at the fact that \(z\) is \sphinxstyleemphasis{one} power of ten, \(y\) is \(-1\) powers of ten, and \(z\) is \(-3\) powers of ten. The logarithm has \sphinxstyleemphasis{collapsed} the huge size difference between \(z\) and the other two values \(x\) and \(y\) by using exponentiation with \sphinxstyleemphasis{base} ten.

\sphinxAtStartPar
In this sense, we can also use the logarithm function for our network to reduce the huge size difference between the values in our activity/hobby network. However, we must first add a slight twist: to do this properly and yield an interpretable adjacency matrix, we need to \sphinxstyleemphasis{augment} the entries of the adjacency matrix \sphinxstyleemphasis{if} it contains zeros. This is because the \(log_{10}(0)\) is \sphinxstyleemphasis{not defined}. To augment the adjacency matrix, we will use the following strategy:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Identify the entries of \(A\) which take a value of zero.

\item {} 
\sphinxAtStartPar
Identify the smallest entry of \(A\) which is not\sphinxhyphen{}zero, and call it \(a_m\).

\item {} 
\sphinxAtStartPar
Compute a value \(\epsilon\) which is an \sphinxstyleemphasis{order of magnitude} smaller than \(a_m\). Since we are taking powers of ten, a single order of magnitude would give us that \(\epsilon = \frac{a_m}{10}\).

\item {} 
\sphinxAtStartPar
Take the augmented adjacency matrix \(A'\) to be defined with entries \(a_{ij}' = a_{ij} + \epsilon\).

\end{enumerate}

\sphinxAtStartPar
Next, since our matrix has values which are now all greater than zero, we can just take the logarithm:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{augment\PYGZus{}zeros}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{X} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{TypeError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The logarithm is not defined for negative values!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{am} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{X} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} the smallest non\PYGZhy{}zero entry of X}
    \PYG{n}{eps} \PYG{o}{=} \PYG{n}{am}\PYG{o}{/}\PYG{l+m+mi}{10}  \PYG{c+c1}{\PYGZsh{} epsilon is one order of magnitude smaller than the smallest non\PYGZhy{}zero entry}
    \PYG{k}{return} \PYG{n}{X} \PYG{o}{+} \PYG{n}{eps}  \PYG{c+c1}{\PYGZsh{} augment all entries of X by epsilon}

\PYG{n}{A\PYGZus{}activity\PYGZus{}aug} \PYG{o}{=} \PYG{n}{augment\PYGZus{}zeros}\PYG{p}{(}\PYG{n}{A\PYGZus{}activity}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} log\PYGZhy{}transform using base 10}
\PYG{n}{A\PYGZus{}activity\PYGZus{}log} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{n}{A\PYGZus{}activity\PYGZus{}aug}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
When we plot the augmented and log\sphinxhyphen{}transformed data, what we see is that many of the edge\sphinxhyphen{}weights we originally might have thought were zero if we only looked at a plot were, in actuality, \sphinxstyleemphasis{not} zero. In this sense, for non\sphinxhyphen{}negative weighted networks, log transforming after zero\sphinxhyphen{}augmentation is often very useful for visualization to get a sense of the magnitudinal differences that might be present between edges.

\sphinxAtStartPar
Our edge\sphinxhyphen{}weight histogram becomes:


\chapter{Why Use Statistical Models?}
\label{\detokenize{representations/ch5/ch5:why-use-statistical-models}}\label{\detokenize{representations/ch5/ch5::doc}}

\section{Why Use Statistical Models?}
\label{\detokenize{representations/ch5/why-use-models:why-use-statistical-models}}\label{\detokenize{representations/ch5/why-use-models::doc}}
\sphinxAtStartPar
In network data science, we typically begin with a question of interest, and a network we use to answer that question. To answer this question, we will turn to statistical models. Consider the simplest possible statistical model which has been used for years: the coin flip model. Let’s say we are playing a game with a gambler, and we bet one dollar. If the coin lands on heads, we get our dollar back, and an additional dollar. If the coin lands on tails, we lose our dollar. We get to watch ten coin flips before deciding whether or not to join the game. So, should we play?

\sphinxAtStartPar
A coin has a probability, which we will denote \(p\), of landing on heads. Since the coin either lands on heads or tails, this means that the probability that the coin lands on tails is \(1-p\). Reasonably, we might guess that the probability that this coin lands on heads is just \(0.5\) (the coin has an equal chance of landing on heads and tails). Unfortunatetly, the universe is a nefarious place! We could easily construct coins which slightly favor heads (perhaps a coin with a chance of landing on heads of \(0.51\)) \sphinxstyleemphasis{or} a coin which slightly favors tails (perhaps a coin with a chance of landing on tails of \(0.51\)).

\sphinxAtStartPar
To understand whether or not we should play this game, we turn to \sphinxstyleemphasis{statistical modelling}. A \sphinxstylestrong{statistical model} is a set of assumptions as to how a random system operates. The statistical model delineates what we think comes down to random chance in our system. In our coin flip example, this means that we describe the coin flip using the \sphinxstyleemphasis{Bernoulli model}, which means that the coin lands on heads with probability \(p\) and tails with probability \(1-p\). The statistical model is the set of all possible coins which could be used for the game.

\sphinxAtStartPar
A \sphinxstylestrong{random variable} is an object whose outcome comes down to random chance. In our coin flip example, the coin flip itself \sphinxstyleemphasis{is} the random variable. The coin possesses a probability \(p\) of landing on heads and a probability \(1-p\) of landing on tails. To learn about the coin, we conduct an \sphinxstyleemphasis{experiment}. The experiment here is watching the coin flip ten times, and observing the outcome of the flips. The outcome of the coin flip is called a \sphinxstyleemphasis{realization} of the random variable. We will never truly understand the coin flip perfectly (we can never say for sure whether the coin will definitely land on heads or tails unless it has two heads or two tails). If we observe enough realizations of coin flips, however, we might be able to describe the coin flip in a way which could work this game in our favor.

\sphinxAtStartPar
The coin flip example is obviously very trivial, but it extends directly to statistical network models which we will use for simple networks. Consider a network of \(100\) students, in which each of the students attends one of two schools. We are interested in understanding whether students are more likely to be friends on a social networking site with students in their school than with students from the other school. Unlike traditional machine learning, in network machine learning we do not observe \(n\) outcomes (or realizations) with \(d\) dimensions. Rather, we see an adjacency matrix \(A\), whose nodes are the \(100\) students, and whose edges are the entries \(a_{ij}\) which take a value of one if the two students \(i\) and \(j\) are friends on the social networking site and a value of zero if the two students are not friends on the social networking site.

\sphinxAtStartPar
Like the coin flip example, there is randomness and uncertainty to our social network. Perhaps a pair of students might be friends in real life, but they never got around to adding each other on the social media site. Maybe our students had a fight and are no longer friends, but never bothered to delete one another as friends on the site. Other factors might exist that we don’t know about (sports, hobbies, special interests) that influence whether two people are friends. Our social network might not capture all of the students, and we might be missing a large portion of the community all together. In many additional ways, our social network is noisy, and in order to address our question of interest, we need procedures which account for this uncertainty.

\sphinxAtStartPar
In machine learning, we typically encounter situations in which we have \(n\) observations in \(d\) dimensions. Traditional statistical models include univariate statistical models (models for data with \(1\) dimension) and multivariate statistical models (models for data with \(d > 1\) dimensions), which can capture this traditional data representation. These models are well suited for discovering new insights about individual observations or collections of individual observations. Why do we need special statistical models for networks? Our realizations are not \(n\) disparate observations in \(d\)\sphinxhyphen{}dimensions; a realization in network machine learning \sphinxstylestrong{is the full network itself}, consisting of nodes, edges, and potential network attributes. We seek to model a representation of the \sphinxstyleemphasis{entire} network so that we can convey insights about properties of the network. To address our question of interest above, we need to characterize how students relate to other students in the network, not describe individual stdents. To this end, we describe our random network using sets of statistical assumptions, referred to as the \sphinxstylestrong{statistical network model}. The coin flip model might haave felt really simple, but we will see how we can use collections of coins to describe pretty complicated random networks throughout this chapter. We break down the key aspects of the coin flip experiment because it is so crucial:

\begin{sphinxadmonition}{note}{The Coin Flip Experiment}

\sphinxAtStartPar
We had the following items we were concerned with in the coin flip example:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The outcomes: The outcomes are either heads or tails. These outcomes will be denoted by the letter \(x\), which takes values which are H (Heads) or T (Tails). The value \(x\) is called a \sphinxstylestrong{realization}.

\item {} 
\sphinxAtStartPar
The coin which was used: The specific coin being used in the coin flip experiment has a probability \(p\) of landing on heads and a probability \(1 - p\) of landing on tails. We will denote the specific coin being used by the letter \(\mathbf x\). The bold\sphinxhyphen{}faced font means that the coin being used has a random outcome (it might be heads, it might be tails) to differentiate it from the coin flips which we saw and have known outcomes indicated by \(x\) (which has a fixed value, since we flipped the coin and \sphinxstyleemphasis{realized} the outcome). We don’t know anything about \(p\) just yet, so we can’t describe the coin’s specific random behavior just yet. The value \(\mathbf x\) is called the \sphinxstylestrong{random variable}.

\item {} 
\sphinxAtStartPar
Feasible coins: A possible coin that could have been used in the coin flipping experiment is one which has a probability \(q\) (which might be different from \(p\)) of landing on heads, and \(1 - q\) of landing on tails. A feasible coin will be denoted by \(Bern(q)\), which just means that the coin lands on heads with probability \(q\) and tails with probability \(1 - q\).

\item {} 
\sphinxAtStartPar
The Bernoulli model: The collection of all feasible coins which could have been used. This is described by the set \(\left\{Bern(q) : q \text{ is a probability between \)0\( and \)1\(}\right\}\). This set is infinitely large, since it contains a feasible coin for \sphinxstyleemphasis{any} specified probability. The commonality between the feasible coins is that they each have a unique probability of landing on heads and a unique probability of landing on tails. The statistical model is simply the collection of all possible feasible coin which feature this commonality. For instance, there is a coin \(Bern(0.1)\) which has a probability \(0.1\) of landing on heads in this set, and another coin \(Bern(0.9)\) which has a probabiliy \(0.9\) of landing on heads in this set.
The specific coin being used, the random variable \(\mathbf x\) which lands on heads with probability \(p\), behaves \sphinxstyleemphasis{exactly} like the coin \(Bern(p)\) from the statistical model. For this reason, we will say that the coin \(\mathbf x\) is a \(Bern(p)\) coin.

\end{enumerate}
\end{sphinxadmonition}


\subsection{Models aren’t Right. Why do we Care?}
\label{\detokenize{representations/ch5/why-use-models:models-aren-t-right-why-do-we-care}}
\sphinxAtStartPar
It is important to clarify that we must pay careful attention to the age old aphorism attributed to George Box, a pioneering British statistician of the 20\(^{th}\) century. George Box stated, “all models are wrong, but some are useful.” In this sense, it is important to remember that the statistical model we select is, in practice, \sphinxstyleemphasis{never} the correct model (this holds for any aspect of machine learning, not just network machine learning). In the context of network science, this means that even if we have a model we think describes our network very well, it is \sphinxstyleemphasis{not} the case that the model we select actually describes the network precisely and correctly. Despite this, it is often valuable to use statistical models for the simple reason that assuming that a stochastic process (that is, some \sphinxstyleemphasis{random} process) which governs our data is what allows us to convey \sphinxstyleemphasis{uncertainty}. To understand the importance of leveraging uncertainty, consider the following scenarios:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Lack of information: In practice, we would never have all of the information about the system that produced the network we observe, and uncertainty can be used in place of that information. For instance, in our social network example, we might only know which school that people are from, but there are many other attributes that would impact the friend circle of a given student. We might not know things like which classes people have taken nor which grade they’re in, but we would expect these facts to impact whether a given pair of people might have a higher chance of being friends. We can use uncertainty in our model to capture the fact that we don’t know the classes nor grades of the students.

\item {} 
\sphinxAtStartPar
We might think the network is deterministic, rather than stochastic: In the extreme case, we might think that if we had \sphinxstyleemphasis{all} of the information which governs the network, then we could determine exactly what realizations would look like with perfect accuracy. Even if we knew exactly what realizations of the network might look like, this description, too, isn’t likely to be very valuable. If we were to develop a model on the basis of everything, our model would be extremely complex and require a large amount of data. For instance, in our social network example, to know whether two people were friends with perfect accuracy, we might need to have intimate knowledge of every single person’s life (Did they just have a fight with somebody and de\sphinxhyphen{}connect with that person? Did they just go to a school dance and meet someone new?).

\item {} 
\sphinxAtStartPar
We learn from uncertainty and simplicity: When we do statistical inference, it is rarely the case that we prioritize a complex, convoluted model that mirrors our data suspiciously closely. Instead, we are usually interested in knowing how faithfully a simpler, more generally applicable model might describe the network. This relates directly to the concept of the bias\sphinxhyphen{}variance tradeoff from machine learning, in which we prefer a model which isn’t too specific (lower bias) but still describes the system effectively (lower variance).

\end{enumerate}

\sphinxAtStartPar
Therefore, it is crucial to incorporate randomness and uncertainty to understand network data. In practice, we select a model which is appropriate from a family of candidate models on the basis of three primary factors:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Utility: The model of interest possesses the level of refinement or complexity that we need to answer our scientific question of interest. What this means is that the coin flip model will allow us to determine whether or not we should gamble.

\item {} 
\sphinxAtStartPar
Estimation: The data has the level of breadth to facilitate estimation of the parameters of the model of interest. This means that we can use the outcomes of coin flips to guess what the probability that the coin will land on heads is.

\item {} 
\sphinxAtStartPar
Appropriateness: The model is appropriate for the data we are given. This means that there are not major factors which are unaccounted for by the statistical model, such as if the coin thrower holds a magnet which will alter the outcome of the coin flip.

\end{enumerate}

\sphinxAtStartPar
For the rest of this section, we will develop intuition for the first point. Later sections will cover estimation of parameters and model selection.


\section{Erdös\sphinxhyphen{}Rényi (ER) Random Networks}
\label{\detokenize{representations/ch5/single-network-models_ER:erdos-renyi-er-random-networks}}\label{\detokenize{representations/ch5/single-network-models_ER::doc}}
\sphinxAtStartPar
We will start our description with the simplest random network model. Consider a social network, with 50 students. Our network will have 50 nodes, where each node represents a single student in the network. Edges in the social network represent whether or not a pair of students are friends. What is the simplest way we could describe whether two people are friends?

\sphinxAtStartPar
In this case, the simplest possible thing to do would be to say, for any two students in our network, there is some probability (which we will call \(p\)) that describes how likely they are to be friends. In the below example, for the sake of argument, we will let \(p=0.3\). What does a realization from this network look like?

\noindent\sphinxincludegraphics{{single-network-models_ER_1_0}.png}

\sphinxAtStartPar
As we mentioned in the preface for this chapter, every statisical model in this book will come down to the coin flip model. In network machine learning, we get to see an adjacency matrix \(A\) whose entries \(a_{ij}\) are one if the students \(i\) and \(j\) are friends on the social networking site, and \(0\) if the students \(i\) and \(j\) are not friends on the social networking site. Just like we had a random coin \(\mathbf x\) which behaved like a \(Bern(p)\) coin from the collection \(\left\{Bern(q) : q \text{ is a probability between \)0\( and \)1\(}\right\}\), we will assume that the network \(A\) is a realization of a random network \(\mathbf A\) which behaves like an element of a collection of random neworks. This might seem like a big stretch, so let’s try to put it into perspective.

\sphinxAtStartPar
Since \(A\) was an \(n \times n\) matrix, \(\mathbf A\) is an \(n \times n\) random matrix. The elements of \(\mathbf A\) will be given by the symbols \(\mathbf a_{ij}\), which means that each edge \(a_{ij}\) of \(A\) is a realization of the random edge \(\mathbf a_{ij}\). Just how do we describe this \(\mathbf a_{ij}\)? Remember that our realizations \(a_{ij}\) are just \(0\)s and \(1\)s, which \sphinxstyleemphasis{feels} a lot like flipping a coin, doesn’t it? Did the coin land on heads, or did it land on tails? Are the two people \(i\) and \(j\) friends, or are they not friends? If we had a coin with some probability of landing on heads, we could describe \(a_{ij}\) as a realization of this coin flip. We could assume that a value of one is analogous to the coin landing on heads, and value of zero is analogous to the coin landing on tails. Perhaps we could even model the network using the same approach we took before with the coin flip. This is starting to go somewhere, so let’s continue with the analogies.


\subsection{The Erdös Rényi random network is parametrized by the independent\sphinxhyphen{}edge probability}
\label{\detokenize{representations/ch5/single-network-models_ER:the-erdos-renyi-random-network-is-parametrized-by-the-independent-edge-probability}}
\sphinxAtStartPar
This simple random network model is called the Erdös Rényi (ER) model1. The way we can think of an ER random network is that the edges depend \sphinxstyleemphasis{only} on a probability, \(p\), and each edge is totally independent of all other edges. We can think of this example as though a coin flip is performed, where the coin has a probability \(p\) of landing on heads, and \(1-p\) of landing on tails. For each edge in the network, we conceptually flip the coin, and if it lands on heads (with probability \(p\)), the edge exists, and if it lands on tails (with probability \(1-p\)) the edge does not exist. The meaning of \sphinxstyleemphasis{independence} is a little technical and goes a bit outside of the scope of this book, so we will leave it at a very high level as meaning that the outcome of particular coin flips do not impact the outcomes of other coin flips. This is not a very precise definition, but it will be plenty for our purposes. If \(\mathbf A\) is a random network which is \(ER_n(p)\) with \(n\) nodes and probability \(p\), we will often say that \(\mathbf A\) is an \(ER_n(p)\) random network.


\subsection{How do we simulate realizations of \protect\(ER_n(p)\protect\) random networks?}
\label{\detokenize{representations/ch5/single-network-models_ER:how-do-we-simulate-realizations-of-er-n-p-random-networks}}
\sphinxAtStartPar
This approach which we will use to describe random networks is called a \sphinxstyleemphasis{generative model}, which means that we have described an observable network realization \(A\) of the random network \(\mathbf A\) in terms of the parameters of \(\mathbf A\). In the case of the \(ER_n(p)\) random networks, we have described \(\mathbf A\) in terms of the probability parameter, \(p\). Generative models are convenient in that we can easily adapt them to tell us exactly how to simulate realizations of the underlying random network. The procedure below will produce for us a network \(A\), which has nodes and edges, where the underlying random network  \(\mathbf A\) is an \(ER_n(p)\) random network:

\begin{sphinxadmonition}{note}{Simulating a realization from an \protect\(ER_n(p)\protect\) random network}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Determine a probability, \(p\), of an edge existing.

\item {} 
\sphinxAtStartPar
Obtain a weighted coin which has a probability \(p\) of landing on heads, and a probability \(1 - p\) of landing on tails. Note that this probability \(p\) might differ from the “traditional” coin with a probability of landing on heads of approximately \(0.5\).

\item {} 
\sphinxAtStartPar
Flip the once for each \sphinxstyleemphasis{possible} edge \((i, j)\) between nodes \(i\) and \(j\) in the network. For a simple network, we will repeat the coin flip \(\binom n 2\) times.

\item {} 
\sphinxAtStartPar
For each coin flip which landed on heads, define that the corresponding edge exists, and define that the corresponding entry \(a_{ij}\) in the adjacency matrix is \(1\). For each coin flip which lands on tails, define that the corresponding edge does not exist, and define that \(a_{ij} = 0\).

\item {} 
\sphinxAtStartPar
The adjacency matrix we produce, \(A\), is a realization of an \(ER_n(p)\) random network.

\end{enumerate}
\end{sphinxadmonition}


\subsection{When do we use an \protect\(ER_n(p)\protect\) Network?}
\label{\detokenize{representations/ch5/single-network-models_ER:when-do-we-use-an-er-n-p-network}}
\sphinxAtStartPar
In practice, the \(ER_n(p)\) model seems like it might be a little too simple to be useful. Why would it ever be useful to think that the best we can do to describe our network is to say that connections exist with some probability? Does this miss a \sphinxstyleemphasis{lot} of useful questions we might want to answer? Fortunately, there are a number of ways in which the simplicity of the \(ER_n(p)\) model is useful. Given a probability and a number of nodes, we can easily describe the properties we would expect to see in a network if that network were ER. For instance, we know how many edges on average the nodes of an \(ER_n(p)\) random nework should have. We can reverse this idea, too: given a network we think might \sphinxstyleemphasis{not} be ER, we could check whether it’s different in some way from an \(ER_n(p)\) random network. For instance, if we see that half the nodes have a ton of edges (meaning, they have a high degree), and half don’t, we might be able to determine that the network is poorly described by an \(ER_n(p)\) random network. If this is the case, we might look for other models that could describe our network which are more complex.



\sphinxAtStartPar
In the next code block, we are going to sample a single \(ER_n(p)\) network with \(50\) nodes and an edge probability \(p\) of \(0.3\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{draw\PYGZus{}multiplot}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{er\PYGZus{}np}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10}  \PYG{c+c1}{\PYGZsh{} network with 50 nodes}
\PYG{n}{p} \PYG{o}{=} \PYG{l+m+mf}{0.3}  \PYG{c+c1}{\PYGZsh{} probability of an edge existing is .3}

\PYG{c+c1}{\PYGZsh{} sample a single simple adjacency matrix from ER(50, .3)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{er\PYGZus{}np}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{p}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} and plot it}
\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}ER\PYGZus{}}\PYG{l+s+si}{\PYGZob{}10\PYGZcb{}}\PYG{l+s+s2}{(0.3)\PYGZdl{} Simulation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{xticklabels}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{yticklabels}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_ER_4_0}.png}

\sphinxAtStartPar
Above, we visualize the network using a heatmap. The dark squares indicate that an edge exists between a pair of nodes, and white squares indicate that an edge does not exist between a pair of nodes.

\sphinxAtStartPar
Next, let’s see what happens when we use a higher edge probability, like \(p=0.7\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p} \PYG{o}{=} \PYG{l+m+mf}{0.7}  \PYG{c+c1}{\PYGZsh{} network has an edge probability of 0.7}

\PYG{c+c1}{\PYGZsh{} sample a single adjacency matrix from ER(50, 0.7)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{er\PYGZus{}np}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{p}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} and plot it}
\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}ER\PYGZus{}}\PYG{l+s+si}{\PYGZob{}10\PYGZcb{}}\PYG{l+s+s2}{(0.7)\PYGZdl{} Simulation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{xticklabels}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{yticklabels}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_ER_7_0}.png}

\sphinxAtStartPar
As the edge probability increases, the sampled adjacency matrix tends to indicate that there are more connections in the network. This is because there is a higher chance of an edge existing when \(p\) is larger.


\subsection{Just how many networks are possible for a network with \protect\(n\protect\) nodes?}
\label{\detokenize{representations/ch5/single-network-models_ER:just-how-many-networks-are-possible-for-a-network-with-n-nodes}}
\sphinxAtStartPar
As you’re going to become accustomed to, we’re going to boil this down again to coin flips. If we had one coin, there are two possible outcomes: either heads or tails. If we had two coins, the first coin could be heads or tails, and the second coin could be heads or tails. Let’s break this down by fixing the outcome of the first coin. If the first coin were heads, there are two possible outcomes for the second coin. If the first coin were tails, there are two possible outcomes for the second coin. This means that the total number of possible outcomes is the sum of the number of possible outcomes if the first coin is heads with the number of possible outcomes if the first coin were tails. This gives us that with two coins, we have four possible outcomes. When we add a third coin, we repeat this process again. If the first coin were heads, the second two coins could take any of four possible outcomes as we just learned. if the first coin were tails, the second two coins could also taake any of four possible outcomes. Therefore, with three coins, we have eight possible outcomes. As we continue this procedure, we quickly will realize that with \(x\) coin flips, we have \(2^x\) possible outcomes.

\sphinxAtStartPar
Remember in Chapter 4 when discussing the {\hyperref[\detokenize{representations/ch5/single-network-models_ER:link?}]{\emph{properties of networks}}}, we determined that there are \(\frac{1}{2}n(n - 1)\) possible edges in a simple network, which we could represent using the notation \(\binom n 2\). In a realized network, each of these edges could exist or not exist, so there are again two possibilities just like the coin flips. Since edges existing or not existing boils down to a coin flip, the number of possible networks with \(n\) nodes is just \(2\) to the power of the number of coin flips that are performed in the network. Here, this is \(2^{\binom n 2}\). This quantity gets \sphinxstyleemphasis{really} big \sphinxstyleemphasis{really} fast! Let’s see just how fast below:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{comb}

\PYG{n}{ns} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{51}\PYG{p}{)}
\PYG{n}{nposs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{comb}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n}{ns}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log10}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_ER_10_0}.png}

\sphinxAtStartPar
This is an enormous quantity! When \(n\) is just \(6\), the number of possible networks is \(2^{\binom 6 2} = 2^{6}\) which is over \(32,000\). When \(n\) is \(15\), the number of posssible networks balloons up to \(2^{\binom{15}{2}} = 2^{105}\) which is over \(10^{30}\). As the number of nodes increases, the number of possible network balloons really, really fast!


\subsection{References}
\label{\detokenize{representations/ch5/single-network-models_ER:references}}
\sphinxAtStartPar
{[}1{]} Erdös P, Rényi A. 1959. “On random graphs, I.” Publ. Math. Debrecen 6:290–297.


\section{Stochastic Block Models (SBM)}
\label{\detokenize{representations/ch5/single-network-models_SBM:stochastic-block-models-sbm}}\label{\detokenize{representations/ch5/single-network-models_SBM::doc}}
\sphinxAtStartPar
Let’s imagine that we have \(100\) students, each of whom can go to one of two possible schools: school one or school two. Our network has \(100\) nodes, and each node represents a single student. The edges of this network represent whether a pair of students are friends. Intuitively, if two students go to the same school, the probably have a higher chance of being friends than if they do not go to the same school. If we were to try to characterize this using an ER random network, we would run into a problem: we have no way to capture the impact that school has on friendships. Intuitively, there must be a better way!

\sphinxAtStartPar
The Stochastic Block Model, or SBM, captures this idea by assigning each of the \(n\) nodes in the network to one of \(K\) communities. A \sphinxstylestrong{community} is a group of nodes within the network. In our example case, the communities would represent the schools that students are able to attend. We use \(K\) here to just denote an integer greater than \(1\) (for example, in the school example we gave above, \(K\) is \(2\)) for the number of \sphinxstyleemphasis{possible} communities that nodes could be members of. In an SBM, instead of describing all pairs of nodes with a fixed probability like with the ER model, we instead describe properties that hold for edges between \sphinxstyleemphasis{pairs of communities}. In our example, what this means is that if two students go to school one, the probability that they are friends might be different than if the two students went to school two, or if one student went to school one and the other to school two. Let’s take a look at what a realization of this setup we have described might look like:

\noindent\sphinxincludegraphics{{single-network-models_SBM_2_0}.png}


\subsection{Defining the paramaters of an SBM random network}
\label{\detokenize{representations/ch5/single-network-models_SBM:defining-the-paramaters-of-an-sbm-random-network}}

\subsubsection{The community assignment vector assigns nodes in the random network to communities}
\label{\detokenize{representations/ch5/single-network-models_SBM:the-community-assignment-vector-assigns-nodes-in-the-random-network-to-communities}}
\sphinxAtStartPar
To describe an SBM random network, we proceed very similarly to an ER random network, with a twist. An SBM random network has a parameter, \(\vec z\), which has a single element for each of the node. We call \(\vec z\) the \sphinxstylestrong{community assignment vector}, which means that for each node of our random network, \(z_i\) tells us which community the node is in. To state this another way, \(\vec z\) is a vector where each element \(z_i\) can take one of \(K\) possible values, where \(K\) is the total number of communities in the network. For example, if we had an SBM random network with four nodes in total, and two total communities, each element \(z_i\) can be either \(1\) or \(2\). If the first two nodes were in community \(1\), and the second two in community \(2\), we would say that \(z_1 = 1\), \(z_2 = 1\), \(z_3 = 2\), and \(z_4 = 2\), which means that \(\vec z\) looks like:
\begin{align*}
    \vec z^\top &= \begin{bmatrix}1 & 1 & 2 & 2\end{bmatrix}
\end{align*}

\subsubsection{The block matrix defines the edge existence probabilities between communities in the random network}
\label{\detokenize{representations/ch5/single-network-models_SBM:the-block-matrix-defines-the-edge-existence-probabilities-between-communities-in-the-random-network}}
\sphinxAtStartPar
The other parameter for an SBM random network is called the block matrix, for which we will use the capital letter \(B\). If there are \(K\) communities in the SBM random network, then \(B\) is a \(K \times K\) matrix, with one entry for each pair of communities. For instance, if \(K\) were two like above, \(B\) would be a \(2 \times 2\) matrix, and would look like this:
\begin{align*}
    B &= \begin{bmatrix}
        b_{11} & b_{12} \\ b_{21} & b_{22}
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Each of the entries of \(B\), which we denote as \(b_{kl}\) in the above matrix, is a probability of an edge existing between a node in community \(k\) and a node in community \(l\).

\sphinxAtStartPar
Fortunately, we can also think of this formulation of a random network using coin flips. In our mini example above, if node \(1\) is in community \(1\) (since \(z_1 = 1\)) and node \(2\) is in community \(1\) (since \(z_2 = 1\)), we have a weighted coin which has a probability \(b_{11}\) (the first row, first column of the block matrix above) of landing on heads, and a \(1 - b_{11}\) chance of landing on tails. An edge between nodes one and two exists if the weighted coin lands on heads, and does not exist if that weighted coin lands on tails. If we wanted to describe an edge between nodes one and three instead, note that \(z_3 = 2\). Therefore, we use the entry \(b_{12}\) as the probability of obtaining a heads for the weighted coin we flip this time. In the general case, to use the block matrix to obtain the probability of an edge \((i, j)\) existing between any pair of nodes \(i\) and \(j\) in our network, we will flip a coin with probability \(b_{z_i z_j}\), where \(z_i\) is the community assignment for the \(i^{th}\) node and \(z_j\) is the community assignment for the \(j^{th}\) node.

\sphinxAtStartPar
If \(\mathbf A\) is a random network which is an \(SBM_n(\vec z, B)\) with \(n\) nodes, the community vector \(\vec z\), and the block matrix \(B\), we say that \(\mathbf A\) is an \(SBM_n(\vec z, B)\) random network.


\subsubsection{How do we simulate realizations of \protect\(SBM_n(\vec z, B)\protect\) random networks?}
\label{\detokenize{representations/ch5/single-network-models_SBM:how-do-we-simulate-realizations-of-sbm-n-vec-z-b-random-networks}}
\sphinxAtStartPar
The procedure below will produce for us a network \(A\), which has nodes and edges, where the underlying random network \(\mathbf A\) is an \(SBM_n(\vec z, B)\) random network:

\begin{sphinxadmonition}{note}{Simulating a realization from an \protect\(SBM_n(\vec z, B)\protect\) random network}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Determine a community assignment vector, \(\vec z\), for each of the \(n\) nodes. Each node should be assigned to one of \(K\) communities.

\item {} 
\sphinxAtStartPar
Determine a block matrix, \(B\), for each pair of the \(K\) communities.

\item {} 
\sphinxAtStartPar
For each pair of communities \(k\) and \(l\), obtain a weighted coin (which we will call the \((k,l)\) coin) which as a \(b_{kl}\) chance of landing on heads, and a \(1 - b_{kl}\) chance of landing on tails.

\item {} 
\sphinxAtStartPar
For each pair of nodes \(i\) and \(j\):
\begin{itemize}
\item {} 
\sphinxAtStartPar
Denote \(z_i\) to be the community assignment of node \(i\), and \(z_j\) to be the community assignment of node \(j\).

\item {} 
\sphinxAtStartPar
Flip the \((z_i, z_j)\) coin, and if it lands on heads, the corresponding entry \(a_{ij}\) in the adjacency matrix is \(1\). If it lands on tails, the corresponding entry \(a_{ij}\) in the adjacency matrix is \(0\).

\end{itemize}

\item {} 
\sphinxAtStartPar
The adjacency matrix we produce, \(A\), is a realization of an \(SBM_n(\vec z, B)\) random network.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
We just covered a lot of intuition! This intuition will come in handy later, but let’s take a break from the theory by working through an example. Let’s use the school example we started above. Say we have \(100\) students, and we know that each student goes to one of two possible schools. Remember that we already know the community assignment vector \(\vec{z}\) ahead of time. We don’t really care too much about the ordering of the students for now, so let’s just assume that the first \(50\) students all go to the first school, and the second \(50\) students all go to the second school.

\begin{sphinxadmonition}{note}{Thought Exercise}

\sphinxAtStartPar
Before you read on, try to think to yourself about what the node\sphinxhyphen{}assignment vector \(\vec z\) looks like.
\end{sphinxadmonition}

\sphinxAtStartPar
Next, let’s plot what \(\vec z\) look like:

\noindent\sphinxincludegraphics{{single-network-models_SBM_4_0}.png}

\sphinxAtStartPar
So as we can see, the first \(50\) students are from the first school, and the second \(50\) students are from second school.

\sphinxAtStartPar
Let’s assume that the students from the first school are better friends in general than the students from the second school, so we’ll say that the probability of two students who both go to the first school being friends is \(0.5\), and the probability of two students who both go to school \(2\) being friends is \(0.3\). Finally, let’s assume that if one student goes to the first school and the other student goes to school \(2\), that the probability that they are friends is \(0.2\).

\begin{sphinxadmonition}{note}{Thought Exercise}

\sphinxAtStartPar
Before you read on, try to think to yourself about what the block matrix \(B\) looks like.
\end{sphinxadmonition}

\sphinxAtStartPar
Next, let’s look at the block matrix \(B\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{2}  \PYG{c+c1}{\PYGZsh{} 2 communities in total}
\PYG{c+c1}{\PYGZsh{} construct the block matrix B as described above}
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,} 
     \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_SBM_7_0}.png}

\sphinxAtStartPar
As we can see, the matrix \(B\) is a symmetric block matrix, since our network is undirected. Finally, let’s sample a single network from the \(SBM_n(\vec z, B)\) with parameters \(\vec z\) and \(B\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{draw\PYGZus{}multiplot}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{c+c1}{\PYGZsh{} sample a graph from SBM\PYGZus{}\PYGZob{}300\PYGZcb{}(tau, B)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{ys} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{ys}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}SBM\PYGZus{}n(z, B)\PYGZdl{} Simulation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_SBM_9_0}.png}

\sphinxAtStartPar
The above network shows students, ordered by the school they are in (first school and the second school, respectively). As we can see in the above network, people from the first school are more connected than people from school \(2\). Also, the connections between people from different schools (the \sphinxstyleemphasis{off\sphinxhyphen{}diagonal} blocks of the adjacency matrix, the lower left and upper right blocks) appear to be a bit \sphinxstyleemphasis{more sparse} (fewer edges) than connections betwen schools (the \sphinxstyleemphasis{on\sphinxhyphen{}diagonal} blocks of the adjacency matrix, the upper left and lower right blocks). The above heatmap can be described as \sphinxstylestrong{modular}: it has clear communities. Remember that the connections for each node are indicated by a single row, or a single column, of the adjacency matrix. The first half of the rows have strong connections with the first half of the columns, which indicates that the first half of students tend to be better friends with other students in the first half. We can duplicate this argument for the second half of students ot see that it seems reasonable to conclude that there are two communities of students here.

\sphinxAtStartPar
Something easy to mistake about a realization of an \(SBM_n(\vec z, B)\) is that the realizations will \sphinxstyleemphasis{not always} have the obvious modular structure we can see above when we look at a heatmap. Rather, this modular structure is \sphinxstyleemphasis{only} made obvious because the students are ordered according to the school in which they are in. What do you think will happen if we look at the students in a random order? Do you think that he structure that exists in this network will be obvious?

\sphinxAtStartPar
The answer is: \sphinxstyleemphasis{No!} Let’s see what happens when we reorder the nodes from the network into a random order, and pretend we don’t know the true community labels ahead of time:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} generate a reordering of the n nodes}
\PYG{n}{vtx\PYGZus{}perm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{n}{Aperm} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{n+nb}{tuple}\PYG{p}{(}\PYG{p}{[}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}
\PYG{n}{yperm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{)}\PYG{p}{[}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}
\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{Aperm}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}SBM\PYGZus{}n(z, B)\PYGZdl{} Simulation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_SBM_11_0}.png}

\sphinxAtStartPar
Now, the students are \sphinxstyleemphasis{not} organized according to school, because they have been randomly reordered. It becomes pretty tough to figure out whether there are communities just by looking at an adjacency matrix, unless you are looking at a network in which the nodes are \sphinxstyleemphasis{already arranged} in an order which respects the community structure. By an \sphinxstyleemphasis{order that respects the community structure}, we mean that the community assignment vector \(\vec z\) is arranged so that all of the nodes in the first community come first, followed by all of the nodes in the second community, followed by all of the nodes in the third community, so on and so forth up to the nodes of the community \(K\).

\sphinxAtStartPar
In practice, this means that if you know ahead of time what natural groupings of the nodes might be (such as knowing which school each student goes to) by way of your node attributes, you can visualize your data according to that grouping. If you don’t know anything about natural groupings of nodes, however, we are left with the problem of \sphinxstyleemphasis{estimating community structure}. A later method, called the \sphinxstyleemphasis{spectral embedding}, will be paired with clustering techniques to allow us to estimate node assignment vectors.


\section{Random Dot Product Graphs (RDPG)}
\label{\detokenize{representations/ch5/single-network-models_RDPG:random-dot-product-graphs-rdpg}}\label{\detokenize{representations/ch5/single-network-models_RDPG::doc}}
\sphinxAtStartPar
In this section, we are going to discuss the Random Dot Product Graph (RDPG), a further generalization of the random network models we have studied. With the RDPG, we can have random networks which are much more complex than those we saw with the \(ER_n(p)\) and the \(SBM_n(\vec z, B)\) random neworks, but \sphinxstyleemphasis{still} have a discernable structure to them. For example, here’s a realization from an RDPG random network:

\noindent\sphinxincludegraphics{{single-network-models_RDPG_2_0}.png}


\subsection{Rethinking the Stocahstic Block Model with a Probability Matrix}
\label{\detokenize{representations/ch5/single-network-models_RDPG:rethinking-the-stocahstic-block-model-with-a-probability-matrix}}
\sphinxAtStartPar
Let’s imagine that we have a network which follows the Stochastic Block Model. To make this example a little bit more concrete, let’s borrow the code example from the \DUrole{xref,myst}{section on Stochastic Block Models}. The nodes of our network represent each of the \(100\) students in our network. Remember that \(z\) is the community assignment vector, which indicates which community (one of two schools) each node (student) is in. Here, the first \(50\) students attend school \(1\), and the second \(50\) students attend school \(2\):

\noindent\sphinxincludegraphics{{single-network-models_RDPG_4_0}.png}

\sphinxAtStartPar
And the block probability matrix \(B\) is a \(2 \times 2\) matrix, where each entry \(b_{kl}\) indicates the probability that a node assigned to community \(k\) is connected to a node assigned to community \(l\):

\noindent\sphinxincludegraphics{{single-network-models_RDPG_6_0}.png}

\sphinxAtStartPar
Are there any other ways to describe this scenario, other than using both the community assignment vector \(\vec z\) and the block matrix \(B\)?


\subsubsection{Probability Matrices Explicitly State the Probability for each Edge}
\label{\detokenize{representations/ch5/single-network-models_RDPG:probability-matrices-explicitly-state-the-probability-for-each-edge}}
\sphinxAtStartPar
Remember, for a given \(\vec z\) and \(B\), that a network which is SBM can be generated using the approach that, given that \(z_i = \ell\) and \(z_j = k\), each edge \((i, j)\) comes down to a coin flip, where the edge exists if the coin lands on heads with probability \(b_{z_i z_j}\) or does not exist if the coin lands on tails with probability \(1- b_{z_i z_j}\). However, there’s another way we could write down this generative model. Suppose we had a \(n \times n\) probability matrix, where for every \(j > i\):
\begin{align*}
    p_{ji} = p_{ij}, p_{ij} = \begin{cases}
        b_{11} & z_i = 1, z_j = 1 \\
        b_{12} & z_i = 1, z_j = 2 \\
        b_{22} & z_i = 2, z_j = 2
    \end{cases}
\end{align*}
\sphinxAtStartPar
This matrix \(P\) with entries \(p_{ij}\) is the probability matrix associated with the SBM. Simply put, this matrix describes the probability \(p_{ij}\) of each edge \((i,j)\) between nodes \(i\) and \(j\) existing. What does \(P\) look like?

\noindent\sphinxincludegraphics{{single-network-models_RDPG_8_0}.png}

\sphinxAtStartPar
As we can see, \(P\) captures a similar modular structure to the actual adjacency matrix corresponding to the SBM network. When we say this network is \sphinxstyleemphasis{modular}, we mean that it looks block\sphinxhyphen{}y, in that there are clusters of edges sharing a similar probability. Also, \(P\) captures the probability of connections between each pair of students. It is the case that \(P\) contains the information of both \(\vec z\) and \(B\). This means that we can write down a generative model by specifying \sphinxstyleemphasis{only} \(P\), and we no longer need to specify \(\vec z\) nor \(B\) at all.

\sphinxAtStartPar
To represent an \(SBM_n(\vec z, B)\) random network using \(P\) uses a lot more space than using the community assignment vector \(\vec z\) and the block matrix \(B\) alone: \(\vec z\) has \(n\) entries, and \(B\) has \(K \times K\) entries, where \(K\) is typically much smaller than \(n\). On the other hand, in this formulation, \(P\) has \(\binom{n}{2}\) entries, which is much bigger than \(n + K \times K\) (since \(K\) is usually much smaller than \(n\)). One thing you might be wondering, however, is can we make things even \sphinxstyleemphasis{more} general by using \(P\) instead of \(\vec z\) and \(B\)? As we can see in our probability matrix above, there are only \(4\) unique entries in total! Can we make \(P\) any more general, and still have a statistical model that simplifies the network with fewer than \(\binom{n}{2}\) entries?


\subsubsection{Decomposing the Probability Matrix into Simpler Components}
\label{\detokenize{representations/ch5/single-network-models_RDPG:decomposing-the-probability-matrix-into-simpler-components}}
\sphinxAtStartPar
As it turns out, for a Stochastic Block Model, the probability matrix \(P\) can be decomposed using a matrix \(X\), where \(P = X X^\top\). This matrix \(X\) is a special matrix called the latent position matrix, which we will discuss further in this section and in the \DUrole{xref,myst}{section on estimation}. We will call a single row of \(X\) the vector \(\vec x_i\). Using this expression, each entry \(p_{ij}\) is going to end up being the product \(\vec x_i^\top \vec x_j\), for all \(i, j\). Like \(P\), \(X\) has \(n\) rows, each of which corresponds to a single node in our network. However, the special property of \(X\) is that it doesn’t \sphinxstyleemphasis{necessarily} have \(n\) columns: rather, \(X\) often will have many fewer columns than rows. For instance, with \(P\) from the example above on Stochastic Block Models, there in fact exists an \(X\) with just \(2\) columns that can be used to describe \(P\). Let’s take a look at what \(X\) looks like. We won’t discuss how to compute this \(X\) just yet, but we’ll try to build some insight into what’s going on here:

\noindent\sphinxincludegraphics{{single-network-models_RDPG_10_0}.png}

\sphinxAtStartPar
One thing we will come back to in a second is the fact that \(X\) in this case is relatively simple: there are \sphinxstyleemphasis{only} \(4\) unique entries, even though there are \(200\) total entries in \(X\) (2 columns for each of \(100\) students).

\sphinxAtStartPar
Like we said previously, it turns out that \(P\) can be described using \sphinxstyleemphasis{only} this matrix \(X\), as \(P = XX^\top\)! Let’s see this in action, by comparing the \(P\) we had above to \(XX^\top\):

\noindent\sphinxincludegraphics{{single-network-models_RDPG_12_0}.png}

\sphinxAtStartPar
As we can see, \(P\) and \(XX^\top\) are identical!

\sphinxAtStartPar
Describing the probability matrix with this matrix \(X\) lends itself to an even further generalization of our single network models. Like we said a few figures ago, \(X\) only has \(4\) unique entries, but there is no reason for this restriction really. The matrix \(X\) can have \sphinxstyleemphasis{any} number of unique entries, so long as the product of \(X\) and its transpose, \(XX^\top\), ends up being a probability matrix (every entry is a number between \(0\) and \(1\)).


\subsubsection{The Latent Position Matrix}
\label{\detokenize{representations/ch5/single-network-models_RDPG:the-latent-position-matrix}}
\sphinxAtStartPar
This matrix \(X\) is called the \sphinxstylestrong{latent position matrix}, and each row \(\vec x_i\) will be called the \sphinxstylestrong{latent position of the node} \(i\). In matrix form, \(X\) looks like this:
\begin{align*}
 X = \begin{bmatrix}
     \vdash & \vec x_1 & \dashv \\
     \vdash & \vec x_2 & \dashv \\
     & \vdots & \\
     \vdash & \vec x_n & \dashv
 \end{bmatrix}
\end{align*}
\sphinxAtStartPar
We will call the columns of \(X\) the \sphinxstylestrong{latent dimensions}, and the total number of columns that \(X\) has will be called the \sphinxstylestrong{latent dimensionality}. We will often use the letter \(d\) to denote the latent dimensionality of the latent position matrix \(X\). For this reason, we say that \(X\) is an \(n\) row (one for each node) and \(d\) column (one for each latent dimension) matrix. Therefore, the latent position of the node \(i\), \(\vec x_i\), is a \(d\)\sphinxhyphen{}dimensional vector. In a few words, the latent dimensionality describes the complexity that the resulting probability matrix \(P = XX^\top\) has: if there are more latent dimensions, \(P\) can look much more complicated than what we have seen thus far!

\sphinxAtStartPar
Let’s think about the latent position matrix in the context of our previous example with the SBM. A common way to explore the latent position matrix is to look at a heatmap (like we did above) or a scatter plot of the latent position matrix. Let’s think about what the scatter plot might look like. The latent dimensionality of the latent position matrix for the SBM example, it turns out, is \(2\), because we have two total latent dimensions for our latent position matrix \(X\). We will take the latent dimensions, and make the first latent dimension the \(x\)\sphinxhyphen{}axis, and the second latent dimension the \(y\)\sphinxhyphen{}axis. We next plot, for each node \(i\), a single point, whose \(x\)\sphinxhyphen{}coordinate will be the first latent dimension for the latent position of node \(i\), and the \(y\)\sphinxhyphen{}coordinate will be the second latent dimension for the latent position of node \(i\). In symbols, we will plot \((x_{i1}, x_{i2})\), for each node \(i\). This means that there will be \(n\)\sphinxhyphen{}total points shown in the plot below:

\noindent\sphinxincludegraphics{{single-network-models_RDPG_14_0}.png}

\sphinxAtStartPar
Why does our scatter plot look like it only has \(2\) points on it? Quite simply, for an SBM, \sphinxstyleemphasis{every} node within the same community has an \sphinxstyleemphasis{identical} latent position! This means that all of the nodes representing students who are in school \(1\) have the same latent positition vector as the other students in school \(1\). Similarly, all of the nodes representing students who are in school \(2\) have the same latent position vector as the other students in school \(2\). Even though it looks like there is only \(1\) point for each school community, there are really \(50\) total latent position vectors for each student within that community and they just happen to overlap. This does not need to be the case for the latent positions, as we will see in a more complicated example later on.


\subsection{The Random Dot Product Graph (RDPG) is parametrized by a latent position matrix}
\label{\detokenize{representations/ch5/single-network-models_RDPG:the-random-dot-product-graph-rdpg-is-parametrized-by-a-latent-position-matrix}}
\sphinxAtStartPar
Now that we have some intuition built up, let’s circle back to random networks. We will call this particular network model the Random Dot Product Graph (RDPG) model. The way that we can think of the \(RDPG\) random network is that the edges depend on a latent position matrix \(X\). For each pair of nodes \(i\) and \(j\), we have a unique coin (we will call this the \((i,j)\) coin) which has a \(\vec x_i^\top \vec x_j\) chance of landing on heads, and a \(1 - \vec x_i^\top \vec x_j\) chance of landing on tails. If the \((i,j)\) coin lands on heads, the edge between nodes \(i\) and \(j\) exists, and if the \((i,j)\) coin lands on tails, the edge between nodes \(i\) and \(j\) does not exist. As before, this coin flip is performed independent of the coin flips for all of the other edges. The notation we use here is just what we are used to in the preceding sections. If \(\mathbf A\) is a random network which is \(RDPG\) with a latent position matrix \(X\), we say that \(\mathbf A\) is an \(RDPG_n(X)\) random network.


\subsection{How do we simulate realizations of \protect\(RDPG_n(X)\protect\) random networks?}
\label{\detokenize{representations/ch5/single-network-models_RDPG:how-do-we-simulate-realizations-of-rdpg-n-x-random-networks}}
\sphinxAtStartPar
The procedure below will produce for us a network \(A\), which has nodes and edges, where the underlying random network \(\mathbf A\) is an \(RDPG_n(X)\) random network:

\begin{sphinxadmonition}{note}{Simulating a realization from an \protect\(RDPG_n(X)\protect\) random network}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Determine a latent position matrix, \(X\), whose rows \(\vec x_i\) are the latent positions of the nodes in the network.

\item {} 
\sphinxAtStartPar
For each pair nodes \(i\) and \(j\):
\begin{itemize}
\item {} 
\sphinxAtStartPar
Obtain a weighted coin \((i,j)\) which has a probability of \(\vec x_i^\top \vec x_j\) of landing on heads, and a \(1 - \vec x_i^\top \vec x_j\) probability of landing on tails.

\item {} 
\sphinxAtStartPar
Flip the \((i,j)\) coin, and if it lands on heads, the corresponding entry \(a_{ij}\) in the adjacency matrix is \(1\). If the coin lands on tails, the corresponding entry \(a_{ij} = 0\).

\end{itemize}

\item {} 
\sphinxAtStartPar
The adjacency matrix we produce, \(A\), is a realization of an \(RDPG_n(X)\) random network.

\end{enumerate}
\end{sphinxadmonition}


\subsubsection{Working out edge probabilities using a latent position matrix}
\label{\detokenize{representations/ch5/single-network-models_RDPG:working-out-edge-probabilities-using-a-latent-position-matrix}}
\sphinxAtStartPar
As we can see above, for the RDPG, we think of the edge probabilities \(p_{ij}\) as a weighted coin, which lands on heads according to the \sphinxstyleemphasis{dot product} \(\vec x_i^\top \vec x_j\). Let’s explore this in the context of our SBM example from above. Remember that the latent position matrix for our example looked like this:

\noindent\sphinxincludegraphics{{single-network-models_RDPG_16_0}.png}

\sphinxAtStartPar
Remember that the first \(50\) students were in school \(1\), and the second \(50\) students were in school \(2\). Looking at the above matrix, we can see that for a student in school \(1\), the latent position vector is:
\begin{align*}
    \vec x_i &= \begin{bmatrix}
        -0.672 \\
        -0.221
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Then for a pair of students \(i\) and \(j\) who are both in school \(1\) (the community assignments \(z_i\) and \(z_j\) are both \(1\)), the probability they are friends is:
\begin{align*}
    b_{11} = p_{ij} &= \vec x_i^\top \vec x_j = \begin{bmatrix}
        -0.672 &
        -0.221
    \end{bmatrix}\begin{bmatrix}
        -0.672 \\
        -0.221
    \end{bmatrix} \\
    &= (-.672)^2 + (-.221)^2 = .5
\end{align*}
\sphinxAtStartPar
For a student in school \(2\), the latent position vector is:
\begin{align*}
    \vec x_i &= \begin{bmatrix}
        -0.415 \\
        0.357
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
So for a pair of students \(i\) and \(j\) who are both in school \(2\) (the community assignments \(z_i\) and \(z_j\) are both \(2\)) the probability that they are friends is:
\begin{align*}
    b_{11} = p_{ij} &= \vec x_i^\top \vec x_j = \begin{bmatrix}
        -0.415 &
        0.357
    \end{bmatrix}\begin{bmatrix}
        -0.415 \\
        0.357
    \end{bmatrix} \\
    &= (-.415)^2 + (.357)^2 = .3
\end{align*}
\sphinxAtStartPar
For a pair of students \(i\) and \(j\) whho are in different schools:
\begin{align*}
b_{12} =  b_{21} = p_{ij} &= \vec x_i^\top \vec x_j = \vec x_j \vec x_i \\
&= \begin{bmatrix}
        -0.672 &
        -0.221
    \end{bmatrix}\begin{bmatrix}
        -0.415 \\
        0.357
    \end{bmatrix} \\
    &= (-.672)\cdot(-.415) + (-.221)\cdot 0.357 = 0.2
\end{align*}
\sphinxAtStartPar
This shows that using only the latent position matrix \(X\), we have been able to deduce the corresponding block matrix \(B\), where:
\begin{align*}
    B &= \begin{bmatrix}
        b_{11} & b_{12} \\
        b_{21} & b_{22}
    \end{bmatrix}
\end{align*}

\subsection{We can do more complicated things with RDPGs than just SBMs}
\label{\detokenize{representations/ch5/single-network-models_RDPG:we-can-do-more-complicated-things-with-rdpgs-than-just-sbms}}
\sphinxAtStartPar
We will let \(X\) be a little more complex than in our preceding example. Our \(X\) will produce a \(P\) that still \sphinxstyleemphasis{somewhat} has a modular structure, but not quite as much as before. Let’s assume that we have \(100\) people who live along a very long road that is \(100\) miles long, and each person is \(1\) mile apart. The nodes of our network represent the people who live along our assumed street. The people at the ends of the street host large parties each week, and invite everyone else on the street to their parties. However, if someone lives closer to one party host, they are going to tend to more frequently go to that host’s parties than the other party host. Consequently, when someone lives near a party host, they are going to tend to be better friends with other people who go to that host’s parties more frequently. What could we use for \(X\)?

\sphinxAtStartPar
Remember that the latent positions for each node \(i\) are denoted by the vector \(\vec x_i\). One possible approach would be to let each \(\vec x_i\) be defined as follows:
\begin{align*}
    \vec x_i = \begin{bmatrix}
        \frac{100 - i}{100} \\
        \frac{i}{100}
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
For instance, \(\vec x_1 = \begin{bmatrix}1 \\ 0\end{bmatrix}\), and \(\vec x_{100} = \begin{bmatrix} 0 \\ 1\end{bmatrix}\). Note that:
\begin{align*}
p_{1,100} = \vec x_1^\top \vec x_j = 1 \cdot 0 + 0 \cdot 1 = 0
\end{align*}
\sphinxAtStartPar
What happens in between?

\sphinxAtStartPar
Let’s consider another person, person \(30\). Note that person \(30\) lives closer to person \(1\) than to person \(100\).  Here, \(\vec x_{30} = \begin{bmatrix} \frac{7}{10}\\ \frac{3}{10}\end{bmatrix}\). This gives us that:
\begin{align*}
p_{1,30} &= \vec x_1^\top \vec x_{30} = \frac{7}{10}\cdot 1 + 0 \cdot \frac{3}{10} = \frac{7}{10} \\
p_{30, 100} &= \vec x_{30}^\top x_{100} = \frac{7}{10} \cdot 0 + \frac{3}{10} \cdot 1 = \frac{3}{10}
\end{align*}
\sphinxAtStartPar
So this means that person \(1\) and person \(30\) have a \(70\%\) probability of being friends, but person \(30\) and \(100\) have onl6 a \(30\%\) probability of being friends.

\sphinxAtStartPar
Intuitively, it seems like our probability matrix \(P\) will capture the intuitive idea we described above. First, we’ll take a look at \(X\), and then we’ll look at \(P\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}  \PYG{c+c1}{\PYGZsh{} the number of nodes in our network}

\PYG{c+c1}{\PYGZsh{} design the latent position matrix X according to }
\PYG{c+c1}{\PYGZsh{} the rules we laid out previously}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{X}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{n}{n} \PYG{o}{\PYGZhy{}} \PYG{n}{i}\PYG{p}{)}\PYG{o}{/}\PYG{n}{n}\PYG{p}{,} \PYG{n}{i}\PYG{o}{/}\PYG{n}{n}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_RDPG_19_0}.png}

\sphinxAtStartPar
The latent position matrix \(X\) that we plotted above is \(n \times d\) dimensions. There are a number of approaches, other than looking at a heatmap of \(X\), with which we can visualize \(X\) to derive insights as to its structure. When \(d=2\), another popular visualization is to look at the latent positions, \(\vec x_i\), as individual points in \(2\)\sphinxhyphen{}dimensional space. This will give us a scatter plot of \(n\) points, each of which has two coordinates. Each point is the latent position for a single node:

\noindent\sphinxincludegraphics{{single-network-models_RDPG_21_0}.png}

\sphinxAtStartPar
The above scatter plot has been subsampled to show only every \(2^{nd}\) latent position, so that the individual \(2\)\sphinxhyphen{}dimensional latent positions are discernable. Due to the way we constructed \(X\), the scatter plot would otherwise appear to be a line (due to points overlapping one another). The reason that the points fall along a vertical line when plotted as a vector is due to the method we used to construct entries of \(X\), described above. Next, we will look at the probability matrix:

\noindent\sphinxincludegraphics{{single-network-models_RDPG_23_0}.png}

\sphinxAtStartPar
Finally, we will sample an RDPG:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{rdpg}
\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{draw\PYGZus{}multiplot}

\PYG{c+c1}{\PYGZsh{} sample an RDPG with the latent position matrix}
\PYG{c+c1}{\PYGZsh{} created above}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{rdpg}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} and plot it}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}RDPG\PYGZus{}}\PYG{l+s+si}{\PYGZob{}100\PYGZcb{}}\PYG{l+s+s2}{(X)\PYGZdl{} Simulation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-network-models_RDPG_25_0}.png}


\section{Multiple Network Models}
\label{\detokenize{representations/ch5/multi-network-models:multiple-network-models}}\label{\detokenize{representations/ch5/multi-network-models::doc}}
\sphinxAtStartPar
Up to this point, we have studied network models which are useful for a single network. What do we do if we have multiple networks?

\sphinxAtStartPar
Let’s imagine that we have a company with \(45\) total employees. This company is a \sphinxstyleemphasis{real} innovator, and is focused on developing software for network machine learning for business use. \(10\) of these employees are company administrative executives, \(25\) of these employees are network machine learning experts, and \(10\) of these employees are marketing experts. For each day over the course of a full \(30\)\sphinxhyphen{}day month, we study the emails that go back and forth between the employees in the company. We summarize the emailing habits within the company using a nework, where the nodes of the network are employees, and the edges indicate the emailing behaviors between each pair of employees. An edge is said to exist if the two employees have exchanged an email on that day, and an edge does not exist if the two employees have not exchanged an email on that day. In most companies, it is common for employees in a similar role to tend to work more closely together, so we might expect that there is some level of a community structure to the emailing habits. For instance, if two employees are both network machine learning experts, they might exchange more emails between one another than a machine learning expert and a marketing expert. For the sake of this example, we will assume that the networks are organized such that the first day is a Monday, the second day is a Tuesday, so on and so forth. Let’s take a look at an example of some possible realizations of the first \(3\) days worth of emails. What we will see below is that all of the networks appear to have the same community organization, though on Wednesday, we will assume there was an executive board meeting, and in the morning leading up to the board meeting, the administrative executives of the company exchanged more emails than on other days. This is reflected in the fact that there are more emails going back and forth between administrative members in the network for “Wednesday”:

\noindent\sphinxincludegraphics{{multi-network-models_1_0}.png}

\sphinxAtStartPar
Remember that a random network has an adjacency matrix denoted by a boldfaced uppercase \(\mathbf A\), and has network realizations \(A\) (which we just call “networks”). When we have multiple networks, we will need to be able to index them individually. For this reason, in this section, we will use the convention that a random network’s adjacency matrix is denoted by a boldfaced uppercase \(\mathbf A^{(m)}\), where \(m\) tells us which network in our collection we are talking about. The capital letter \(N\) defines the \sphinxstyleemphasis{total} number of random networks in our collection. In our email example, since we have email networks for \(30\) days, \(N\) is \(30\). When we use the letter \(m\) itself, we will typically be referring to an arbitrary random network amongst the collection of random networks, where \(m\) beween \(1\) and \(N\). When we have \(N\) total networks, we will write down the entire \sphinxstylestrong{collection of random networks} using the notation \(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\). With what we already know, for a random network \(\mathbf A^{(m)}\), we would be able to use a single nework model to describe \(\mathbf A^{(m)}\). This means, for instance, if we thought that each email network could be represented by an RDPG, that we would have a \sphinxstyleemphasis{different} latent position matrix \(X^{(m)}\) to define each of the \(30\) networks. In symbols, we would write that each \(\mathbf A^{(m)}\) is an RDPG random nework with latent position matrix \(X^{(m)}\). What is the problem with this description?

\sphinxAtStartPar
What this description lacks is that, over the course of a given \(30\) days, a \sphinxstyleemphasis{lot} of the networks are going to show similar emailing patterns. We might expect that this implies that, on some level, the latent position matrices should also show some sort of common structure. However, since we used a \sphinxstyleemphasis{unique} latent position matrix \(X^{(m)}\) for each random network \(\mathbf A^{(m)}\), we have inherently stated that we think that the networks have completely distinct latent position matrices. If we were to perform a task downstream, such as whether we could identify which employees are in which community, we would have to analyze each latent position matrix individually, and we would not be able to learn a latent position matrix with shared structure across all \(30\) days. Before we jump into multiple network models, let’s provide some context as tp how we will build these up.

\sphinxAtStartPar
In the below descriptions, we will tend to build off the Random Dot Product Graph (RDPG), and closely related variations of it. Why? Well, as it turns out, the RDPG is extremely flexible, in that we can represent both ER and SBMs as RDPGs, too. This means that building off the RDPG gives us multiple random network models that will be inherently flexible. Further, as we will see in the later section on {\hyperref[\detokenize{representations/ch5/multi-network-models:link?}]{\emph{Estimation}}}, the RDPG is extremely well\sphinxhyphen{}suited for the situation in which we want to analyze SBMs, but do not know which communities the nodes are in ahead of time. This situation is extremely common across numerous disciplines of network machine learning, such as social networking, neuroscience, and many other fields.

\sphinxAtStartPar
So, how can we model our collection of random networks with shared structure?


\subsection{Joint Random Dot Product Graphs (JRDPG) Model}
\label{\detokenize{representations/ch5/multi-network-models:joint-random-dot-product-graphs-jrdpg-model}}
\sphinxAtStartPar
In our example on email networks, notice that the Monday and Tuesday emails do not look \sphinxstyleemphasis{too} qualitatively different. It looks like they have relatively similar connectivity patterns between the different employee working groups, and we might even think that the underlying random process that governs these two email networks are \sphinxstyleemphasis{identical}. In statisical science, when we have a collection of \(N\) random networks that have the same underlying random process, we describe this with the term \sphinxstylestrong{homogeneity}. Let’s put what this means into context using our coin flip example. If a pair of coins are \sphinxstyleemphasis{homogeneous}, this means that the probability that they land on heads is identical. Likewise, this intuition extends directly to random networks. A \sphinxstylestrong{homogeneous} collection of random networks \(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\) is one in which \sphinxstyleemphasis{all} of the \(N\) random networks have the \sphinxstyleemphasis{same probability matrix}. Remember that the probability matrix \(P^{(m)}\) is the matrix whose entries \(p^{(m)}_{ij}\) indicate the probability that an edge exists between nodes \(i\) and \(j\). On the other hand, a \sphinxstylestrong{heterogeneous} collection of random networks is a collection of networks \(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\) is one in which the probability matrices are \sphinxstyleemphasis{not} the same for all of the \(N\) networks.

\sphinxAtStartPar
The probability matrices \(P^{(1)}\) and \(P^{(2)}\) for the random networks \(\mathbf A^{(1)}\) and \(\mathbf A^{(2)}\) for Monday and Tuesday are shown in the following figure. We also show the difference between the two probability matrices, to make really clear that they are the same:

\noindent\sphinxincludegraphics{{multi-network-models_3_0}.png}

\sphinxAtStartPar
The Joint Random Dot Product Graphs (JRDPG) is the simplest way we can extend the RDPG random network model to multiple random networks. The way we can think of the JRDPG model is that for each of our \(N\) total random neworks, the edges depend on a latent position matrix \(X\). We say that a collection of random networks \(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\) with \(n\) nodes is \(JRDPG_n(X)\) if each random network \(\mathbf A^{(m)}\) is \(RDPG_n(X)\) and if the \(N\) networks are independent.


\subsubsection{The JRDPG model does not allow us to convey unique aspects about the networks}
\label{\detokenize{representations/ch5/multi-network-models:the-jrdpg-model-does-not-allow-us-to-convey-unique-aspects-about-the-networks}}
\sphinxAtStartPar
Under the JRDPG model, each of the \(N\) random networks share the same latent position matrix. Remember that for an RDPG, the probability matrix \(P = XX^\top\). This means that for all of the \(N\) networks, \(P^{(m)} = XX^\top\) under the JRDPG model. hence, \(P^{(1)} = P^{(2)} = ... = P^{(N)}\), and \sphinxstyleemphasis{all} of the probability matrices are \sphinxstyleemphasis{identical}! This means that the \(N\) random networks are \sphinxstylestrong{homogeneous}.

\sphinxAtStartPar
For an RDPG, since \(P = XX^\top\), the probability matrix depends \sphinxstyleemphasis{only} on the latent positions \(X\). This means that we can tell whether a collection of networks are homogeneous just by looking at the latent position matrices! It turns out that the random networks underlying the realizations for the email networks in our given example were just SBMs. From the section on {\hyperref[\detokenize{representations/ch5/multi-network-models:link?}]{\emph{Random Dot Product Graphs}}}, we learned that SBMs are just RDPGs with a special latent position matrix. Let’s try this first by looking at the latent position matrices for \(\mathbf A^{(1)}\) and \(\mathbf A^{(2)}\) from the random networks for Monday and Tuesday first:

\noindent\sphinxincludegraphics{{multi-network-models_5_0}.png}

\sphinxAtStartPar
So the latent position matrices for Monday and Tuesday are exactly identical. Therefore, the collection of random networks \(\left\{\mathbf A^{(1)}, \mathbf A^{(2)}\right\}\) are homogeneous, and we could model this pair of networks using the JRDPG.

\sphinxAtStartPar
What about Wednesday? Well, as it turns out, Wednesday had a \sphinxstyleemphasis{different} latent position matrix from both Monday and Tuesday:

\noindent\sphinxincludegraphics{{multi-network-models_7_0}.png}

\sphinxAtStartPar
So, \(\mathbf A^{(1)}\) and \(\mathbf A^{(3)}\) do not have the same latent position matrices. This means that the collections of random networks \(\left\{\mathbf A^{(1)}, \mathbf A^{(3)}\right\}\), \(\left\{\mathbf A^{(2)}, \mathbf A^{(3)}\right\}\), and \(\left\{\mathbf A^{(1)}, \mathbf A^{(2)}, \mathbf A^{(3)}\right\}\) are \sphinxstyleemphasis{heterogeneous}, because their probability matrices will be different. So, unfortunately, the JRDPG cannot handle the hetereogeneity between the random networks of Monday and Tuesday with the random network for Wednesday. To remove this restrictive homogeneity property of the JRDPG, we will need a new single network model, called the Inhomogeneous Erdos\sphinxhyphen{}Renyi (IER) random network model.


\subsubsection{The Inhomogeous Erdos\sphinxhyphen{}Renyi (IER) Random Network}
\label{\detokenize{representations/ch5/multi-network-models:the-inhomogeous-erdos-renyi-ier-random-network}}
\sphinxAtStartPar
The IER random network is the most general random network model for a binary graph. The way we can think of the \(IER\) random network is that a probability matrix \(P\) with \(n\) rows and \(n\) columns defines each of the edge\sphinxhyphen{}existence probabilities for pairs of nodes in the network. For each pair of nodes \(i\) and \(j\), we have a unique coin which has a \(p_{ij}\) chance of landing on heads, and a \(1 - p_{ij}\) chance of landing on tails. If the coin lands on heads, the edge between nodes \(i\) and \(j\) exists, and if the coin lands on tails, the edge between nodes \(i\) and \(j\) does not exist. This coin flip is performed independently of the coin flips for all of the other edges. If \(\mathbf A\) is a random network which is \(IER\) with a probability matrix \(P\), we say that \(\mathbf A\) is an \(IER_n(P)\) random network.

\sphinxAtStartPar
As before, we can develop a procedure to produce for us a network \(A\), which has nodes and edges, where the underlying random network \(\mathbf A\) is an \(IER_n(P)\) random network:

\begin{sphinxadmonition}{note}{Simulating a realization from an \protect\(IER_n(P)\protect\) random network}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Determine a probability matrix \(P\), whose entries \(p_{ij}\) are probabilities.

\item {} 
\sphinxAtStartPar
For each pair of nodes \(i\) an \(j\):
\begin{itemize}
\item {} 
\sphinxAtStartPar
Obtain a weighted coin \((i,j)\) which has a probability \(p_{ij}\) of landing on heads, and a \(1 - p_{ij}\) probability of landing on tails.

\item {} 
\sphinxAtStartPar
Flip the \((i,j)\) coin, andd if it lands on heads, the corresponding entry \(a_{ij}\) in the adjacency matrix is \(1\). If the coin lands on tails, the corresponding entry \(a_{ij}\) is \(0\).

\end{itemize}

\item {} 
\sphinxAtStartPar
The adjacency matrix we produce, \(A\), is a realization of an \(IER_n(P)\) random network.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
It is important to realize that all of the networks we have described so far are also \(IER_n(P)\) random networks. The previous single network models we have described to date simply place restrictions on the way in which we acquire \(P\). For instance, in an \(ER_n(p)\) random network, all entries \(p_{ij}\) of \(P\) are equal to \(p\). To see that an \(SBM_n(\vec z, B)\) random network is also \(IER_n(P)\), we can construct the probability matrix \(P\) such that \(p_{ij} = b_{kl}\) of the block matrix \(B\) when the community of node \(i\) \(z_i = k\) and the community of node \(j\) \(z_j = l\). To see that an \(RDPG_n(X)\) random network is also \(IER_n(P)\), we can construct the probability matrix \(P\) such that \(P = XX^\top\). This shows that the IER random network is the most general of the single network models we have studied so far. Next, let’s see how the IER random network can help us address this heterogeneity.


\subsection{Common Subspace Independent Edge (COSIE) Model}
\label{\detokenize{representations/ch5/multi-network-models:common-subspace-independent-edge-cosie-model}}
\sphinxAtStartPar
In our example on email networks, notice that the Monday and Wednesday emails looked relatively similar, but had an important difference: on Wednesday, there was an administrative meeting, and the employees on the administrative team exchanged far more emails than usual amongst one another. It turns out that, in fact, the random networks \(\mathbf A^{(1)}\) and \(\mathbf A^{(3)}\) which underly the email networks \(A^{(1)}\) and \(A^{(3)}\) were also different, because the probability matrices were different:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}prob\PYGZus{}block\PYGZus{}annot}\PYG{p}{(}\PYG{n}{P1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}P\PYGZca{}}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{(1)\PYGZcb{}\PYGZdl{} Monday Prob. Matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}prob\PYGZus{}block\PYGZus{}annot}\PYG{p}{(}\PYG{n}{P3}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}P\PYGZca{}}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{(3)\PYGZcb{}\PYGZdl{} Wednesday Prob. Matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}prob\PYGZus{}block\PYGZus{}annot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{P1} \PYG{o}{\PYGZhy{}} \PYG{n}{P3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}|P\PYGZca{}}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{(1)\PYGZcb{} \PYGZhy{} P\PYGZca{}}\PYG{l+s+s2}{\PYGZob{}}\PYG{l+s+s2}{(3)\PYGZcb{}|\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multi-network-models_9_0}.png}

\sphinxAtStartPar
Notice that the difference in the probability of an email being exchanged between the members of the administrative team is \(0.6\) higher in the probability matrix for Wednesday than for Monday. This is because the random networks \(\mathbf A^{(1)}\) and \(\mathbf A^{(3)}\) are heterogeneous. To reflect this heterogeneity, we will need to turn to the COmmon Subspace Independent Edge (COSIE) model.


\subsubsection{The COSIE Model is defined by a collection of score matrices and a shared low\sphinxhyphen{}rank subspace}
\label{\detokenize{representations/ch5/multi-network-models:the-cosie-model-is-defined-by-a-collection-of-score-matrices-and-a-shared-low-rank-subspace}}
\sphinxAtStartPar
Even though \(P^{(1)}\) and \(P^{(3)}\) are not \sphinxstyleemphasis{identical}, we can see they still share \sphinxstyleemphasis{some} structure: the employee teams are the same between the two email networks, and much of the probability matrix is unchanged. For this reason, it will be useful for us to have a network model which allows us to convey \sphinxstyleemphasis{some} shared structure, but still lets us convey aspects of the different networks which are \sphinxstyleemphasis{unique}. The COSIE model will accomplish this using a \sphinxstyleemphasis{shared latent position matrix}, and unique \sphinxstyleemphasis{score matrices} for each of the random networks.


\paragraph{The Shared Latent Position Matrix Describes Similarities}
\label{\detokenize{representations/ch5/multi-network-models:the-shared-latent-position-matrix-describes-similarities}}
\sphinxAtStartPar
The \sphinxstyleemphasis{shared latent position matrix} for the COSIE model is quite similar to the latent position matrix for an RDPG. Like the latent position matrix, the shared latent position matrix \(V\) is a matrix with \(n\) rows (one for each node) and \(d\) columns. The \(d\) columns behave very similarly to the \(d\) columns ffor the latent position matrix of an RDPG, and \(d\) is referred to as the \sphinxstyleemphasis{latent dimensionality} of the COSIE random networks. Like before, each row of the shared latent position matrix \(v_i\) will be referred to as the shared latent position vector for node \(i\).

\sphinxAtStartPar
We will also add an additional restriction to \(V\): it will be a matrix with orthonormal columns. What this means is that for each column of \(V\), the dot product of the column with itself is \(1\), and the dot product of the column with any other column is \(0\). This has the implication that \(V^\top V = I\), the identity matrix.

\sphinxAtStartPar
The shared latent position matrix conveys the \sphinxstyleemphasis{common structure} between the COSIE random networks, and will be a parameter for each of the neworks. Remember that with the \(JRDPG_n(X)\) model, we were able to capture the homogeneity of the email networks on Monday and Tuesday, but we could not capture the heterogeneity of the email nework on Wednesday. However, we want the shared latent position matrix \(V\) to convey the commonality amongst the three email networks; that is, that the employees are are always working on the same employee teams. Let’s take a look at the shared latent position matrix \(V\) for the email example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{MultipleASE} \PYG{k}{as} \PYG{n}{MASE}

\PYG{n}{embedder} \PYG{o}{=} \PYG{n}{MASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{V} \PYG{o}{=} \PYG{n}{embedder}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{p}{[}\PYG{n}{P1}\PYG{p}{,} \PYG{n}{P2}\PYG{p}{,} \PYG{n}{P3}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{plot\PYGZus{}latent}\PYG{p}{(}\PYG{n}{V}\PYG{p}{,} \PYG{n}{nodetix}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{24}\PYG{p}{,}\PYG{l+m+mi}{34}\PYG{p}{,}\PYG{l+m+mi}{44}\PYG{p}{]}\PYG{p}{,} \PYG{n}{nodelabs}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{25}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{35}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{45}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{dimtix}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{1.5}\PYG{p}{,}\PYG{l+m+mf}{2.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dimlabs}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}V\PYGZdl{}, Shared Latent Positions}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multi-network-models_11_0}.png}

\sphinxAtStartPar
\(V\) reflects the fact that the first \(25\) employees are the ML experts, and all \(25\) have the same latent position vector. The next \(10\) employees are the administrative members, and share a different latent position vector from the ML experts. Finally, the last \(10\) employees are the marketing members, share a different latent position vector from the ML experts and the administrative team. In this way, the orthonormal matrix \(V\) has conveyed the community structure (the employee roles and who they tend to email amongst) that is shared across all three days.


\paragraph{Score Matrices Describe Differences}
\label{\detokenize{representations/ch5/multi-network-models:score-matrices-describe-differences}}
\sphinxAtStartPar
The \sphinxstyleemphasis{score matrices} for the COSIE random networks essentially tell us how to assemble the shared latent position matrix to obtain the unique probability matrix for each network. The score matrix \(R^{(m)}\) for a random network \(m\) is a matrix with \(d\) columns and \(d\) rows. Therefore, it is a square matrix whose number of dimensions is equal to the latent dimensionality of the COSIE random networks.

\sphinxAtStartPar
The probability matrix for each network under the COSIE model is the matrix:
\begin{align*}
    P^{(m)} &= VR^{(m)}V^\top
\end{align*}
\sphinxAtStartPar
In our email example, we want the score matrices to reflect that Monday and Tuesday share a probability matrix, but Monday and Wednesday do not. Consequently, we would expect that the score matrices from Monday and Tuesday should be the same, but the score matrix for Wednesday will be different:

\noindent\sphinxincludegraphics{{multi-network-models_13_0}.png}

\sphinxAtStartPar
As we can see, the scores \(R^{(1)}\) and \(R^{(2)}\) are the same for Monday and Tuesday, but different for \(R^{(3)}\) Wednesday.

\sphinxAtStartPar
Finally, let’s relate all of this back to the COSIE model. The way we can think about the COSIE model is that for each random network \(m\) of our \(N\) total networks, the probability matrix \(P^{((m)}\) depends on the shared latent position matrix \(V\) and the score matrix \(R^{(m)}\). The probability matrix \(P^{(m)}\) for the \(m^{th}\) random network is defined so that \(P^{(m)} = VR^{(m)}V^\top\). This means that each entry \(p_{ij}^{(m)} = \vec v_i^\top R^{(m)} \vec v_j\). We say that a collection of random networks \(\left\{\mathbf A^{(1)}, ..., \mathbf A^{(N)}\right\}\) with \(n\) nodes is \(COSIE_n\left(V, \left\{R^{(1)},...,R^{(N)}\right\}\right)\) if each random network \(\mathbf A^{(m)}\) is \(IER(P^{(m)})\). Stated another way, each of the \(N\) random networks share the same orthonormal matrix \(V\), but a unique score matrix \(R^{(m)}\). This allows the random networks to share some underlying structure (which is conveyed by \(V\)) but each random network still has a combination of this shared structure (conveyed by \(R^{(m)}\)).

\sphinxAtStartPar
Since the probability matrix \(P^{(m)} = VR^{(m)}V^\top\), we can see that two random networks with the same score matrix will be homogeneous, and two random networks with different score matrices will be heterogeneous. In this way, we were able to capture the homogeneity between the random networks for Monday and Tuesday emails, while also capturing the heterogeneity between the random networks for Monday and Wednesday emails.


\subsection{Correlated Network Models}
\label{\detokenize{representations/ch5/multi-network-models:correlated-network-models}}
\sphinxAtStartPar
Finally, we get to a special case of network models, known as correlated network models. Let’s say that we have a group of people in a city, and we know that each people in our group have both a Facebook and a Twitter. The nodes in our network are the people we have. The first network consists of Facebook connections amongst the people, where an edge exists between two people if they are friends on Facebook. The second network consists of Twitter connections amongst the people, where an edge exists between two people if they follow one another on Twitter. We think that if two people are friends on Facebook, there is a good chance that they follow one another on Twitter, and vice versa. How do we reflect this similarity through a multiple network model?

\sphinxAtStartPar
At a high level, network correlation between a pair of networks describes the property that the existence of edges in one network provides us with some level of information about edges in the other network, much like the Facebook/Twitter example we just discussed. In this book, we will focus on the \(\rho\)\sphinxhyphen{}\sphinxstyleemphasis{correlated} network models. What the \(\rho\)\sphinxhyphen{}correlated network models focus on is that given two random networks with the same number of nodes, each edge has a correlation of \(\rho\) between the two networks. To define this a little more rigorously, a pair of random networks \(\mathbf A^{(1)}\) and \(\mathbf A^{(2)}\) are called \(\rho\)\sphinxhyphen{}\sphinxstylestrong{correlated} if all of the edges across both networks are mutually independent, except that for all pairs of indices \(i\) and \(j\), \(corr(\mathbf a_{ij}^{(1)}, \mathbf a_{ij}^{(2)}) = \rho\), where \(corr(\mathbf x, \mathbf y)\) is the Pearson correlation between two random variables \(\mathbf x\) and \(\mathbf y\). In our example, this means that whether two people are friends on Facebook is \sphinxstyleemphasis{correlated} with whether they are following one another on Twitter.

\sphinxAtStartPar
At a high level, the Pearson correlation describes whether one variable being large/small gives information that the other variable is large/small (positive correlation, between \(0\) and \(1\)) or whether one variable being large/small gives information that the other variable will be small/large (negative correlation, between \(-1\) and \(0\)). If the two networks are positively correlated and we know that one of the edges \(\mathbf a_{ij}^{(1)}\) has a value of one, then we have information that \(\mathbf a_{ij}^{(2)}\) might also be one, and vice\sphinxhyphen{}versa for taking values of zero. If the two networks are negatively correlated and we know that one of the edges \(\mathbf a_{ij}^{(1)}\) has a value of one, then we have information that \(\mathbf a_{ij}^{(2)}\) might be zero, and vice\sphinxhyphen{}versa. If the two networks are not correlated (\(\rho = 0\)) we do not learn anything about edges of network two by looking at edges from network one.


\subsubsection{\protect\(\rho\protect\)\sphinxhyphen{}Correlated RDPG}
\label{\detokenize{representations/ch5/multi-network-models:rho-correlated-rdpg}}
\sphinxAtStartPar
The \(\rho\)\sphinxhyphen{}correlated RDPG is the most general correlated network model we will need for the purposes of this book. Remembering that both ER and SBM random networks are special cases of the RDPG (for a given choice of the latent position matrix), the \(\rho\)\sphinxhyphen{}correlated RDPG can therefore be used to construct \(\rho\)\sphinxhyphen{}correlated ER and \(\rho\)\sphinxhyphen{}correlated SBMs, too. The way we can think about the \(\rho\)\sphinxhyphen{}correlated RDPG is that like for the normal RDPG, a latent position matrix \(X\) with \(n\) rows and a latent dimensionality of \(d\) is used to define the edge\sphinxhyphen{}existence probabilities for the networks \(\mathbf A^{(1)}\) and \(\mathbf A^{(2)}\). We begin by defining that \(\mathbf A^{(1)}\) is \(RDPG_n(X)\). Next, we define the second network as follows. We use a coin for each edge \((i, j)\), which has a probability that depends on the values that the first network takes. If the edge \(\mathbf a_{ij}^{(1)}\) takes the value of one, then we use a coin which has a probability of landing on heads of \(\vec x_i^\top \vec x_j + \rho(1 - \vec x_i^\top \vec x_j)\). If the edge \(\mathbf a_{ij}^{(1)}\) takes the value of zero, then we use a coin which has a probability of landing on heads of \((1 - \rho)\vec x_i^\top \vec x_j\). We flip this coin, and if it lands on heads, then the edge \(\mathbf a_{ij}^{(2)}\) takes the value of one. If it lands on tails, then the edge \(\mathbf a_{ij}^{(2)}\) takes the value of zero. If \(\mathbf A^{(1)}\) and \(A^{(2)}\) are random networks which are \(\rho\)\sphinxhyphen{}correlated RDPGs with latent position matrix \(X\), we say that the pair \(\left\{\mathbf A^{(1)}, A^{(2)}\right\}\) are \(\rho-RDPG_n(X)\).

\begin{sphinxadmonition}{note}{Simulating realizations of \protect\(\rho\protect\)\sphinxhyphen{}correlated RDPGs}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Determine a latent position matrix \(X\), where rows \(\vec x_i\) are the latent position vectors for the nodes \(i\).

\item {} 
\sphinxAtStartPar
Determine a correlation between the two networks of \(\rho\), where \(\rho \geq -1\) and \(\rho \leq 1\).

\item {} 
\sphinxAtStartPar
Simulate a realization \(A^{(1)}\) which is a realization  of an \(RDPG_n(X)\) random network.

\item {} 
\sphinxAtStartPar
For each pair of nodes \(i\) and \(j\):
\begin{itemize}
\item {} 
\sphinxAtStartPar
If the edge \(a_{ij}^{(1)}\) has a value of one, obtain a coin which has a probability of landing on heads of \(\vec x_i^\top \vec x_j + \rho(1 - \vec x_i^\top \vec x_j)\). If the edge \(a_{ij}^{(2)}\) has a value of zero, obtain a coin which has a probability of landing on heads of \((1 - \rho)\vec x_i^\top \vec x_j\).

\item {} 
\sphinxAtStartPar
Flip the \((i,j)\) coin, andd if it lands on heads, the corresponding entry \(a_{ij}^{(2)}\) in the adjacency matrix is \(1\). If the coin lands on tails, the corresponding entry \(a_{ij}^{(2)}\) is \(0\).

\end{itemize}

\item {} 
\sphinxAtStartPar
The adjacency matrices \(A^{(1)}\) and \(A^{(2)}\) are realizations of \(\rho-RDPG_n(X)\) random networks.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
Fortunately, graspologic makes sampling \(\rho\)\sphinxhyphen{}correlated RDPGs relatively simple. Let’s say that in our Facebook/Twitter example, we have \(100\) people across two schools, like our standard example from the SBM section. The first \(50\) students attend school one, and the second \(50\) students attend school two. To recap, the latent position matrix looks like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{svd}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}

\PYG{c+c1}{\PYGZsh{} deine a probability matrix for a stochastic block model with two communities}
\PYG{c+c1}{\PYGZsh{} where the first 50 students are from community one and the second 50 students are}
\PYG{c+c1}{\PYGZsh{} from community two}
\PYG{n}{P} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,}\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{P}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{50}\PYG{p}{]} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{5}
\PYG{n}{P}\PYG{p}{[}\PYG{l+m+mi}{50}\PYG{p}{:}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{:}\PYG{l+m+mi}{100}\PYG{p}{]} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{3}
\PYG{n}{P}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{:}\PYG{l+m+mi}{100}\PYG{p}{]} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{2}
\PYG{n}{P}\PYG{p}{[}\PYG{l+m+mi}{50}\PYG{p}{:}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{50}\PYG{p}{]} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{2}

\PYG{c+c1}{\PYGZsh{} use the singular value decomposition to obtain the corresponding latent}
\PYG{c+c1}{\PYGZsh{} position matrix}
\PYG{n}{U}\PYG{p}{,} \PYG{n}{S}\PYG{p}{,} \PYG{n}{V} \PYG{o}{=} \PYG{n}{svd}\PYG{p}{(}\PYG{n}{P}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{U}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{@} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{S}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multi-network-models_17_0}.png}

\sphinxAtStartPar
To sample two networks which are \(\rho\)\sphinxhyphen{}correlated SBMs, let’s assume that the correlation between the two networks is high, so we will assume \(\rho = 0.7\). We use the graspologic function to obtain a realization for each network. We show the two networks, as well as the edges which are different between the two networks. We summarize this edge difference plot with \(diff(A^{(F)} - A^{(T)})\), which simply counts the number of edges which are different:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{rdpg\PYGZus{}corr}

\PYG{n}{rho} \PYG{o}{=} \PYG{l+m+mf}{0.7}
\PYG{n}{A\PYGZus{}facebook}\PYG{p}{,} \PYG{n}{A\PYGZus{}twitter} \PYG{o}{=} \PYG{n}{rdpg\PYGZus{}corr}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{n}{rho}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multi-network-models_20_0}.png}

\sphinxAtStartPar
On the other hand, if the correlation were \(\rho = 0\) (the two networks are uncorrelated), we can see that the number of edges which are different is much higher:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rho} \PYG{o}{=} \PYG{l+m+mf}{0.0}
\PYG{n}{A\PYGZus{}facebook}\PYG{p}{,} \PYG{n}{A\PYGZus{}twitter} \PYG{o}{=} \PYG{n}{rdpg\PYGZus{}corr}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{n}{rho}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multi-network-models_23_0}.png}


\section{Network Models with Covariates}
\label{\detokenize{representations/ch5/models-with-covariates:network-models-with-covariates}}\label{\detokenize{representations/ch5/models-with-covariates::doc}}

\section{Single network model theory}
\label{\detokenize{representations/ch5/single-network-models_theory:single-network-model-theory}}\label{\detokenize{representations/ch5/single-network-models_theory::doc}}

\subsection{Foundation}
\label{\detokenize{representations/ch5/single-network-models_theory:foundation}}
\sphinxAtStartPar
To understand network models, it is crucial to understand the concept of a network as a random quantity, taking a probability distribution. We have a realization \(A\), and we think that this realization is random in some way. Stated another way, we think that there exists a network\sphinxhyphen{}valued random variable \(\mathbf A\) that governs the realizations we get to see. Since \(\mathbf A\) is a random variable, we can describe it using a probability distribution. The distribution of the random network \(\mathbf A\) is the function \(\mathbb P\) which assigns probabilities to every possible configuration that \(\mathbf A\) could take. Notationally, we write that \(\mathbf A \sim \mathbb P\), which is read in words as “the random network \(\mathbf A\) is distributed according to \(\mathbb P\).”

\sphinxAtStartPar
In the preceding description, we made a fairly substantial claim: \(\mathbb P\) assigns probabilities to every possible configuration that realizations of \(\mathbf A\), denoted by \(A\), could take. How many possibilities are there for a network with \(n\) nodes? Let’s limit ourselves to simple networks: that is, \(A\) takes values that are unweighted (\(A\) is \sphinxstyleemphasis{binary}), undirected (\(A\) is \sphinxstyleemphasis{symmetric}), and loopless (\(A\) is \sphinxstyleemphasis{hollow}). In words, \(\mathcal A_n\) is the set of all possible adjacency matrices \(A\) that correspond to simple networks with \(n\) nodes. Stated another way: every \(A\) that is found in \(\mathcal A\) is a \sphinxstyleemphasis{binary} \(n \times n\) matrix (\(A \in \{0, 1\}^{n \times n}\)), \(A\) is symmetric (\(A = A^\top\)), and \(A\) is \sphinxstyleemphasis{hollow} (\(diag(A) = 0\), or \(A_{ii} = 0\) for all \(i = 1,...,n\)). We describe \(\mathcal A_n\) as:
\begin{align*}
    \mathcal A_n = \left\{A : A \textrm{ is an $n \times n$ matrix with $0$s and $1$s}, A\textrm{ is symmetric}, A\textrm{ is hollow}\right\}
\end{align*}
\sphinxAtStartPar
To summarize the statement that \(\mathbb P\) assigns probabilities to every possible configuration that realizations of \(\mathbf A\) can take, we write that \(\mathbb P : \mathcal A_n \rightarrow [0, 1]\). This means that for any \(A \in \mathcal A_n\) which is a possible realization of a random network \(\mathbf A\), that \(\mathbb P(\mathbf A = A)\) is a probability (it takes a value between \(0\) and \(1\)). If it is completely unambiguous what the random variable \(\mathbf A\) refers to, we might abbreviate \(\mathbb P(\mathbf A = A)\) with \(\mathbb P(A)\). This statement can alternatively be read that the probability that the random variable \(\mathbf A\) takes the value \(A\) is \(\mathbb P(A)\). Finally, let’s address that question we had in the previous paragraph. How many possible adjacency matrices are in \(\mathcal A_n\)?

\sphinxAtStartPar
Let’s imagine what just one \(A \in \mathcal A_n\) can look like. Note that each matrix \(A\) has \(n \times n = n^2\) possible entries, in total, since \(A\) is an \(n \times n\) matrix. There are \(n\) possible self\sphinxhyphen{}loops for a network, but since \(\mathbf A\) is simple, it is loopless. This means that we can subtract \(n\) possible edges from \(n^2\), leaving us with \(n^2 - n = n(n-1)\) possible edges that might not be unconnected. If we think in terms of a realization \(A\), this means that we are ignoring the diagonal entries \(a_{ii}\), for all \(i \in [n]\).  Remember that a simple network is also undirected. In terms of the realization \(A\), this means that for every pair \(i\) and \(j\), that \(a_{ij} = a_{ji}\). If we were to learn about an entry in the upper triangle of \(A\) where \(a_{ij}\) is such that \(j > i\), note that we have also learned what \(a_{ji}\) is, too. This symmetry of \(A\) means that of the \(n(n-1)\) entries that are not on the diagonal of \(A\), we would, in fact, “double count” the possible number of unique values that \(A\) could have. This means that \(A\) has a total of \(\frac{1}{2}n(n - 1)\) possible entries which are \sphinxstyleemphasis{free}, which is equal to the expression \(\binom{n}{2}\). Finally, note that for each entry of \(A\), that the adjacency can take one of two possible values: \(0\) or \(1\). To write this down formally, for every possible edge which is randomly determined, we have \sphinxstyleemphasis{two} possible values that edge could take. Let’s think about building some intuition here:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
If \(A\) is \(2 \times 2\), there are \(\binom{2}{2} = 1\) unique entry of \(A\), which takes one of \(2\) values. There are \(2\) possible ways that \(A\) could look:

\end{enumerate}
\begin{align*}
    \begin{bmatrix}
        0 & 1 \\
        1 & 0
    \end{bmatrix}\textrm{ or }
    \begin{bmatrix}
        0 & 0 \\
        0 & 0
    \end{bmatrix}
\end{align*}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
If \(A\) is \(3 \times 3\), there are \(\binom{3}{2} = \frac{3 \times 2}{2} = 3\) unique entries of \(A\), each of which takes one of \(2\) values. There are \(8\) possible ways that \(A\) could look:

\end{enumerate}
\begin{align*}
&\begin{bmatrix}
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 & 1 & 0 \\
    1 & 0 & 1 \\
    0 & 1 & 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 & 0 & 1 \\
    0 & 0 & 1 \\
    1 & 1 & 0
    \end{bmatrix}
    \textrm{ or }\\
&\begin{bmatrix}
    0 & 1 & 1 \\
    1 & 0 & 0 \\
    1 & 0 & 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 & 0 & 1 \\
    0 & 0 & 0 \\
    1 & 0 & 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 & 0 & 0 \\
    0 & 0 & 1 \\
    0 & 1 & 0
    \end{bmatrix}\textrm{ or }\\
&\begin{bmatrix}
    0 & 1 & 0 \\
    1 & 0 & 0 \\
    0 & 0 & 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 & 0 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 0
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
How do we generalize this to an arbitrary choice of \(n\)? The answer is to use \sphinxstyleemphasis{combinatorics}. Basically, the approach is to look at each entry of \(A\) which can take different values, and multiply the total number of possibilities by \(2\) for every element which can take different values. Stated another way, if there are \(2\) choices for each one of \(x\) possible items, we have \(2^x\) possible ways in which we could select those \(x\) items. But we already know how many different elements there are in \(A\), so we are ready to come up with an expression for the number. In total, there are \(2^{\binom n 2}\) unique adjacency matrices in \(\mathcal A_n\). Stated another way, the \sphinxstyleemphasis{cardinality} of \(\mathcal A_n\), described by the expression \(|\mathcal A_n|\), is \(2^{\binom n 2}\). The \sphinxstylestrong{cardinality} here just means the number of elements that the set \(\mathcal A_n\) contains. When \(n\) is just \(15\), note that \(\left|\mathcal A_{15}\right| = 2^{\binom{15}{2}} = 2^{105}\), which when expressed as a power of \(10\), is more than \(10^{30}\) possible networks that can be realized with just \(15\) nodes! As \(n\) increases, how many unique possible networks are there? In the below figure, look at the value of \(|\mathcal A_n| = 2^{\binom n 2}\) as a function of \(n\). As we can see, as \(n\) gets big, \(|\mathcal A_n|\) grows really really fast!

\noindent\sphinxincludegraphics{{single-network-models_theory_2_0}.png}

\sphinxAtStartPar
So, now we know that we have probability distributions on networks, and a set \(\mathcal A_n\) which defines all of the adjacency matrices that every probability distribution must assign a probability to. Now, just what is a network model? A \sphinxstylestrong{network model} is a set \(\mathcal P\) of probability distributions on \(\mathcal A_n\). Stated another way, we can describe \(\mathcal P\) to be:
\begin{align*}
    \mathcal P &\subseteq \{\mathbb P: \mathbb P\textrm{ is a probability distribution on }\mathcal A_n\}
\end{align*}
\sphinxAtStartPar
In general, we will simplify \(\mathcal P\) through something called \sphinxstyleemphasis{parametrization}. We define \(\Theta\) to be the set of all possible parameters of the random network model, and \(\theta \in \Theta\) is a particular parameter choice that governs the parameters of a specific network\sphinxhyphen{}valued random variaable \(\mathbf A\). In this case, we will write \(\mathcal P\) as the set:
\begin{align*}
    \mathcal P(\Theta) &= \left\{\mathbb P_\theta : \theta \in \Theta\right\}
\end{align*}
\sphinxAtStartPar
If \(\mathbf A\) is a random network that follows a network model, we will write that \(\mathbf A \sim \mathbb P_\theta\), for some choice \(\theta\). We will often use the shorthand \(\mathbf A \sim \mathbb P\).

\sphinxAtStartPar
If you are used to traditional univariate or multivariate statistical modelling, an extremely natural choice for when you have a discrete sample space (like \(\mathcal A_n\), which is discrete because we can count it) would be to use a categorical model. In the categorical model, we would have a single parameter for all possible configurations of an \(n\)\sphinxhyphen{}node network; that is, \(|\theta| = \left|\mathcal A_n\right| = 2^{\binom n 2}\). What is wrong with this model? The limitations are two\sphinxhyphen{}fold:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
As we explained previously, when \(n\) is just \(15\), we would need over \(10^{30}\) bits of storage just to define \(\theta\). This amounts to more than \(10^{8}\) zetabytes, which exceeds the storage capacity of \sphinxstyleemphasis{the entire world}.

\item {} 
\sphinxAtStartPar
With a single network observed (or really, any number of networks we could collect in the real world) we would never be able to get a reasonable estimate of \(2^{\binom n 2}\) parameters for any reasonably non\sphinxhyphen{}trivial number of nodes \(n\). For the case of one observed network \(A\), an estimate of \(\theta\) (referred to as \(\hat\theta\)) would simply be for \(\hat\theta\) to have a \(1\) in the entry corresponding to our observed network, and a \(0\) everywhere else. Inferentially, this would imply that the network\sphinxhyphen{}valued random variable \(\mathbf A\) which governs realizations \(A\) is deterministic, even if this is not the case. Even if we collected potentially \sphinxstyleemphasis{many} observed networks, we would still (with very high probability) just get \(\hat \theta\) as a series of point masses on the observed networks we see, and \(0\)s everywhere else. This would mean our parameter estimates \(\hat\theta\) would not generalize to new observations at \sphinxstyleemphasis{all}, with high probability.

\end{enumerate}

\sphinxAtStartPar
So, what are some more reasonable descriptions of \(\mathcal P\)? We explore some choices below. Particularly, we will be most interested in the \sphinxstyleemphasis{independent\sphinxhyphen{}edge} networks. These are the families of networks in which the generative procedure which governs the random networks assume that the edges of the network are generated \sphinxstyleemphasis{independently}. \sphinxstylestrong{Statistical Independence} is a property which greatly simplifies many of the modelling assumptions which are crucial for proper estimation and rigorous statistical inference, which we will learn more about in the later chapters.


\subsubsection{Equivalence Classes}
\label{\detokenize{representations/ch5/single-network-models_theory:equivalence-classes}}
\sphinxAtStartPar
In all of the below models, we will explore the concept of the \sphinxstylestrong{probability equivalence class}, or an \sphinxstyleemphasis{equivalence class}, for short. The probability is a function which in general, describes how effective a particular observation can be described by a random variable \(\mathbf A\) with parameters \(\theta\), written \(\mathbf A \sim F(\theta)\). The probability will be used to describe the probability \(\mathbb P_\theta(\mathbf A)\) of observing the realization \(A\) if the underlying random variable \(\mathbf A\) has parameters \(\theta\).  Why does this matter when it comes to equivalence classes? An equivalence class is a subset of the sample space \(E \subseteq \mathcal A_n\), which has the following properties. Holding the parameters \(\theta\) fixed:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
If \(A\) and \(A'\) are members of the same equivalence class \(E\) (written \(A, A' \in E\)), then \(\mathbb P_\theta(A) = \mathbb P_\theta(A')\).

\item {} 
\sphinxAtStartPar
If \(A\) and \(A''\) are members of different equivalence classes; that is, \(A \in E\) and \(A'' \in E'\) where \(E, E'\) are equivalence classes, then \(\mathbb P_\theta(A) \neq \mathbb P_\theta(A'')\).

\item {} 
\sphinxAtStartPar
Using points 1 and 2, we can establish that if \(E\) and \(E'\) are two different equivalence classes, then \(E \cap E' = \varnothing\). That is, the equivalence classes are \sphinxstylestrong{mutually disjoint}.

\item {} 
\sphinxAtStartPar
We can use the preceding properties to deduce that given the sample space \(\mathcal A_n\) and a probability function \(\mathbb P_\theta\), we can define a partition of the sample space into equivalence classes \(E_i\), where \(i \in \mathcal I\) is an arbitrary indexing set. A \sphinxstylestrong{partition} of \(\mathcal A_n\) is a sequence of sets which are mutually disjoint, and whose union is the whole space. That is, \(\bigcup_{i \in \mathcal I} E_i = \mathcal A_n\).

\end{enumerate}

\sphinxAtStartPar
We will see more below about how the equivalence classes come into play with network models, and in a later section, we will see their relevance to the estimation of the parameters \(\theta\).


\subsubsection{Independent\sphinxhyphen{}Edge Random Networks}
\label{\detokenize{representations/ch5/single-network-models_theory:independent-edge-random-networks}}\label{\detokenize{representations/ch5/single-network-models_theory:representations-whyuse-networkmodels-iern}}
\sphinxAtStartPar
The below models are all special families of something called \sphinxstylestrong{independent\sphinxhyphen{}edge random networks}. An independent\sphinxhyphen{}edge random network is a network\sphinxhyphen{}valued random variable, in which the collection of edges are all independent. In words, this means that for every adjacency \(\mathbf a_{ij}\) of the network\sphinxhyphen{}valued random variable \(\mathbf A\), that \(\mathbf a_{ij}\) is independent of \(\mathbf a_{i'j'}\), any time that \((i,j) \neq (i',j')\). When the networks are simple, the easiest thing to do is to assume that each edge \((i,j)\) is connected with some probability (which might be different for each edge) \(p_{ij}\). We use the \(ij\) subscript to denote that this probability is not necessarily the same for each edge. This simple model can be described as \(\mathbf a_{ij}\) has the distribution \(Bern(p_{ij})\), for every \(j > i\), and is independent of every other edge in \(\mathbf A\). We only look at the entries \(j > i\), since our networks are simple. This means that knowing a realization of \(\mathbf a_{ij}\) also gives us the realizaaion of \(\mathbf a_{ji}\) (and thus \(\mathbf a_{ji}\) is a \sphinxstyleemphasis{deterministic} function of \(\mathbf a_{ij}\)). Further, we know that the random network is loopless, which means that every \(\mathbf a_{ii} = 0\). We will call the matrix \(P = (p_{ij})\) the \sphinxstylestrong{probability matrix} of the network\sphinxhyphen{}valued random variable \(\mathbf A\). In general, we will see a common theme for the probabilities of a realization \(A\) of a network\sphinxhyphen{}valued random variable \(\mathbf A\), which is that it will greatly simplify our computation. Remember that if \(\mathbf x\) and \(\mathbf y\) are binary variables which are independent, that \(\mathbb P(\mathbf x = x, \mathbf y = y) = \mathbb P(\mathbf x = x) \mathbb P(\mathbf y = y)\). Using this fact:
\begin{align*}
\mathbb P(\mathbf A = A) &= \mathbb P(\mathbf a_{11} = a_{11}, \mathbf a_{12} = a_{12}, ..., \mathbf a_{nn} = a_{nn}) \\
    &= \mathbb P(\mathbf a_{ij} = a_{ij} \text{ for all }j > i) \\
    &= \prod_{j > i}\mathbb P(\mathbf a_{ij} = a_{ij}), \;\;\;\;\textrm{Independence Assumption}
\end{align*}
\sphinxAtStartPar
Next, we will use the fact that if a random variable \(\mathbf a_{ij}\) has the Bernoulli distribution with probability \(p_{ij}\), that \(\mathbb P(\mathbf a_{ij} = a_{ij}) = p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - p_{ij}}\):
\begin{align*}
    \mathbb P_\theta(A) &= \prod_{j > i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - p_{ij}}
\end{align*}
\sphinxAtStartPar
Now that we’ve specified a probability and a very generalizable model, we’ve learned the full story behind network models and are ready to skip to estimating parameters, right? \sphinxstyleemphasis{Wrong!} Unfortunately, if we tried too estimate anything about each \(p_{ij}\) individually, we would obtain that \(p_{ij} = a_{ij}\) if we only have one realization \(A\). Even if we had many realizations of \(\mathbf A\), this still would not be very interesting, since we have a \sphinxstyleemphasis{lot} of \(p_{ij}\)s to estimate, and we’ve ignored any sort of structural model that might give us deeper insight into \(\mathbf A\). In the below sections, we will learn successively less restrictive (and hence, \sphinxstyleemphasis{more expressive}) assumptions about \(p_{ij}\)s, which will allow us to convey fairly complex random networks, but \sphinxstyleemphasis{still} enable us with plenty of intteresting things to learn about later on.


\subsection{Erdös\sphinxhyphen{}Rényi (ER) Random Networks}
\label{\detokenize{representations/ch5/single-network-models_theory:erdos-renyi-er-random-networks}}
\sphinxAtStartPar
The Erdös Rényi model formalizes this relatively simple situation with a single parameter and an \(iid\) assumption:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
\(p\)
&
\sphinxAtStartPar
\([0, 1]\)
&
\sphinxAtStartPar
Probability that an edge exists between a pair of nodes, which is identical for all pairs of nodes
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
From here on out, when we talk about an Erdös Rényi random variable, we will simply call it an ER network. In an ER network, each pair of nodes is connected with probability \(p\), and therefore not connected with probability \(1-p\). Statistically, we say that for each edge \(\mathbf{a}_{ij}\) for every pair of nodes where \(j > i\) (in terms of the adjacency matrix, this means all of the edges in the \sphinxstyleemphasis{upper right} triangle), that \(\mathbf{a}_{ij}\) is sampled independently and identically from a \sphinxstyleemphasis{Bernoulli} distribution with probability \(p\). The word “independent” means that edges in the network occurring or not occurring do not affect one another. For instance, this means that if we knew a student named Alice was friends with Bob, and Alice was also friends with Chadwick, that we do not learn any information about whether Bob is friends with Chadwick. The word “identical” means that every edge in the network has the same probability \(p\) of being connected. If Alice and Bob are friends with probability \(p\), then Alice and Chadwick are friends with probability \(p\), too. We assume here that the networks are undirected, which means that if an edge \(\mathbf a_{ij}\) exists from node \(i\) to \(j\), then the edge \(\mathbf a_{ji}\) also exists from node \(j\) to node \(i\). We also assume that the networks are loopless, which means that no edges \(\mathbf a_{ii}\) can go from node \(i\) to itself. If \(\mathbf A\) is the adjacency matrix for an ER network with probability \(p\), we write that \(\mathbf A \sim ER_n(p)\).

\sphinxAtStartPar
Next, let’s formalize an example of one of the limitations of an ER random network. Remember that we said that ER random networks are often too simple. Well, one way in which they are simple is called \sphinxstylestrong{degree homogeneity}, which is a property in which \sphinxstyleemphasis{all} of the nodes in an ER network have the \sphinxstyleemphasis{exact} same expected node degree! What this means is that if we were to take an ER random network \(\mathbf A\), we would expect that \sphinxstyleemphasis{all} of the nodes in the network had the same degree. Let’s see how this works:

\begin{sphinxadmonition}{note}{Working Out the Expected Degree in an Erdös\sphinxhyphen{}Rényi Network}

\sphinxAtStartPar
Suppose that \(\mathbf A\) is a simple network which is random. The network has \(n\) nodes \(\mathcal V = (v_i)_{i = 1}^n\). Recall that the in a simple network, the node degree is \(deg(v_i) = \sum_{j = 1}^n \mathbf a_{ij}\). What is the expected degree of a node \(v_i\) of a random network \(\mathbf A\) which is Erdös\sphinxhyphen{}Rényi?

\sphinxAtStartPar
To describe this, we will compute the expectated value of the degree \(deg(v_i)\), written \(\mathbb E\left[deg(v_i)\right]\). Let’s see what happens:
\begin{align*}
    \mathbb E\left[deg(v_i)\right] &= \mathbb E\left[\sum_{j = 1}^n \mathbf a_{ij}\right] \\
    &= \sum_{j = 1}^n \mathbb E[\mathbf a_{ij}]
\end{align*}
\sphinxAtStartPar
We use the \sphinxstyleemphasis{linearity of expectation} in the line above, which means that the expectation of a sum with a finite number of terms being summed over (\(n\), in this case) is the sum of the expectations. Finally, by definition, all of the edges \(A_{ij}\) have the same distribution: \(Bern(p)\). The expected value of a random quantity which takes a Bernoulli distribution is just the probability \(p\). This means every term \(\mathbb E[\mathbf a_{ij}] = p\). Therefore:
\begin{align*}
    \mathbb E\left[deg(v_i)\right] &= \sum_{j = 1}^n p = n\cdot p
\end{align*}
\sphinxAtStartPar
Since all of the \(n\) terms being summed have the same expected value. This holds for \sphinxstyleemphasis{every} node \(v_i\), which means that the expected degree of all nodes is an undirected ER network is the same number, \(n \cdot p\).
\end{sphinxadmonition}


\subsubsection{Probability}
\label{\detokenize{representations/ch5/single-network-models_theory:probability}}
\sphinxAtStartPar
What is the probability for realizations of Erdös\sphinxhyphen{}Rényi networks? Remember that for Independent\sphinxhyphen{}edge graphs, that the probability can be written:
\begin{align*}
    \mathbb P_{\theta}(A) &= \prod_{j > i} \mathbb P_\theta(\mathbf{a}_{ij} = a_{ij})
\end{align*}
\sphinxAtStartPar
Next, we recall that by assumption of the ER model, that the probability matrix \(P = (p)\), or that \(p_{ij} = p\) for all \(i,j\). Therefore:
\begin{align*}
    \mathbb P_\theta(A) &= \prod_{j > i} p^{a_{ij}}(1 - p)^{1 - a_{ij}} \\
    &= p^{\sum_{j > i} a_{ij}} \cdot (1 - p)^{\binom{n}{2} - \sum_{j > i}a_{ij}} \\
    &= p^{m} \cdot (1 - p)^{\binom{n}{2} - m}
\end{align*}
\sphinxAtStartPar
This means that the probability \(\mathbb P_\theta(A)\) is a function \sphinxstyleemphasis{only} of the number of edges \(m = \sum_{j > i}a_{ij}\) in the network represented by adjacency matrix \(A\). The equivalence class on the Erdös\sphinxhyphen{}Rényi networks are the sets:
\begin{align*}
    E_{i} &= \left\{A \in \mathcal A_n : m = i\right\}
\end{align*}
\sphinxAtStartPar
where \(i\) index from \(0\) (the minimum number of edges possible) all the way up to \(n^2\) (the maximum number of edges possible). All of the relationships for equivalence classes discussed above apply to the sets \(E_i\).


\subsection{Network Models for networks which aren’t simple}
\label{\detokenize{representations/ch5/single-network-models_theory:network-models-for-networks-which-aren-t-simple}}
\sphinxAtStartPar
To make the discussions a little more easy to handle, in the above descriptions and all our successive descriptions, we will describe network models for \sphinxstylestrong{simple networks}. To recap, networks which are simple are binary networks which are both loopless and undirected. Stated another way, simple networks are networks whose adjacency matrices are only \(0\)s and \(1\)s, they are hollow (the diagonal is entirely \sphinxstyleemphasis{0}), and symmetric (the lower and right triangles of the adjacency matrix are the \sphinxstyleemphasis{same}). What happens our networks don’t quite look this way?

\sphinxAtStartPar
For now, we’ll keep the assumption that the networks are binary, but we will discuss non\sphinxhyphen{}binary network models in a later chapter. We have three possibilities we can consider, and we will show how the “relaxations” of the assumptions change a description of a network model. A \sphinxstyleemphasis{relaxation}, in statistician speak, means that we are taking the assumptions that we had (in this case, that the networks are \sphinxstyleemphasis{simple}), and progressively making the assumptions weaker (more \sphinxstyleemphasis{relaxed}) so that they apply to other networks, too. We split these out so we can be as clear as possible about how the generative model changes with each relaxation step.

\sphinxAtStartPar
We will compare each relaxation to the statement about the generative model for the ER generative model. To recap, for a simple network, we wrote:

\sphinxAtStartPar
“Statistically, we say that for each edge \(\mathbf{a}_{ij}\) for every pair of nodes where \(j > i\) (in terms of the adjacency matrix, this means all of the nodes in the \sphinxstyleemphasis{upper right} triangle), that \(\mathbf{a}_{ij}\) is sampled independently and identically from a \sphinxstyleemphasis{Bernoulli} distribution with probability \(p\)….  We assume here that the networks are undirected, which means that if an edge \(\mathbf a_{ij}\) exists from node \(i\) to \(j\), then the edge \(\mathbf a_{ji}\) also exists from node \(j\) to node \(i\). We also assume that the networks are loopless, which means that no edges \(\mathbf a_{ii}\) can go from node \(i\) to itself.”

\sphinxAtStartPar
Any additional parts that are added are expressed in \sphinxstylestrong{green} font. Omitted parts are struck through with red font.

\sphinxAtStartPar
Note that these generalizations apply to \sphinxstyleemphasis{any} of the successive networks which we describe in the Network Models section, and not just the ER model!


\subsubsection{Binary network model which has loops, but is undirected}
\label{\detokenize{representations/ch5/single-network-models_theory:binary-network-model-which-has-loops-but-is-undirected}}
\sphinxAtStartPar
Here, all we want to do is relax the assumption that the network is loopless. We simply ignore the statement that edges \(\mathbf a_{ii}\) cannot exist, and allow that the \(\mathbf a_{ij}\) which follow a Bernoulli distribution (with some probability which depends on the network model choice) \sphinxstyleemphasis{now} applies to \(j \geq i\), and not just \(j > i\). We keep that an edge \(\mathbf a_{ij}\) existing implies that \(\mathbf a_{ji}\) also exists, which maintains the symmetry of \(\mathbf A\) (and consequently, the undirectedness of the network).

\sphinxAtStartPar
Our description of the ER network changes to:

\sphinxAtStartPar
Statistically, we say that for each edge \(\mathbf{a}_{ij}\) for every pair of nodes where \(\mathbf{\color{green}{j \geq i}}\) (in terms of the adjacency matrix, this means all of the nodes in the \sphinxstyleemphasis{upper right} triangle \sphinxstylestrong{and the diagonal}), that \(\mathbf{a}_{ij}\) is sampled independently and identically from a \sphinxstyleemphasis{Bernoulli} distribution with probability \(p\)….  We assume here that the networks are undirected, which means that if an edge \(\mathbf a_{ij}\) exists from node \(i\) to \(j\), then the edge \(\mathbf a_{ji}\) also exists from node \(j\) to node \(i\). We also assume that the networks are loopless, which means that no edges \(\mathbf a_{ii}\) can go from node \(i\) to itself.


\subsubsection{Binary network model which is loopless, but directed}
\label{\detokenize{representations/ch5/single-network-models_theory:binary-network-model-which-is-loopless-but-directed}}
\sphinxAtStartPar
Like above, we simply ignore the statement that \(\mathbf a_{ji} = \mathbf a_{ij}\), which removes the symmetry of \(\mathbf A\) (and consequently, removes the undirectedness of the network). We allow that the \(\mathbf a_{ij}\) which follows a Bernoulli distribution now apply to \(j \neq i\), and not just \(j > i\). We keep that \(\mathbf a_{ii} = 0\), which maintains the hollowness of \(\mathbf A\) (and consequently, the undirectedness of the network).

\sphinxAtStartPar
Our description of the ER network changes to:

\sphinxAtStartPar
Statistically, we say that for each edge \(\mathbf{a}_{ij}\) for every pair of nodes where \(\mathbf{\color{green}{j \neq i}}\) (in terms of the adjacency matrix, this means all of the nodes in the \sphinxstyleemphasis{upper right} triangle\sphinxstylestrong{which are not along the diagonal}), that \(\mathbf{a}_{ij}\) is sampled independently and identically from a \sphinxstyleemphasis{Bernoulli} distribution with probability \(p\)….  We assume here that the networks are undirected, which means that if an edge \(\mathbf a_{ij}\) exists from node \(i\) to \(j\), then the edge \(\mathbf a_{ji}\) also exists from node \(j\) to node \(i\). We also assume that the networks are loopless, which means that no edges \(\mathbf a_{ii}\) can go from node \(i\) to itself.


\subsubsection{Binary network model which is has loops and is directed}
\label{\detokenize{representations/ch5/single-network-models_theory:binary-network-model-which-is-has-loops-and-is-directed}}
\sphinxAtStartPar
Finally, for a network which has loops and is directed, we combine the above two approaches. We ignore the statements that \(\mathbf a_{ji} = \mathbf a_{ij}\), and the statement that \(\mathbf a_{ii} = 0\).

\sphinxAtStartPar
Our descriptiomn of the ER network changes to:

\sphinxAtStartPar
Statistically, we say that for each edge \(\mathbf{a}_{ij}\)  where \(j > i\) (in terms of the adjacency matrix, this means all of the nodes in the \sphinxstyleemphasis{upper right} triangle), that \(\mathbf{a}_{ij}\) is sampled independently and identically from a \sphinxstyleemphasis{Bernoulli} distribution with probability \(p\), for all possible combinations of nodes \(j\) and \(i\). We assume here that the networks are undirected, which means that if an edge \(\mathbf a_{ij}\) exists from node \(i\) to \(j\), then the edge \(\mathbf a_{ji}\) also exists from node \(j\) to node \(i\). We also assume that the networks are loopless, which means that no edges \(\mathbf a_{ii}\) can go from node \(i\) to itself.


\subsection{\sphinxstyleemphasis{A Priori} Stochastic Block Model}
\label{\detokenize{representations/ch5/single-network-models_theory:a-priori-stochastic-block-model}}
\sphinxAtStartPar
The \sphinxstyleemphasis{a priori} SBM is an SBM in which we know ahead of time (\sphinxstyleemphasis{a priori}) which nodes are in which communities. Here, we will use the variable \(K\) to denote the maximum number of different communities. The ordering of the communities does not matter; the community we call \(1\) versus \(2\) versus \(K\) is largely a symbolic distinction (the only thing that matters is that they are \sphinxstyleemphasis{different}). The \sphinxstyleemphasis{a priori} SBM has the following parameter:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
\(B\)
&
\sphinxAtStartPar
{[}0,1{]}\(^{K \times K}\)
&
\sphinxAtStartPar
The block matrix, which assigns edge probabilities for pairs of communities
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
To describe the \sphinxstyleemphasis{A Priori} SBM, we will designate the community each node is a part of using a vector, which has a single community assignment for each node in the network. We will call this \sphinxstylestrong{node assignment vector} \(\vec{\tau}\), and it is a \(n\)\sphinxhyphen{}length vector (one element for each node) with elements which can take values from \(1\) to \(K\). In symbols, we would say that \(\vec\tau \in \{1, ..., K\}^n\). What this means is that for a given element of \(\vec \tau\), \(\tau_i\), that \(\tau_i\) is the community assignment (either \(1\), \(2\), so on and so forth up to \(K\)) for the \(i^{th}\) node. If there we hahd an example where there were \(2\) communities (\(K = 2\)) for instance, and the first two nodes are in community \(1\) and the second two in community \(2\), then \(\vec\tau\) would be a vector which looks like:
\begin{align*}
    \vec\tau &= \begin{bmatrix}1 & 1 & 2 & 2\end{bmatrix}^\top
\end{align*}
\sphinxAtStartPar
Next, let’s discuss the matrix \(B\), which is known as the \sphinxstylestrong{block matrix} of the SBM. We write down that \(B \in [0, 1]^{K \times K}\), which means that the block matrix is a matrix with \(K\) rows and \(K\) columns. If we have a pair of nodes and know which of the \(K\) communities each node is from, the block matrix tells us the probability that those two nodes are connected. If our networks are simple, the matrix \(B\) is also symmetric, which means that if \(b_{kk'} = p\) where \(p\) is a probability, that \(b_{k'k} = p\), too. The requirement of \(B\) to be symmetric exists \sphinxstyleemphasis{only} if we are dealing with undirected networks.

\sphinxAtStartPar
Finally, let’s think about how to write down the generative model for the \sphinxstyleemphasis{a priori} SBM. Intuitionally what we want to reflect is, if we know that node \(i\) is in community \(k'\) and node \(j\) is in community \(k\), that the \((k', k)\) entry of the block matrix is the probability that \(i\) and \(j\) are connected. We say that given  \(\tau_i = k'\) and \(\tau_j = k\), \(\mathbf a_{ij}\) is sampled independently from a \(Bern(b_{k' k})\) distribution for all \(j > i\). Note that the adjacencies \(\mathbf a_{ij}\) are not \sphinxstyleemphasis{necessarily} identically distributed, because the probability depends on the community of edge \((i,j)\). If \(\mathbf A\) is an \sphinxstyleemphasis{a priori} SBM network with parameter \(B\), and \(\vec{\tau}\) is a realization of the node\sphinxhyphen{}assignment vector, we write that \(\mathbf A \sim SBM_{n,\vec \tau}(B)\).


\subsubsection{Probability}
\label{\detokenize{representations/ch5/single-network-models_theory:id1}}
\sphinxAtStartPar
What does the probability for the \sphinxstyleemphasis{a priori} SBM look like? In our previous description, we admittedly simplified things to an extent to keep the wording down. In truth, we model the \sphinxstyleemphasis{a priori} SBM using a \sphinxstyleemphasis{latent variable} model, which means that the node assignment vector, \(\vec{\pmb \tau}\), is treated as \sphinxstyleemphasis{random}. For the case of the \sphinxstyleemphasis{a priori} SBM, it just so happens that we \sphinxstyleemphasis{know} the specific value that this latent variable \(\vec{\pmb \tau}\) takes, \(\vec \tau\), ahead of time.

\sphinxAtStartPar
Fortunately, since \(\vec \tau\) is a \sphinxstyleemphasis{parameter} of the \sphinxstyleemphasis{a priori} SBM, the probability is a bit simpler than for the \sphinxstyleemphasis{a posteriori} SBM. This is because the \sphinxstyleemphasis{a posteriori} SBM requires an integration over potential realizations of \(\vec{\pmb \tau}\), whereas the \sphinxstyleemphasis{a priori} SBM does not, since we already know that \(\vec{\pmb \tau}\) was realized as \(\vec\tau\).

\sphinxAtStartPar
Putting these steps together gives us that:
\begin{align*}
\mathbb P_\theta(A) &= \mathbb P_{\theta}(\mathbf A = A | \vec{\pmb \tau} = \vec\tau) \\
&= \prod_{j > i} \mathbb P_\theta(\mathbf a_{ij} = a_{ij} | \vec{\pmb \tau} = \vec\tau),\;\;\;\;\textrm{Independence Assumption}
\end{align*}
\sphinxAtStartPar
Next, for the \sphinxstyleemphasis{a priori} SBM, we know that each edge \(\mathbf a_{ij}\) only \sphinxstyleemphasis{actually} depends on the community assignments of nodes \(i\) and \(j\), so we know that \(\mathbb P_{\theta}(\mathbf a_{ij} = a_{ij} | \vec{\pmb \tau} = \vec\tau) = \mathbb P(\mathbf a_{ij} = a_{ij} | \tau_i = k', \tau_j = k)\), where \(k\) and \(k'\) are any of the \(K\) possible communities. This is because the community assignments of nodes that are not nodes \(i\) and \(j\) do not matter for edge \(ij\), due to the independence assumption.

\sphinxAtStartPar
Next, let’s think about the probability matrix \(P = (p_{ij})\) for the \sphinxstyleemphasis{a priori} SBM. We know that, given that \(\tau_i = k'\) and \(\tau_j = k\),  each adjacency \(\mathbf a_{ij}\) is sampled independently and identically from a \(Bern(b_{k',k})\) distribution. This means that \(p_{ij} = b_{k',k}\). Completing our analysis from above:
\begin{align*}
    \mathbb P_\theta(A) &= \prod_{j > i} b_{k'k}^{a_{ij}}(1 - b_{k'k})^{1 - a_{ij}} \\
    &= \prod_{k,k' \in [K]}b_{k'k}^{m_{k'k}}(1 - b_{k'k})^{n_{k'k} - m_{k'k}}
\end{align*}
\sphinxAtStartPar
Where \(n_{k' k}\) denotes the total number of edges possible between nodes assigned to community \(k'\) and nodes assigned to community \(k\). That is, \(n_{k' k} = \sum_{j > i} \mathbb 1_{\tau_i = k'}\mathbb 1_{\tau_j = k}\). Further, we will use \(m_{k' k}\) to denote the total number of edges observed between these two communities. That is, \(m_{k' k} = \sum_{j > i}\mathbb 1_{\tau_i = k'}\mathbb 1_{\tau_j = k}a_{ij}\). Note that for a single \((k',k)\) community pair, that the probability is analogous to the probability of a realization of an ER random variable.



\sphinxAtStartPar
Like the ER model, there are again equivalence classes of the sample space \(\mathcal A_n\) in terms of their probability. For a two\sphinxhyphen{}community setting, with \(\vec \tau\) and \(B\) given, the equivalence classes are the sets:
\begin{align*}
    E_{a,b,c}(\vec \tau, B) &= \left\{A \in \mathcal A_n : m_{11} = a, m_{21}=m_{12} = b, m_{22} = c\right\}
\end{align*}
\sphinxAtStartPar
The number of equivalence classes possible scales with the number of communities, and the manner in which nodes are assigned to communities (particularly, the number of nodes in each community).


\subsection{\sphinxstyleemphasis{A Posteriori} Stochastic Block Model}
\label{\detokenize{representations/ch5/single-network-models_theory:a-posteriori-stochastic-block-model}}
\sphinxAtStartPar
In the \sphinxstyleemphasis{a posteriori} Stochastic Block Model (SBM), we consider that node assignment to one of \(K\) communities is a random variable, that we \sphinxstyleemphasis{don’t} know already like te \sphinxstyleemphasis{a priori} SBM. We’re going to see a funky word come up, that you’re probably not familiar with, the \sphinxstylestrong{\(K\) probability simplex}. What the heck is a probability simplex?

\sphinxAtStartPar
The intuition for a simplex is probably something you’re very familiar with, but just haven’t seen a word describe. Let’s say I have a vector, \(\vec\pi = (\pi_k)_{k \in [K]}\), which has a total of \(K\) elements. \(\vec\pi\) will be a vector, which indicates the \sphinxstyleemphasis{probability} that a given node is assigned to each of our \(K\) communities, so we need to impose some additional constraints. Symbolically, we would say that, for all \(i\), and for all \(k\):
\begin{align*}
    \pi_k = \mathbb P(\pmb\tau_i = k)
\end{align*}
\sphinxAtStartPar
The \(\vec \pi\) we’re going to use has a very special property: all of its elements are non\sphinxhyphen{}negative: for all \(\pi_k\), \(\pi_k \geq 0\). This makes sense since \(\pi_k\) is being used to represent the probability of a node \(i\) being in group \(k\), so it certainly can’t be negative. Further, there’s another thing that we want our \(\vec\pi\) to have: in order for each element \(\pi_k\) to indicate the probability of something to be assigned to \(k\), we need all of the \(\pi_k\)s to sum up to one. This is because of something called the Law of Total Probability. If we have \(K\) total values that \(\pmb \tau_i\) could take, then it is the case that:
\begin{align*}
    \sum_{k=1}^K \mathbb P(\pmb \tau_i = k) = \sum_{k = 1}^K \pi_k = 1
\end{align*}
\sphinxAtStartPar
So, back to our question: how does a probability simplex fit in? Well, the \(K\) probability simplex describes all of the possible values that our vector \(\vec\pi\) could take! In symbols, the \(K\) probability simplex is:
\begin{align*}
\left\{\vec\pi : \text{for all $k$ }\pi_k \geq 0, \sum_{k = 1}^K \pi_k = 1 \right\}
\end{align*}
\sphinxAtStartPar
So the \(K\) probability simplex is just the space for all possible vectors which could indicate assignment probabilities to one of \(K\) communities.

\sphinxAtStartPar
What does the probability simplex look like? Below, we take a look at the \(2\)\sphinxhyphen{}probability simplex (2\sphinxhyphen{}d \(\vec\pi\)s) and the \(3\)\sphinxhyphen{}probability simplex (3\sphinxhyphen{}dimensional \(\vec\pi\)s):

\noindent\sphinxincludegraphics{{single-network-models_theory_8_0}.png}

\sphinxAtStartPar
The values of \(\vec\pi = (\pi)\) that are in the \(K\)\sphinxhyphen{}probability simplex are indicated by the shaded region of each figure. This comprises the \((\pi_1, \pi_2)\) pairs that fall along a diagonal line from \((0,1)\) to \((1,0)\) for the \(2\)\sphinxhyphen{}simplex, and the \((\pi_1, \pi_2, \pi_3)\) tuples that fall on the surface of the triangular shape above with nodes at \((1,0,0)\), \((0,1,0)\), and \((0,0,1)\).

\sphinxAtStartPar
This model has the following parameters:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
\(\vec \pi\)
&
\sphinxAtStartPar
the \(K\) probability simplex
&
\sphinxAtStartPar
The probability of a node being assigned to community \(K\)
\\
\hline
\sphinxAtStartPar
\(B\)
&
\sphinxAtStartPar
{[}0,1{]}\(^{K \times K}\)
&
\sphinxAtStartPar
The block matrix, which assigns edge probabilities for pairs of communities
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The \sphinxstyleemphasis{a posteriori} SBM is a bit more complicated than the \sphinxstyleemphasis{a priori} SBM. We will think about the \sphinxstyleemphasis{a posteriori} SBM as a variation of the \sphinxstyleemphasis{a priori} SBM, where instead of the node\sphinxhyphen{}assignment vector being treated as a known fixed value (the community assignments), we will treat it as \sphinxstyleemphasis{unknown}. \(\vec{\pmb \tau}\) is called a \sphinxstyleemphasis{latent variable}, which means that it is a quantity that is never actually observed, but which will be useful for describing our model. In this case, \(\vec{\pmb \tau}\) takes values in the space \(\{1,...,K\}^n\). This means that for a given realization of \(\vec{\pmb \tau}\), denoted by \(\vec \tau\), that for each of the \(n\) nodes in the network, we suppose that an integer value between \(1\) and \(K\) indicates which community a node is from. Statistically, we write that the node assignment for node \(i\), denoted by \(\pmb \tau_i\), is sampled independently and identically from \(Categorical(\vec \pi)\). Stated another way, the vector \(\vec\pi\) indicates the probability \(\pi_k\) of assignment to each community \(k\) in the network.

\sphinxAtStartPar
The matrix \(B\) behaves exactly the same as it did with the \sphinxstyleemphasis{a posteriori} SBM. Finally, let’s think about how to write down the generative model in the \sphinxstyleemphasis{a posteriori} SBM. The model for the \sphinxstyleemphasis{a posteriori} SBM is, in fact, nearly the same as for the \sphinxstyleemphasis{a priori} SBM: we still say that given \(\tau_i = k'\) and \(\tau_j = k\), that \(\mathbf a_{ij}\) are independent \(Bern(b_{k'k})\). Here, however, we also describe that \(\pmb \tau_i\) are sampled independent and identically from \(Categorical(\vec\pi)\), as we learned above. If \(\mathbf A\) is the adjacency matrix for an \sphinxstyleemphasis{a posteriori} SBM network with parameters \(\vec \pi\) and \(B\), we write that \(\mathbf A \sim SBM_n(\vec \pi, B)\).


\subsubsection{Probability}
\label{\detokenize{representations/ch5/single-network-models_theory:id2}}
\sphinxAtStartPar
What does the probability for the \sphinxstyleemphasis{a posteriori} SBM look like? In this case, \(\theta = (\vec \pi, B)\) are the parameters for the model, so the probability for a realization \(A\) of \(\mathbf A\) is:
\begin{align*}
\mathbb P_\theta(A) &= \mathbb P_\theta(\mathbf A = A)
\end{align*}
\sphinxAtStartPar
Next, we use the fact that the probability that \(\mathbf A = A\) is, in fact, the \sphinxstyleemphasis{integration} (over realizations of \(\vec{\pmb \tau}\)) of the joint \((\mathbf A, \vec{\pmb \tau})\). In this case, we will let \(\mathcal T = \{1,...,K\}^n\) be the space of all possible realizations that \(\vec{\pmb \tau}\) could take:
\label{equation:representations/ch5/single-network-models_theory:9c16a564-ea42-456d-a5ee-f3a10fedbeb1}\begin{align}
\mathbb P_\theta(A)&= \sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A, \vec{\pmb \tau} = \vec \tau) 
\end{align}
\sphinxAtStartPar
Next, remember that by definition of a conditional probability for a random variable \(\mathbf x\) taking value \(x\) conditioned on random variable \(\mathbf y\) taking the value \(y\), that \(\mathbb P(\mathbf x = x | \mathbf y = y) = \frac{\mathbb P(\mathbf x = x, \mathbf y = y)}{\mathbb P(\mathbf y = y)}\). Note that by multiplying through by \(\mathbf P(\mathbf y = y)\), we can see that \(\mathbb P(\mathbf x = x, \mathbf y = y) = \mathbb P(\mathbf x = x| \mathbf y = y)\mathbb P(\mathbf y = y)\). Using this logic for \(\mathbf A\) and \(\vec{\pmb \tau}\):
\begin{align*}
\mathbb P_\theta(A) &=\sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A| \vec{\pmb \tau} = \vec \tau)\mathbb P(\vec{\pmb \tau} = \vec \tau)
\end{align*}
\sphinxAtStartPar
Intuitively, for each term in the sum, we are treating \(\vec{\pmb \tau}\) as taking a fixed value, \(\vec\tau\), to evaluate this probability statement.

\sphinxAtStartPar
We will start by describing \(\mathbb P(\vec{\pmb \tau} = \vec\tau)\). Remember that for \(\vec{\pmb \tau}\), that each entry \(\pmb \tau_i\) is sampled \sphinxstyleemphasis{independently and identically} from \(Categorical(\vec \pi)\).The probability mass for a \(Categorical(\vec \pi)\)\sphinxhyphen{}valued random variable is \(\mathbb P(\pmb \tau_i = \tau_i; \vec \pi) = \pi_{\tau_i}\). Finally, note that if we are taking the products of \(n\) \(\pi_{\tau_i}\) terms, that many of these values will end up being the same. Consider, for instance, if the vector \(\tau = [1,2,1,2,1]\). We end up with three terms of \(\pi_1\), and two terms of \(\pi_2\), and it does not matter which order we multiply them in. Rather, all we need to keep track of are the counts of each \(\pi\) term. Written another way, we can use the indicator that \(\tau_i = k\), given by \(\mathbb 1_{\tau_i = k}\), and a running counter over all of the community probability assignments \(\pi_k\) to make this expression a little more sensible. We will use the symbol \(n_k = \sum_{i = 1}^n \mathbb 1_{\tau_i = k}\) to denote this value, which is the number of nodes in community \(k\):
\begin{align*}
\mathbb P_\theta(\vec{\pmb \tau} = \vec \tau) &= \prod_{i = 1}^n \mathbb P_\theta(\pmb \tau_i = \tau_i),\;\;\;\;\textrm{Independence Assumption} \\
&= \prod_{i = 1}^n \pi_{\tau_i} ,\;\;\;\;\textrm{p.m.f. of a Categorical R.V.}\\
&= \prod_{k = 1}^K \pi_{k}^{n_k},\;\;\;\;\textrm{Reorganizing what we are taking products of}
\end{align*}
\sphinxAtStartPar
Next, let’s think about the conditional probability term, \(\mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau)\). Remember that the entries are all independent conditional on \(\vec{\pmb \tau}\) taking the value \(\vec\tau\). It turns out this is exactly the same result that we obtained for the \sphinxstyleemphasis{a priori} SBM:
\begin{align*}
\mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau)
&= \prod_{k',k} b_{\ell k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}
\end{align*}
\sphinxAtStartPar
Combining these into the integrand gives:
\begin{align*}
\mathbb P_\theta(A) &= \sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau) \mathbb P_\theta(\vec{\pmb \tau} = \vec \tau) \\
&= \sum_{\vec \tau \in \mathcal T} \prod_{k = 1}^K \left[\pi_k^{n_k}\cdot \prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\right]
\end{align*}
\sphinxAtStartPar
Evaluating this sum explicitly proves to be relatively tedious and is a bit outside of the scope of this book, so we will omit it here.


\subsection{Degree\sphinxhyphen{}Corrected Stochastic Block Model (DCSBM)}
\label{\detokenize{representations/ch5/single-network-models_theory:degree-corrected-stochastic-block-model-dcsbm}}
\sphinxAtStartPar
Let’s think back to our school example for the Stochastic Block Model. Remember, we had 100 students, each of whom could go to one of two possible schools: school one or school two. Our network had 100 nodes, representing each of the students. We said that the school for which each student attended was represented by their node assignment \(\tau_i\) to one of two possible communities. The matrix \(B\) was the block probaability matrix, where \(b_{11}\) was the probability that students in school one were friends, \(b_{22}\) was the probability that students in school two were friends, and \(b_{12} = b_{21}\) was the probability that students were friends if they did not go to the same school. In this case, we said that \(\mathbf A\) was an \(SBM_n(\tau, B)\) random network.

\sphinxAtStartPar
When would this setup not make sense? Let’s say that Alice and Bob both go to the same school, but Alice is more popular than Bob. In general since Alice is more popular than Bob, we might want to say that for any clasasmate, Alice gets an additional “popularity benefit” to her probability of being friends with the other classmate, and Bob gets an “unpopularity penalty.” The problem here is that within a single community of an SBM, the SBM assumes that the \sphinxstylestrong{node degree} (the number of nodes each nodes is connected to) is the \sphinxstyleemphasis{same} for all nodes within a single community. This means that we would be unable to reflect this benefit/penalty system to Alice and Bob, since each student will have the same number of friends, on average. This problem is referred to as \sphinxstylestrong{community degree homogeneity} in a Stochastic Block Model Network. Community degree homogeneity just means that the node degree is \sphinxstyleemphasis{homogeneous}, or the same, for all nodes within a community.

\begin{sphinxadmonition}{note}{Degree Homogeneity in a Stochastic Block Model Network}

\sphinxAtStartPar
Suppose that \(\mathbf A \sim SBM_{n, \vec\tau}(B)\), where \(\mathbf A\) has \(K=2\) communities. What is the node degree of each node in \(\mathbf A\)?

\sphinxAtStartPar
For an arbitrary node \(v_i\) which is in community \(k\) (either one or two), we will compute the expectated value of the degree \(deg(v_i)\), written \(\mathbb E\left[deg(v_i); \tau_i = k\right]\). We will let \(n_k\) represent the number of nodes whose node assignments \(\tau_i\) are to community \(k\). Let’s see what happens:
\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &= \mathbb E\left[\sum_{j = 1}^n \mathbf a_{ij}\right] \\
    &= \sum_{j = 1}^n \mathbb E[\mathbf a_{ij}]
\end{align*}
\sphinxAtStartPar
We use the \sphinxstyleemphasis{linearity of expectation} again to get from the top line to the second line. Next, instead of summing over all the nodes, we’ll break the sum up into the nodes which are in the same community as node \(i\), and the ones in the \sphinxstyleemphasis{other} community \(k'\). We use the notation \(k'\) to emphasize that \(k\) and \(k'\) are different values:
\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &= \sum_{j : i \neq j, \tau_j = k} \mathbb E\left[\mathbf a_{ij}\right] + \sum_{j : \tau_j =k'} \mathbb E[\mathbf a_{ij}]
\end{align*}
\sphinxAtStartPar
In the first sum, we have \(n_k-1\) total edges (the number of nodes that aren’t node \(i\), but are in the same community), and in the second sum, we have \(n_{k'}\) total edges (the number of nodes that are in the other community). Finally, we will use that the probability of an edge in the same community is \(b_{kk}\), but the probability of an edge between the communities is \(b_{k' k}\). Finally, we will use that the expected value of an adjacency \(\mathbf a_{ij}\) which is Bernoulli distributed is its probability:
\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &= \sum_{j : i \neq j, \tau_j = k} b_{kk} + \sum_{j : \tau_j = \ell} b_{kk'},\;\;\;\;\mathbf a_{ij}\textrm{ are Bernoulli distributed} \\
    &= (n_k - 1)b_{kk} + n_{k'} b_{kk'}
\end{align*}
\sphinxAtStartPar
This holds for any node \(i\) which is in community \(k\). Therefore, the expected node degree is the same, or \sphinxstylestrong{homogeneous}, within a community of an SBM.
\end{sphinxadmonition}

\sphinxAtStartPar
To address this limitation, we turn to the Degree\sphinxhyphen{}Corrected Stochastic Block Model, or DCSBM. As with the Stochastic Block Model, there is both a \sphinxstyleemphasis{a priori} and \sphinxstyleemphasis{a posteriori} DCSBM.


\subsubsection{\sphinxstyleemphasis{A Priori} DCSBM}
\label{\detokenize{representations/ch5/single-network-models_theory:a-priori-dcsbm}}
\sphinxAtStartPar
Like the \sphinxstyleemphasis{a priori} SBM, the \sphinxstyleemphasis{a priori} DCSBM is where we know which nodes are in which communities ahead of time. Here, we will use the variable \(K\) to denote the number of different communiies. The \sphinxstyleemphasis{a priori} DCSBM has the following two parameters:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
\(B\)
&
\sphinxAtStartPar
{[}0,1{]}\(^{K \times K}\)
&
\sphinxAtStartPar
The block matrix, which assigns edge probabilities for pairs of communities
\\
\hline
\sphinxAtStartPar
\(\vec\theta\)
&
\sphinxAtStartPar
\(\mathbb R^n_+\)
&
\sphinxAtStartPar
The degree correction vector, which adjusts the degree for pairs of nodes
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The latent community assignment vector \(\vec{\pmb \tau}\) with a known \sphinxstyleemphasis{a priori} realization \(\vec{\tau}\) and the block matrix \(B\) are exactly the same for the \sphinxstyleemphasis{a priori} DCSBM as they were for the \sphinxstyleemphasis{a priori} SBM.

\sphinxAtStartPar
The vector \(\vec\theta\) is the degree correction vector. Each entry \(\theta_i\) is a positive scalar. \(\theta_i\) defines how much more (or less) edges associated with node \(i\) are connected due to their association with node \(i\).

\sphinxAtStartPar
Finally, let’s think about how to write down the generative model for the \sphinxstyleemphasis{a priori} DCSBM. We say that \(\tau_i = k'\) and \(\tau_j = k\), \(\mathbf a_{ij}\) is sampled independently from a \(Bern(\theta_i \theta_j b_{k'k})\) distribution for all \(j > i\). As we can see, \(\theta_i\) in a sense is “correcting” the probabilities of each adjacency to node \(i\) to be higher, or lower, depending on the value of \(\theta_i\) that that which is given by the block probabilities \(b_{\ell k}\). If \(\mathbf A\) is an \sphinxstyleemphasis{a priori} DCSBM network with parameters and \(B\), we write that \(\mathbf A \sim DCSBM_{n,\vec\tau}(\vec \theta, B)\).


\paragraph{Probability}
\label{\detokenize{representations/ch5/single-network-models_theory:id3}}
\sphinxAtStartPar
The derivation for the probability is the same as for the \sphinxstyleemphasis{a priori} SBM, with the change that \(p_{ij} = \theta_i \theta_j b_{k'k}\) instead of just \(b_{k'k}\). This gives that the probability turns out to be:
\begin{align*}
    \mathbb P_\theta(A) &= \prod_{j > i} \left(\theta_i \theta_j b_{k'k}\right)^{a_{ij}}\left(1 - \theta_i \theta_j b_{k'k}\right)^{1 - a_{ij}}
\end{align*}
\sphinxAtStartPar
The expression doesn’t simplify much more due to the fact that the probabilities are dependent on the particular \(i\) and \(j\), so we can’t just reduce the statement in terms of \(n_{k'k}\) and \(m_{k'k}\) like for the SBM.


\subsubsection{\sphinxstyleemphasis{A Posteriori} DCSBM}
\label{\detokenize{representations/ch5/single-network-models_theory:a-posteriori-dcsbm}}
\sphinxAtStartPar
The \sphinxstyleemphasis{a posteriori} DCSBM is to the \sphinxstyleemphasis{a posteriori} SBM what the \sphinxstyleemphasis{a priori} DCSBM was to the \sphinxstyleemphasis{a priori} SBM. The changes are very minimal, so we will omit explicitly writing it all down here so we can get this section wrapped up, with the idea that the preceding section on the \sphinxstyleemphasis{a priori} DCSBM should tell you what needs to change. We will leave it as an exercise to the reader to write down a model and probability statement for realizations of the DCSBM.


\subsection{Random Dot Product Graph (RDPG)}
\label{\detokenize{representations/ch5/single-network-models_theory:random-dot-product-graph-rdpg}}

\subsubsection{\sphinxstyleemphasis{A Priori} RDPG}
\label{\detokenize{representations/ch5/single-network-models_theory:a-priori-rdpg}}
\sphinxAtStartPar
The \sphinxstyleemphasis{a priori} Random Dot Product Graph is an RDPG in which we know \sphinxstyleemphasis{a priori} the latent position matrix \(X\). The \sphinxstyleemphasis{a priori} RDPG has the following parameter:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
\(X\)
&
\sphinxAtStartPar
\( \mathbb R^{n \times d}\)
&
\sphinxAtStartPar
The matrix of latent positions for each node \(n\).
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\(X\) is called the \sphinxstylestrong{latent position matrix} of the RDPG. We write that \(X \in \mathbb R^{n \times d}\), which means that it is a matrix with real values, \(n\) rows, and \(d\) columns. We will use the notation \(\vec x_i\) to refer to the \(i^{th}\) row of \(X\). \(\vec x_i\) is referred to as the \sphinxstylestrong{latent position} of a node \(i\). This looks something like this:
\begin{align*}
    X = \begin{bmatrix}
     \vec x_{1}^\top \\
     \vdots \\
     \vec x_n^\top
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Noting that \(X\) has \(d\) columns, this implies that \(\vec x_i \in  \mathbb R^d\), or that each node’s latent position is a real\sphinxhyphen{}valued \(d\)\sphinxhyphen{}dimensional vector.

\sphinxAtStartPar
What is the generative model for the \sphinxstyleemphasis{a priori} RDPG? As we discussed above, given \(X\), for all \(j > i\), \(\mathbf a_{ij} \sim Bern(\vec x_i^\top \vec x_j)\) independently. If \(i < j\), \(\mathbf a_{ji} = \mathbf a_{ij}\) (the network is \sphinxstyleemphasis{undirected}), and \(\mathbf a_{ii} = 0\) (the network is \sphinxstyleemphasis{loopless}). If \(\mathbf A\) is an \sphinxstyleemphasis{a priori} RDPG with parameter \(X\), we write that \(\mathbf A \sim RDPG_n(X)\).




\paragraph{Probability}
\label{\detokenize{representations/ch5/single-network-models_theory:id4}}
\sphinxAtStartPar
Given \(X\), the probability for an RDPG is relatively straightforward, as an RDPG is another Independent\sphinxhyphen{}Edge Random Graph. The independence assumption vastly simplifies our resulting expression. We will also use many of the results we’ve identified above, such as the p.m.f. of a Bernoulli random variable. Finally, we’ll note that the probability matrix \(P = (\vec x_i^\top \vec x_j)\), so \(p_{ij} = \vec x_i^\top \vec x_j\):
\begin{align*}
    \mathbb P_\theta(A) &= \mathbb P_\theta(A) \\
    &= \prod_{j > i}\mathbb P(\mathbf a_{ij} = a_{ij}),\;\;\;\; \textrm{Independence Assumption} \\
    &= \prod_{j > i}(\vec x_i^\top \vec x_j)^{a_{ij}}(1 - \vec x_i^\top \vec x_j)^{1 - a_{ij}},\;\;\;\; a_{ij} \sim Bern(\vec x_i^\top \vec x_j)
\end{align*}
\sphinxAtStartPar
Unfortunately, the probability equivalence classes are a bit harder to understand intuitionally here compared to the ER and SBM examples so we won’t write them down here, but they still exist!


\subsubsection{\sphinxstyleemphasis{A Posteriori} RDPG}
\label{\detokenize{representations/ch5/single-network-models_theory:a-posteriori-rdpg}}
\sphinxAtStartPar
Like for the \sphinxstyleemphasis{a posteriori} SBM, the \sphinxstyleemphasis{a posteriori} RDPG introduces another strange set: the \sphinxstylestrong{intersection of the unit ball and the non\sphinxhyphen{}negative orthant}. Huh? This sounds like a real mouthful, but it turns out to be rather straightforward. You are probably already very familiar with a particular orthant: in two\sphinxhyphen{}dimensions, an orthant is called a quadrant. Basically, an orthant just extends the concept of a quadrant to spaces which might have more than \(2\) dimensions. The non\sphinxhyphen{}negative orthant happens to be the orthant where all of the entries are non\sphinxhyphen{}negative. We call the \sphinxstylestrong{\(K\)\sphinxhyphen{}dimensional non\sphinxhyphen{}negative orthant} the set of points in \(K\)\sphinxhyphen{}dimensional real space, where:
\begin{align*}
    \left\{\vec x \in \mathbb R^K : x_k \geq 0\text{ for all $k$}\right\}
\end{align*}
\sphinxAtStartPar
In two dimensions, this is the traditional upper\sphinxhyphen{}right portion of the standard coordinate axis. To give you a picture, the \(2\)\sphinxhyphen{}dimensional non\sphinxhyphen{}negative orthant is the blue region of the following figure:

\noindent\sphinxincludegraphics{{single-network-models_theory_18_0}.png}

\sphinxAtStartPar
Now, what is the unit ball? You are probably familiar with the idea of the unit ball, even if you haven’t heard it called that specifically. Remember that the Euclidean norm for a point \(\vec x\) which has coordinates \(x_i\) for \(i=1,...,K\) is given by the expression:
\begin{align*}
    \left|\left|\vec x\right|\right|_2 = \sqrt{\sum_{i = 1}^K x_i^2}
\end{align*}
\sphinxAtStartPar
The Euclidean unit ball is just the set of points whose Euclidean norm is at most \(1\). To be more specific, the \sphinxstylestrong{closed unit ball} with the Euclidean norm is the set of points:
\begin{align*}
    \left\{\vec x \in \mathbb R^K :\left|\left|\vec x\right|\right|_2 \leq 1\right\}
\end{align*}
\sphinxAtStartPar
We draw the \(2\)\sphinxhyphen{}dimensional unit ball with the Euclidean norm below, where the points that make up the unit ball are shown in red:

\noindent\sphinxincludegraphics{{single-network-models_theory_20_0}.png}

\sphinxAtStartPar
Now what is their intersection? Remember that the intersection of two sets \(A\) and \(B\) is the set:
\begin{align*}
    A \cap B &= \{x : x \in A, x \in B\}
\end{align*}
\sphinxAtStartPar
That is, each element must be in \sphinxstyleemphasis{both} sets to be in the intersection. The interesction of the unit ball and the non\sphinxhyphen{}negative orthant will be the set:
\begin{align*}
   \mathcal X_K = \left\{\vec x \in \mathbb R^K :\left|\left|\vec x\right|\right|_2 \leq 1, x_k \geq 0 \textrm{ for all $k$}\right\}
\end{align*}
\sphinxAtStartPar
visually, this will be the set of points in the \sphinxstyleemphasis{overlap} of the unit ball and the non\sphinxhyphen{}negative orthant, which we show below in purple:

\noindent\sphinxincludegraphics{{single-network-models_theory_22_0}.png}

\sphinxAtStartPar
This space has an \sphinxstyleemphasis{incredibly} important corollary. It turns out that if \(\vec x\) and \(\vec y\) are both elements of \(\mathcal X_K\), that \(\left\langle \vec x, \vec y \right \rangle = \vec x^\top \vec y\), the \sphinxstylestrong{inner product}, is at most \(1\), and at least \(0\). Without getting too technical, this is because of something called the Cauchy\sphinxhyphen{}Schwartz inequality and the properties of \(\mathcal X_K\). If you remember from linear algebra, the Cauchy\sphinxhyphen{}Schwartz inequality states that \(\left\langle \vec x, \vec y \right \rangle\) can be at most the product of \(\left|\left|\vec x\right|\right|_2\) and \(\left|\left|\vec y\right|\right|_2\). Since \(\vec x\) and \(\vec y\) have norms both less than or equal to \(1\) (since they are on the \sphinxstyleemphasis{unit ball}), their inner\sphinxhyphen{}product is at most \(1\). Further, since \(\vec x\) and \(\vec y\) are in the non\sphinxhyphen{}negative orthant, their inner product can never be negative. This is because both \(\vec x\) and \(\vec y\) have entries which are not negative, and therefore their element\sphinxhyphen{}wise products can never be negative.

\sphinxAtStartPar
The \sphinxstyleemphasis{a posteriori} RDPG is to the \sphinxstyleemphasis{a priori} RDPG what the \sphinxstyleemphasis{a posteriori} SBM was to the \sphinxstyleemphasis{a priori} SBM. We instead suppose that we do \sphinxstyleemphasis{not} know the latent position matrix \(X\), but instead know how we can characterize the individual latent positions. We have the following parameter:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
F
&
\sphinxAtStartPar
inner\sphinxhyphen{}product distributions
&
\sphinxAtStartPar
A distribution which governs each latent position.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The parameter \(F\) is what is known as an \sphinxstylestrong{inner\sphinxhyphen{}product distribution}. In the simplest case, we will assume that \(F\) is a distribution on a subset of the possible real vectors that have \(d\)\sphinxhyphen{}dimensions with an important caveat: for any two vectors within this subset, their inner product \sphinxstyleemphasis{must} be a probability. We will refer to the subset of the possible real vectors as \(\mathcal X_K\), which we learned about above. This means that for any \(\vec x_i, \vec x_j\) that are in \(\mathcal X_K\), it is always the case that \(\vec x_i^\top \vec x_j\) is between \(0\) and \(1\). This is essential because like previously, we will describe the distribution of each edge in the adjacency matrix using \(\vec x_i^\top \vec x_j\) to represent a probability. Next, we will treat the latent position matrix as a matrix\sphinxhyphen{}valued random variable which is \sphinxstyleemphasis{latent} (remember, \sphinxstyleemphasis{latent} means that we don’t get to see it in our real data). Like before, we will call \(\vec{\mathbf x}_i\) the random latent positions for the nodes of our network. In this case, each \(\vec {\mathbf x}_i\) is sampled independently and identically from the inner\sphinxhyphen{}product distribution \(F\) described above. The latent\sphinxhyphen{}position matrix is the matrix\sphinxhyphen{}valued random variable \(\mathbf X\) whose entries are the latent vectors \(\vec {\mathbf x}_i\), for each of the \(n\) nodes.

\sphinxAtStartPar
The model for edges of the \sphinxstyleemphasis{a posteriori} RDPG can be described by conditioning on this unobserved latent\sphinxhyphen{}position matrix. We write down that, conditioned on \(\vec {\mathbf x}_i = \vec x\) and \(\vec {\mathbf x}_j = \vec y\), that if \(j > i\), then \(\mathbf a_{ij}\) is sampled independently from a \(Bern(\vec x^\top \vec y)\) distribution. As before, if \(i < j\), \(\mathbf a_{ji} = \mathbf a_{ij}\) (the network is \sphinxstyleemphasis{undirected}), and \(\mathbf a_{ii} = 0\) (the network is \sphinxstyleemphasis{loopless}). If \(\mathbf A\) is the adjacency matrix for an \sphinxstyleemphasis{a posteriori} RDPG with parameter \(F\), we write that \(\mathbf A \sim RDPG_n(F)\).


\paragraph{Probability}
\label{\detokenize{representations/ch5/single-network-models_theory:id5}}
\sphinxAtStartPar
The probability for the \sphinxstyleemphasis{a posteriori} RDPG is fairly complicated. This is because, like the \sphinxstyleemphasis{a posteriori} SBM, we do not actually get to see the latent position matrix \(\mathbf X\), so we need to use \sphinxstyleemphasis{integration} to obtain an expression for the probability. Here, we are concerned with realizations of \(\mathbf X\). Remember that \(\mathbf X\) is just a matrix whose rows are \(\vec {\mathbf x}_i\), each of which individually have have the distribution \(F\); e.g., \(\vec{\mathbf x}_i \sim F\) independently. For simplicity, we will assume that \(F\) is a disrete distribution on \(\mathcal X_K\). This makes the logic of what is going on below much simpler since the notation gets less complicated, but does not detract from the generalizability of the result (the only difference is that sums would be replaced by multivariate integrals, and probability mass functions replaced by probability density functions).

\sphinxAtStartPar
We will let \(p\) denote the probability mass function (p.m.f.) of this discrete distribution function \(F\). The strategy will be to use the independence assumption, followed by integration over the relevant rows of \(\mathbf X\):
\begin{align*}
\mathbb P_\theta(A) &= \mathbb P_\theta(\mathbf A = A) \\
&= \prod_{j > i} \mathbb P(\mathbf a_{ij} = a_{ij}), \;\;\;\;\textrm{Independence Assumption} \\
\mathbb P(\mathbf a_{ij} = a_{ij})&= \sum_{\vec x \in \mathcal X_K}\sum_{\vec y \in \mathcal X_K}\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y),\;\;\;\;\textrm{integration over }\vec {\mathbf x}_i \textrm{ and }\vec {\mathbf x}_j
\end{align*}
\sphinxAtStartPar
Next, we will simplify this expression a little bit more, using the definition of a conditional probability like we did before for the SBM:
\begin{align*}
\\
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &= \mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) \mathbb P(\vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y)
\end{align*}
\sphinxAtStartPar
Further, remember that if \(\mathbf a\) and \(\mathbf b\) are independent, then \(\mathbb P(\mathbf a = a, \mathbf b = b) = \mathbb P(\mathbf a = a)\mathbb P(\mathbf b = b)\). Using that \(\vec x_i\) and \(\vec x_j\) are independent, by definition:
\begin{align*}
\mathbb P(\vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &= \mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}
\sphinxAtStartPar
Which means that:
\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &=  \mathbb P(\mathbf a_{ij} = a_{ij} | \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y)\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}
\sphinxAtStartPar
Finally, we that conditional on \(\vec{\mathbf x}_i = \vec x_i\) and \(\vec{\mathbf x}_j = \vec x_j\), \(\mathbf a_{ij}\) is \(Bern(\vec x_i^\top \vec x_j)\). This means that in terms of our probability matrix, each entry \(p_{ij} = \vec x_i^\top \vec x_j\). Therefore:
\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &= (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}
\end{align*}
\sphinxAtStartPar
This implies that:
\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &=  (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}
\sphinxAtStartPar
So our complete expression for the probability is:
\begin{align*}
\mathbb P_\theta(A) &= \prod_{j > i}\sum_{\vec x \in \mathcal X_K}\sum_{\vec y \in \mathcal X_K} (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}

\subsection{Generalized Random Dot Product Graph (GRDPG)}
\label{\detokenize{representations/ch5/single-network-models_theory:generalized-random-dot-product-graph-grdpg}}
\sphinxAtStartPar
The Generalized Random Dot Product Graph, or GRDPG, is the most general random network model we will consider in this book. Note that for the RDPG, the probability matrix \(P\) had entries \(p_{ij} = \vec x_i^\top \vec x_j\). What about \(p_{ji}\)? Well, \(p_{ji} = \vec x_j^\top \vec x_i\), which is exactly the same as \(p_{ij}\)! This means that even if we were to consider a directed RDPG, the probabilities that can be captured are \sphinxstyleemphasis{always} going to be symmetric. The generalized random dot product graph, or GRDPG, relaxes this assumption. This is achieved by using \sphinxstyleemphasis{two} latent positin matrices, \(X\) and \(Y\), and letting \(P = X Y^\top\). Now, the entries \(p_{ij} = \vec x_i^\top \vec y_j\), but \(p_{ji} = \vec x_j^\top \vec y_i\), which might be different.


\subsubsection{\sphinxstyleemphasis{A Priori} GRDPG}
\label{\detokenize{representations/ch5/single-network-models_theory:a-priori-grdpg}}
\sphinxAtStartPar
The \sphinxstyleemphasis{a priori} GRDPG is a GRDPG in which we know \sphinxstyleemphasis{a priori} the latent position matrices \(X\) and \(Y\). The \sphinxstyleemphasis{a priori} GRDPG has the following parameters:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
\(X\)
&
\sphinxAtStartPar
\( \mathbb R^{n \times d}\)
&
\sphinxAtStartPar
The matrix of left latent positions for each node \(n\).
\\
\hline
\sphinxAtStartPar
\(Y\)
&
\sphinxAtStartPar
\( \mathbb R^{n \times d}\)
&
\sphinxAtStartPar
The matrix of right latent positions for each node \(n\).
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\(X\) and \(Y\) behave nearly the same as the latent position matrix \(X\) for the \sphinxstyleemphasis{a priori} RDPG, with the exception that they will be called the \sphinxstylestrong{left latent position matrix} and the \sphinxstylestrong{right latent position matrix} respectively. Further, the vectors \(\vec x_i\) will be the left latent positions, and \(\vec y_i\) will be the right latent positions, for a given node \(i\), for each node \(i=1,...,n\).

\sphinxAtStartPar
What is the generative model for the \sphinxstyleemphasis{a priori} GRDPG? As we discussed above, given \(X\) and \(Y\), for all \(j \neq i\), \(\mathbf a_{ij} \sim Bern(\vec x_i^\top \vec y_j)\) independently. If we consider only loopless networks, \(\mathbf a_{ij} = 0\). If \(\mathbf A\) is an \sphinxstyleemphasis{a priori} GRDPG with left and right latent position matrices \(X\) and \(Y\), we write that \(\mathbf A \sim GRDPG_n(X, Y)\).


\subsubsection{\sphinxstyleemphasis{A Posteriori} GRDPG}
\label{\detokenize{representations/ch5/single-network-models_theory:a-posteriori-grdpg}}
\sphinxAtStartPar
The \sphinxstyleemphasis{A Posteriori} GRDPG is very similar to the \sphinxstyleemphasis{a posteriori} RDPG. We have two parameters:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
F
&
\sphinxAtStartPar
inner\sphinxhyphen{}product distributions
&
\sphinxAtStartPar
A distribution which governs the left latent positions.
\\
\hline
\sphinxAtStartPar
G
&
\sphinxAtStartPar
inner\sphinxhyphen{}product distributions
&
\sphinxAtStartPar
A distribution which governs the right latent positions.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Here, we treat the left and right latent position matrices as latent variable matrices, like we did for \sphinxstyleemphasis{a posteriori} RDPG. That is, the left latent positions are sampled independently and identically from \(F\), and the right latent positions \(\vec y_i\) are sampled independently and identically from \(G\).

\sphinxAtStartPar
The model for edges of the \sphinxstyleemphasis{a posteriori} RDPG can be described by conditioning on the unobserved left and right latent\sphinxhyphen{}position matrices. We write down that, conditioned on \(\vec {\mathbf x}_i = \vec x\) and \(\vec {\mathbf y}_j = \vec y\), that if \(j \neq i\), then \(\mathbf a_{ij}\) is sampled independently from a \(Bern(\vec x^\top \vec y)\) distribution. As before, assuming the network is loopless, \(\mathbf a_{ii} = 0\). If \(\mathbf A\) is the adjacency matrix for an \sphinxstyleemphasis{a posteriori} RDPG with parameter \(F\), we write that \(\mathbf A \sim GRDPG_n(F, G)\).


\subsection{Inhomogeneous Erdös\sphinxhyphen{}Rényi (IER)}
\label{\detokenize{representations/ch5/single-network-models_theory:inhomogeneous-erdos-renyi-ier}}
\sphinxAtStartPar
In the preceding models, we typically made assumptions about how we could characterize the edge\sphinxhyphen{}existence probabilities using fewer than \(\binom n 2\) different probabilities (one for each edge). The reason for this is that in general, \(n\) is usually relatively large, so attempting to actually learn \(\binom n 2\) different probabilities is not, in general, going to be very feasible (it is \sphinxstyleemphasis{never} feasible when we have a single network, since a single network only one observation for each independent edge). Further, it is relatively difficult to ask questions for which assuming edges share \sphinxstyleemphasis{nothing} in common (even if they don’t share the same probabilities, there may be properties underlying the probabilities, such as the \sphinxstyleemphasis{latent positions} that we saw above with the RDPG, that we might still want to characterize) is actually favorable.

\sphinxAtStartPar
Nonetheless, the most general model for an independent\sphinxhyphen{}edge random network is known as the Inhomogeneous Erdös\sphinxhyphen{}Rényi (IER) Random Network. An IER Random Network is characterized by the following parameters:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Space
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\hline
\sphinxAtStartPar
\(P\)
&
\sphinxAtStartPar
{[}0,1{]}\(^{n \times n}\)
&
\sphinxAtStartPar
The edge probability matrix.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The probability matrix \(P\) is an \(n \times n\) matrix, where each entry \(p_{ij}\) is a probability (a value between \(0\) and \(1\)). Further, if we restrict ourselves to the case of simple networks like we have done so far, \(P\) will also be symmetric (\(p_{ij} = p_{ji}\) for all \(i\) and \(j\)). The generative model is similar to the preceding models we have seen: given the \((i, j)\) entry of \(P\), denoted \(p_{ij}\), the edges \(\mathbf a_{ij}\) are independent \(Bern(p_{ij})\), for any \(j > i\). Further, \(\mathbf a_{ii} = 0\) for all \(i\) (the network is \sphinxstyleemphasis{loopless}), and \(\mathbf a_{ji} = \mathbf a_{ij}\) (the network is \sphinxstyleemphasis{undirected}). If \(\mathbf A\) is the adjacency maatrix for an IER network with probability matarix \(P\), we write that \(\mathbf A \sim IER_n(P)\).

\sphinxAtStartPar
It is worth noting that \sphinxstyleemphasis{all} of the preceding models we have discussed so far are special cases of the IER model. This means that, for instance, if we were to consider only the probability matrices where all of the entries are the same, we could represent the ER models. Similarly, if we were to only to consider the probability matrices \(P\) where \(P = XX^\top\), we could represent any RDPG.

\sphinxAtStartPar
The IER Random Network can be thought of as the limit of Stochastic Block Models, as the number of communities equals the number of nodes in the network. Stated another way, an SBM Random Network where each node is in its own community is equivalent to an IER Random Network. Under this formulation, note that the block matarix for such an SBM, \(B\), would have \(n \times n\) unique entries. Taking \(P\) to be this block matrix shows that the IER is a limiting case of SBMs.


\subsubsection{Probability}
\label{\detokenize{representations/ch5/single-network-models_theory:id6}}
\sphinxAtStartPar
The probability for a network which is IER is very straightforward. We use the independence assumption, and the p.m.f. of a Bernoulli\sphinxhyphen{}distributed random\sphinxhyphen{}variable \(\mathbf a_{ij}\):
\begin{align*}
    \mathbb P_\theta(A) &= \mathbb P(\mathbf A = A) \\
    &= \prod_{j > i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - a_{ij}}
\end{align*}

\chapter{Learning Network Representations}
\label{\detokenize{representations/ch6/ch6:learning-network-representations}}\label{\detokenize{representations/ch6/ch6::doc}}

\section{Estimating Parameters in Network Models via MLE}
\label{\detokenize{representations/ch6/estimating-parameters_mle:estimating-parameters-in-network-models-via-mle}}\label{\detokenize{representations/ch6/estimating-parameters_mle::doc}}
\sphinxAtStartPar
When we learned about random networks which can be described using single network models, one of the key things we covered were the \sphinxstyleemphasis{parameters} that define the underlying random networks. If we see a network which is a realization of a random network, we do \sphinxstyleemphasis{not}, in practice, know what those parameters that describe the random network are. However, we have a slight problem, because learning about the underlying random network \sphinxstyleemphasis{requires} us to have some understanding of the parameters that define it. What are we to do?

\sphinxAtStartPar
To overcome this hurdle, we must \sphinxstyleemphasis{estimate} the parameters of the underlying random network. At a very high level, \sphinxstylestrong{estimation} is a procedure to calculate properties about a random variable (or a set of random variables) using only the data we are given: finitely many (in network statistics, often just one) samples which we assume are realizations of the random variable we want to learn about. Here, what we want to obtain are ways in which we can \sphinxstyleemphasis{estimate} the parameters of the underlying random network, when we have a realization of a random network.


\subsection{Erdös\sphinxhyphen{}Rényi (ER)}
\label{\detokenize{representations/ch6/estimating-parameters_mle:erdos-renyi-er}}
\sphinxAtStartPar
Recall that the Erdös\sphinxhyphen{}Rényi (ER) network has a single parameter: the probability of each edge existing, which we termed \(p\). Due to the simplicity of a random network which is ER, we can resort to the Maximum Likelihood technique we described above, and it turns out we obtain virtually the same result. We find that the best estimate of the probability of an edge existing in an ER random network is just the ratio of the total number of edges in the network, \(m\), divided by the total number of edges possible in the network, which is \(\binom n 2\)! Our result is:
\begin{align*}
    \hat p &= \frac{m}{\binom n 2}.
\end{align*}
\sphinxAtStartPar
Intuitively, the estimate of the probability \(p\) is the ratio of how many edges we see in the network, \(m\), and how many edges we could have seen \(\binom n 2\)! To bring this back to our coin flip example, this is like we are saying that there is a single coin. We flip the coin once for every possible edge between those pairs of communities, \(\binom n 2\). When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads we obtained, \(m\), and divide by the number of coin flips we made, \(\binom n 2\).

\sphinxAtStartPar
Let’s work on an example. We will use a realization of a random network which is ER, with \(40\) nodes and an edge probability of \(0.2\). We begin by simulating and visualizing the appropriate network:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{er\PYGZus{}np}
\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{draw\PYGZus{}multiplot}

\PYG{n}{A} \PYG{o}{=} \PYG{n}{er\PYGZus{}np}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{40}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Simulated ER(0.2)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_mle_2_0}.png}

\sphinxAtStartPar
Next, we fit the appropriate model, from graspologic, and plot the estimated probability matrix \(\hat P\) against the true probability matrix \(P\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{plot} \PYG{k+kn}{import} \PYG{n}{heatmap}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{EREstimator}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{EREstimator}\PYG{p}{(}\PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n}{Phat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{p\PYGZus{}mat\PYGZus{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{Phat}\PYG{p}{,}
        \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{font\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,}
        \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{hat P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}ER\PYGZcb{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{P} \PYG{o}{=} \PYG{l+m+mf}{0.2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{40}\PYG{p}{,} \PYG{l+m+mi}{40}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} default entries to 0.2}
\PYG{n}{P} \PYG{o}{=} \PYG{n}{P} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{P}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{P}\PYG{p}{,}
        \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{font\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,}
        \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}ER\PYGZcb{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{Phat} \PYG{o}{\PYGZhy{}} \PYG{n}{P}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{font\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,}
        \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}|}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{hat P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}ER\PYGZcb{}}\PYG{l+s+s2}{ \PYGZhy{} P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}ER\PYGZcb{}}\PYG{l+s+s2}{|\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_mle_5_0}.png}

\sphinxAtStartPar
Not half bad! The estimated probability matrix \(\hat P\) looks extremely similar to the true probability matrix \(P\).


\subsection{Stochastic Block Model}
\label{\detokenize{representations/ch6/estimating-parameters_mle:stochastic-block-model}}
\sphinxAtStartPar
The Stochastic Block Model also has a single parameter: the block matrix, \(B\), whose entries \(b_{kk'}\) denote the probabilities of edges existing or not existing between pairs of communities in the Stochastic Block Model. When we apply the method of MLE to the SBM, what we find is that, where \(m_{kk'}\) is the total number of edges between nodes in communities \(k\) and \(k'\), and \(n_{kk'}\) is the number of edges possible between nodes in communities \(k\) and \(k'\):
\begin{align*}
    \hat b_{kk'} = \frac{m_{kk'}}{n_{kk'}}.
\end{align*}
\sphinxAtStartPar
Intuitively, the estimate of the block probability \(b_{kk'}\) is the ratio of how many edges we see between communities \(k\) and \(k'\) \(m_{kk'}\) and how many edges we could have seen \(n_{kk'}\)! To bring this back to our coin flip example, this is like we are saying that there is one coin called coin \((k, k')\) for each pair of communities in our network. We flip each coin once for every possible edge between those pairs of communities, \(n_{kk'}\). When that coin lands on heads, that particular edge is determined to exist, and when it lands on tails, that edge does not exist. Our best guess, then, is just to count the number of heads we obtained, \(m_{kk'}\), and divide by the number of coin flips we made, \(n_{kk'}\).

\sphinxAtStartPar
Let’s work through an example network, with 20 nodes in each community, and a block matrix of:
\begin{align*}
    B &= \begin{bmatrix}
        .8 & .2 \\
        .2 & .8
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Which corresponds to a probability matrix \(P\) where each entry is:
\begin{align*}
    p_{ij} &= \begin{cases}
    0.8 & i, j \leq 20 \text{ or }i, j \geq 20 \\
    0.2 & \text{otherwise}
    \end{cases}
\end{align*}
\sphinxAtStartPar
We begin by simulating an appropriate SBM:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}

\PYG{n}{n} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{]}
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{A} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{)}

\PYG{n}{y} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{y}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Simulated SBM(B)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_mle_8_0}.png}

\sphinxAtStartPar
Next, let’s fit an appropriate SBM, and investigate the estimate of \(B\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{SBMEstimator}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{SBMEstimator}\PYG{p}{(}\PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{y}\PYG{p}{)}
\PYG{n}{Bhat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{block\PYGZus{}p\PYGZus{}}
\PYG{n}{Phat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{p\PYGZus{}mat\PYGZus{}}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_mle_11_0}.png}

\sphinxAtStartPar
And our estimate \(\hat B\) is very similar to the true block matrix \(B\). This is further reflected by looking at the probability matrix, like we did for the ER example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{Phat}\PYG{p}{,}
        \PYG{n}{inner\PYGZus{}hier\PYGZus{}labels}\PYG{o}{=}\PYG{n}{y}\PYG{p}{,}
        \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{font\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,}
        \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{hat P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}SBM\PYGZcb{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{P} \PYG{o}{=} \PYG{l+m+mf}{0.2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{+} \PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{+} \PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} default entries to 0.2}
\PYG{n}{P}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mf}{0.8}  \PYG{c+c1}{\PYGZsh{} B11}
\PYG{n}{P}\PYG{p}{[}\PYG{l+m+mi}{20}\PYG{p}{:}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{:}\PYG{l+m+mi}{40}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mf}{0.8}  \PYG{c+c1}{\PYGZsh{} B22}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{fill\PYGZus{}diagonal}\PYG{p}{(}\PYG{n}{P}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} loopless}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{P}\PYG{p}{,}
        \PYG{n}{inner\PYGZus{}hier\PYGZus{}labels}\PYG{o}{=}\PYG{n}{y}\PYG{p}{,}
        \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{font\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,}
        \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}SBM\PYGZcb{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{Phat} \PYG{o}{\PYGZhy{}} \PYG{n}{P}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{inner\PYGZus{}hier\PYGZus{}labels}\PYG{o}{=}\PYG{n}{y}\PYG{p}{,}
        \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
        \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{font\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,}
        \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}|}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{hat P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}SBM\PYGZcb{}}\PYG{l+s+s2}{ \PYGZhy{} P\PYGZus{}}\PYG{l+s+si}{\PYGZob{}SBM\PYGZcb{}}\PYG{l+s+s2}{|\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_mle_13_0}.png}


\section{Why embed networks?}
\label{\detokenize{representations/ch6/why-embed-networks:why-embed-networks}}\label{\detokenize{representations/ch6/why-embed-networks::doc}}
\sphinxAtStartPar
Networks by themselves can have interesting properties, but a network is not how we traditionally organize data in machine learning. In almost any ML algorithm \sphinxhyphen{} whether you’re using a neural network or a decision tree, whether your goal is to classify observations or to predict values using regression \sphinxhyphen{} you’ll see data organized into a matrix, where the rows represent observations and the columns represent features, or variables. Each observation, or row of the matrix, is traditionally represented as a single point in \(d\)\sphinxhyphen{}dimensional space (if there are \(d\) columns in the matrix). If you have two columns, for instance, you could represent data organized in this way on an x/y coordinate plane. The first column would represent the x\sphinxhyphen{}axis, and the second column would represent the y\sphinxhyphen{}axis.

\sphinxAtStartPar
For example, the data below is organized traditionally. On the left is the data matrix; each observation has its own row, with two features across the columns. The x\sphinxhyphen{}column contains the first feature for each observation, and the y\sphinxhyphen{}column contains the second feature for each observation. We can see the two clusters of data numerically, through the color mapping.

\sphinxAtStartPar
On the right is the same data, but plotted in Euclidean space. Each column of the data matrix gets its own axis in the plot. The x and y axis location of the \(i^{th}\) point in the scatterplot is the same as the x and y values of the \(i^{th}\) row of the data matrix. We can see the two clusters of data geometrically, through the location of the points.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{load\PYGZus{}ext} autoreload
\PYG{o}{\PYGZpc{}}\PYG{k}{autoreload} 1

\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}blobs}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} make the data}
\PYG{n}{centers} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} 
                    \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{X}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{make\PYGZus{}blobs}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{cluster\PYGZus{}std}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,}
                  \PYG{n}{centers}\PYG{o}{=}\PYG{n}{centers}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} convert data into a DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{why-embed-networks_3_0}.png}

\sphinxAtStartPar
It’s often useful for our data to be organized like this, since it opens the door to a wide variety of machine learning methods. With the data above, for example, we could use scikit\sphinxhyphen{}learn to perform simple K\sphinxhyphen{}Means Clustering to find the two clusters of observations. Below, we import scikit\sphinxhyphen{}learn’s K\sphinxhyphen{}Means clustering algorithm. K\sphinxhyphen{}Means finds a pre\sphinxhyphen{}determined number of clusters in your data by setting randomly determined starting\sphinxhyphen{}points, and then iterating to get those points closer to the true cluster means. It outputs the community membership labels for each datapoint, which you can see below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}

\PYG{n}{predicted\PYGZus{}labels} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Predicted labels: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}labels}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Predicted labels:  [0 0 0 0 0 1 1 1 1 1]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{why-embed-networks_6_0}.png}

\sphinxAtStartPar
Network\sphinxhyphen{}valued data are different. Take the Stochastic Block Model below, shown as both a layout plot and an adjacency matrix. Say your goal is to view the nodes as particular observations, and you’d like to cluster the data in the same way you clustered the Euclidean data above. Intuitively, you’d expect to find two groups: one for the first set of heavily connected nodes, and one for the second set. Unfortunately, traditional machine learning algorithms won’t work on data represented as a network: it doesn’t live in the traditional rows\sphinxhyphen{}as\sphinxhyphen{}observations, columns\sphinxhyphen{}as\sphinxhyphen{}features format.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{networkx} \PYG{k}{as} \PYG{n+nn}{nx}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{p} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{9}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{l+m+mi}{25}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{why-embed-networks_9_0}.png}

\sphinxAtStartPar
You, of course, \sphinxstyleemphasis{can} make up methods which work directly on networks \sphinxhyphen{} algorithms which run by traversing along edges, for instance, or which use network statistics like node degree to learn, and so on \sphinxhyphen{} and data scientists have developed many algorithms like this. But to be able to use the entire toolbox that machine learning offers, you’d like to be able to figure out a way to \sphinxstyleemphasis{represent} networks in Euclidean space as tabular data. This is why having good embedding methods, like Spectral Embedding (which we’ll learn about soon), is useful. There’s another problem with networks that make embedding into lower\sphinxhyphen{}dimensional space useful.


\subsection{High Dimensionality of Network Data}
\label{\detokenize{representations/ch6/why-embed-networks:high-dimensionality-of-network-data}}
\sphinxAtStartPar
The other problem with network data is its high dimensionality. You could view each element of an adjacency matrix as its own (binary, for unweighted networks) dimension, for instance – although you could also make the argument that talking about dimensionality doesn’t even make \sphinxstyleemphasis{sense} with network data, since it doesn’t live in Euclidean space. Regardless, if you were to view the elements of the adjacency matrix as their own dimensions, you can get to a fairly unmanageable number of dimensions fairly quickly. Many dimensions can generally be unmanageable largely because of a machine learning concept called the \sphinxstyleemphasis{curse of dimensionality}, described below.

\begin{sphinxadmonition}{note}{The Curse of Dimensionality}

\sphinxAtStartPar
Our intuition often fails when observations have a lot of features – meaning, observations that, when you think of them geometrically, are points in very high\sphinxhyphen{}dimensional space.

\sphinxAtStartPar
For example, pick a point randomly in a 10,000\sphinxhyphen{}dimensional unit hypercube (meaning, a \(1 \times 1 \times \dots \times 1\) cube, with ten thousand 1s). You can also just think of this point as a vector with 10,000 elements, each of which has a value between 0 and 1. There’s a probability greater than 99.999999\% that the point will be located a distance less than .001 from a border of the hypercube. This probability is only 0.4\% in a unit square. This actually makes intuitive sense: if you think about measuring a lot of attributes of an object, there’s a decent chance it’ll be extreme in at least one of those attributes. Take yourself, for example. You’re probably normal in a lot of ways, but I’m sure you can think of a part of yourself which is extreme compared to other people.

\sphinxAtStartPar
An even bigger shocker: if you pick two random points in a unit square with two dimensions, they’ll be on average 0.52 units of distance away from each other. However, if you pick two random points in a unit hypercube with a million dimensions, they’ll be around 408 units away from each other. This implies that, on average, any set of points that you generate from some random process when you’re in high dimensions will be extremely far away from each other.

\sphinxAtStartPar
What this comes down to is that almost every point in ultra\sphinxhyphen{}high dimensions is extremely lonely, hugging the edge of the space it lives in, all by itself. These facts mess with many traditional machine learning methods which use relative distances, or averages (very few points in high\sphinxhyphen{}dimensional space will actually be anywhere near their average!) 
\end{sphinxadmonition}

\sphinxAtStartPar
This is where network embedding methods come into play. Because networks represented as adjacency matrices are extremely high\sphinxhyphen{}dimensional, they run into many of the issues described above. Embedding, much like traditional dimensionality reduction methods in machine learning like Principle Component Analysis (PCA), allows us to move down to a more manageable number of dimensions while still preserving useful information about the network.


\subsection{We Often Embed To Estimate Latent Positions}
\label{\detokenize{representations/ch6/why-embed-networks:we-often-embed-to-estimate-latent-positions}}
\sphinxAtStartPar
The embedding methods which we’ll explore the most in this book are the spectral methods. These methods pull heavily from linear algebra to keep only the information about our network which is useful \sphinxhyphen{} and use that information to place nodes in Euclidean space. We’ll explore other methods as well. It’s worth it to know a bit of linear algebra review here, particularly on concepts like eigenvectors and eigenvalues, as well as the properties of symmetric matrices. We’ll guide you as clearly as possible through the math in future sections.

\sphinxAtStartPar
Spectral embedding methods in particular, which we’ll talk about in the next section, will estimate an embedding called the latent position matrix. This is an \(n \times d\) matrix (where this are \(n\) rows, one for each node, and \(d\) dimensions for each row). The latent position matrix is thus organized like a traditional data table, with nodes corresponding to observations, and you could plot the rows as points in Euclidean space.


\subsection{What The Heck Is The Latent Position Matrix, Anyway?}
\label{\detokenize{representations/ch6/why-embed-networks:what-the-heck-is-the-latent-position-matrix-anyway}}
\sphinxAtStartPar
What actually is a latent position? How can we interpret a latent position matrix?

\sphinxAtStartPar
Well, assuming you’re viewing your network as some type of random dot product graph (remember that this can include SBMs, ER networks, and more), you can think of every node as being secretly associated with a position in Euclidean space. This position (relative to the positions associated with other nodes) tells you the probability that one node will have an edge with another node.

\sphinxAtStartPar
Let’s call the latent position matrix, \(X\). Remember that \(X\) has \(n\) rows (the number of nodes) and \(d\) columns (the number of dimensions). Although in practice you almost never know what the latent position matrix \sphinxstyleemphasis{actually} is, you can \sphinxstyleemphasis{estimate it} by embedding your network.

\sphinxAtStartPar
We’re going to cheat a bit and use an embedding method (in this case, adjacency spectral embedding) before we’ve discussed it, just to show what this looks like. In the next section, you’ll learn how this embedding is happening, but for now, just think of it as a way to estimate the latent positions for the nodes of a network and move from network space to Euclidean space.

\sphinxAtStartPar
Below we make a network, which in this case is an SBM. From the network, we can estimate a set of latent positions, where \(n=20\) rows for each node and \(d=2\) dimensions. Usually when something is an estimation for something else in statistics, you put a hat over it: \(\hat{X}\). We’ll do that here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} make a network}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{]}\PYG{p}{,} 
              \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{]}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} embed}
\PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{why-embed-networks_18_0}.png}

\sphinxAtStartPar
It’s good to emphasize here that we’re modeling our networks as \sphinxstyleemphasis{random dot\sphinxhyphen{}product graphs} (RDPGs). One implication is that we can think of a network as having some underlying probability distribution, and any specific network is one of many possible realizations of that distribution. It also means that each edge in our network has some \sphinxstyleemphasis{probability} of existing: nodes 0 and 3, for instance, may or may not have an edge. The concept of a latent position only works under the assumption that the network is drawn from an RDPG.


\subsubsection{The Latent Position Matrix Tells You About Edge Probabilities}
\label{\detokenize{representations/ch6/why-embed-networks:the-latent-position-matrix-tells-you-about-edge-probabilities}}
\sphinxAtStartPar
We mentioned before that the relative locations of latent positions tell you about edge probabilities, but it’s good to be a bit more specific. If you take the dot product (or the weighted sum) of row \(i\) of the latent position matrix \(X\) with row \(j\), you’ll get the probability that nodes \(i\) and \(j\) have an edge between them. Incidentally, this means that the dot product between any two rows of the latent position matrix has to be bound between 0 and 1.


\paragraph{Making A Block Probability Matrix From The Latent Positions}
\label{\detokenize{representations/ch6/why-embed-networks:making-a-block-probability-matrix-from-the-latent-positions}}
\sphinxAtStartPar
Similarly, you can find the block probability matrix \(P\) for your network using the latent positions. How would you generate \(P\) from \(X\)?

\sphinxAtStartPar
Well, you’d just multiply it by its transpose: \(P = XX^\top\). This operation will take the dot product between every row of \(X\) and put it in the result. \((XX^\top)_{ij}\) will just be the dot product between rows \(i\) and \(j\) of the latent position matrix (which is the probability that nodes \(i\) and \(j\) will be connected). So, \(XX^\top\) is just the \(n \times n\) block probability matrix \sphinxhyphen{} and if you’ve estimated your latent positions using real\sphinxhyphen{}world data, you can also estimate the block probability matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{text}


\PYG{n}{shape} \PYG{o}{=} \PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{B0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{full}\PYG{p}{(}\PYG{n}{shape}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{8}\PYG{p}{)}
\PYG{n}{B1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{full}\PYG{p}{(}\PYG{n}{shape}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} block probability matrix}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{block}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{B0}\PYG{p}{,} \PYG{n}{B1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{n}{B1}\PYG{p}{,} \PYG{n}{B0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}



\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{heatmap}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{X}\PYG{n+nd}{@X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Estimated block }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{probability matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Actual block }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{probability matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} text}
\PYG{n}{text}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.8}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{75}\PYG{p}{)}
\PYG{n}{text}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.8}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{75}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{)}
\PYG{n}{text}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{)}
\PYG{n}{text}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{75}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{75}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{why-embed-networks_24_0}.png}


\subsubsection{Thinking About Latent Positions Geometrically}
\label{\detokenize{representations/ch6/why-embed-networks:thinking-about-latent-positions-geometrically}}
\sphinxAtStartPar
You can also think about this stuff geometrically. The dot product between any two vectors \(u_i\) and \(u_j\), geometrically, is their lengths multiplied together and then weighted by the cosine of the angle between them. Smaller angles have cosines close to 1, and larger angles have cosines close to 0. So, nodes whose latent positions have larger angles between them tend to have lower edge probabilities, and nodes whose latent positions have smaller angles between them tend to have higher edge probabilities. This is the core intuition you need to understand why you can find communities and do downstream inference with latent position matrices: two nodes whose latent positions are further apart will have a smaller probability of having an edge between them!

\noindent\sphinxincludegraphics{{why-embed-networks_27_0}.png}

\sphinxAtStartPar
If you have an \sphinxstyleemphasis{estimate} for the latent positions, there’s math that shows that you get a pretty good estimate for the block probability matrix as well. In practice, that’s what you’re actually doing: getting an estimate of the latent positions with spectral embedding, then using those to do more downstream tasks or estimating block probability matrices.


\section{Spectral Embedding Methods}
\label{\detokenize{representations/ch6/spectral-embedding:spectral-embedding-methods}}\label{\detokenize{representations/ch6/spectral-embedding::doc}}
\sphinxAtStartPar
One of the primary embedding tools we’ll use in this book is a set of methods called \sphinxstyleemphasis{spectral embedding} . You’ll see spectral embedding and variations on it repeatedly, both throughout this section and when we get into applications, so it’s worth taking the time to understand spectral embedding deeply. If you’re familiar with Principal Component Analysis (PCA), this method has a lot of similarities. We’ll need to get into a bit of linear algebra to understand how it works.

\sphinxAtStartPar
Remember that the basic idea behind any network embedding method is to take the network and put it into Euclidean space \sphinxhyphen{} meaning, a nice data table with rows as observations and columns as features (or dimensions), which you can then plot on an x\sphinxhyphen{}y axis. In this section, you’ll see the linear algebra\sphinxhyphen{}centric approach that spectral embedding uses to do this.

\sphinxAtStartPar
Spectral methods are based on a bit of linear algebra, but hopefully a small enough amount to still be understandable. The overall idea has to do with eigenvectors, and more generally, something called “singular vectors” \sphinxhyphen{} a generalization of eigenvectors. It turns out that the biggest singular vectors of a network’s adjacency matrix contain the most information about that network \sphinxhyphen{} and as the singular vectors get smaller, they contain less information about the network (we’re glossing over what ‘information’ means a bit here, so just think about this as a general intuition). So if you represent a network in terms of its singular vectors, you can drop the smaller ones and still retain most of the information. This is the essence of what spectral embedding is about (here “biggest” means “the singular vector corresponding to the largest singular value”).

\begin{sphinxadmonition}{note}{Singular Values and Singular Vectors}

\sphinxAtStartPar
If you don’t know what singular values and singular vectors are, don’t worry about it. You can think of them as a generalization of eigenvalues/vectors (it’s also ok if you don’t know what those are): all matrices have singular values and singular vectors, but not all matrices have eigenvalues and eigenvectors. In the case of square, symmetric matrices with positive eigenvalues, the eigenvalues/vectors and singular values/vectors are the same thing.

\sphinxAtStartPar
If you want some more background information on eigenstuff and singularstuff, there are some explanations in the Math Refresher section in the introduction. They’re an important set of vectors associated with matrices with a bunch of interesting properties. A lot of linear algebra is built around exploring those properties.
\end{sphinxadmonition}

\sphinxAtStartPar
You can see visually how Spectral Embedding works below. We start with a 20\sphinxhyphen{}node Stochastic Block Model with two communities, and then found its singular values and vectors. It turns out that because there are only two communities, only the first two singular vectors contain information – the rest are just noise! (you can see this if you look carefully at the first two columns of the eigenvector matrix). So, we took these two columns and scaled them by the first two singular vectors of the singular value matrix \(D\). The final embedding is that scaled matrix, and the plot you see takes the rows of that matrix and puts them into Euclidean space (an x\sphinxhyphen{}y axis) as points. This matrix is called the \sphinxstyleemphasis{latent position matrix}, and the embeddings for the nodes are called the \sphinxstyleemphasis{latent positions}. Underneath the figure is a list that explains how the algorithm works, step\sphinxhyphen{}by\sphinxhyphen{}step.

\noindent\sphinxincludegraphics{{spectral-embedding_2_0}.png}

\begin{sphinxadmonition}{note}{The Spectral Embedding Algorithm}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Take a network’s adjacency matrix. Optionally take its Laplacian as a network representation.

\item {} 
\sphinxAtStartPar
Decompose it into a a singular vector matrix, a singular value matrix, and the singular vector matrix’s transpose.

\item {} 
\sphinxAtStartPar
Remove every column of the singular vector matrix except for the first \(k\) vectors, corresponding to the \(k\) largest singular values.

\item {} 
\sphinxAtStartPar
Scale the \(k\) remaining columns by their corresponding singular values to create the embedding.

\item {} 
\sphinxAtStartPar
The rows of this embedding matrix are the locations in Euclidean space for the nodes of the network (called the latent positions). The embedding matrix is an estimate of the latent position matrix (which we talked about in the ‘why embed networks’ section)

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
We need to dive into a few specifics to understand spectral embedding better. We need to figure out how to find our network’s singular vectors, for instance, and we also need to understand why those singular vectors can be used to form a representation of our network. To do this, we’ll explore a few concepts from linear algebra like matrix rank, and we’ll see how understanding these concepts connects to understanding spectral embedding.

\sphinxAtStartPar
Let’s scale down and make a simple network, with only six nodes. We’ll take its Laplacian just to show what that optional step looks like, and then we’ll find its singular vectors with a technique we’ll explore called Singular Value Decomposition. Then, we’ll explore why we can use the first \(k\) singular values and vectors to find an embedding. Let’s start with creating the simple network.


\subsection{A Simple Network}
\label{\detokenize{representations/ch6/spectral-embedding:a-simple-network}}
\sphinxAtStartPar
Say we have the simple network below. There are six nodes total, numbered 0 through 5, and there are two distinct connected groups (called “connected components” in network theory land). Nodes 0 through 2 are all connected to each other, and nodes 3 through 5 are also all connected to each other.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{itertools} \PYG{k+kn}{import} \PYG{n}{combinations}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{k}{def} \PYG{n+nf}{add\PYGZus{}edge}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{edge}\PYG{p}{:} \PYG{n+nb}{tuple}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Add an edge to an undirected graph.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{i}\PYG{p}{,} \PYG{n}{j} \PYG{o}{=} \PYG{n}{edge}
    \PYG{n}{A}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{n}{A}\PYG{p}{[}\PYG{n}{j}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{k}{return} \PYG{n}{A}

\PYG{n}{A} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{edge} \PYG{o+ow}{in} \PYG{n}{combinations}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{add\PYGZus{}edge}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{edge}\PYG{p}{)}
    
\PYG{k}{for} \PYG{n}{edge} \PYG{o+ow}{in} \PYG{n}{combinations}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{add\PYGZus{}edge}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{edge}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see the adjacency matrix and network below. Notice that there are two distrinct blocks in the adjacency matrix: in its upper\sphinxhyphen{}left, you can see the edges between the first three nodes, and in the bottom right, you can see the edges between the second three nodes.

\noindent\sphinxincludegraphics{{spectral-embedding_9_0}.png}


\subsection{The Laplacian Matrix}
\label{\detokenize{representations/ch6/spectral-embedding:the-laplacian-matrix}}
\sphinxAtStartPar
With spectral embedding, we’ll either find the singular vectors of the Laplacian or the singular vectors of the Adjacency Matrix itself (For undirected Laplacians, the singular vectors are the same thing as the eigenvectors). Since we already have the adjacency matrix, let’s take the Laplacian just to see what that looks like.

\sphinxAtStartPar
Remember from chapter four that there are a few different types of Laplacian matrices. By default, for undirected networks, Graspologic uses the normalized Laplacian \(L = D^{-1/2} A D^{-1/2}\), where \(D\) is the degree matrix. Remember that the degree matrix has the degree, or number of edges, of each node along the diagonals. Variations on the normalized Laplacian are generally what we use in practice, but for simplicity and illustration, we’ll just use the basic, cookie\sphinxhyphen{}cutter version of the Laplacian \(L = D - A\).

\sphinxAtStartPar
Here’s the degree matrix \(D\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Build the degree matrix D}
\PYG{n}{degrees} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{count\PYGZus{}nonzero}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{D} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{degrees}\PYG{p}{)}
\PYG{n}{D}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[2, 0, 0, 0, 0, 0],
       [0, 2, 0, 0, 0, 0],
       [0, 0, 2, 0, 0, 0],
       [0, 0, 0, 2, 0, 0],
       [0, 0, 0, 0, 2, 0],
       [0, 0, 0, 0, 0, 2]])
\end{sphinxVerbatim}

\sphinxAtStartPar
And here’s the Laplacian matrix, written out in full.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Build the Laplacian matrix L}
\PYG{n}{L} \PYG{o}{=} \PYG{n}{D}\PYG{o}{\PYGZhy{}}\PYG{n}{A}
\PYG{n}{L}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[ 2., \PYGZhy{}1., \PYGZhy{}1.,  0.,  0.,  0.],
       [\PYGZhy{}1.,  2., \PYGZhy{}1.,  0.,  0.,  0.],
       [\PYGZhy{}1., \PYGZhy{}1.,  2.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  2., \PYGZhy{}1., \PYGZhy{}1.],
       [ 0.,  0.,  0., \PYGZhy{}1.,  2., \PYGZhy{}1.],
       [ 0.,  0.,  0., \PYGZhy{}1., \PYGZhy{}1.,  2.]])
\end{sphinxVerbatim}

\sphinxAtStartPar
Below, you can see these matrices visually.

\noindent\sphinxincludegraphics{{spectral-embedding_17_0}.png}


\subsection{Finding Singular Vectors With Singular Value Decomposition}
\label{\detokenize{representations/ch6/spectral-embedding:finding-singular-vectors-with-singular-value-decomposition}}
\sphinxAtStartPar
Now that we have a Laplacian matrix, we’ll want to find its singular vectors. To do this, we’ll need to use a technique called \sphinxstyleemphasis{Singular Value Decomposition}, or SVD.

\sphinxAtStartPar
SVD is a way to break a single matrix apart (also known as factorizing) into three distinct new matrices – In our case, the matrix will be the Laplacian we just built. These three new matrices correspond to the singular vectors and singular values of the original matrix: the algorithm will collect all of the singular vectors as columns of one matrix, and the singular values as the diagonals of another matrix.

\sphinxAtStartPar
In the case of the Laplacian (as with all symmetric matrices that have real, positive eigenvalues), remember that the singular vectors/values and the eigenvectors/values are the same thing. For more technical and generalized details on how SVD works, or for explicit proofs, we would recommend a Linear Algebra textbook {[}Trefethan, LADR{]}. Here, we’ll look at the SVD with a bit more detail here in the specific case where we start with a matrix which is square, symmetric, and has real eigenvalues.

\sphinxAtStartPar
\sphinxstylestrong{Singular Value Decomposition} Suppose you have a square, symmetrix matrix \(X\) with real eigenvalues. In our case, \(X\) corresponds to the Laplacian \(L\) (or the adjacency matrix \(A\)).
\begin{align*}
\begin{bmatrix}
    x_{11} & & & " \\
    & x_{22} & & \\
    & & \ddots & \\
    " & & & x_{nn}
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Then, you can find three matrices \sphinxhyphen{} one which rotates vectors in space, one which scales them along each coordinate axis, and another which rotates them back \sphinxhyphen{} which, when you multiply them all together, recreate the original matrix \(X\). This is the essence of singular value decomposition: you can break down any linear transformation into a rotation, a scaling, and another rotation. Let’s call the matrix which rotates \(U\) (this type of matrix is called “orthogonal”), and the matrix that scales \(S\).
\begin{align*}
    X &= U S V^T
\end{align*}
\sphinxAtStartPar
Since \(U\) is a matrix that just rotates any vector, all of its column\sphinxhyphen{}vectors are orthogonal (all at right angles) from each other and they all have the unit length of 1. These columns are more generally called the \sphinxstylestrong{singular vectors} of X. In some specific cases, these are also called the eigenvectors. Since \(S\) just scales, it’s a diagonal matrix: there are values on the diagonals, but nothing (0) on the off\sphinxhyphen{}diagonals. The amount that each coordinate axis is scaled are the values on the diagonal entries of \(S\), \(\sigma_{i}\). These are \sphinxstylestrong{singular values} of the matrix \(X\), and, also when some conditions are met, these are also the eigenvalues. Assuming our network is undirected, this will be the case with the Laplacian matrix, but not necessarily the adjacency matrix.
\begin{align*}
    X &= \begin{bmatrix}
    \uparrow & \uparrow &  & \uparrow \\
    u_1 & \vec u_2 & ... & \vec u_n \\
    \downarrow & \downarrow &  & \downarrow
    \end{bmatrix}\begin{bmatrix}
    \sigma_1 & &  & \\
    & \sigma_2 &  & \\
    & & \ddots & \\
    & & & \sigma_n
    \end{bmatrix}\begin{bmatrix}
    \leftarrow & \vec u_1^T & \rightarrow \\
    \leftarrow & \vec u_2^T & \rightarrow \\
    & \vdots & \\
    \leftarrow & \vec u_n^T & \rightarrow \\
    \end{bmatrix}
\end{align*}

\subsection{Breaking Down Our Network’s Laplacian matrix}
\label{\detokenize{representations/ch6/spectral-embedding:breaking-down-our-network-s-laplacian-matrix}}
\sphinxAtStartPar
Now we know how to break down any random matrix into singular vectors and values with SVD, so let’s apply it to our toy network. We’ll break down our Laplacian matrix into \(U\), \(S\), and \(V^\top\). The Laplacian is a special case where the singular values and singular vectors are the same as the eigenvalues and eigenvectors, so we’ll just refer to them as eigenvalues and eigenvectors from here on, since those terms are more common. For similar (actually the same) reasons, in this case \(V^\top = U^\top\).

\sphinxAtStartPar
Here, the leftmost column of \(U\) (and the leftmost eigenvalue in \(S\)) correspond to the eigenvector with the highest eigenvalue, and they’re organized in descending order (this is standard for Singular Value Decomposition).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{svd}
\PYG{n}{U}\PYG{p}{,} \PYG{n}{S}\PYG{p}{,} \PYG{n}{Vt} \PYG{o}{=} \PYG{n}{svd}\PYG{p}{(}\PYG{n}{L}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{spectral-embedding_23_0}.png}

\sphinxAtStartPar
So now we have a collection of eigenvectors organized into a matrix with \(U\), and a collection of their corresponding eigenvalues organized into a matrix with \(S\). Remember that with Spectral Embedding, we keep only the largest eigenvalues/vectors and “clip” columns off of \(U\).

\sphinxAtStartPar
Why exactly do these matrices reconstruct our Laplacian when multiplied together? Why does the clipped version of \(U\) give us a lower\sphinxhyphen{}dimensional representation of our network? To answer that question, we’ll need to start talking about a concept in linear algebra called the \sphinxstyleemphasis{rank} of a matrix.

\sphinxAtStartPar
The essential idea is that you can turn each eigenvector/eigenvalue pair into a low\sphinxhyphen{}information matrix instead of a vector and number. Summing all of these matrices lets you reconstruct \(L\). Summing only a few of these matrices lets you get \sphinxstyleemphasis{close} to \(L\). In fact, if you were to unwrap the two matrices into single vectors, the vector you get from summing is as close in Euclidean space as you possibly can get to \(L\) given the information you deleted when you removed the smaller eigenvectors.

\sphinxAtStartPar
Let’s dive into it!


\subsection{Why We Care About Taking Eigenvectors: Matrix Rank}
\label{\detokenize{representations/ch6/spectral-embedding:why-we-care-about-taking-eigenvectors-matrix-rank}}
\sphinxAtStartPar
When we embed anything to create a new representation, we’re essentially trying to find a simpler version of that thing which preserves as much information as possible. This leads us to the concept of \sphinxstylestrong{matrix rank}.

\sphinxAtStartPar
\sphinxstylestrong{Matrix Rank}: The rank of a matrix \(X\), defined \(rank(X)\), is the number of linearly independent rows and columns of \(X\).

\sphinxAtStartPar
At a very high level, we can think of the matrix rank as telling us just how “simple” \(X\) is. A matrix which is rank \(1\) is very simple: all of its rows or columns can be expressed as a weighted sum of just a single vector. On the other hand, a matrix which has “full rank”, or a rank equal to the number of rows (or columns, whichever is smaller), is a bit more complex: no row nor column can be expressed as a weighted sum of other rows or columns.

\sphinxAtStartPar
There are a couple ways that the rank of a matrix and the singular value decomposition interact which are critical to understand: First, you can make a matrix from your singular vectors and values (eigenvectors and values, in our Laplacian’s case), and summing all of them recreates your original, full\sphinxhyphen{}rank matrix. Each matrix that you add to the sum increases the rank of the result by one. Second, summing only a few of them gets you to the best estimation of the original matrix that you can get to, given the low\sphinxhyphen{}rank result. Let’s explore this with a bit more depth.

\sphinxAtStartPar
We’ll be using the Laplacian as our examples, which has the distinctive quality of having its eigenvectors be the same as its singular vectors. For the adjacency matrix, this theory all still works, but you’d just have to replace \(\vec u_i \vec u_i^\top\) with \(\vec u_i \vec v_i^\top\) throughout (the adjacency matrices’ SVD is \(A = U S V^\top\), since the right singular vectors might be different than the left singular vectors).


\subsubsection{Summing Rank 1 Matrices Recreates The Original Matrix}
\label{\detokenize{representations/ch6/spectral-embedding:summing-rank-1-matrices-recreates-the-original-matrix}}
\sphinxAtStartPar
You can actually create an \(n \times n\) matrix using any one of the original Laplacian’s eigenvectors \(\vec u_i\) by taking its outer product \(\vec{u_i} \vec{u_i}^T\). This creates a rank one matrix which only contains the information stored in the first eigenvector. Scale it by its eigenvalue \(\sigma_i\) and you have something that feels suspiciously similar to how we take the first few singular vectors of \(U\) and scale them in the spectral embedding algorithm.

\sphinxAtStartPar
It turns out that we can express any matrix \(X\) as the sum of all of these rank one matrices.
Take the \(i^{th}\) column of \(U\). Remember that we’ve been calling this \(\vec u_i\): the \(i^{th}\) eigenvector of our Laplacian. Its corresponding eigenvalue is the \(i^{th}\) element of the diagonal eigenvalue matrix \(E\). You can make a rank one matrix from this eigenvalue/eigenvector pair by taking the outer product and scaling the result by the eigenvalue: \(\sigma_i \vec u_i \vec u_i^T\).

\sphinxAtStartPar
It turns out that when we take the sum of all of these rank \(1\) matrices–each one corresponding to a particular eigenvalue/eigenvector pair–we’ll recreate the original matrix.
\begin{align*}
    X &= \sum_{i = 1}^n \sigma_i \vec u_i \vec u_i^T = \sigma_1 \begin{bmatrix}\uparrow \\ \vec u_1 \\ \downarrow\end{bmatrix}\begin{bmatrix}\leftarrow & \vec u_1^T & \rightarrow \end{bmatrix} + 
    \sigma_2 \begin{bmatrix}\uparrow \\ \vec u_2 \\ \downarrow\end{bmatrix}\begin{bmatrix}\leftarrow & \vec u_2^T & \rightarrow \end{bmatrix} + 
    ... + 
    \sigma_n \begin{bmatrix}\uparrow \\ \vec u_n \\ \downarrow\end{bmatrix}\begin{bmatrix}\leftarrow & \vec u_n^T & \rightarrow \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Here are all of the \(\sigma_i \vec u_i \vec u_i^T\) for our Laplacian L. Since there were six nodes in the original network, there are six eigenvalue/vector pairs, and six rank 1 matrices.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}nodes} \PYG{o}{=} \PYG{n}{U}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} For each eigenvector/value,}
\PYG{c+c1}{\PYGZsh{} find its outer product,}
\PYG{c+c1}{\PYGZsh{} and append it to a list.}
\PYG{n}{low\PYGZus{}rank\PYGZus{}matrices} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{node} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}nodes}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ui} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{atleast\PYGZus{}2d}\PYG{p}{(}\PYG{n}{U}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{node}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
    \PYG{n}{vi} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{atleast\PYGZus{}2d}\PYG{p}{(}\PYG{n}{Vt}\PYG{o}{.}\PYG{n}{T}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{node}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
    \PYG{n}{low\PYGZus{}rank\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{S}\PYG{p}{[}\PYG{n}{node}\PYG{p}{]} \PYG{o}{*} \PYG{n}{ui} \PYG{o}{@} \PYG{n}{vi}\PYG{o}{.}\PYG{n}{T}
    \PYG{n}{low\PYGZus{}rank\PYGZus{}matrices}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{low\PYGZus{}rank\PYGZus{}matrix}\PYG{p}{)}
    
\PYG{c+c1}{\PYGZsh{} Take the elementwise sum of every matrix in the list.}
\PYG{n}{laplacian\PYGZus{}sum} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{low\PYGZus{}rank\PYGZus{}matrices}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see the result of the sum below. On the left are all of the low\sphinxhyphen{}rank matrices \sphinxhyphen{} one corresponding to each eigenvector \sphinxhyphen{} and on the right is the sum of all of them. You can see that the sum is just our Laplacian!

\noindent\sphinxincludegraphics{{spectral-embedding_31_0}.png}

\sphinxAtStartPar
Next up, we’ll estimate the Laplacian by only taking a few of these matrices. You can already kind of see in the figure above that this’ll work \sphinxhyphen{} the last two matrices don’t even have anything in them (they’re just 0)!


\subsubsection{We can approximate our simple Laplacian by only summing a few of the low\sphinxhyphen{}rank matrices}
\label{\detokenize{representations/ch6/spectral-embedding:we-can-approximate-our-simple-laplacian-by-only-summing-a-few-of-the-low-rank-matrices}}
\sphinxAtStartPar
When you sum the first few of these low\sphinxhyphen{}rank \(\sigma_i u_i u_i^T\), you can \sphinxstyleemphasis{approximate} your original matrix.

\sphinxAtStartPar
This tells us something interesting about Spectral Embedding: the information in the first few eigenvectors of a high rank matrix lets us find a more simple approximation to it. You can take a matrix that’s extremely complicated (high\sphinxhyphen{}rank) and project it down to something which is much less complicated (low\sphinxhyphen{}rank).

\sphinxAtStartPar
Look below. In each plot, we’re summing more and more of these low\sphinxhyphen{}rank matrices. By the time we get to the fourth sum, we’ve totally recreated the original Laplacian.

\noindent\sphinxincludegraphics{{spectral-embedding_35_0}.png}


\subsubsection{Approximating becomes extremely useful when we have a bigger (now regularized) Laplacian}
\label{\detokenize{representations/ch6/spectral-embedding:approximating-becomes-extremely-useful-when-we-have-a-bigger-now-regularized-laplacian}}
\sphinxAtStartPar
This becomes even more useful when we have huge networks with thousands of nodes, but only a few communities. It turns out, especially in this situation, we can usually sum a very small number of low\sphinxhyphen{}rank matrices and get to an excellent approximation for our network that uses much less information.

\sphinxAtStartPar
Take the network below, for example. It’s generated from a Stochastic Block Model with 1000 nodes total (500 in one community, 500 in another). We took its normalized Laplacian (remember that this means \(L = D^{-1/2} A D^{-1/2}\)), decomposed it, and summed the first two low\sphinxhyphen{}rank matrices that we generated from the eigenvector columns.

\sphinxAtStartPar
The result is not exact, but it looks pretty close. And we only needed the information from the first two singular vectors instead of all of the information in our full \(n \times n\) matrix!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{to\PYGZus{}laplacian}

\PYG{c+c1}{\PYGZsh{} Make network}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{]}\PYG{p}{,} 
              \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{l+m+mi}{25}\PYG{p}{]}
\PYG{n}{A2}\PYG{p}{,} \PYG{n}{labels2} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Form new laplacian}
\PYG{n}{L2} \PYG{o}{=} \PYG{n}{to\PYGZus{}laplacian}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} decompose}
\PYG{n}{k} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{U2}\PYG{p}{,} \PYG{n}{E2}\PYG{p}{,} \PYG{n}{Ut2} \PYG{o}{=} \PYG{n}{svd}\PYG{p}{(}\PYG{n}{L2}\PYG{p}{)}

\PYG{n}{k\PYGZus{}matrices} \PYG{o}{=} \PYG{n}{U2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{k}\PYG{p}{]}
\PYG{n}{low\PYGZus{}rank\PYGZus{}approximation} \PYG{o}{=} \PYG{n}{U2}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{n}{k}\PYG{p}{]} \PYG{o}{@} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{E2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{o}{@} \PYG{n}{Ut2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{n}{k}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Plotting}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{l2\PYGZus{}hm} \PYG{o}{=} \PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{L2}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}L\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{l2approx\PYGZus{}hm} \PYG{o}{=} \PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{low\PYGZus{}rank\PYGZus{}approximation}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sum\PYGZus{}}\PYG{l+s+s2}{\PYGZob{}\PYGZob{}}\PYG{l+s+s2}{i = 1\PYGZcb{}\PYGZcb{}\PYGZca{}}\PYG{l+s+si}{\PYGZob{}2\PYGZcb{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZus{}i u\PYGZus{}i u\PYGZus{}i\PYGZca{}T\PYGZdl{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{l2\PYGZus{}hm}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Full\PYGZhy{}rank Laplacian for a 50\PYGZhy{}node matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{15}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{l2approx\PYGZus{}hm}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sum of only two low\PYGZhy{}rank matrices}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontdict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{15}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{;}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Summing only two low\PYGZhy{}rank matrices approximates the normalized Laplacian pretty well!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{24}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{spectral-embedding_38_0}.png}

\sphinxAtStartPar
This is where a lot of the power of an SVD comes from: you can approximate extremely complicated (high\sphinxhyphen{}rank) matrices with extremely simple (low\sphinxhyphen{}rank) matrices.


\subsection{How This Matrix Rank Stuff Helps Us Understand Spectral Embedding}
\label{\detokenize{representations/ch6/spectral-embedding:how-this-matrix-rank-stuff-helps-us-understand-spectral-embedding}}
\sphinxAtStartPar
Remember the actual spectral embedding algorithm: we take a network, decompose it with Singular Value Decomposition into its singular vectors and values, and then cut out everything but the top \(k\) singular vector/value pairs. Once we scale the columns of singular vectors by their corresponding values, we have our embedding. That embedding is called the latent position matrix, and the locations in space for each of our nodes are called the latent positions.

\sphinxAtStartPar
Let’s go back to our original, small (six\sphinxhyphen{}node) network and make an estimate of the latent position matrix from it. We’ll embed down to three dimensions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{U\PYGZus{}cut} \PYG{o}{=} \PYG{n}{U}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{n}{k}\PYG{p}{]}
\PYG{n}{E\PYGZus{}cut} \PYG{o}{=} \PYG{n}{E}\PYG{p}{[}\PYG{p}{:}\PYG{n}{k}\PYG{p}{]}

\PYG{n}{latents\PYGZus{}small} \PYG{o}{=} \PYG{n}{U\PYGZus{}cut} \PYG{o}{@} \PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{E\PYGZus{}cut}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{spectral-embedding_43_0}.png}

\sphinxAtStartPar
How does what we just talked about help us understand spectral embedding?

\sphinxAtStartPar
Well, each column of the latent position matrix is the \(i^{th}\) eigenvector scaled by the \(i^{th}\) eigenvalue: \(\sigma_i \vec{u_i}\). If we right\sphinxhyphen{}multiplied one of those columns by its unscaled transpose \(\vec{u_i}^\top\), we’d have one of our rank one matrices. This means that you can think of our rank\sphinxhyphen{}one matrices as essentially just fancy versions of the columns of a latent position matrix (our embedding). They contain all the same information \sphinxhyphen{} they’re just matrices instead of vectors!

\noindent\sphinxincludegraphics{{spectral-embedding_45_0}.png}

\sphinxAtStartPar
In fact, you can express the sum we did earlier \sphinxhyphen{} our lower\sphinxhyphen{}rank estimation of L \sphinxhyphen{} with just our latent position matrix! Remember that \(U_k\) is the first \(k\) eigenvectors of our Laplacian, and \(S_k\) is the diagonal matrix with the first \(k\) eigenvalues (and that we named them \(\sigma_1\) through \(\sigma_k\)).

\noindent\sphinxincludegraphics{{spectral-embedding_47_0}.png}

\sphinxAtStartPar
This helps gives an intuition for why our latent position matrix gives a representation of our network. You can take columns of it, turn those columns into matrices, and sum those matrices, and then estimate the Laplacian for the network. That means the columns of our embedding network contain all of the information necessary to estimate the network!


\subsection{Figuring Out How Many Dimensions To Embed Your Network Into}
\label{\detokenize{representations/ch6/spectral-embedding:figuring-out-how-many-dimensions-to-embed-your-network-into}}
\sphinxAtStartPar
One thing we haven’t addressed is how to figure out how many dimensions to embed down to. We’ve generally been embedding into two dimensions throughout this chapter (mainly because it’s easier to visualize), but you can embed into as many dimensions as you want.

\sphinxAtStartPar
If you don’t have any prior information about the “true” dimensionality of your latent positions, by default you’d just be stuck guessing. Fortunately, there are some rules\sphinxhyphen{}of\sphinxhyphen{}thumb to make your guess better, and some methods people have developed to make fairly decent guesses automatically.

\sphinxAtStartPar
The most common way to pick the number of embedding dimensions is with something called a scree plot. Essentially, the intuition is this: the top singular vectors of an adjacency matrix contain the most useful information about your network, and as the singular vectors have smaller and smaller singular values, they contain less important and so are less important (this is why we’re allowed to cut out the smallest \(n-k\) singular vectors in the spectral embedding algorithm).

\sphinxAtStartPar
The scree plot just plots the singular values by their indices: the first (biggest) singular value is in the beginning, and the last (smallest) singular value is at the end.

\sphinxAtStartPar
You can see the scree plot for the Laplacian we made earlier below. We’re only plotting the first ten singular values for demonstration purposes.

\noindent\sphinxincludegraphics{{spectral-embedding_51_0}.png}

\sphinxAtStartPar
You’ll notice that there’s a marked area called the “elbow”. This is an area where singular values stop changing in magnitude as much when they get smaller: before the elbow, singular values change rapidly, and after the elbow, singular values barely change at all. (It’s called an elbow because the plot kind of looks like an arm, viewed from the side!)

\sphinxAtStartPar
The location of this elbow gives you a rough indication for how many “true” dimensions your latent positions have. The singular values after the elbow are quite close to each other and have singular vectors which are largely noise, and don’t tell you very much about your data. It looks from the scree plot that we should be embedding down to two dimensions, and that adding more dimensions would probably just mean adding noise to our embedding.

\sphinxAtStartPar
One drawback to this method is that a lot of the time, the elbow location is pretty subjective \sphinxhyphen{} real data will rarely have a nice, pretty elbow like the one you see above. The advantage is that it still generally works pretty well; embedding into a few more dimensions than you need isn’t too bad, since you’ll only have a few noies dimensions and there still may be \sphinxstyleemphasis{some} signal there.

\sphinxAtStartPar
In any case, Graspologic automates the process of finding an elbow using a popular method developed in 2006 by Mu Zhu and Ali Ghodsi at the University of Waterloo. We won’t get into the specifics of how it works here, but you can usually find fairly good elbows automatically.


\subsection{Using Graspologic to embed networks}
\label{\detokenize{representations/ch6/spectral-embedding:using-graspologic-to-embed-networks}}
\sphinxAtStartPar
It’s pretty straightforward to use graspologic’s API to embed a network. The setup works like an SKlearn class: you instantiate an AdjacencySpectralEmbed class, and then you use it to transform data. You set the number of dimensions to embed to (the number of eigenvector columns to keep!) with \sphinxcode{\sphinxupquote{n\_components}}.


\subsubsection{Adjacency Spectral Embedding}
\label{\detokenize{representations/ch6/spectral-embedding:adjacency-spectral-embedding}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}

\PYG{c+c1}{\PYGZsh{} Generate a network from an SBM}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{]}\PYG{p}{,} 
              \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{l+m+mi}{25}\PYG{p}{]}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Instantiate an ASE model and find the embedding}
\PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{embedding} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}latents}\PYG{p}{(}\PYG{n}{embedding}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Adjacency Spectral Embedding}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{spectral-embedding_57_0}.png}


\subsubsection{Laplacian Spectral Embedding}
\label{\detokenize{representations/ch6/spectral-embedding:laplacian-spectral-embedding}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{LaplacianSpectralEmbed} \PYG{k}{as} \PYG{n}{LSE}

\PYG{n}{embedding} \PYG{o}{=} \PYG{n}{LSE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}latents}\PYG{p}{(}\PYG{n}{embedding}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Laplacian Spectral Embedding}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}left\PYGZsq{}:\PYGZsq{}Laplacian Spectral Embedding\PYGZsq{}\PYGZcb{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{spectral-embedding_60_1}.png}


\subsection{When should you use ASE and when should you use LSE?}
\label{\detokenize{representations/ch6/spectral-embedding:when-should-you-use-ase-and-when-should-you-use-lse}}
\sphinxAtStartPar
Throughout this article, we’ve primarily used LSE, since Laplacians have some nice properties (such as having singular values being the same as eigenvalues) that make stuff like SVD easier to explain. However, you can embed the same network with either ASE or LSE, and you’ll get two different (but equally true) embeddings.

\sphinxAtStartPar
Since both embeddings will give you a reasonable clustering, how are they different? When should you use one compared to the other?

\sphinxAtStartPar
Well, it turns out that LSE and ASE capture different notions of “clustering”. Carey Priebe and collaborators at Johns Hopkins University investigated this recently \sphinxhyphen{} in 2018 \sphinxhyphen{} and discovered that LSE lets you capture “affinity” structure, whereas ASE lets you capture “core\sphinxhyphen{}periphery” structure (their paper is called “On a two\sphinxhyphen{}truths phenomenon in spectral graph clustering” \sphinxhyphen{} it’s an interesting read for the curious). The difference between the two types of structure is shown in the image below.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=400\sphinxpxdimen]{{two-truths}.jpeg}
\caption{Affinity vs. Core\sphinxhyphen{}periphery Structure}\label{\detokenize{representations/ch6/spectral-embedding:two-truths}}\end{figure}

\sphinxAtStartPar
The “affinity” structure \sphinxhyphen{} the one that LSE is good at finding \sphinxhyphen{} means that you have two groups of nodes which are well\sphinxhyphen{}connected within the groups, and aren’t very connected with each other. Think of a friend network in two schools, where people within the same school are much more likely to be friends than people in different schools. This is a type of structure we’ve seen a lot in this book in our Stochastic Block Model examples. If you think the communities in your data look like this, you should apply LSE to your network.

\sphinxAtStartPar
The name “core\sphinxhyphen{}periphery” is a good description for this type of structure (which ASE is good at finding). In this notion of clustering, you have a core group of well\sphinxhyphen{}connected nodes surrounded by a bunch of “outlier” nodes which just don’t have too many edges with anything in general. Think of a core of popular, well\sphinxhyphen{}liked, and charismatic kids at a high school, with a periphery of loners or people who prefer not to socialize as much.


\section{Estimating Parameters in Network Models via Spectral Methods}
\label{\detokenize{representations/ch6/estimating-parameters_spectral:estimating-parameters-in-network-models-via-spectral-methods}}\label{\detokenize{representations/ch6/estimating-parameters_spectral::doc}}
\sphinxAtStartPar
Now that we have the ability to estimate parameters via MLE for the SBM, you might think we are done discussing SBM estimation. Unfortunately, we have skipped a relatively fundamental problem with SBM parameter estimation. You will notice that everything we have covered about the SBM to date assumes that we the assignments to one of \(K\) possible communities for each node, which is given by the node assignment variable \(\tau_i\) for each node in the network. Why is this problematic? Well, quite simply, when we are learning about \sphinxstyleemphasis{many} different networks we might come across, we might not actually \sphinxstyleemphasis{observe} the communities of each node.

\sphinxAtStartPar
Consider, for instance, the school example we have worked extensively with. In the context of the SBM, it makes sense to guess that two individuals will have a higher chaance of being friends if they attend the same school than if they did not go to the same school. Remember, when we knew what school each student was from and could \sphinxstyleemphasis{order} the students by school ahead of time, the network looked like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{draw\PYGZus{}multiplot}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{ns} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} network with 100 nodes}
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} block matrix}

\PYG{c+c1}{\PYGZsh{} sample a single simple adjacency matrix from SBM(z, B)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{ns}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{zs} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_2_0}.png}

\sphinxAtStartPar
The block structure is \sphinxstyleemphasis{completely obvious}, and it seems like we could almost just guess which nodes are from which communities by looking at the adjacency matrix (with the proper ordering). Ant therein lies the issue: if we did not know which school each student was from, we would have \sphinxstyleemphasis{no way} of actually using the technique we described in the preceding chapter to estimate parameters for our SBM. If our nodes were just randomly ordered, we might see something instead like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} generate a reordering of the n nodes}
\PYG{n}{vtx\PYGZus{}perm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{n}{Aperm} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{n+nb}{tuple}\PYG{p}{(}\PYG{p}{[}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_5_0}.png}

\sphinxAtStartPar
It is extremely unclear which nodes are in which community, because there is no immediate block structure to the network. So, what \sphinxstyleemphasis{can} we do?

\sphinxAtStartPar
In fact, the way we will proceed is to devise a technique for estimation for RDPGs, and then we will use this technique for RDPG estimation to produce meaningful community assignments for our nodes. Effectively, what we will do is we will use a clever technique we discussed in the preceding sections, called an \sphinxstyleemphasis{embedding}, to learn not only about each edge, but to \sphinxstyleemphasis{learn about each node in relation to all of the other nodes}. What do we mean by this? What we mean is that, while looking at a single edge in our network, we only have two possible choices: the edge exists or it does not exist. However, if we instead consider nodes in \sphinxstyleemphasis{relation} to one another, we can start to deduce patterns about how our nodes might be organized in the community sense. While seeing that two students Bill and Stephanie were friends will not tell us whether Bill and Stephanie were in the same school, if we knew that Bill and Stephanie also shared many other friends (such as Denise, Albert, and Kwan), and those friends also tended to be friends, that piece of information might tell us that they all might happen to be school mates.

\sphinxAtStartPar
The embedding technique we employ, the \sphinxstyleemphasis{spectral embedding}, allows us to pick up on these \sphinxstyleemphasis{community} sorts of tendencies. When we call a set of nodes a \sphinxstylestrong{community} in the context of a network, what we mean is that these nodes tend to be more connected (more edges exist between and amongst them) than with other nodes in the network. The spectral embeddings will help us to identify these communities of nodes, and hopefully, when we review the communities of nodes we learn, we will be able to derive some sort of insight. For instance, in our school example, ideally, we might pick up on two communities of nodes, one for each school.


\subsection{Random Dot Product Graph}
\label{\detokenize{representations/ch6/estimating-parameters_spectral:random-dot-product-graph}}
\sphinxAtStartPar
The Random Dot Product Graph has a single parameter, the latent position matrix \(X\). Remember that the latent position matrix has \(n\) rows, one for each node in the network, and \(d\) columns, one for each latent dimension in the network.

\sphinxAtStartPar
Remember that when we see a network which we think might be a realization of an RDPG random network, we do not actually have the latent position matrix \(X\) ahead of time. For this reason, we must \sphinxstyleemphasis{estimate} \(X\). Unfortunately, we cannot just use MLE like we did for the ER and SBM networks. Instead, we will use spectral methods.

\sphinxAtStartPar
In order to produce an estimate of \(X\), we also need to know the number of latent dimensions of \(\pmb A\), \(d\). We might have a reasonable ability to “guess” what \(d\) is ahead of time, but this will often not be the case. For this reason, we can instead estimate \(d\) using the strategy described in \DUrole{xref,myst}{6.3.7}. Once we have an adjacency matrix \(A\) and a number of latent dimensions \(d\) (or an estimate of the number of latent dimensions, which we will call \(\hat d\)), producing an estimate of the latent position matrix is very straightforward. We simply use the adjacency spectral embedding to embed the adjacency matrix \(A\) into \(d\) (or, \(\hat d\)) dimensions. The resulting embedded adjacency matrix \sphinxstyleemphasis{is} the estimate of the latent position matrix. We will call the estimate of the latent position matrix \(\hat X\).

\sphinxAtStartPar
Let’s try an example of an \sphinxstyleemphasis{a priori} RDPG. We will use the same example that we used in the {\hyperref[\detokenize{representations/ch6/estimating-parameters_spectral:link?}]{\emph{section on RDPGs}}}, where we had \(100\) people living along a \(100\) mile road, with each person one mile apart. The nodes represented people, and two people were connected if they were friends. The people at the ends of the street hosted parties, and invite everyone along the street to their parties. If a person was closer to one party host, they would tend to go to that host’s parties more than the other party host. The latent positions for each person \(i\) were the vectors:
\begin{align*}
    \vec x_i &= \begin{bmatrix}
        \frac{100 - i}{100} \\ \frac{i}{100}
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
In this case, since each \(\vec x_i\) is \(2\)\sphinxhyphen{}dimensional, the number of latent dimensions in \(X\) is \(d=2\). Let’s simulate an example network:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{rdpg}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}  \PYG{c+c1}{\PYGZsh{} the number of nodes in our network}

\PYG{c+c1}{\PYGZsh{} design the latent position matrix X according to }
\PYG{c+c1}{\PYGZsh{} the rules we laid out previously}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{X}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n} \PYG{o}{\PYGZhy{}} \PYG{n}{i}\PYG{p}{)}\PYG{o}{/}\PYG{n}{n}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{i}\PYG{o}{/}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{P} \PYG{o}{=} \PYG{n}{X} \PYG{o}{@} \PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{rdpg}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_8_0}.png}

\sphinxAtStartPar
What happens when we fit a \sphinxcode{\sphinxupquote{rdpg}} model to \(A\)? We will evaluate the performance of the RDPG estimator by comparing the estimated probability matrix, \(\hat P = \hat X \hat X^\top\), to the true probability matrix, \(P = XX^\top\). We can do this using the \sphinxcode{\sphinxupquote{RDPGEstimator}} object, provided directly by graspologic:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{RDPGEstimator}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{RDPGEstimator}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} number of latent dimensions is 2}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n}{Xhat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{latent\PYGZus{}}
\PYG{n}{Phat} \PYG{o}{=} \PYG{n}{Xhat} \PYG{o}{@} \PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_11_0}.png}

\sphinxAtStartPar
Note that our estimated probability matrix tends to preserve the pattern in the true probability matrix, where the probabilities are highest for pairs of nodes which are closer together, but lower for pairs of nodes which are farther apart.

\sphinxAtStartPar
What if we did not know that there were two latent dimensions ahead of time? The RDPG Estimator handles this situation just as well, and we can estimate the number of latent dimensions with \(\hat d\) instead:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{RDPGEstimator}\PYG{p}{(}\PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} number of latent dimensions is not known}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n}{Xhat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{latent\PYGZus{}}
\PYG{n}{Phat} \PYG{o}{=} \PYG{n}{Xhat} \PYG{o}{@} \PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{)}
\PYG{n}{dhat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{latent\PYGZus{}}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Estimated number of latent dimensions: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{dhat}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Estimated number of latent dimensions: 2
\end{sphinxVerbatim}

\sphinxAtStartPar
So we can see that choosing the best\sphinxhyphen{}fit elbow instead yielded \(\hat d = 3\); that is, the number of latent dimensions are estimated to be \(3\). Again, looking at the estimated and true probability matrices:

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_15_0}.png}

\sphinxAtStartPar
Which also is a decent estimate of the true probability matrix \(P\).


\subsection{Stochastic Block Model with unknown communities}
\label{\detokenize{representations/ch6/estimating-parameters_spectral:stochastic-block-model-with-unknown-communities}}
\sphinxAtStartPar
Finally, we can return to our original goal, which was to estimate the parameters of an Stochastic Block Model when we don’t know the communities different nodes are in.


\subsubsection{Number of communities \protect\(K\protect\) is known}
\label{\detokenize{representations/ch6/estimating-parameters_spectral:number-of-communities-k-is-known}}
\sphinxAtStartPar
When the number of communities is known (even if we don’t know which community each node is in), the procedure for fitting a Stochastic Block Model to a network is relatively straightforward. Let’s consider a similar example to the scenario we had {\hyperref[\detokenize{representations/ch6/estimating-parameters_spectral:link?}]{\emph{in our introduction}}}, but with \(3\) communities instead of \(2\). We will have a block matrix given by:
\begin{align*}
    B &= \begin{bmatrix}
        0.8 & 0.2 & 0.2 \\
        0.2 & 0.8 & 0.2 \\
        0.2 & 0.2 & 0.8
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Which is a Stochastic block model in which the within\sphinxhyphen{}community edge probability is \(0.8\), and exceeds the between\sphinxhyphen{}community edge probability of \(0.2\). We will let the probability of each node being assigned to different blocks be equal, and we will produce a matrix with \(100\) nodes in total.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}

\PYG{n}{pi\PYGZus{}vec} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{3}\PYG{p}{]}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{c+c1}{\PYGZsh{} randomly assign nodes to one of three communities by sampling}
\PYG{c+c1}{\PYGZsh{} node counts}
\PYG{n}{ns} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{multinomial}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{pi\PYGZus{}vec}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{ns}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{B}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the true community labels}
\PYG{n}{y} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ns}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ns}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_19_0}.png}

\sphinxAtStartPar
Remember, however, that we do not \sphinxstyleemphasis{actually} know the community labels of each node in \(A\), so this problem is a little more difficult than it might seem. If we reordered the nodes, the coommunity each node is assigned to would not be as visually obvious as it is here in this example, as we showed back in \DUrole{xref,myst}{Chapter 5}. In real data, the nodes might not actually be ordered in a manner which makes the community structure as readily apparent.

\sphinxAtStartPar
Our goal is to learn about the block matrix, \(B\), which is the parameter that we care about for the SBM. However, we cannot just plug \(A\) into the \sphinxcode{\sphinxupquote{SBMEstimator}} class like we did back when we \DUrole{xref,myst}{fit an SBM using MLE}. This is because the \sphinxcode{\sphinxupquote{SBMEstimator}} uses node community assignments, which we do not have. Instead, what we will do is turn again to the adjacency spectral embedding, as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
We begin by reduce the observed network \(A\) to a an estimated latent position matrix, \(\hat X\) using the adjacency spectral embedding.

\item {} 
\sphinxAtStartPar
We will use K\sphinxhyphen{}Means clustering (or an alternative clustering technique, such as Gaussian Mixture Model) to assign each node’s latent position to a particular community. These will be called our estimates of the community labels for the nodes.

\item {} 
\sphinxAtStartPar
Finally, we will use the communities to which each node is assigned to estimate the block matrix, \(B\), by using the estimated community labels in conjunction with the \sphinxcode{\sphinxupquote{SBMEstimator}} class.

\end{enumerate}

\sphinxAtStartPar
We will demonstrate how to use K\sphinxhyphen{}means clustering to infer block labels here. We begin by first embedding \(A\) to estimate a latent position matrix:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed}

\PYG{n}{ase} \PYG{o}{=} \PYG{n}{AdjacencySpectralEmbed}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking}
\PYG{n}{Xhat} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
How do we explore the estimated latent position matrix \(\hat X\) to figure out whether we might be able to uncover useful community assignments for the nodes in our network?


\subsubsection{Pairs Plots}
\label{\detokenize{representations/ch6/estimating-parameters_spectral:pairs-plots}}
\sphinxAtStartPar
When embedding a matrix using any embedding technique in \sphinxcode{\sphinxupquote{graspologic}}, it is important to figure out how good that embedding is. One particularly useful way to figure out this “latent structure” (community assignments which are present, but \sphinxstyleemphasis{unknown} by us ahead of time) from a network we suspect might be well\sphinxhyphen{}fit by a Stochastic Block Model is known as a “pairs plot”. In a pairs plot, we investigate how effectively the embedding “separates” nodes within the dataset into individual “clusters”. We will ultimately exploit these “clusters” that appear in the latent positions to generate community assignments for each node. To demonstrate the case where the “pairs plot” shows obvious latent community structure, we will use the predicted latent position matrix we just produced, from an adjacency matrix which is a realization of a random network which is truly a Stochastic Block Model. The pairs plot looks like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{plot} \PYG{k+kn}{import} \PYG{n}{pairplot}

\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Pairs plot for network with communities}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_23_0}.png}

\sphinxAtStartPar
As we can see, the pairs plot is a \sphinxcode{\sphinxupquote{d x d}} matrix of plots, where \sphinxcode{\sphinxupquote{d}} is the total number of features of the matrix for which a pairs plot is being produced. For each off\sphinxhyphen{}diagonal plot (the plots with the dots), the \(k^{th}\) row and \(l^{th}\) column scatter plot has the points \((x_{ik}, x_{il})\) for each node \(i\) in the network. Stated another way, the off\sphinxhyphen{}diagonal plot is a scatter plot for each node of the \(k^{th}\) dimension and the \(l^{th}\) dimension of the latent position matrix. That these scatter plots indicate that the points appear to be separated into individual clusters provides evidence that there might be community structure in the network.

\sphinxAtStartPar
The diagonal elements of the pairs plot simply represent histograms of latent positions for each dimension. Higher bars indicate that more points have latent position estimates in that range. For instance, the top\sphinxhyphen{}left histogram indicates a histogram of the first latent dimension for all nodes, the middle histogram is a histogram of the second latent dimension for all nodes, so on and so forth.

\sphinxAtStartPar
Next, we will show a brief example of what happens when adjacency spectral embedding does not indicate that there is latent community structure. Our example network here will be a realization of a random network which is ER, with a probability of \(0.5\) for an edge existing between any pair of nodes. As an ER network does not have community structure, we wouldn’t expect the pairs plot to indicate that there are obvious clusters.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{er\PYGZus{}np}

\PYG{n}{A\PYGZus{}er} \PYG{o}{=} \PYG{n}{er\PYGZus{}np}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A\PYGZus{}er}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ER(0.5)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_25_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ase\PYGZus{}er} \PYG{o}{=} \PYG{n}{AdjacencySpectralEmbed}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking}
\PYG{n}{Xhat\PYGZus{}er} \PYG{o}{=} \PYG{n}{ase\PYGZus{}er}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A\PYGZus{}er}\PYG{p}{)}

\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{Xhat\PYGZus{}er}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Pairs plot for network without communities}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_26_0}.png}

\sphinxAtStartPar
Unlike with the SBM, we can’t see any obvious clusters in this pairs plot.

\sphinxAtStartPar
Next, let’s return to our SBM example and obtain some predicted community assignments for our points. Since we do not have any information as to which cluster each node is assigned to, we must use an unsupervised clustering method. We will use the \sphinxcode{\sphinxupquote{KMeans}} function from \sphinxcode{\sphinxupquote{sklearn}}’s cluster module to do so. Since we know that the SBM has 3 communities, we will use 3 clusters for the KMeans algorithm. The clusters produced by the \sphinxcode{\sphinxupquote{KMeans}} algorithm will be our “predicted” community assignments.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}

\PYG{n}{labels\PYGZus{}kmeans} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Since we have simulated data, we have the benefit of being able to evaluate the quality of our predicted community assignments to the true community assignments. We will use the Adjusted Rand Index (ARI), which is a measure of the clustering accuracy. A high ARI (near \(1\)) indicates a that the predicted community assignments are good relative the true community assignments, and a low ARI (near \(0\)) indicates that the predicted community assignments are not good relative the true community assignments. The ARI is agnostic to the names of the different communities, which means that even if the community labels assigned by unsupervised learning do not match the community labels in the true realized network, the ARI is still a legitimate statistic we can investigate. We will look more at the implications of this in the following paragraph.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}

\PYG{n}{ari\PYGZus{}kmeans} \PYG{o}{=} \PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}\PYG{p}{(}\PYG{n}{labels\PYGZus{}kmeans}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ARI(predicted communities, true communities) = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{ari\PYGZus{}kmeans}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ARI(predicted communities, true communities) = 1.0
\end{sphinxVerbatim}

\sphinxAtStartPar
The ARI of \(1\) indicates that the true communities and the predicted communities are in complete agreement!

\sphinxAtStartPar
When using unsupervised learning to learn about labels (such as, in this case, community assignments) for a given set of points (such as, in this case, the latent positions of each of the \(n\) \sphinxstyleemphasis{nodes} of our realized network), a truly unsupervised approach knows \sphinxstyleemphasis{nothing} about the true labels for the set of points. This has the implication that the assigned community labels may not make sense in the context of the true labels, or may not align. For instance, a predicted community of \(2\) may not mean the same thing as the true community being \(2\), since the true community assignments did not have any \sphinxstyleemphasis{Euclidean} relevance to the set of points we clustered. This means that we may have to remap the labels from the unsupervised learning predictions to better match the true labels so that we can do further diagnostics. For this reason, the \sphinxcode{\sphinxupquote{graspologic}} package offers the \sphinxcode{\sphinxupquote{remap\_labels}} utility function:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{remap\PYGZus{}labels}

\PYG{n}{labels\PYGZus{}kmeans\PYGZus{}remap} \PYG{o}{=} \PYG{n}{remap\PYGZus{}labels}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{labels\PYGZus{}kmeans}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
We can use these remapped labels to understand when \sphinxcode{\sphinxupquote{KMeans}} is, or is not, producing reasonable labels for our investigation. We begin by first looking at a pairs plot, which now will color the points by their assigned community:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{,}
         \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels\PYGZus{}kmeans\PYGZus{}remap}\PYG{p}{,}
         \PYG{n}{title}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{KMeans on embedding, ARI: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{ari\PYGZus{}kmeans}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{legend\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mf}{3.5}\PYG{p}{,}
         \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{muted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_34_0}.png}

\sphinxAtStartPar
The final utility of the pairs plot is that we can investigate which points, if any, the clustering technique is getting wrong. We can do this by looking at the classification error of the nodes to each community:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{error} \PYG{o}{=} \PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{labels\PYGZus{}kmeans\PYGZus{}remap}  \PYG{c+c1}{\PYGZsh{} compute which assigned labels from labels\PYGZus{}kmeans\PYGZus{}remap differ from the true labels y}
\PYG{n}{error} \PYG{o}{=} \PYG{n}{error} \PYG{o}{!=} \PYG{l+m+mi}{0}  \PYG{c+c1}{\PYGZsh{} if the difference between the community labels is non\PYGZhy{}zero, an error has occurred}
\PYG{n}{er\PYGZus{}rt} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{error}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} error rate is the frequency of making an error}

\PYG{n}{palette} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{)}\PYG{p}{,}
           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wrong}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mf}{0.8}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n}{error\PYGZus{}label} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{o}{*}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} initialize numpy array for each node}
\PYG{n}{error\PYGZus{}label}\PYG{p}{[}\PYG{n}{error}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wrong}\PYG{l+s+s1}{\PYGZsq{}}  \PYG{c+c1}{\PYGZsh{} add label \PYGZsq{}Wrong\PYGZsq{} for each error that is made}

\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{,}
         \PYG{n}{labels}\PYG{o}{=}\PYG{n}{error\PYGZus{}label}\PYG{p}{,}
         \PYG{n}{title}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error from KMeans, Error rate: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{er\PYGZus{}rt}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{legend\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mf}{3.5}\PYG{p}{,}
         \PYG{n}{palette}\PYG{o}{=}\PYG{n}{palette}\PYG{p}{,}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_36_0}.png}

\sphinxAtStartPar
Great! Our classification has not made any errors.

\sphinxAtStartPar
To learn about the block matrix \(B\), we can now use the \sphinxcode{\sphinxupquote{SBMEstimator}} class, with our predicted labels:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{SBMEstimator}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{SBMEstimator}\PYG{p}{(}\PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{labels\PYGZus{}kmeans\PYGZus{}remap}\PYG{p}{)}
\PYG{n}{Bhat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{block\PYGZus{}p\PYGZus{}}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_40_0}.png}

\begin{sphinxadmonition}{note}{Recap of inference for Stochastic Block Model with known number of communities}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
We learned that the adjacency spectral embedding is a key algorithm for making sense of networks which are realizations of SBM random networks. The estimates of latent positions produced by ASE are critical for learning community assignments.

\item {} 
\sphinxAtStartPar
We learned that unsuperised learning (such as K\sphinxhyphen{}means) allows us to ues the estimated latent positions to learn community assignments for each node in our network.

\item {} 
\sphinxAtStartPar
We can use \sphinxcode{\sphinxupquote{remap\_labels}} to align predicted labels with true labels, if true labels are known. This is useful for benchmarking techniques on networks with known community labels.

\item {} 
\sphinxAtStartPar
We evaluate the quality of unsupervised learning by plotting the predicted node labels and (if we know the true labels) the errorfully classified nodes. The ARI and the error rate summarize how effective our unsupervised learning techniques performed.

\end{enumerate}
\end{sphinxadmonition}


\subsubsection{Number of communities \protect\(K\protect\) is not known}
\label{\detokenize{representations/ch6/estimating-parameters_spectral:number-of-communities-k-is-not-known}}
\sphinxAtStartPar
In real data, we almost never have the beautiful canonical modular structure obvious to us from a Stochastic Block Model. This means that it is \sphinxstyleemphasis{extremely infrequently} going to be the case that we know exactly how we should set the number of communities, \(K\), ahead of time.

\sphinxAtStartPar
Let’s first remember back to the single network models section, when we took a Stochastic block model with obvious community structure, and let’s see what happens when we just move the nodes of the adjacency matrix around. We begin with a similar adjacency matrix to \(A\) given above, for the \(3\)\sphinxhyphen{}community SBM example, but with the within and between\sphinxhyphen{}community edge probabilities a bit closer together so that we can see what happens when we experience errors. The communities are still slightly apparent, but less so than before:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.45}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mf}{0.45}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.45}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.45}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{ns}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{B}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the true community labels}
\PYG{n}{y} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{1} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ns}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ns}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{draw\PYGZus{}multiplot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{y}\PYG{p}{,} \PYG{n}{xticklabels}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{yticklabels}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Simulated SBM(\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{pi\PYGZdl{}, B)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_42_0}.png}

\sphinxAtStartPar
Next, we permute the nodes around to reorder the realized adjacency matrix:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} generate a reordering of the n nodes}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{vtx\PYGZus{}perm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{A}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{n}{A\PYGZus{}permuted} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{n+nb}{tuple}\PYG{p}{(}\PYG{p}{[}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}
\PYG{n}{y\PYGZus{}perm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{[}\PYG{n}{vtx\PYGZus{}perm}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_45_0}.png}

\sphinxAtStartPar
We only get to see the adjacency matrix in the \sphinxstyleemphasis{left} panel; the panel in the \sphinxstyleemphasis{right} is constructed by using the true labels (which we do \sphinxstyleemphasis{not} have!). This means that we proceed for statistical inference about the random network underlying our realized network using \sphinxstyleemphasis{only} the heatmap we have at right. It is not immediately obvious that this is the realization of a random network which is an SBM with \(3\) communities.

\sphinxAtStartPar
Our procedure is \sphinxstyleemphasis{very} similar to what we did previously {\hyperref[\detokenize{representations/ch6/estimating-parameters_spectral:link?}]{\emph{when the number of communities was known}}}. We again embed using the “elbow picking” technique:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ase\PYGZus{}perm} \PYG{o}{=} \PYG{n}{AdjacencySpectralEmbed}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking}
\PYG{n}{Xhat\PYGZus{}permuted} \PYG{o}{=} \PYG{n}{ase\PYGZus{}perm}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A\PYGZus{}permuted}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
We examine the pairs plot, \sphinxstyleemphasis{just} like in the section on {\hyperref[\detokenize{representations/ch6/estimating-parameters_spectral:link?}]{\emph{pairs plots}}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{Xhat\PYGZus{}permuted}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{SBM adjacency spectral embedding}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_49_0}.png}

\sphinxAtStartPar
We can still see that there is some level of latent community structure apparent in the pairs plot. This is evident from, for instance, the plots of Dimension 2 against Dimension 3, where we can see that the latent positions of respective nodes \sphinxstyleemphasis{appear} to be clustering in some way.

\sphinxAtStartPar
Next, we have the biggest difference with the approach we took previously. Since we do \sphinxstyleemphasis{not} know the optimal number of clusters \(K\) \sphinxstyleemphasis{nor} the true community assignments, we must choose an unsupervised clustering technique which allows us to \sphinxstyleemphasis{compare} clusterings with different choices of clusters. We can again perform this using the \sphinxcode{\sphinxupquote{KMeans}} algorithm that we used previously. Here, we will compare the quality of a clustering with one number of clusters to the quality of a clustering with a \sphinxstyleemphasis{different} number of clusters using the silhouette score. The optimal clustering is selected to be the clustering which has the largest silhouette score across all attempted numbers of clusters.

\sphinxAtStartPar
This feature is implemented automatically in the \sphinxcode{\sphinxupquote{KMeansCluster}} function of \sphinxcode{\sphinxupquote{graspologic}}. We will select the number of clusters which maximizes the silhouette score, and will allow at most \(10\) clusters total to be produced:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeansCluster}

\PYG{n}{km\PYGZus{}clust} \PYG{o}{=} \PYG{n}{KMeansCluster}\PYG{p}{(}\PYG{n}{max\PYGZus{}clusters} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{km\PYGZus{}clust} \PYG{o}{=} \PYG{n}{km\PYGZus{}clust}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{Xhat\PYGZus{}permuted}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we visualize the silhouette score as a function of the number of clusters:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{pandas} \PYG{k+kn}{import} \PYG{n}{DataFrame} \PYG{k}{as} \PYG{n}{df}

\PYG{n}{nclusters} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} graspologic nclusters goes from 2 to max\PYGZus{}clusters}
\PYG{n}{silhouette} \PYG{o}{=} \PYG{n}{km\PYGZus{}clust}\PYG{o}{.}\PYG{n}{silhouette\PYGZus{}}  \PYG{c+c1}{\PYGZsh{} obtain the respective silhouette scores}

\PYG{n}{silhouette\PYGZus{}df} \PYG{o}{=} \PYG{n}{df}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of Clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{nclusters}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Silhouette Score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{silhouette}\PYG{p}{\PYGZcb{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} place into pandas dataframe}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{silhouette\PYGZus{}df}\PYG{p}{,}\PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of Clusters}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Silhouette Score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Silhouette Analysis of KMeans Clusterings}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_53_0}.png}

\sphinxAtStartPar
As we can see, Silhouette Analysis has indicated the best number of clusters as \(3\) (which, is indeed, \sphinxstyleemphasis{correct} since we are performing a simulation where we know the right answer). Next, let’s take a look at the pairs plot for the optimal classifier. We begin by producing the predicted labels for each of our nodes, and remapping to the true community assignment labels, exactly as we did previously for further analysis:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{labels\PYGZus{}autokmeans} \PYG{o}{=} \PYG{n}{km\PYGZus{}clust}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{Xhat\PYGZus{}permuted}\PYG{p}{)}
\PYG{n}{labels\PYGZus{}autokmeans} \PYG{o}{=} \PYG{n}{remap\PYGZus{}labels}\PYG{p}{(}\PYG{n}{y\PYGZus{}perm}\PYG{p}{,} \PYG{n}{labels\PYGZus{}autokmeans}\PYG{p}{)}

\PYG{n}{ari\PYGZus{}kmeans} \PYG{o}{=} \PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}\PYG{p}{(}\PYG{n}{labels\PYGZus{}autokmeans}\PYG{p}{,} \PYG{n}{y\PYGZus{}perm}\PYG{p}{)}

\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{Xhat\PYGZus{}permuted}\PYG{p}{,}
         \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels\PYGZus{}autokmeans}\PYG{p}{,}
         \PYG{n}{title}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{KMeans on embedding, ARI: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{ari\PYGZus{}kmeans}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{legend\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mf}{3.5}\PYG{p}{,}
         \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{muted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_55_0}.png}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{KMeans}} was still able to find relatively stable clusters, which align quite well (ARI of \(0.855\), which is not perfect but closer to \(1\) than to \(0\)!) with the true labels! Next, we will look at which points \sphinxcode{\sphinxupquote{KMeans}} tends to get \sphinxstyleemphasis{wrong} to see if any patterns arise:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{error} \PYG{o}{=} \PYG{n}{y\PYGZus{}perm} \PYG{o}{\PYGZhy{}} \PYG{n}{labels\PYGZus{}autokmeans}  \PYG{c+c1}{\PYGZsh{} compute which assigned labels from labels\PYGZus{}kmeans\PYGZus{}remap differ from the true labels y}
\PYG{n}{error} \PYG{o}{=} \PYG{n}{error} \PYG{o}{!=} \PYG{l+m+mi}{0}  \PYG{c+c1}{\PYGZsh{} if the difference between the community labels is non\PYGZhy{}zero, an error has occurred}
\PYG{n}{er\PYGZus{}rt} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{error}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} error rate is the frequency of making an error}

\PYG{n}{palette} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{)}\PYG{p}{,}
           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wrong}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mf}{0.8}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n}{error\PYGZus{}label} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{o}{*}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} initialize numpy array for each node}
\PYG{n}{error\PYGZus{}label}\PYG{p}{[}\PYG{n}{error}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wrong}\PYG{l+s+s1}{\PYGZsq{}}  \PYG{c+c1}{\PYGZsh{} add label \PYGZsq{}Wrong\PYGZsq{} for each error that is made}

\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{Xhat\PYGZus{}permuted}\PYG{p}{,}
         \PYG{n}{labels}\PYG{o}{=}\PYG{n}{error\PYGZus{}label}\PYG{p}{,}
         \PYG{n}{title}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error from KMeans, Error rate: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{er\PYGZus{}rt}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{legend\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mf}{3.5}\PYG{p}{,}
         \PYG{n}{palette}\PYG{o}{=}\PYG{n}{palette}\PYG{p}{,}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_57_0}.png}

\sphinxAtStartPar
And there do not appear to be any dramatic issues in our clustering which woul suggest systematic errors are present. To learn about \(B\), we would proceed exactly as we did previously, by using these labels with the \sphinxcode{\sphinxupquote{SBMEstimator}} class to perform inference:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{SBMEstimator}\PYG{p}{(}\PYG{n}{directed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A\PYGZus{}permuted}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{labels\PYGZus{}autokmeans}\PYG{p}{)}
\PYG{n}{Bhat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{block\PYGZus{}p\PYGZus{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{o}{\PYGZlt{}}\PYG{n}{ipython}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{input}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{34}\PYG{o}{\PYGZhy{}}\PYG{n}{a112421e2ac5}\PYG{o}{\PYGZgt{}} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{15}         \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{16} 
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{17} \PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{Bhat} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{18}         \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{19}         \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}

\PYG{n+ne}{ValueError}: operands could not be broadcast together with shapes (2,2) (3,3) 
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{estimating-parameters_spectral_60_1}.png}

\sphinxAtStartPar
Which appears very close to the true \(B\).

\begin{sphinxadmonition}{note}{Recap of inference for Stochastic Block Model with unknown number of communities}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The Adjacency Spectral Embedding is used to generate \sphinxstyleemphasis{estimates of latent positions}. We verify the quality of these estimates using pairs plots.

\item {} 
\sphinxAtStartPar
We use unsupervised learning with an objective quality metric (such as the silhoutte score) to learn both the number of communities \sphinxstyleemphasis{and} the community assignment for nodes within our network. The use of an objective quality metric that allows us to evaluate classification performance across different numbers of communities is the key difference between how we perform inference with unknown community labels when we knew vs did not know the number of communities in our network.

\item {} 
\sphinxAtStartPar
We \sphinxstyleemphasis{align} the labels produced by unsupervised learning with true labels for our network using \sphinxcode{\sphinxupquote{remap labels}}.

\item {} 
\sphinxAtStartPar
We evaluate the nuances of the unsupervised learning technique using pairs plots colored with the predicted labels, and the classification errors. Again, we ue the ARI and the error rate to evaluate classifier performance.

\end{enumerate}
\end{sphinxadmonition}


\section{Random\sphinxhyphen{}Walk and Diffusion\sphinxhyphen{}based Methods}
\label{\detokenize{representations/ch6/random-walk-diffusion-methods:random-walk-and-diffusion-based-methods}}\label{\detokenize{representations/ch6/random-walk-diffusion-methods::doc}}
\sphinxAtStartPar
Although this book puts a heavy emphasis on spectral methods, there are many ways to learn lower\sphinxhyphen{}dimensional representations for networks which don’t involve spectral clustering in any way. These methods might be more computationally efficient on large networks, or they might be more easily parallelizable; they might avoid problems like nonidentifiability, or they could provide representations which are better for the particular downstream machine learning task that you want to use the representation for. Maybe you want to cluster in a way that groups together different types of nodes than spectral methods. Regardless of your reasoning, this chapter is intended to give you a some insight into the world of alternative ways to represent networks.


\subsection{node2vec}
\label{\detokenize{representations/ch6/random-walk-diffusion-methods:node2vec}}
\sphinxAtStartPar
node2vec is one such method. Instead of relying on taking eigenvectors or eigenvalues, like a Laplacian, node2vec uses a random walk to preserve the relationships between nodes and their \sphinxstyleemphasis{local neighborhoods}: all of the nodes which you can get to by walking along a small number of edges from your starting node. For example, take

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{node2vec\PYGZus{}embed}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{plot} \PYG{k+kn}{import} \PYG{n}{heatmap}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{networkx} \PYG{k}{as} \PYG{n+nn}{nx}

\PYG{c+c1}{\PYGZsh{} Start with some simple parameters}
\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{300}  \PYG{c+c1}{\PYGZsh{} Total number of nodes}
\PYG{n}{n} \PYG{o}{=} \PYG{n}{N} \PYG{o}{/}\PYG{o}{/} \PYG{l+m+mi}{3}  \PYG{c+c1}{\PYGZsh{} Nodes per community}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Our block probability matrix}

\PYG{c+c1}{\PYGZsh{} Make our Stochastic Block Model}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{p}{[}\PYG{n}{n}\PYG{p}{,} \PYG{n}{n}\PYG{p}{]}\PYG{p}{,} \PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{random-walk-diffusion-methods_4_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}

\PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{latents} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{latents}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}seaborn.axisgrid.PairGrid at 0x164e99580\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{random-walk-diffusion-methods_5_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{plot} \PYG{k+kn}{import} \PYG{n}{pairplot}

\PYG{n}{networkx\PYGZus{}sbm} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{Graph}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{,} \PYG{n}{nodes} \PYG{o}{=} \PYG{n}{node2vec\PYGZus{}embed}\PYG{p}{(}\PYG{n}{networkx\PYGZus{}sbm}\PYG{p}{,} \PYG{n}{dimensions}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{walk\PYGZus{}length}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
WARNING:gensim.models.base\PYGZus{}any2vec:consider setting layer size to a multiple of 4 for greater performance
WARNING:gensim.models.base\PYGZus{}any2vec:under 10 jobs per worker: consider setting a smaller `batch\PYGZus{}words\PYGZsq{} for smoother alpha decay
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}seaborn.axisgrid.PairGrid at 0x165a39f40\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{random-walk-diffusion-methods_6_2}.png}


\section{Graph Neural Networks}
\label{\detokenize{representations/ch6/graph-neural-networks:graph-neural-networks}}\label{\detokenize{representations/ch6/graph-neural-networks::doc}}

\section{Multiple\sphinxhyphen{}Network Representation Learning}
\label{\detokenize{representations/ch6/multigraph-representation-learning:multiple-network-representation-learning}}\label{\detokenize{representations/ch6/multigraph-representation-learning::doc}}

\subsection{Aliens and Humans}
\label{\detokenize{representations/ch6/multigraph-representation-learning:aliens-and-humans}}
\sphinxAtStartPar
Say you’re a brain researcher, and you have a bunch of scans of brains \sphinxhyphen{} some are scans of people, and some are scans of aliens. You have some code that estimates networks from your scans, so you turn all your scans into networks. The nodes represent the brain regions which are common to both humans and aliens (isn’t evolution amazing?), and the edges represent communication between these brain regions. You want to know if the human and alien networks share a common grouping of regions (your research topic is titled, “Do Alien Brains Have The Same Hemispheres That We Do?”). What do you do? How do you even deal with situations in which you have a lot of networks whose nodes all represent the same objects, but whose edges might come from totally different distributions?

\sphinxAtStartPar
Well, if your goal is to find the shared grouping of regions between the human and alien networks, you could try embedding your networks and then seeing what those embeddings look like. This would serve the dual purpose of having less stuff to deal with and having some way to directly compare all of your networks in the same space. Finding an embedding is also simply useful in general, because embedding a network or group of networks opens the door to machine learning methods designed for tabular data.

\sphinxAtStartPar
For example, say you have four alien networks and four human networks. Since alien brain networks aren’t currently very accessible, we’ll just simulate our human and alien networks with Stochastic Block Models. The communities that we’re trying to group all of the brain regions into are the two hemispheres of the brain. We’ll design the human brains to have strong connections within hemispheres, and we’ll design the alien brains to have strong connections between hemispheres – but the same regions still correspond to the same hemispheres.

\sphinxAtStartPar
we’ll use a relatively small number of nodes and fairly small block probabilities. You can see the specific parameters in the code below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}

\PYG{c+c1}{\PYGZsh{} Generate networks from an SBM, given some parameters}
\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}sbm}\PYG{p}{(}\PYG{o}{*}\PYG{n}{probs}\PYG{p}{,} \PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pa}\PYG{p}{,} \PYG{n}{pb}\PYG{p}{,} \PYG{n}{pc}\PYG{p}{,} \PYG{n}{pd} \PYG{o}{=} \PYG{n}{probs}
    \PYG{n}{P} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{pa}\PYG{p}{,} \PYG{n}{pb}\PYG{p}{]}\PYG{p}{,} 
                  \PYG{p}{[}\PYG{n}{pc}\PYG{p}{,} \PYG{n}{pd}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{sbm}\PYG{p}{(}\PYG{p}{[}\PYG{n}{n}\PYG{p}{,} \PYG{n}{n}\PYG{p}{]}\PYG{p}{,} \PYG{n}{P}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{n}{return\PYGZus{}labels}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} make nine human networks}
\PYG{c+c1}{\PYGZsh{} and nine alien networks}
\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{06}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{03}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{labels} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{n}{n} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{n}{n}
\PYG{n}{humans} \PYG{o}{=} \PYG{p}{[}\PYG{n}{make\PYGZus{}sbm}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p1}\PYG{p}{,} \PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{aliens} \PYG{o}{=} \PYG{p}{[}\PYG{n}{make\PYGZus{}sbm}\PYG{p}{(}\PYG{n}{p3}\PYG{p}{,} \PYG{n}{p1}\PYG{p}{,} \PYG{n}{p1}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
The human and alien networks come from very different distributions. As you can see from the Stochastic Block Model structure below, the regions in the human and the alien brains can both be separated into two communities. These communities represent the two hemispheres of the brain (who knew aliens also have bilateralized brains!). Although both humans and aliens have the same regions belonging to their respective hemispheres, as we planned, the alien networks have a strange property: their brain regions have more connections with regions in the opposite hemisphere than the same one.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_6_0}.png}


\subsection{Different ways to Embed the Networks}
\label{\detokenize{representations/ch6/multigraph-representation-learning:different-ways-to-embed-the-networks}}
\sphinxAtStartPar
Remember, our goal is to find community structure common to both humans and aliens, and in our case that community structure is the brain hemispheres. We’re going to try to to embed our brain networks into some lower\sphinxhyphen{}dimensional space \sphinxhyphen{} that way, we can use standard clustering methods from machine learning to figure out which regions are grouped. Try to think about how you might find a lower\sphinxhyphen{}dimensional embedding where the location of each node’s latent positions uses information from all of the networks.


\subsubsection{Averaging Separately}
\label{\detokenize{representations/ch6/multigraph-representation-learning:averaging-separately}}
\sphinxAtStartPar
The first idea you might come up with is to average your networks together, and then embed the result of that averaging with Spectral Embedding. It turns out that this is actually the right idea in the very special case where all of your networks come from the same probability distribution. In our case, we’ll try averaging our groups of networks separately: we’ll treat the human networks as one group, and the alien networks as another group, and we’ll average each independently. In the end, we’ll have two separate embeddings.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}

\PYG{c+c1}{\PYGZsh{} Compute the average adjacency matrix for }
\PYG{c+c1}{\PYGZsh{} human brains and alien brains}
\PYG{n}{human\PYGZus{}mean\PYGZus{}network} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{humans}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{alien\PYGZus{}mean\PYGZus{}network} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{aliens}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Embed both matrices}
\PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{human\PYGZus{}latents} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{human\PYGZus{}mean\PYGZus{}network}\PYG{p}{)}
\PYG{n}{alien\PYGZus{}latents} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{alien\PYGZus{}mean\PYGZus{}network}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Below, you can see what happens when we embed the averaged human and alien networks separately. Like all of our embedding plots, each dot represents the latent positions for a particular node.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_13_0}.png}

\sphinxAtStartPar
Both of these embeddings have clear clustering: there are two communities of nodes in both the human and the alien networks. We can recover the labels for these communities fairly easily using our pick of unsupervised clustering method. We know that the latent positions in each community of an Adjacency Spectral Embedding are normally distributed under this simulation setting, and we have two communities. That means that the above embeddings are distributed according to a Gaussian Mixture. Here, “Gaussian” just means “normal”, and a gaussian mixture just means that we have groups of normally distributed data clusters. As a result, it makes sense to cluster these data using scikit\sphinxhyphen{}learn’s GaussianMixture implementation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{mixture} \PYG{k+kn}{import} \PYG{n}{GaussianMixture} \PYG{k}{as} \PYG{n}{GMM}

\PYG{c+c1}{\PYGZsh{} Predict labels for the human and alien brains}
\PYG{n}{human\PYGZus{}labels} \PYG{o}{=} \PYG{n}{GMM}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{human\PYGZus{}latents}\PYG{p}{)}
\PYG{n}{alien\PYGZus{}labels} \PYG{o}{=} \PYG{n}{GMM}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{alien\PYGZus{}latents}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see a plot that predicts our community structure below. Success! When we embed the human and the alien networks separately, averaging them clearly lets us cluster the brain regions by hemisphere. However, as you can see, the colors are flipped: the communities are in different places relative to each other. This is because the alien networks are drawn from a different distribution than the human networks.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_17_0}.png}


\subsubsection{Averaging Together}
\label{\detokenize{representations/ch6/multigraph-representation-learning:averaging-together}}
\sphinxAtStartPar
But what if you wanted to embed \sphinxstyleemphasis{all} of the networks into the same space, both the human and the alien networks, so that there’s only one plot? Let’s try it. We’ll take all of the networks and then average them together, and then do an Adjacency Spectral Embedding. This will result in a single plot, with each point representing a single brain region. Do you think we’ll still find this nice community separation?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{total\PYGZus{}mean\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{humans} \PYG{o}{+} \PYG{n}{aliens}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{all\PYGZus{}latents} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{total\PYGZus{}mean\PYGZus{}matrix}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_21_0}.png}

\sphinxAtStartPar
Nope, bummer. Our community separation into discrete hemispheres is gone \sphinxhyphen{} the human networks and the alien networks cancelled each other out. As far as anybody can tell, our latent positions have just become meaningless noise, so we can’t cluster and find communities like we did before.


\paragraph{Why Did Averaging Together Fail?}
\label{\detokenize{representations/ch6/multigraph-representation-learning:why-did-averaging-together-fail}}
\sphinxAtStartPar
Why did this happen? Well, let’s go back and compare one human brain network with one alien brain network.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_25_0}.png}

\sphinxAtStartPar
The human network has more edges in the upper\sphinxhyphen{}left and lower\sphinxhyphen{}left quadrants of the heatmap. This implies that two regions in the same hemisphere are more likely to be connected for humans than two regions in opposite hemispheres.

\sphinxAtStartPar
The alien network tells a different story. For aliens, two regions in opposite hemispheres are more likely to be connected than two regions in the same hemisphere.

\sphinxAtStartPar
But what happens when you average these two adjacency matrices together?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{combined} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{humans}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{aliens}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{averaged} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{combined}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_28_0}.png}

\sphinxAtStartPar
By averaging, we’ve lost all of the community structure used to exist. That’s why our big averaged embedding failed.

\sphinxAtStartPar
We’ve just discovered that even though it’s oten a great idea to simply average all of your networks together \sphinxhyphen{} for example, if they were drawn from the same distribution \sphinxhyphen{} it’s often a horrible idea to average all of your networks if they might come from different distributions. This is a case of averaging networks which are “heterogeneous”: Not only are your networks slightly different, but they’re \sphinxstyleemphasis{should} to be different because their edge probabilities aren’t the same. Sampling a lot of heterogenous networks and then averaging them, as you can see from our exploration above, can result in losing the community signal you might have had.

\sphinxAtStartPar
We’d like to find a way to compare these heterogeneous networks directly, so that we can embed all of our networks into the same space and still keep that nice community structure. Figuring out the best way to do this is a topic under active research, and the set of techniques and tools that have developed as a result are together called multiple\sphinxhyphen{}network representation learning.


\subsection{Different Types of Multiple\sphinxhyphen{}Network Representation Learning}
\label{\detokenize{representations/ch6/multigraph-representation-learning:different-types-of-multiple-network-representation-learning}}
\sphinxAtStartPar
Let’s take a moment to explore some of the possible general approaches we could take in multiple\sphinxhyphen{}network representation learning. At some point we need to combine the many individual representations of our networks into one, and there are at least three possible places where we could do this: combining the networks together, combining the networks separately, and combining the embeddings. Each of these eventually results in a latent position representation for our networks. It’s important to note that in all of these approaches, we’re simply learning representations for our groups of networks. You can do whatever you want with these representations; in our case, we’ll illustrate that we can use them to classify our nodes.


\subsubsection{Combining the Networks Together}
\label{\detokenize{representations/ch6/multigraph-representation-learning:combining-the-networks-together}}
\sphinxAtStartPar
With this approach, you’ll start with a set of networks, and then you’ll combine them all into a single network prior to doing anything else. You can then embed and classify this network directly. What we did before, averaging the human and alien networks, was an example of combining our networks – we just averaged all of our adjacency matrices, and then we embedded the result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 0 Axes\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_34_1}.png}


\subsubsection{Combining The Networks Separately}
\label{\detokenize{representations/ch6/multigraph-representation-learning:combining-the-networks-separately}}
\sphinxAtStartPar
The above approach is nice for collapsing our information into a single embedding – with each point in our final embedding representing a single node of our network. However, there are situations in which we might want to keep our embeddings separate, but make sure that they’re in the same latent space – meaning, the embeddings aren’t rotations of each other. That way, we can directly compare the embeddings of our separate embeddings.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_37_0}.png}


\subsubsection{Combining the embeddings}
\label{\detokenize{representations/ch6/multigraph-representation-learning:combining-the-embeddings}}
\sphinxAtStartPar
The final approach to multiple\sphinxhyphen{}network representation learning that we’ll talk about is combining the embeddings themselves. With this approach, you’re waiting until you’ve already embnedded all of your networks separately before you combine them, either with Adjacency Spectral Embedding or with some other single\sphinxhyphen{}network embedding method. Multiple Adjacency Spectral Embedding, which we’ll be talking about soon, is an example of this approach.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_40_0}.png}

\sphinxAtStartPar
For the rest of this section, we’ll explore the strengths and weaknesses of different particular techniques which use these approaches. The first we’ll look at is combines the embeddings, like above. It’s called Multiple Adjacency Spectral Embedding, or MASE for short.


\subsection{Multiple Adjacency Spectral Embedding}
\label{\detokenize{representations/ch6/multigraph-representation-learning:multiple-adjacency-spectral-embedding}}
\sphinxAtStartPar
MASE is a technique which combines embeddings by concatennating and re\sphinxhyphen{}embedding the separate latent positions into a single space. It’s nice because you don’t actually need each network to be generated from the same distribution \sphinxhyphen{} you only need the nodes of the different networks to be aligned and for them to belong to the same communities.

\sphinxAtStartPar
MASE is probably the easiest to understand if you know how Adjacency Spectral Embeddings work. Say you have some number of networks, and (like we said above) their nodes are aligned. The goal of MASE is to embed the networks into a single space, with each point in that space representing a single node \sphinxhyphen{} but, unlike simply averaging, MASE lets you combine networks which aren’t necessarily drawn from the same distribution. MASE is based on the common subspace independent\sphinxhyphen{}edge (COSIE) model from the multi\sphinxhyphen{}network models section of chapter 5, so we’re operating under the assumption that there \sphinxstyleemphasis{is} some low\sphinxhyphen{}dimensional space common to all of our networks that we can embed into in the first place.

\sphinxAtStartPar
Let’s go back to our group of human and alien brains and try using MASE to embed them rather than averaging. Then, we’ll dive deeper into what’s going on under the hood. First, we’ll instantiate a MASE classifier and embed down to two dimensions. Then we’ll create a combined list of the human and alien brains, and use MASE to find the latent positions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{MultipleASE} \PYG{k}{as} \PYG{n}{MASE}

\PYG{c+c1}{\PYGZsh{} Use MASE to embed everything}
\PYG{n}{mase} \PYG{o}{=} \PYG{n}{MASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{latents\PYGZus{}mase} \PYG{o}{=} \PYG{n}{mase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{humans} \PYG{o}{+} \PYG{n}{aliens}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_45_0}.png}

\sphinxAtStartPar
Unlike the disastrous results from simply averaging all of our networks together, MASE manages to keep the community structure that we found when we averaged our networks separately. Let’s see what’s under the hood.


\subsubsection{How Does MASE Work?}
\label{\detokenize{representations/ch6/multigraph-representation-learning:how-does-mase-work}}
\sphinxAtStartPar
Below, you can see how MASE works. We start with networks, drawn as nodes in space connected to each other. We turn them into adjacency matrices, and then we embed the adjacency matrices of a bunch of networks separately, using our standard Adjacency Spectral Embedding. Then, we take all of those embeddings, concatenate horizontally into a single matrix, and embed the entire concatenated matrix. The colors are the true communities each node belongs to: there’s a red and an orange community. MASE is an unsupervised learning technique and so it doesn’t need any information about the true communities to embed, but they’re useful to see.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=400\sphinxpxdimen]{{mase1}.jpeg}
\caption{The MASE algorithm}\label{\detokenize{representations/ch6/multigraph-representation-learning:mase-fig}}\end{figure}


\paragraph{A Collection of Networks}
\label{\detokenize{representations/ch6/multigraph-representation-learning:a-collection-of-networks}}
\sphinxAtStartPar
We’ll illustrate what’s happening in the MASE algorithm by running through all of its steps ourselves, with a set of example networks.

\sphinxAtStartPar
Suppose we have a set of networks generated from Stochastic Block Models with two communities in each network. The networks have aligned nodes – meaning that the \(i_{th}\) row of all of their adjacency matrices represent the edges for the same node \(i\). The nodes also all belong to the same communities. However, edge probabilities might change depending on the network. In the first network, you might have nodes in the same community having a high chance of connecting to each other, whereas in the second network, nodes are much more likely to be connected to other nodes in different communities. You want to end up with a classification that distinctly groups the nodes into their respective communities, using the information from all of the networks. Because MASE takes approach of combining the embeddings, we start by embedding each network separately with an Adjacency Spectral Embedding.

\sphinxAtStartPar
Below is Python code which generates four networks with Stochastic Block Models. Each of the networks is drawn from a different distribution (the block probability matrices are different), but the labels are the same across the networks (which means that nodes have a consistent community no matter which network you’re looking at). If you’re interested in the particular parameters used to generate these SBMs, you can see them in the code below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{06}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{03}
\PYG{n}{A1}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{make\PYGZus{}sbm}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p1}\PYG{p}{,} 
                      \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{A2} \PYG{o}{=} \PYG{n}{make\PYGZus{}sbm}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{)}
\PYG{n}{A3} \PYG{o}{=} \PYG{n}{make\PYGZus{}sbm}\PYG{p}{(}\PYG{n}{p3}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p2}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{)}
\PYG{n}{A4} \PYG{o}{=} \PYG{n}{make\PYGZus{}sbm}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{,} \PYG{n}{p3}\PYG{p}{)}

\PYG{n}{networks} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A1}\PYG{p}{,} \PYG{n}{A2}\PYG{p}{,} \PYG{n}{A3}\PYG{p}{,} \PYG{n}{A4}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_53_0}.png}


\paragraph{Embedding our networks}
\label{\detokenize{representations/ch6/multigraph-representation-learning:embedding-our-networks}}
\sphinxAtStartPar
Next, we embed each of the four networks separately using Adjacency Spectral Embedding. This step is pretty straightforward, so we won’t dive into it too much: remember, we’re combining the embeddings, not the networks, so we’re not doing anything fancy. The python code below just groups the four networks into a list, and then loops through the list, embedding each network into two dimensions and saving the resulting embeddings into a variable.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}

\PYG{n}{networks} \PYG{o}{=} \PYG{p}{[}\PYG{n}{A1}\PYG{p}{,} \PYG{n}{A2}\PYG{p}{,} \PYG{n}{A3}\PYG{p}{,} \PYG{n}{A4}\PYG{p}{]}
\PYG{n}{latents\PYGZus{}mase} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{network} \PYG{o+ow}{in} \PYG{n}{networks}\PYG{p}{:}
    \PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{latent} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{network}\PYG{p}{)}
    \PYG{n}{latents\PYGZus{}mase}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{latent}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_57_0}.png}

\sphinxAtStartPar
It’s important to keep in mind that these embeddings don’t live in the same \sphinxstyleemphasis{latent space}. What this means is that averaging these networks together would result in essentially meaningless noise. This is because of the rotational invariance of latent positions: you can only recover the latent positions of any network up to a rotation.


\paragraph{Combining our embeddings}
\label{\detokenize{representations/ch6/multigraph-representation-learning:combining-our-embeddings}}
\sphinxAtStartPar
Now comes the interesting part. Our goal is to find some way to take each of these individual embeddings and combine them. We want to find a reasonable way of doing this.

\sphinxAtStartPar
We can visualize each of our four embeddings a different way. Instead of the using the two latent position dimensions as the x\sphinxhyphen{}axis and the y\sphinxhyphen{}axis of our plot, we can just visualize our latent position matrices directly. Each latent position now corresponds to rows in one of these matrices. The two columns are the two latent position dimensions, and the two colors in each row corresponds to the latent position value. We’re essentially substituting location for color.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_61_0}.png}

\sphinxAtStartPar
Because the rows of these matrices are all aligned \sphinxhyphen{} meaning, row 0 corresponds to node 0 for all four matrices \sphinxhyphen{} we can actually think of each node as having (in this case) eight latent position dimensions: two for each of our four networks. Eight is a somewhat arbitrary number here: each network contributes two dimensions simply because we originally chose to embed all of our networks down to two dimensions with ASE, and the number of networks is of course even more arbitrary. You’ll usually have more than four.

\sphinxAtStartPar
In the more general sense, we can think of each node as having \(m \times d\) latent position dimensions, where \(m\) is the number of networks, and \(d\) is the number of dimensions we embed each network into. We don’t actually need separate matrices to express this idea: the natural thing to do would be to just concatenate all of the matrices horizontally into a single \(m \times d\) matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Concatenate our four matrices horizontally into a single m by d matrix}
\PYG{n}{concatenated} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{n}{latents\PYGZus{}mase}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_64_0}.png}


\paragraph{Embedding our Combination To Create a Joint Embedding}
\label{\detokenize{representations/ch6/multigraph-representation-learning:embedding-our-combination-to-create-a-joint-embedding}}
\sphinxAtStartPar
So now we have a combined representation for our separate embeddings, but we have a new problem: our latent positions suddenly have way too many dimensions. In this example they have eight (the number of columns in our combined matrix), but remember that in general we’d have \(m \times d\). This somewhat defeats the purpose of an embedding: we took a bunch of high\sphinxhyphen{}dimensional objects and turned them all into a single high\sphinxhyphen{}dimensional object. Big whoop. We can’t see what our combined embedding look like in euclidean space, unless we can somehow visualize \(m \times d\) dimensional space (hint: we can’t). We’d like to just have \sphinxcode{\sphinxupquote{d}} dimensions \sphinxhyphen{} that was the whole point of using \sphinxcode{\sphinxupquote{d}} components for each of our Adjacency Spectral Embeddings in the first place!

\sphinxAtStartPar
There’s an obvious solution here: why don’t we just embed \sphinxstyleemphasis{again}? Nothing stops us from doing a Singular Value Decomposition on a nonsquare matrix, and so we can just create a joint embedding of our combined matrix and go back down to a healthy \(d\) columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{select\PYGZus{}svd}
\PYG{n}{joint\PYGZus{}embedding}\PYG{p}{,} \PYG{o}{*}\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{select\PYGZus{}svd}\PYG{p}{(}\PYG{n}{concatenated}\PYG{p}{,} \PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_68_0}.png}

\sphinxAtStartPar
Looks like this idea worked well \sphinxhyphen{} Our nodes are clearly grouped into two distinct communities, and all of our networks were drawn from the same distribution! To reiterate, what we did was:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Embed each of our four networks separately into two\sphinxhyphen{}dimensional space

\item {} 
\sphinxAtStartPar
Think of all of the resulting latent positions for a particular node as a single vector

\item {} 
\sphinxAtStartPar
With the intuition from 2, horizontally concatenate our four latent position matrices into a single matrix

\item {} 
\sphinxAtStartPar
embed that new matrix down to 2 dimensions

\end{enumerate}


\subsubsection{Using Graspologic}
\label{\detokenize{representations/ch6/multigraph-representation-learning:using-graspologic}}
\sphinxAtStartPar
In practice, you don’t actually have to implement any of this stuff yourself. Graspologic’s MultipleASE class implements it all for you under the hood. You can see the embedding below \sphinxhyphen{} you give MultipleASE a list of networks, and it spits out a set of joint latent positions. Graspologic’s implementation of MASE is doing pretty much exactly what we just did: it embeds all of the networks you pass in, concatenates them horizontally, and then re\sphinxhyphen{}embeds the concatenated matrix. You can see this in the figure – MASE’s embedding looks just like the one we made above.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{MultipleASE} \PYG{k}{as} \PYG{n}{MASE}

\PYG{n}{mase} \PYG{o}{=} \PYG{n}{MASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{latents} \PYG{o}{=} \PYG{n}{mase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_73_0}.png}


\subsubsection{Score Matrices*}
\label{\detokenize{representations/ch6/multigraph-representation-learning:score-matrices}}
\sphinxAtStartPar
Exactly how is the joint embedding we created related to all of separate, original networks? Well, to understand this, we need to introduce the concept of \sphinxstyleemphasis{score matrices}.

\sphinxAtStartPar
In MASE, each network is associated with its own score matrix. Just like the joint embedding describes how the networks are similar, the score matrices describe how each network is different.

\sphinxAtStartPar
Suppose we have a set of networks with adjacency matrices \(A^{(1)}, ..., A^{(m)}\), with each network being unweighted. In the joint embedding we made before, for instance, we had \(m=4\).

\sphinxAtStartPar
Now, we run MASE using the method described above, and we get a joint embedding \(V\). Then each adjacency matrix, \(A^{(i)}\), can be decomposed into \(VR^{(i)} V^\top\), where \(R^{(i)}\) is the score matrix corresponding to the \(i_{th}\) network:
\begin{align*}
    A^{(i)} = VR^{(i)} V^\top
\end{align*}
\sphinxAtStartPar
This is how the score matrix of a particular network \(R^{(i)}\) and the single joint embedding \(V\) is related to the original network \(A^{(i)}\).


\paragraph{Finding Score Matrices}
\label{\detokenize{representations/ch6/multigraph-representation-learning:finding-score-matrices}}
\sphinxAtStartPar
Any particular score matrix, \(R^{(i)}\), is square and \(d \times d\). The dimension, \(d\), corresponds to the number of embedding dimensions – so if we wanted to embed down to two dimensions, each \(R^{(i)}\) would be a \(2 \times 2\) matrix.

\sphinxAtStartPar
Now, here’s the interesting part: how do we find our score matrices? Well, there’s a theorem in linear algebra about matrices which are \sphinxstyleemphasis{orthogonal}, meaning that the columns all perpendicular to each other. This theorem says that the inverse of an orthogonal matrix is its transpose. So, for an orthogonal matrix \(O\),
\begin{align*}
    O^\top = O^{-1}
\end{align*}
\sphinxAtStartPar
Interestingly, the column\sphinxhyphen{}vectors of our joint embedding matrix (let’s call it \(V\)) are all perpendicular. Since definitionally, what it means for two vectors to be perpendicular is that they have a dot product of 0, we can check this below:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{V} \PYG{o}{=} \PYG{n}{joint\PYGZus{}embedding}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Take the dot product of the columns of our joint latent position matrix}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{V}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{@} \PYG{n}{V}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.0
\end{sphinxVerbatim}

\sphinxAtStartPar
What this all means is that \(V^\top V\) is just the identity matrix \(I\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{V}\PYG{o}{.}\PYG{n}{T}\PYG{n+nd}{@V}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[1., 0.],
       [0., 1.]])
\end{sphinxVerbatim}

\sphinxAtStartPar
and so, finally, we can use the above two facts to find the score matrix for a particular network. We just take our original formula \(A^{(i)} = VR^{(i)} V^\top\), left\sphinxhyphen{}multiply by \(V^\top\), and right\sphinxhyphen{}multiply by \(V\).
\begin{align*}
    A^{(i)} &= VR^{(i)} V^\top \\
    V^{\top} A^{(i)} V &= (V^\top V) R^{(i)} (V^\top V) \\
    V^\top A^{(i)} V &= R^{(i)} 
\end{align*}
\sphinxAtStartPar
Below, we turn the list of four networks we already embedded into a 3\sphinxhyphen{}D numpy array, and then do the above multiplication to get a new 3D numpy array of scores matrices. Because we embedded into two dimensions, each score matrix is \(2 \times 2\), and the four score matrices are “slices” along the 0th axis of the numpy array.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{networks\PYGZus{}array} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{asarray}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{)}
\PYG{n}{scores} \PYG{o}{=} \PYG{n}{V}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{networks\PYGZus{}array} \PYG{o}{@} \PYG{n}{V}
\PYG{n}{scores}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(4, 2, 2)
\end{sphinxVerbatim}

\sphinxAtStartPar
Now, here’s something interesting: it turns out that we can estimate the edge probability matrix which generated any graph with \( P^{(i)} = V R^{(i)} V^\top\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{P\PYGZus{}0} \PYG{o}{=} \PYG{n}{V} \PYG{o}{@} \PYG{n}{scores}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{@} \PYG{n}{V}\PYG{o}{.}\PYG{n}{T}
\end{sphinxVerbatim}

\sphinxAtStartPar
Below and to the left, you can see the original adjacency matrix for the first matrix. In the center, you can see the heatmap for the first network’s score matrix. Next to it, you can see the recreation of the first network. Remember that we only used the score matrix to recreate it. The first network has a block probability matrix of
\label{equation:representations/ch6/multigraph-representation-learning:4e7701b0-7d2e-42da-b41b-e765a9d92127}\begin{align}
\begin{bmatrix}
.12 & .03 \\
.03 & .06 \\
\end{bmatrix}
\end{align}
\sphinxAtStartPar
and so we should expect the edges in the top\sphinxhyphen{}left block of our adjacency matrix to be more connected, the edges in the two off\sphinxhyphen{}diagonal blocks to not be very connected, and the edges in the bottom\sphinxhyphen{}right block to be kind of connected.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_86_0}.png}

\sphinxAtStartPar
So we’ve learned that MASE is useful when you want a joint embedding that combines all of your networks together, and when you want to estimate edge probabilities for one of your networks. What if we wanted to keep our separate embeddings, but put them all in the same space? That’s what the Omnibus Embedding gives, and what we’ll explore now.


\subsection{Omnibus Embedding}
\label{\detokenize{representations/ch6/multigraph-representation-learning:omnibus-embedding}}
\sphinxAtStartPar
The Omnibus Embedding combines networks separately to put them all into the same latent space. What this means is that the embeddings for each network after the omnibus embedding are \sphinxstyleemphasis{directly comparable}: none of the embeddings are rotations of each other, and distances between nodes across embeddings actually means something. You can use the omnibus embedding to answer a variety of questions about the interacting properties of a collection of networks. For example, you could figure out which nodes or subgraphs are responsible for similarities or differences across your networks, or you could determine whether subcommunities in your networks are statistically similar or different. You could try to figure out which underlying parameters of your network are the same, and which are different.

\sphinxAtStartPar
In the next section, we’ll explore how the Omnibus Embedding works. Sections in future chapters will explore some the things you can do with your separate embeddings to learn about your networks.


\subsubsection{OMNI on our four networks}
\label{\detokenize{representations/ch6/multigraph-representation-learning:omni-on-our-four-networks}}
\sphinxAtStartPar
We’ll begin with an example. Let’s go back to the four networks we created in the MASE section and look at their embeddings. Notice that the way the blue cluster of points and the red cluster of points is rotated is somewhat arbitrary across the embeddings for our different networks \sphinxhyphen{} this is because of the nonidentifiability problem in spectral embeddings.

\begin{sphinxadmonition}{note}{Non\sphinxhyphen{}Identifiability}

\sphinxAtStartPar
Let’s take a network generated from an RDPG with \(n\) nodes. Each of these \(n\) nodes is associated with a latent position vector, corresponding to that node’s row in the network’s embedding. What it means for a node to have a latent position vector is that the probability for an edge to exist between two nodes \(i\) and \(j\) is the dot product of their latent position vectors.

\sphinxAtStartPar
More specifically, if \(\textbf{P}\) is a matrix of edge probabilities, and \(\textbf{X}\) is our latent position matrix, then \(\textbf{P} = \textbf{X} \textbf{X}^\top\).

\sphinxAtStartPar
The nonidentifiability problem is as follows: Take any orthogonal matrix (a matrix which only rotates or flips other matrices). Call it \(\textbf{W}\). By definition, the transpose of any orthogonal matrix is its inverse: \(\textbf{W} \textbf{W}^\top = \textbf{I}\), where \(\textbf{I}\) is the identity matrix. So,
\label{equation:representations/ch6/multigraph-representation-learning:a3d94e9d-ac1b-44d1-a6b5-0dd9990ceb32}\begin{align}
P &= \textbf{X} \textbf{X}^\top \\
  &= \textbf{X} \textbf{I} \textbf{X}^\top \\
  &= (\textbf{X} \textbf{W}) (\textbf{W}^\top \textbf{X}^\top) \\
  &= (\textbf{X} \textbf{W}) (\textbf{X} \textbf{W})^\top \\
\end{align}
\sphinxAtStartPar
What this means is that you can take any latent position matrix and rotate it, and the rotated version will still generate the same matrix of edge probabilities. So, when you try to estimate latent positions, separate estimations can produce rotated versions of each other.

\sphinxAtStartPar
You need to be aware of this in situations where you’re trying to directly compare more than one embedding. You wouldn’t be able to figure out the average position of a node, for instance, when you have multiple embeddings of that node.
\end{sphinxadmonition}

\sphinxAtStartPar
You can see the nonidentifiability problem in action below. The embeddings for network 1 and for network 2 are particularly illustrative; community 0 is generally top in network 1, but on the right in network two. There isn’t a way to compare any two nodes directly. Another way to say this is that, right now, all of our embeddings live in different \sphinxstyleemphasis{latent spaces}: direct comparison between embeddings for nodes in network 1 and nodes in network 2 isn’t possible. You can also see the latent position corresponding to the first node as a big red circle in each network so that you can track a single point \sphinxhyphen{} you can see that the red points are likely to be rotated or flipped across networks.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_94_0}.png}


\subsubsection{OMNI on our four heterogeneous networks}
\label{\detokenize{representations/ch6/multigraph-representation-learning:omni-on-our-four-heterogeneous-networks}}
\sphinxAtStartPar
Let’s see what happens when, instead of embedding our networks separately as above, we find their latent positions with an Omnibus Embedding. Again, we’ll plot a particular node with a circle so that we can track it across embeddings.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{OmnibusEmbed}

\PYG{n}{omni} \PYG{o}{=} \PYG{n}{OmnibusEmbed}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{latents\PYGZus{}omni} \PYG{o}{=} \PYG{n}{omni}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{multigraph-representation-learning_98_0}.png}

\sphinxAtStartPar
Unlike when we embedded the four networks separately, the clusters created by the Omnibus Embedding \sphinxstyleemphasis{live in the same space}: you don’t have to rotate or flip your points to line them up across embeddings. The cluster of blue points is always in the top left, and the cluster of red points is always in the bottom right. This means that we can compare points directly; the relative location of the node in your network corresponding to the red circle, for instance, now means something across the four networks, and we can do stuff like measure the distance of the red circle in network 1 to the red circle in network two to gain information.


\subsubsection{How Does OMNI work?}
\label{\detokenize{representations/ch6/multigraph-representation-learning:how-does-omni-work}}
\sphinxAtStartPar
At a high level, the omnibus embedding is fairly simple. It:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Combines the adjacency matrices for all of our networks into a single, giant matrix (the Omnibus Matrix)

\item {} 
\sphinxAtStartPar
Embeds that matrix using a standard Adjacency or Laplacian Spectral Embedding.

\end{enumerate}

\sphinxAtStartPar
The omnibus matrix itself just has every original adjacency or laplacian matrix along its diagonal, and the elementwise average of every pair of original matrices on the off\sphinxhyphen{}diagonals. This means that the Omnibus Matrix is \sphinxstyleemphasis{huge}: if you have \(m\) networks, each of which has \(n\) nodes, the Omnibus Matrix will be a \(mn \times mn\) matrix.

\sphinxAtStartPar
For example, say we only have two networks. Let’s name their adjacency matrices \(A^{(1)}\) and \(A^{(2)}\). Then, the omnibus embedding looks like this:
\label{equation:representations/ch6/multigraph-representation-learning:bc82a287-3c5e-42ed-85ba-d7904e618257}\begin{align}
\begin{bmatrix}
A^{(1)} & \frac{A^{(1)} + A^{(2)}}{2} \\
\frac{A^{(2)} + A^{(1)}}{2} & A^{(2)} \\
\end{bmatrix}
\end{align}
\sphinxAtStartPar
where each entry on the diagonal is itself a matrix. In general, when we have \(m\) networks, the \(i_{th}\) diagonal entry is \(A^{(i)}\) and the \((i, j)_{th}\) entry is \(\frac{A^{(i)} + A^{(j)}}{2}\). What this means is that you just stick each of your adjacency matrices on the diagonal of a large matrix, and you fill in the off\sphinxhyphen{}diagonals with the averages of each pair of two adjacency matrices.

\sphinxAtStartPar
You can see this in code below. Below, we just use numpy’s block function to generate our simple Omnibus Matrix from two networks.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a0}\PYG{p}{,} \PYG{n}{a1} \PYG{o}{=} \PYG{n}{networks}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{networks}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{omni} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{block}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{a0}\PYG{p}{,} \PYG{p}{(}\PYG{n}{a0}\PYG{o}{+}\PYG{n}{a1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
                 \PYG{p}{[}\PYG{p}{(}\PYG{n}{a1}\PYG{o}{+}\PYG{n}{a0}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{a1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Below you can see the resulting Omnibus Matrix. The first and second networks are shown as heatmaps on the left, and their Omnibus Matrix is shown on the right.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_104_0}.png}


\paragraph{Creating the Omnibus Matrix For All Four Networks}
\label{\detokenize{representations/ch6/multigraph-representation-learning:creating-the-omnibus-matrix-for-all-four-networks}}
\sphinxAtStartPar
Here’s the Omnibus Matrix for all four of our networks. You can see adjacency matrices for the original four networks on the diagonal blocks, highlighted in blue, and all possible pairs of averages of adjacency matrices on the off\sphinxhyphen{}diagonal blocks, highlighted in orange.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_107_0}.png}


\paragraph{Embedding the Omnibus Matrix}
\label{\detokenize{representations/ch6/multigraph-representation-learning:embedding-the-omnibus-matrix}}
\sphinxAtStartPar
You should understand the next step fairly well by now. We embed the Omnibus Matrix normally, using ASE, as if it were just a normal adjacency matrix. This will create an \(nm \times d\) sized latent position matrix (where, remember, \(n\) is the number of nodes in each network, \(m\) is the number of networks, and \(d\) is the number of embedding dimensions). Here, since each of our four networks has 200 nodes, \(mn\) is 800, and we chose to embed down to two dimensions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{select\PYGZus{}svd}

\PYG{n}{U}\PYG{p}{,} \PYG{n}{D}\PYG{p}{,} \PYG{n}{V} \PYG{o}{=} \PYG{n}{select\PYGZus{}svd}\PYG{p}{(}\PYG{n}{omni}\PYG{p}{,} \PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{joint\PYGZus{}embedding} \PYG{o}{=} \PYG{n}{U} \PYG{o}{@} \PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{D}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{joint\PYGZus{}embedding}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(800, 2)
\end{sphinxVerbatim}


\paragraph{Creating Separate Latent Positions In The Same Latent Space}
\label{\detokenize{representations/ch6/multigraph-representation-learning:creating-separate-latent-positions-in-the-same-latent-space}}
\sphinxAtStartPar
Now, the only question we have remaining is how to actually pull the separate latent positions for each network from this matrix. It turns out that the individual latent positions for each network are actually stacked on top of each other: the first \(n\) rows of the joint matrix we just made correspond to the nodes of the first network, the second \(n\) rows correspond to the nodes of the second network, and so on.

\sphinxAtStartPar
If we want, we can pull out the separate latent positions for each network explicitly. Below, we reshape our 2\sphinxhyphen{}dimensional \(mn \times d\) numpy array for the omnbus embedding into a \(m \times n \times d\) array: the embeddings for each network are now simply stacked on top of each other on the third dimension (and the first axis of our numpy array).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{m} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{latent\PYGZus{}networks} \PYG{o}{=} \PYG{n}{joint\PYGZus{}embedding}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{latent\PYGZus{}networks}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(4, 200, 2)
\end{sphinxVerbatim}

\sphinxAtStartPar
Below, you can see the embeddings we just created. On the left is the full \(mn \times d\) omnibus matrix, and on the right are the slices of the \(m \times n \times d\) 3\sphinxhyphen{}D array we created above. If you look carefully, you can see that the top two blocks of colors (row\sphinxhyphen{}wise) in the larger embedding correspond to the latent positions for network 1, the second two blocks correspond to the latent positions for network 2, and so on. They’re a bit squished, so that everything lines up nicely, but they’re there.

\noindent\sphinxincludegraphics{{multigraph-representation-learning_115_1}.png}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{multigraph-representation-learning_115_0}.png}
\caption{This is a \sphinxstylestrong{caption}, with an embedded \sphinxcode{\sphinxupquote{\{glue:text\}}} element: !}\label{\detokenize{representations/ch6/multigraph-representation-learning:fig-omnibus}}\end{figure}

\sphinxAtStartPar
And finally, below is the above embeddings, plotted in Euclidean space. Each point is a row of the embedding above, and the dots are colored according to their class label. The big matrix on the left (the joint OMNI embedding) just contains every latent position we have, across all of our networks. This means that, on the lefthand plot, there will be four points for every node (remember that we’re operating under the assumption that we have the same set of nodes across all of our networks).

\noindent\sphinxincludegraphics{{multigraph-representation-learning_118_0}.png}


\subsection{How Can You Use The Omnibus Embedding?}
\label{\detokenize{representations/ch6/multigraph-representation-learning:how-can-you-use-the-omnibus-embedding}}
\sphinxAtStartPar
Fundamentally, the omnibus embedding is useful is because it lets you avoid the somewhat annoying and noise\sphinxhyphen{}generating process of figuring out a good way to rotate your separate embeddings to line them up. For instance, say you want to figure out if two networks are generated from the same distribution (This means that the matrix that contains edge probabilities, \(\textbf{P}\), is the same for both networks). Then, it’s reasonable to assume that their latent positions will be pretty close to each other. Look at the equation below:

\sphinxAtStartPar
\(\min_{W} ||{\hat{N_1} - \hat{N_2}W}||_F\)

\sphinxAtStartPar
Here:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(W\) is a matrix that just rotates or flips (called an isometry, or an orthonormal matrix)

\item {} 
\sphinxAtStartPar
\(\hat{N_1}\) and \(\hat{N_2}\) are the estimated latent positions for networks one and two, respectively

\end{itemize}

\sphinxAtStartPar
the \(||X||_F\) syntax means that we’re taking the frobenius norm of \(X\). Taking the frobenius norm of a matrix is the same as unwrapping the matrix into a giant vector and measuring that vector’s length. So, this equation is saying that the latent position for a given node in network one should be close to the latent position in network two.

\sphinxAtStartPar
But, there’s that \(W\) there, the rotation matrix. We actually wish we didn’t have to find it. We have to because of the same problem we keep running into: you can rotate latent positions and they’ll still have the same dot product relative to each other, and so you can only embed a network up to a rotation. In practice, you can find this matrix and rotate latent positions for separate networks using it to compare them directly, but again, it’s annoying, adds compute power that you probably don’t want to use, and it’ll add noise to any kind of inference you want to do later.

\sphinxAtStartPar
The Omnibus Embedding is fundamentally a solution to this problem. Because the embeddings for all of your networks live in the same space, you don’t have to rotate them manually – and you cut out the noise that gets created when you have to \sphinxstyleemphasis{infer} a good rotation matrix. We’ll explore all the downstream use cases in future chapters, but below is a sneak peak.

\sphinxAtStartPar
The figure below (adapted from Gopalakrishnan et al. 2021 {[}{]},  is the omnibus embedding for 32 networks created from a bunch of mouse brains, some of which have been genetically modified. The nodes of these networks represent the regions of a mouse brain and the edges represent how well\sphinxhyphen{}connected the neurons in a given pair of regions are. The figure below actually only shows two nodes: the node representing one region in the left hemisphere, and the node representing its corresponding region in the right hemisphere.

\sphinxAtStartPar
So what we’re actually \sphinxstyleemphasis{plotting} in this embedding is a bit different than normal, because rather than being nodes, the points we plot are \sphinxstyleemphasis{networks}: one for each of our thirty\sphinxhyphen{}two mice. The only reason we’re able to get away with doing this is the omnibus embedding: each network lives in the same space!

\noindent\sphinxincludegraphics{{multigraph-representation-learning_121_0}.png}

\sphinxAtStartPar
You can clearly see a difference between the genetically modified mice and the normal mice. The genetically modified mice are off in their own cluster; if you’re familiar with classical statistics, you could do a MANOVA here and find that the genetically modified mice are significantly different from the rest \sphinxhyphen{} if we wanted to, we could figure out which mice are genetically modified, even without having that information in advance!


\section{Joint Representation Learning}
\label{\detokenize{representations/ch6/joint-representation-learning:joint-representation-learning}}\label{\detokenize{representations/ch6/joint-representation-learning::doc}}
\sphinxAtStartPar
In many problems, our network might be more than just the information contained in its adjacency matrix (called its topology, or its collection of nodes and edges). If we were investigating a social network, we might have access to extra information about each person – their gender, for instance, or their age. If we were investigating a brain network, we might have information about the physical location of neurons, or the volume of a brain region. When we we embed a network, it seems like we should be able to use these extra bits of information \sphinxhyphen{} called the “features” or “covariates” of a network \sphinxhyphen{} to somehow improve our analysis. The techniques and tools that we’ll explore in this section use both the covariates and the topology of a network to create and learn from new representations of the network. Because they jointly use both the topology of the network and its extra covariate information, these techniques and tools are called joint representation learning.

\sphinxAtStartPar
There are two primary reasons that we might want to explore using node covariates in addition to topological structure. First, they might improve our standard embedding algorithms, like Laplacian and Adjacency Spectral Embedding. For example, if the latent structure of the covariates of a network lines up with the latent structure of its topology, then we might be able to reduce noise when we embed, even if the communities in our network don’t overlap perfectly with the communities in our covariates. Second, figuring out what the clusters of an embedding actually mean can sometimes be difficult and covariates create a natural structure in our network that we can explore. Covariate information in brain networks telling us where in the brain each node is, for instance, might let us better understand the types of characteristics that distinguish between different brain regions.

\sphinxAtStartPar
In this section, we’ll explore different ways to learn from our data when we have access to the covariates of a network in addition to its topological structure. We’ll explore \sphinxstyleemphasis{Covariate\sphinxhyphen{}Assisted Spectral Embedding} (CASE), a variation on Spectral Embedding. In CASE, instead of embedding just the adjacency matrix or its regularized Laplacian, we’ll combine the Laplacian and our covariates into a new matrix and embed that.

\sphinxAtStartPar
A good way to illustrate how using covariates might help us is to use a model in which some of our community information is in the covariates and some is in our topology. Using the Stochastic Block Model, we’ll create a simulation using three communities: the first and second community will be indistinguishable in the topological structure of a network, and the second and third community will be indistinguishable in its covariates. By combining the topology and the covariates, we’ll get a nice embedding that lets us find three distinct community clusters.


\subsection{Stochastic Block Model}
\label{\detokenize{representations/ch6/joint-representation-learning:stochastic-block-model}}
\sphinxAtStartPar
Suppose we have a network generated from a Stochastic Block Model (or, commonly, SBM) with three communities. In our adjacency matrix, which contains only our network’s topological information, we’d like to create a situation where the first two communities are completely indistinguishable: Any random node in the first community will have exactly the same chance of being connected to another node in the first community or to a node in the second community. We’d like the third community to be distinct, with only a small probability that nodes in it will connect to nodes in either of the other two communities.

\sphinxAtStartPar
The Python code below generates a matrix that looks like this. There are 1500 nodes, with 500 nodes per community. Because the \(3 \times 3\) block probability matrix that generated this SBM has the same probability values (.3) in its upper\sphinxhyphen{}left \(2 \times 2\) square, a node in community 1 has a 30\% chance of being connected to either another node in community 1 or a node in community 2. As a result, in our adjacency matrix, we’ll see the nodes in communities one and two as a single giant block. On the other hand, nodes in community three only have a 15\% chance to connect to nodes in the first community. So, the end result is that we’ve created a situation where we have three communities that we’d like to separate into distinct clusters, but the topological structure in the adjacency matrix can’t separate the three groups by itself.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}

\PYG{c+c1}{\PYGZsh{} Start with some simple parameters}
\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{1500}  \PYG{c+c1}{\PYGZsh{} Total number of nodes}
\PYG{n}{n} \PYG{o}{=} \PYG{n}{N} \PYG{o}{/}\PYG{o}{/} \PYG{l+m+mi}{3}  \PYG{c+c1}{\PYGZsh{} Nodes per community}
\PYG{n}{p}\PYG{p}{,} \PYG{n}{q} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{15}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{15}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{15}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Our block probability matrix}

\PYG{c+c1}{\PYGZsh{} Make our Stochastic Block Model}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{p}{[}\PYG{n}{n}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{n}{n}\PYG{p}{]}\PYG{p}{,} \PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here you can see what our adjacency matrix looks like. Notice the giant block in the top\sphinxhyphen{}left: this block contains both nodes in both of the first two communities, and they’re indistinguishable from each other.

\noindent\sphinxincludegraphics{{joint-representation-learning_8_0}.png}

\sphinxAtStartPar
If we wanted to embed this graph using our Laplacian or Adjacency Spectral Embedding methods, we’d find the first and second communities layered on top of each other (though we wouldn’t be able to figure that out from our embedding if we didn’t cheat by knowing in advance which community each node is supposed to belong to). The python code below embeds our latent positions all the way down to two dimensions. Below it, you can see a plot of the latent positions, with each node color\sphinxhyphen{}coded by its true community.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{LaplacianSpectralEmbed} \PYG{k}{as} \PYG{n}{LSE}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{to\PYGZus{}laplacian}

\PYG{n}{L} \PYG{o}{=} \PYG{n}{to\PYGZus{}laplacian}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{form}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{R\PYGZhy{}DAD}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{L\PYGZus{}latents} \PYG{o}{=} \PYG{n}{LSE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{L}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{joint-representation-learning_11_0}.png}

\sphinxAtStartPar
As you can see, we’d have a tough time clustering this \sphinxhyphen{} the first and second community are completely indistinguishable. It would be nice if we could use extra information to more clearly distinguish between them. We don’t have this information in our adjacency matrix: it needs to come from somewhere else.


\subsection{Covariates}
\label{\detokenize{representations/ch6/joint-representation-learning:covariates}}
\sphinxAtStartPar
But we’re in luck \sphinxhyphen{} we have a set of covariates for each node! These covariates contain the extra information we need that allows us to separate our first and second community. However, with only these extra covariate features, we can no longer distinguish between the last two communities \sphinxhyphen{} they contain the same information.

\sphinxAtStartPar
Below is Python code that generates these covariates. Each node is associated with its own group of 30 covariates (thirty being chosen primarily to visualize what’s happening more clearly). We’ll organize this information into a matrix, where the \(i_{th}\) row contains the covariates associated with node \(i\). Remember that we have 1500 nodes in our network, so there will be 1500 rows. We’ll draw all the covariates for each node from the same Beta distribution (with values ranging from 0 to 1), but the nodes in the first community will be drawn from a different Beta distribution than the nodes in the last two.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{beta}

\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}community}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{beta}\PYG{o}{.}\PYG{n}{rvs}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{gen\PYGZus{}covariates}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c1} \PYG{o}{=} \PYG{n}{make\PYGZus{}community}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{n}{c2} \PYG{o}{=} \PYG{n}{make\PYGZus{}community}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{c3} \PYG{o}{=} \PYG{n}{make\PYGZus{}community}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

    \PYG{n}{covariates} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{(}\PYG{n}{c1}\PYG{p}{,} \PYG{n}{c2}\PYG{p}{,} \PYG{n}{c3}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{covariates}
    

\PYG{c+c1}{\PYGZsh{} Generate a covariate matrix}
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{gen\PYGZus{}covariates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here is a visualization of the covariates we just created. The first community is represented by the lighter\sphinxhyphen{}colored rows, and the last two are represented by the darker\sphinxhyphen{}colored rows.

\noindent\sphinxincludegraphics{{joint-representation-learning_17_0}.png}

\sphinxAtStartPar
We can play almost the same game here as we did with the Laplacian. If we embed the information contained in this matrix of covariates into lower dimensions, we can see the reverse situation as before \sphinxhyphen{} the first community is separate, but the last two are overlayed on top of each other.

\sphinxAtStartPar
Below is Python code that embeds the covariates. We’ll use custom embedding code rather than graspologic’s LSE class, because it isn’t technically right to act as if we’re embedding a Laplacian (even though we’re doing essentially the same thing under the hood). Underneath it is a plot of the resulting embedding.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{extmath} \PYG{k+kn}{import} \PYG{n}{randomized\PYGZus{}svd}

\PYG{k}{def} \PYG{n+nf}{embed}\PYG{p}{(}\PYG{n}{matrix}\PYG{p}{,} \PYG{o}{*}\PYG{p}{,} \PYG{n}{dimension}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{latents}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{randomized\PYGZus{}svd}\PYG{p}{(}\PYG{n}{matrix}\PYG{p}{,} \PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{n}{dimension}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{latents}

\PYG{n}{Y\PYGZus{}latents} \PYG{o}{=} \PYG{n}{embed}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{dimension}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
As you can see, we’re in a similar situation as we were in with the adjacency matrix, but with different communities: instead of the first and second communities being indistinguishable, now the second and third are. We’d like to see full separation between all three communities, so we need some kind of representation of our network that allows us to use both the information in the topology and the information in the covariates. This is where CASE comes in.


\subsection{Covariate\sphinxhyphen{}Assisted Spectral Embedding}
\label{\detokenize{representations/ch6/joint-representation-learning:covariate-assisted-spectral-embedding}}
\sphinxAtStartPar
Covariate\sphinxhyphen{}Assisted Spectral Embedding, or CASE1, is a simple way of combining our network and our covariates into a single model. In the most straightforward version of CASE, we combine the network’s regularized Laplacian matrix \(L\) and a function of our covariate matrix \(YY^\top\). Here, \(Y\) is just our covariate matrix, in which row \(i\) contains the covariates associated with node \(i\). Notice the word “regularized” \sphinxhyphen{} This means (from the Laplacian section earlier) that our Laplacian looks like \(L = L_{\tau} = D_{\tau}^{-1/2} A D_{\tau}^{-1/2}\) (remember, \(D\) is a diagonal matrix with \(D_{ii}\) telling us the degree of node \(i\)).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Suppose that \(Y\) only contains 0’s and 1’s. To interpret \(YY^T\), notice we’re effectively taking the the dot product of each row of \(Y\) with each other row, because the transpose operation turns rows into columns. Now, look at what happens below when we take the dot product of two example vectors with only 0’s and 1’s in them:
\label{equation:representations/ch6/joint-representation-learning:7276f209-1403-45e3-a060-ac6cd49c643e}\begin{align}
\begin{bmatrix}
1 & 1 & 1
\end{bmatrix} \cdot 
\begin{bmatrix}
0 \\
1 \\
1 \\
\end{bmatrix} = 1\times 0 + 1\times 1 + 1\times 1 = 2
\end{align}
\sphinxAtStartPar
If there are two overlapping 1’s in the same position of the left vector and the right vector, then there will be an additional 1 added to their weighted sum. So, in the case of a binary \(YY^T\), when we matrix\sphinxhyphen{}multiply a row of \(Y\) by a column of \(Y^T\), the resulting value, \((YY^T)_{i, j}\), will be equal to the number of shared locations in which vectors \(i\) and \(j\) both have ones.
\end{sphinxadmonition}

\sphinxAtStartPar
A particular value in \(YY^\top\), \((YY^\top)_{i, j}\), can be interpreted as measuring the “agreement” or “similarity” between row \(i\) and row \(j\) of our covariate matrix. Since each row represents the covariates for a particular node, the higher the value of \((YY^\top)_{i, j}\), the more similar the covariates of the \(i_{th}\) and \(j_{th}\) nodes are. The overall result is a matrix that looks fairly similar to our Laplacian!

\sphinxAtStartPar
The following Python code generates our covariate similarity matrix \(YY^\top\). We’ll also normalize the rows of our covariate matrix to have unit length using scikit\sphinxhyphen{}learn \sphinxhyphen{} this is because we want the scale for our covariate matrix to be roughly the same as the scale for our adjacency matrix. Later, we’ll weight \(YY^\top\) to help with this as well.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{normalize}

\PYG{n}{Y} \PYG{o}{=} \PYG{n}{normalize}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{YYt} \PYG{o}{=} \PYG{n}{Y}\PYG{n+nd}{@Y}\PYG{o}{.}\PYG{n}{T}
\end{sphinxVerbatim}

\sphinxAtStartPar
Below, you can see the Laplacian we generated earlier next to \(YY^\top\). Remember, each matrix contains information about our communities that the other doesn’t have \sphinxhyphen{} and our goal is to combine them in a way that lets us distinguish between all three communities.

\noindent\sphinxincludegraphics{{joint-representation-learning_27_0}.png}

\sphinxAtStartPar
The way we’ll combine the two matrices will simply be a weighted sum of these two matrices \sphinxhyphen{} this is what CASE is doing under the hood. The weight (here called \(\alpha\)) is multiplied by \(YY^\top\) \sphinxhyphen{} that way, both matrices contribute an equal amount of useful information to the embedding.
\begin{equation*}
\begin{split}
L + \alpha YY^\top
\end{split}
\end{equation*}

\subsubsection{Exploring Possible Weights}
\label{\detokenize{representations/ch6/joint-representation-learning:exploring-possible-weights}}
\sphinxAtStartPar
An obvious question here is how to weight the covariates. If we simply summed the two matrices by themselves, we’d unfortunately be in a situation where whichever matrix contained larger numbers would dominate over the other. In our current setup, without a weight on \(YY^\top\), the covariates of our network would dominate over its topology.

\noindent\sphinxincludegraphics{{joint-representation-learning_31_0}.png}

\sphinxAtStartPar
What do different potential weights look like? Let’s do a comparison. Below you can see the embeddings for 9 possible weights on \(YY^\top\), ranging between \(10^{-5}\) and 100.

\noindent\sphinxincludegraphics{{joint-representation-learning_33_0}.png}

\sphinxAtStartPar
It looks like we’d probably want a weight somewhere between 0.01 and 0.5  \sphinxhyphen{} then, we’ll have three communities which are fairly distinct from each other, implying that we’re pulling good information from both our network’s topology and its covariates. We could just pick our weight manually, but it would be nice to have some kind of algorithm or equation which lets us pick a reasonable weight automatically.


\subsubsection{Finding a Reasonable Weight Automatically}
\label{\detokenize{representations/ch6/joint-representation-learning:finding-a-reasonable-weight-automatically}}
\sphinxAtStartPar
In general, we’d like to embed in a way that lets us distinguish between communities. This means that if we knew which community each node belonged to, we’d like to be able to correctly retrieve the correct commmunities for each node as possible with a clustering algorithm after embedding. This also implies that the communities will be as distinct as possible.

\sphinxAtStartPar
We already found a range of possible weights, embedded our combined matrix for every value in this range, and then looked at which values produced the best clustering. But, how do we find a weight which lets us consistently use useful information from both the topology and the covariates?

\sphinxAtStartPar
When we embed symmetric matrices, keep in mind that the actual points we’re plotting are the components of the eigenvectors with the biggest eigenvalues. When we embed into two\sphinxhyphen{}dimensional space, for instance, the X\sphinxhyphen{}axis values of our points are the components of the eigenvector with the biggest eigenvalue, and the Y\sphinxhyphen{}axis values are the components of the eigenvector with the second\sphinxhyphen{}biggest eigenvalue. This means that we should probably be thinking about how much information the Laplacian and \(YY^\top\) contributes to the biggest eigenvalue/eigenvector pairs.

\sphinxAtStartPar
Thinking about this more, if we have a small weight, \(YY^\top\) will contribute only a small amount to the biggest eigenvalue/vector pair. If we have a large weight, \(YY^\top\) will contribute a large amount to the biggest eigenvalue/vector pair. The weight that causes the Laplacian and \(YY^\top\) to contribute the same amount of information, then, is just the ratio of the biggest eigenvalue of \(L\) and the biggest eigenvalue of \(YY^\top\):
\begin{equation*}
\begin{split}
weight = \frac{\lambda_1 (L)}{\lambda_1 (YY^\top)}
\end{split}
\end{equation*}
\sphinxAtStartPar
Let’s check what happens when we combine our Laplacian and covariates matrix using the weight described in the equation above. Our embedding works the same as it does in Laplacian Spectral Embedding: we decompose our combined matrix using Singular Value Decomposition, truncating the columns, and then we visualize the rows of the result. Remember, we’ll be embedding \(L + \alpha YY^\top\), where \(\alpha\) is our weight. We’ll embed all the way down to two dimensions, just to make visualization simpler.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{sparse}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{eigsh}

\PYG{c+c1}{\PYGZsh{} Find the biggest eigenvalues in both of our matrices}
\PYG{n}{leading\PYGZus{}eigval\PYGZus{}L}\PYG{p}{,} \PYG{o}{=} \PYG{n}{eigsh}\PYG{p}{(}\PYG{n}{L}\PYG{p}{,} \PYG{n}{return\PYGZus{}eigenvectors}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{leading\PYGZus{}eigval\PYGZus{}YYt}\PYG{p}{,} \PYG{o}{=} \PYG{n}{eigsh}\PYG{p}{(}\PYG{n}{YYt}\PYG{p}{,} \PYG{n}{return\PYGZus{}eigenvectors}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Per our equation above, we get the weight using}
\PYG{c+c1}{\PYGZsh{} the ratio of the two biggest eigenvalues.}
\PYG{n}{weight} \PYG{o}{=} \PYG{n}{leading\PYGZus{}eigval\PYGZus{}L} \PYG{o}{/} \PYG{n}{leading\PYGZus{}eigval\PYGZus{}YYt}

\PYG{c+c1}{\PYGZsh{} Do our weighted sum, then embed}
\PYG{n}{L\PYGZus{}} \PYG{o}{=} \PYG{n}{L} \PYG{o}{+} \PYG{n}{weight}\PYG{o}{*}\PYG{n}{YYt}
\PYG{n}{latents\PYGZus{}} \PYG{o}{=} \PYG{n}{embed}\PYG{p}{(}\PYG{n}{L\PYGZus{}}\PYG{p}{,} \PYG{n}{dimension}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{joint-representation-learning_39_0}.png}

\sphinxAtStartPar
Success! We’ve managed to achieve separation between all three communities. Below we can see (from left to right) a comparison of our network’s latent position when we only use its topological information, when we only use the information contained in its covariates, and finally our embedding using the weight we found.

\noindent\sphinxincludegraphics{{joint-representation-learning_41_0}.png}


\subsection{Using Graspologic}
\label{\detokenize{representations/ch6/joint-representation-learning:using-graspologic}}
\sphinxAtStartPar
Graspologic’s CovariateAssistedSpectralEmbedding class implements CASE directly. The following code applies CASE to reduce the dimensionality of \(L + aYY^T\) down to two dimensions, and then plots the latent positions to show the clustering.

\begin{sphinxadmonition}{note}{Non\sphinxhyphen{}Assortative Networks}

\sphinxAtStartPar
We don’t always necessarily want to embed \(L + \alpha YY^\top\). Using the regularized Laplacian by itself, for instance, isn’t always best. If your network is \sphinxstyleemphasis{non\sphinxhyphen{}assortative} \sphinxhyphen{} meaning, the between\sphinxhyphen{}block probabilities are greater than the within\sphinxhyphen{}block probabilities \sphinxhyphen{} it’s better to square our Laplacian. This is because the adjacency matrices of non\sphinxhyphen{}assortative networks have a lot of negative eigenvalues; squaring the Laplacian gets rid of a lot of annoying negative eigenvalues, and we end up with a better embedding. In the non\sphinxhyphen{}assortative case, we end up embedding \(LL + aYY^\top\). The \sphinxcode{\sphinxupquote{embedding\_alg}} parameter controls this: you can write \sphinxcode{\sphinxupquote{embedding\_alg="non\sphinxhyphen{}assortative"}} if you’re in the non\sphinxhyphen{}assortative situation.
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{CovariateAssistedEmbed} \PYG{k}{as} \PYG{n}{CASE}

\PYG{n}{casc} \PYG{o}{=} \PYG{n}{CASE}\PYG{p}{(}\PYG{n}{assortative}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{latents} \PYG{o}{=} \PYG{n}{casc}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{covariates}\PYG{o}{=}\PYG{n}{Y}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}latents}\PYG{p}{(}\PYG{n}{latents}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Embedding our model using graspologic}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{joint-representation-learning_45_0}.png}


\subsection{Omnibus Joint Embedding}
\label{\detokenize{representations/ch6/joint-representation-learning:omnibus-joint-embedding}}
\sphinxAtStartPar
If you’ve read the Multiple\sphinxhyphen{}Network Representation Learning section, you’ve seen the Omnibus Embedding (if you haven’t read that section, you should go read it before reading this one!). To recap, the way the omnibus embedding works is:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Have a bunch of networks

\item {} 
\sphinxAtStartPar
Combine the adjacency matrices from all of those networks into a single, huge network

\item {} 
\sphinxAtStartPar
Embed that huge network

\end{enumerate}

\sphinxAtStartPar
Remember that the Omnibus Embedding is a way to bring all of your networks into the same space (meaning, you don’t run into any rotational nonidentifiability issues when you embed). Once you embed the Omnibus Matrix, it’ll produce a huge latent position matrix, which you can break apart along the rows to recover latent positions for your individual networks.

\sphinxAtStartPar
You might be able to predict where this is going. What if we created an Omnibus embedding not with a set of networks, but with a network and covariates?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{OmnibusEmbed}

\PYG{c+c1}{\PYGZsh{} embed with Omni}
\PYG{n}{omni} \PYG{o}{=} \PYG{n}{OmnibusEmbed}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Normalize the covariates first}
\PYG{n}{YYt} \PYG{o}{=} \PYG{n}{Y}\PYG{n+nd}{@Y}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{YYt} \PYG{o}{/}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{YYt}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create a joint embedding}
\PYG{n}{joint\PYGZus{}embedding} \PYG{o}{=} \PYG{n}{omni}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{YYt}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{joint-representation-learning_49_0}.png}

\sphinxAtStartPar
There’s a few things going on here. First, we had to normalize the covariates by dividing \(YY^\top\) by its maximum. This is because if we didn’t, the covariates and the adjacency matrix would contribute vastly different amounts of information to the omnibus matrix. You can see that by looking at the average value of \(YY^\top\) compared to the average value of \(A\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Mean of Y@Y.T:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{Y}\PYG{n+nd}{@Y}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Mean of A:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Mean of Y@Y.T: 0.02
Mean of A: 0.23
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Mean of normalized Y@Y.T:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{YYt}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Mean of A:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Mean of normalized Y@Y.T: 0.42
Mean of A: 0.23
\end{sphinxVerbatim}

\sphinxAtStartPar
Remember the way this data is set up: you can separate the first community from the last two with the topology, you can separate the last community from the first two with the covariates, but you need both data sources to separate all three.

\sphinxAtStartPar
Here, you can see that \sphinxstyleemphasis{both embeddings} are able to separate all three communities. This is because the Omnibus Embedding induces dependence on the latent positions it outputs. Remember that the off\sphinxhyphen{}diagonals of the Omnibus Matrix contain the averages of pairs of networks fed into it. These off\sphinxhyphen{}diagonal elements are responsible for some “information leakage”: so the topology embedding contains information from the covariates, and the covariate embedding contains information from the topology.


\subsection{MASE Joint Embedding}
\label{\detokenize{representations/ch6/joint-representation-learning:mase-joint-embedding}}
\sphinxAtStartPar
Just like you can use the OMNI to do a joint embedding, you can also use MASE to do a joint embedding. This fundamentally comes down to the fact that both embeddings fundamentally just eat matrices as their input \sphinxhyphen{} whether those matrices are the adjacency matrix or \(YY^\top\) doesn’t really matter.

\sphinxAtStartPar
Just like OMNI, we’ll quickly recap how MASE works here:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Have a bunch of networks

\item {} 
\sphinxAtStartPar
Embed them all separately with ASE or LSE

\item {} 
\sphinxAtStartPar
Concate those embeddings into a single latent position matrix with a lot more dimensions

\item {} 
\sphinxAtStartPar
Embed that new matrix

\end{enumerate}

\sphinxAtStartPar
The difference here is the same as with Omni – we have the adjacency matrix (topology) and its covariates for a single network. So instead of embedding a bunch of adjacency matrices or Laplacians, we embed the adjacency matrix (or Laplacian) and the similarity matrix for the covariates \(YY^\top\) separately, concatenate, and then embed again.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{MultipleASE} \PYG{k}{as} \PYG{n}{MASE}

\PYG{c+c1}{\PYGZsh{} Remmeber that YY\PYGZca{}T is still normalized!}
\PYG{n}{mase} \PYG{o}{=} \PYG{n}{MASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{joint\PYGZus{}embedding} \PYG{o}{=} \PYG{n}{mase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{p}{[}\PYG{n}{A}\PYG{p}{,} \PYG{n}{YYt}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{joint-representation-learning_57_0}.png}

\sphinxAtStartPar
As you can see, MASE lets us get fairly clear separation between communities. The covariates are still normalized, as with OMNI, so that they can contribute the same amount to the embedding as the adjacency matrix.


\subsubsection{References}
\label{\detokenize{representations/ch6/joint-representation-learning:references}}
\sphinxAtStartPar
{[}1{]} N. Binkiewicz, J. T. Vogelstein, K. Rohe, Covariate\sphinxhyphen{}assisted spectral clustering, Biometrika, Volume 104, Issue 2, June 2017, Pages 361–377, https://doi.org/10.1093/biomet/asx008{[}2{]} Lloyd, S. (1982). Least squares quantization in PCM. IEEE transactions on information theory, 28(2), 129\sphinxhyphen{}137.{[}3{]} https://scikit\sphinxhyphen{}learn.org/stable/modules/clustering.html\#k\sphinxhyphen{}means{[}4{]} Hotelling, H. (1936). Relations between two sets of variates. Biometrika 28, 321–77.


\section{Model Estimation Theory}
\label{\detokenize{representations/ch6/estimating-parameters_theory:model-estimation-theory}}\label{\detokenize{representations/ch6/estimating-parameters_theory::doc}}
\sphinxAtStartPar
Throughout Chapter 5, we spent a lot of attention developing intuition for many of the network models that are essential to understanding random networks. Recall that the notation that we use for a random network (more specifically, a network\sphinxhyphen{}valued random variable), \(\mathbf A\), does \sphinxstyleemphasis{not} refer to any network we could ever hope to see (or as we introduced in the previous chapter, \sphinxstyleemphasis{realize}) in the real world. This issue is extremely important in network machine learning, so we will try to drive it home one more time: no matter how much data we collected (unless we could get infinite data, which we \sphinxstyleemphasis{can’t}), we can never hope to understand the true distribution of \(\mathbf A\). As network scientists, this leaves us with a bit of a problem: what, then, can we do to make useful claims about \(\mathbf A\), if we can’t actually see \(\mathbf A\) nor its distribution?

\sphinxAtStartPar
This is where statistics, particularly, \sphinxstylestrong{estimation}, comes into play. At a very high level, estimation is a procedure to calculate properties about a random variable (or a set of random variables) using \sphinxstyleemphasis{only} the data we are given: finitely many (in network statistics, often just \sphinxstyleemphasis{one}) samples which we assume are \sphinxstyleemphasis{realizations} of the random variable we want to learn about. The properties of the random variable that we seek to learn about are called \sphinxstylestrong{estimands}, and  In the case of our network models, in particular, we will attempt to obtain reasonable estimates of the parameters (our \sphinxstyleemphasis{estimands}) associated with random networks.

\sphinxAtStartPar
The most useful property that we will leverage which was developed in Chapter \(5\) is the independent\sphinxhyphen{}edge assumption. As we discussed, when working with independent\sphinxhyphen{}edge random network models, we will assume that edges in our random network are \sphinxstyleemphasis{independent}. This means that the probability of observing a particular realization of a random network is, in fact, the product of the probabilities of observing each edge in the random network. Notationally, what this means is that if \(\mathbf A\) is a random network with \(n\) nodes and edges \(\mathbf a_{ij}\), and \(A\) is a realization of that random network with edges \(a_{ij}\), then:
\begin{align*}
    \mathbb P_\theta(\mathbf A = A) &= \mathbb P(\mathbf a_{11} = a_{11}, \mathbf a_{12} = a_{12}, ..., \mathbf a_{nn} = a_{nn}), \\
    &= \prod_{i, j} \mathbb P_\theta(\mathbf a_{ij} = a_{ij}).
\end{align*}
\sphinxAtStartPar
In the special case where our networks are simple (undirected and loopless), this simplifies to:
\begin{align*}
    \mathbb P_\theta(\mathbf A = A) &= \prod_{i < j} \mathbb P_\theta(\mathbf a_{ij} = a_{ij}).
\end{align*}
\sphinxAtStartPar
for any network realization \(A\) which is simple. This is because if \(\mathbf a_{ij} = a\), then we also know that \(\mathbf a_{ji} = a\), and we only need to worry about one of the edges (we chose the edges in the upper right triangle of the adjacency matrix arbitrarily).  Further, since \(A\) is also simple, then we know hat \(\mathbf a_{ii} = 0\); that is, no nodes have loops, so we don’t need to worry about the case where \(i = j\) either.

\sphinxAtStartPar
We will set the scene for the later examples using a common example. Let’s say we flip a coin \(10\) times, and see \(6\) heads. What is the probability that the coin lands on heads? Intuitively, the answer is rather simple! It feels like it should just be \(\frac{6}{10}\). And in one particular way, that really is the \sphinxstyleemphasis{best} guess we could make!

\sphinxAtStartPar
Below, we discuss the nitty\sphinxhyphen{}gritty technical details of how we learn about random networks using a particular method known as Maximum Likelihood Estimation (MLE). Maximum likelihood estimation is why \(\frac{6}{10}\) is a great guess for our coin flip example. Finding MLEs can be pretty difficult, so we leave the details in starred sections. If you aren’t familiar with MLE, you can skip these, and still learn how to use the results!


\subsection{The Method of Maximum Likelihood Estimation (MLE)}
\label{\detokenize{representations/ch6/estimating-parameters_theory:the-method-of-maximum-likelihood-estimation-mle}}
\sphinxAtStartPar
Let’s think about what exactly this means using an example that you are likely familiar with. I have a single coin, and I want to know the probability of the outcome of a roll of that coin being a heads. For sake of argument, we will call this coin \sphinxstyleemphasis{fair}, which means that the true probability it lands on heads (or tails) is \(0.5\). In this case, I would call the outcome of the \(i^{th}\) coin flip the random variable \(\mathbf x_i\), and it can produce realizations which take one of two possible values: a heads (an outcome of a \(1\)) or a tails (an outcome of a \(0\)). We will say that we see \(10\) total coin flips. We will number these realizations as \(x_i\), where \(i\) goes from \(1\) to \(10\). To recap, the boldfaced \(\mathbf x_i\) denotes the random variable, and the unbolded \(x_i\) denotes the realization which we actually see. Our question of interest is: how do we estimate the probability of the coin landing on a heads, if we don’t know anything about the true probability value \(p\), other than the outcomes of the coin flips we got to observe?

\sphinxAtStartPar
Here, since \(\mathbf x_i\) takes the value \(1\) or \(0\) each with probability \(0.5\), we would say that \(\mathbf x_i\) is a \(Bernoulli(0.5)\) random variable. This means that the random variable \(\mathbf x\) has the Bernoulli distribution, and the probability of a heads, \(p\), is \(0.5\). All \(10\) of our \(\mathbf x_i\) are called \sphinxstyleemphasis{identically distributed}, since they all have the same \(Bernoulli(0.5)\) distribution.

\sphinxAtStartPar
We will also assume that the outcomes of the coin flips are mutually independent, which is explained in the terminology section.

\sphinxAtStartPar
For any one coin flip, the probability of observing the outcome \(i\) is, by definition of the Bernoulli distribution:
\begin{align*}
    \mathbb P_\theta(\mathbf x_i = x_i) = p^{x_i} (1 - p)^{1 - x_i}.
\end{align*}
\sphinxAtStartPar
Note that we use the notation \(\mathbb P_\theta\) to indicate that the probability is a function of the parameter set \(\theta\) for the random variable \(\mathbf x_i\). Here, since the only parameter for each \(\mathbf x_i\) is a probability \(p\), then \(\theta = p\).

\sphinxAtStartPar
If we saw \(n\) total outcomes, the probability is, using the definition of mutual independence:
\begin{align*}
    \mathbb P_\theta(\mathbf x_1 = x_1, ..., \mathbf x_{n} = x_{n}; p) &= \prod_{i = 1}^{n}\mathbb P(\mathbf x_i = x_i), \\
    &= \prod_{i = 1}^n p^{x_i}(1 - p)^{1 - x_i}, \\
    &= p^{\sum_{i = 1}^n x_i}(1 - p)^{n - \sum_{i = 1}^n x_i}.
\end{align*}
\sphinxAtStartPar
What if we saw \(10\) coin flips, and \(6\) were heads? Can we take a “guess” at what \(p\) might be? Intuitively your first reaction might be to say a good guess of \(p\), which we will abbreviate \(\hat p\), would be \(0.6\), which is \(6\) heads of \(10\) outcomes. In many ways, this intuitive guess is spot on. However, in network machine learning, we like to be really specific about why, exactly, this guess makes sense.

\sphinxAtStartPar
Looking at the above equation, one thing we can do is use the technique of \sphinxstylestrong{maximum likelihood estimation}. We call the function \(\mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p)\) the \sphinxstyleemphasis{likelihood} of our sequence, for a given value of \(p\). Note that we have added the term “\(; p\)” to our notation, which is simply to emphasize the dependence of the likelihood on the probability. So, what we \sphinxstyleemphasis{really} want to do is find the value that \(p\) could take, which \sphinxstyleemphasis{maximizes} the likelihood. Let’s see what the likelihood function looks like as a function of different values of \(p\):

\noindent\sphinxincludegraphics{{estimating-parameters_theory_1_0}.png}

\sphinxAtStartPar
As we can see, it turns out that our intuitive answer, that \(p=0.6\), is in fact the Maximum Likelihood Estimate for the Bernoulli probability parameter \(p\). Now how do we go about showing this rigorously?

\sphinxAtStartPar
An easier problem, we often will find, is to instead maximize the \sphinxstyleemphasis{log likelihood} rather than the likelihood itself. This is because the log function is \sphinxstyleemphasis{monotone}, which means that if \(\mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_1) < \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_2)\), then \(\log\mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_1) < \log \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_n = x_n; p_2)\) as well for some choices \(p_1\) and \(p_2\). Without going too down in the weeds, the idea is that the \(\log\) function does not change any critical points of the likelihood. The log likelihood of the above expression is:
\begin{align*}
\log \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_{n} = x_{n}; p) &= \log \left[p^{\sum_{i = 1}^n x_i}(1 - p)^{n - \sum_{i = 1}^n x_i}\right], \\
&= \sum_{i = 1}^n x_i \log(p) + \left(n - \sum_{i = 1}^n x_i\right)\log(1 - p).
\end{align*}
\sphinxAtStartPar
And visually, the log\sphinxhyphen{}likelihood now looks instead like this:

\noindent\sphinxincludegraphics{{estimating-parameters_theory_4_0}.png}

\sphinxAtStartPar
Although we can see that the two plots look \sphinxstyleemphasis{almost} nothing alike, the key is the word \sphinxstyleemphasis{almost} here. Notice that the absolute maximum is, in fact, the same regardless of whether we use the likelihood or the log\sphinxhyphen{}likelihood. Further, notice that at the maximum, the slope of the tangent line is \(0\). You may recall from calculus that this is how we typically go about finding a critical point of a function. Now, let’s get make our argument a little more technical. Remembering from calculus \(1\) and \(2\), to find a maximal point of the log\sphinxhyphen{}likelihood function with respect to some variable \(p\), our process looks like this:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Take the derivative of the log\sphinxhyphen{}likelihood with respect to \(p\),

\item {} 
\sphinxAtStartPar
Set it equal to \(0\) and solve for the critical point \(\tilde p\),

\item {} 
\sphinxAtStartPar
Verify that the critical point \(\tilde p\) is indeed an estimate of a maximum, \(\hat p\).

\end{enumerate}

\sphinxAtStartPar
Proceeding using the result we derived above, and using the fact that \(\frac{d}{du} \log(u) = \frac{1}{u}\) and that \(\frac{d}{du} \log(1 - u) = -\frac{1}{1 - u}\):
\begin{align*}
\frac{d}{d p}\log \mathbb P(\mathbf x_1 = x_1, ..., \mathbf x_{n} = x_{n}; p) &= \frac{\sum_{i = 1}^n x_i}{p} - \frac{n - \sum_{i = 1}^n x_i}{1 - p} = 0, \\
\Rightarrow \frac{\sum_{i = 1}^n x_i}{p} &= \frac{n - \sum_{i = 1}^n x_i}{1 - p}, \\
\Rightarrow (1 - p)\sum_{i = 1}^n x_i &= p\left(n - \sum_{i = 1}^n x_i\right), \\
\sum_{i = 1}^n x_i - p\sum_{i = 1}^n x_i &= pn - p\sum_{i = 1}^n x_i ,\\
\Rightarrow \tilde p &= \frac{1}{n}\sum_{i = 1}^n x_i.
\end{align*}
\sphinxAtStartPar
We use the notation \(\tilde p\) here to denote that \(\tilde p\) is a critical point of the function.

\sphinxAtStartPar
Finally, we must check that this is an estimate of a maximum, which we can do by taking the second derivative and checking that the second derivative is negative. We will omit this since it’s a bit intricate and tangential from our argument, but if you work it through, you will find that the second derivative is indeed negative at \(\tilde p\). This means that \(\tilde p\) is indeed an estimate of a maximum, which we would denote by \(\hat p\).

\sphinxAtStartPar
Finally, using this result, we find that with \(6\) heads in \(10\) outcomes, we would obtain an estimate:
\begin{align*}
    \hat p &= \frac{6}{10} = 0.6.
\end{align*}
\sphinxAtStartPar
which exactly aligns with our intuition.

\sphinxAtStartPar
So, why do we need estimation tools, if in our example, our intuition gave us the answer a whole lot faster? Unfortunately, the particular scenario we described was one of the \sphinxstyleemphasis{simplest possible examples} in which a parameter requires estimation. As the scenario grows more complicated, and \sphinxstyleemphasis{especially} when we extend to network\sphinxhyphen{}valued data, figuring out good ways to estimate parameters is extremely difficult. For this reason, we will describe some tools which are very relevant to network machine learning to learn about network parameters.


\subsubsection{MLE for ER}
\label{\detokenize{representations/ch6/estimating-parameters_theory:mle-for-er}}
\sphinxAtStartPar
In Chapter 5, we explored the derivation for the probability of observing a realization \(A\) of a given random network \(\mathbf A\) which is ER, which is equivalent to the likelihood of \(A\). Recall this was:
\begin{align*}
    \mathbb P_\theta(A) &= p^{m} \cdot (1 - p)^{\binom{n}{2} - m}.
\end{align*}
\sphinxAtStartPar
where \(m = \sum_{i < j} a_{ij}\) is the total number of edges in the observed network \(A\). Our approach here parallels directly the approach for the coin; we begin by taking the log of the probability:
\begin{align*}
    \log \mathbb P_\theta(A) &= \log \left[p^{m} \cdot (1 - p)^{\binom{n}{2} - m}\right], \\
    &= m \log p + \left(\binom n 2 - m\right)\log (1 - p).
\end{align*}
\sphinxAtStartPar
Next, we take the derivative with respect to \(p\), set equal to \(0\), and we end up with:
\begin{align*}
\frac{d}{d p}\log \mathbb P_\theta(A) &= \frac{m}{p} - \frac{\binom n 2 - m}{1 - p} = 0, \\
\Rightarrow \tilde p &= \frac{m}{\binom n 2}.
\end{align*}
\sphinxAtStartPar
We omitted several detailed steps due to the fact that we show the rigorous derivation above. Checking the second derivative, which we omit since it is rather mathematically tedious, we see that the second derivative at \(\tilde p\) is negative, so we indeed have found an estimate of the maximum, and will be denoted by \(\hat p\). This gives that the Maximum Likelihood Estimate (or, the MLE, for short) of the probability \(p\) for a random network \(\mathbf A\) which is ER is:
\begin{align*}
    \hat p &= \frac{m}{\binom n 2}.
\end{align*}

\subsubsection{MLE for SBM}
\label{\detokenize{representations/ch6/estimating-parameters_theory:mle-for-sbm}}
\sphinxAtStartPar
When we derived the probability for a realization \(A\) of a random network \(\mathbf A\) which could be characterized using the \sphinxstyleemphasis{a priori} Stochasic Block Model, we obtained that:
\begin{align*}
    \mathbb P_\theta(A) &= \prod_{k, k' \in [K]}b_{k'k}^{m_{k'k}} \cdot (1 - b_{k'k})^{n_{k'k - m_{k'k}}},
\end{align*}
\sphinxAtStartPar
where \(n_{k'k} = \sum_{i < j}\mathbb 1_{\tau_i = k}\mathbb 1_{\tau_j = k'}\) was the number of possible edges between nodes in community \(k\) and \(k'\), and \(m_{k'k} = \sum_{i < j}\mathbb 1_{\tau_i = k}\mathbb 1_{\tau_j = k'}a_{ij}\) was the number of edges in the realization \(A\) between nodes within communities \(k\) and \(k'\).

\sphinxAtStartPar
Noting that the log of the product is the sum of the logs, or that \(\log \prod_i x_i = \sum_i \log x_i\), the log of the probability is:
\begin{align*}
    \log \mathbb P_\theta(A) &= \sum_{k, k' \in [K]} m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k}).
\end{align*}
\sphinxAtStartPar
We notice a side\sphinxhyphen{}note that we mentioned briefly in the network models section: in a lot of ways, the probability (and consequently, the log probability) of a random network which is an \sphinxstyleemphasis{a priori} SBM behaves very similarly to that of a random network which is ER, with the caveat that the probability term \(p\), the total number of possible edges \(\binom n 2\), and the total number of edges \(m\) have been replaced with the probability term \(b_{k'k}\), the total number of possible edges \(n_{k'k}\), and the total number of edges \(m_{k'k}\) which \sphinxstyleemphasis{apply only to that particular pair of communities}. In this sense, the \sphinxstyleemphasis{a priori} SBM is kind of like a collection of communities of ER networks. Pretty neat right? Well, it doesn’t stop there. When we take the partial derivative of \(\log \mathbb P_\theta(A)\) with respect to any of the probability terms \(b_{l'l}\), we see an even more direct consequence of this observation:
\begin{align*}
    \frac{\partial }{\partial b_{l' l}}\log \mathbb P_\theta(A) &= \frac{\partial}{\partial b_{l'l}}\sum_{k, k' \in [K]} m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k}), \\
    &= \sum_{k, k' \in [K]} \frac{\partial}{\partial b_{l'l}}\left[m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k})\right].
\end{align*}
\sphinxAtStartPar
Now what? Notice that any of the summands in which \(k \neq l\) and \(k' \neq l'\), the partial derivative with respect to \(b_{l'l}\) is in fact exactly \(0\)! Why is this? Well, let’s consider a \(k\) which is different from \(l\), and a \(k'\) which is different from \(l'\). Notice that:
\begin{align*}
\frac{\partial}{\partial b_{l'l}}\left[m_{k'k}\log b_{k'k} + \left(n_{k'k} - m_{k'k}\right)\log(1 - b_{k'k})\right] = 0,
\end{align*}
\sphinxAtStartPar
which simply follows since the quantity to the right of the partial derivative is not a funcion of \(b_{l'l}\) at all! Therefore:
\begin{align*}
    \frac{\partial }{\partial b_{l' l}}\log \mathbb P_\theta(A) &= 0 + \frac{\partial}{\partial b_{l'l}}\left[m_{l'l}\log b_{l'l} + \left(n_{l'l} - m_{l'l}\right)\log(1 - b_{l'l})\right] \\
    &= \frac{m_{l'l}}{b_{l'l}} - \frac{n_{l'l} - m_{l'l}}{1 - b_{l'l}} = 0, \\
\Rightarrow b_{l'l}^* &= \frac{m_{l'l}}{n_{l'l}}.
\end{align*}
\sphinxAtStartPar
Like above, we omit the second derivative test, and conclude that the MLE of the block matrix \(B\) for a random network \(\mathbf A\) which is \sphinxstyleemphasis{a priori} SBM is the matrix \(\hat B\) with entries:
\begin{align*}
    \hat b_{l'l} &= \frac{m_{l'l}}{n_{l'l}}.
\end{align*}

\subsection{Spectral Methods}
\label{\detokenize{representations/ch6/estimating-parameters_theory:spectral-methods}}

\subsection{Why don’t we just use MLE?*}
\label{\detokenize{representations/ch6/estimating-parameters_theory:why-don-t-we-just-use-mle}}
\sphinxAtStartPar
The a posteriori Stochastic Block Model has a pair of parameters, the block matrix, \(B\), and the community probability vector, \(\vec \pi\). If you are keeping up with the log\sphinxhyphen{}likelihood derivations in the single network models section, you will recall that the log\sphinxhyphen{}likelihood for an a posteriori Stochastic Block Model, we obtain that:
\begin{align*}
    \mathbb P_\theta(A) &= \sum_{\vec \tau \in \mathcal T} \prod_{k = 1}^K \left[\pi_k^{n_k}\cdot \prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\right]
\end{align*}
\sphinxAtStartPar
That expression, it turns out, is a lot more complicated than what we had to deal with for the \sphinxstyleemphasis{a priori} Stochastic Block Model. Taking the log gives us that:
\begin{align*}
\log 
    \mathbb P_\theta(A) &= \log\left(\sum_{\vec \tau \in \mathcal T} \prod_{k = 1}^K \left[\pi_k^{n_k}\cdot \prod_{k'=1}^K b_{k' k}^{m_{k' k}}(1 - b_{k' k})^{n_{k' k} - m_{k' k}}\right]\right)
\end{align*}
\sphinxAtStartPar
Whereas the log of a product of terms is the sum of the logs of the terms, no such easy simplification exists for the log of a \sphinxstyleemphasis{sum} of terms. This means that we will have to get a bit creative here. Instead, we will turn first to the \sphinxstyleemphasis{a priori} Random Dot Product Graph, and then figure out how to estimate parameters from an \sphinxstyleemphasis{a posteriori} SBM using that.


\chapter{Theoretical Results}
\label{\detokenize{representations/ch7/ch7:theoretical-results}}\label{\detokenize{representations/ch7/ch7::doc}}

\section{Theory for Single Network Models}
\label{\detokenize{representations/ch7/theory-single-network:theory-for-single-network-models}}\label{\detokenize{representations/ch7/theory-single-network::doc}}

\section{Theory for Multiple\sphinxhyphen{}Network Models}
\label{\detokenize{representations/ch7/theory-multigraph:theory-for-multiple-network-models}}\label{\detokenize{representations/ch7/theory-multigraph::doc}}

\section{Theory for Graph Matching}
\label{\detokenize{representations/ch7/theory-matching:theory-for-graph-matching}}\label{\detokenize{representations/ch7/theory-matching::doc}}

\part{Applications}


\chapter{Applications When You Have One Network}
\label{\detokenize{applications/ch8/ch8:applications-when-you-have-one-network}}\label{\detokenize{applications/ch8/ch8::doc}}

\section{Community Detection}
\label{\detokenize{applications/ch8/community-detection:community-detection}}\label{\detokenize{applications/ch8/community-detection::doc}}

\section{Testing for Differences between Communities}
\label{\detokenize{applications/ch8/testing-differences:testing-for-differences-between-communities}}\label{\detokenize{applications/ch8/testing-differences::doc}}

\section{Model Selection}
\label{\detokenize{applications/ch8/model-selection:model-selection}}\label{\detokenize{applications/ch8/model-selection::doc}}

\section{Single\sphinxhyphen{}Network Vertex Nomination}
\label{\detokenize{applications/ch8/single-vertex-nomination:single-network-vertex-nomination}}\label{\detokenize{applications/ch8/single-vertex-nomination::doc}}
\sphinxAtStartPar
Say you’re a criminal investigator trying to uncover a human trafficking ring. You build a network of potential suspects: some of the nodes in the network represent human traffickers, and some represent innocent people. The edges of the network represent a working relationship between a given pair of individuals.

\sphinxAtStartPar
Your team has limited resources, and so it’s difficult to scrutinize everybody in the network directly to see if they are human traffickers. Ideally, you’d like to use your network to nominate potential suspects, so that you can prioritize your investigative efforts. You’ve already done some work: you have a list of a few nodes of the network who are known to be traffickers, and you have a list of a few who you know are not. Your goal, then, is to build an ordered list of nodes in the network that are most similar to the nodes you already know belong to human traffickers. Ideally, the first nodes in the list would be more likely to be traffickers, and the nodes would get less and less likely the further down in the list you go.

\sphinxAtStartPar
This is the idea behind \sphinxstyleemphasis{single\sphinxhyphen{}network vertex nomination}. You have a group of “seed nodes” that you know have the right community membership, and then you take the rest of the nodes in your network and order them by their relationship to the seed nodes in terms of that community membership. The nomination task here isn’t just classification: it’s prioritization. You’re prioritizing how important the rest of your nodes are with respect to the seed nodes, with the most important nodes at the top.


\subsection{Spectral Vertex Nomination}
\label{\detokenize{applications/ch8/single-vertex-nomination:spectral-vertex-nomination}}
\sphinxAtStartPar
There are a few approaches to vertex nomination. You can take a likelihood\sphinxhyphen{}maximization approach, or a bayes\sphinxhyphen{}optimal approach \sphinxhyphen{} but what we’ll focus on is \sphinxstyleemphasis{Spectral Vertex Nomination}. The general idea is that you embed your network, and then you just order the latent positions by how close they are to the seed node latent positions. There are a few ways of doing this: you could create a \sphinxstyleemphasis{separate} set of nominees for each node, for instance. This would correspond to finding the people closest to \sphinxstyleemphasis{each} human trafficker, rather than finding a single list of nominees. You could also just get a single list of nominees: you could first take the centroid of the latent positions of your seed nodes, and then find the closest nodes to that \sphinxstyleemphasis{centroid}. There are also a few different ways of defining what it means to be “close” to seed node latent positions. The obvious way is euclidean distance, which is what you’d traditionally think of as the distance between two points, but you could also use something like the Mahalanobis distance, which is essentially Euclidean distance but with a coordinate system and a rescaling defined by the covariance in your data.

\sphinxAtStartPar
In any case, all forms of Spectral Vertex Nomination involve finding embeddings and then taking distances. In contrast to the other approaches, it scales well with very large networks (since you’re essentially just doing an embedding followed by a simple calculation) and doesn’t require any prior knowledge of community membership.

\sphinxAtStartPar
Let’s see what spectral vertex nomination looks like. Below, we see the latent positions for a network with three communities, where two of the communities are more closely linked than the third community. We do a standard adjacency spectral embedding, and we end up with a set of latent positions. Our seed nodes \sphinxhyphen{} the ones whose community membership we know \sphinxhyphen{} are marked.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}

\PYG{c+c1}{\PYGZsh{} construct network}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.35}\PYG{p}{,} \PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.65}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create a network from and SBM, then embed}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{p}{[}\PYG{n}{n}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{n}{n}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Let\PYGZsq{}s say we know that the first five nodes belong to the first community.}
\PYG{c+c1}{\PYGZsh{} We\PYGZsq{}ll say that those are our seed nodes.}
\PYG{n}{seeds} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} grab a set of seed nodes}
\PYG{n}{memberships} \PYG{o}{=} \PYG{n}{labels}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{memberships}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n}{seed\PYGZus{}idx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{seeds}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{mask}\PYG{p}{[}\PYG{n}{seed\PYGZus{}idx}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{memberships}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{mask}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{bool}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}

\PYG{c+c1}{\PYGZsh{} find the latent positions for the seed nodes}
\PYG{n}{seed\PYGZus{}latents} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{n}{memberships}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{bool}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-vertex-nomination_5_0}.png}

\sphinxAtStartPar
Now, we’d like to order the rest of the vertices in this network by their degree of similarity to the seed nodes. Remember that we talked about two ways of doing this: we could find a separate set of nominations for each seed node, or we could find a single set of nominations for all of the seed nodes. Let’s start by finding a single set, using the centroid.


\subsubsection{Finding a single set of nominations}
\label{\detokenize{applications/ch8/single-vertex-nomination:finding-a-single-set-of-nominations}}
\sphinxAtStartPar
Computing the centroid is as easy as just taking the mean value for the seed latent positions along each coordinate axis. Since our example is in 2 dimensions, we can just take our \(m \times 2\) matrix of seed latent positions and take the mean along the first axis to create a \(1 \times 2\) vector. That vector will be the centroid, and its location in Euclidean space will be right in the middle of the seeds. You can see the centroid (red star) along with the seed latent positions (red circles) below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{centroid} \PYG{o}{=} \PYG{n}{seed\PYGZus{}latents}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-vertex-nomination_10_0}.png}

\sphinxAtStartPar
Now, all we do is order the rest of the latent positions (the blue dots in the figure above) by their distance to the centroid. The nodes corresponding to the closer latent positions will be higher up in our nomination list. Scikit\sphinxhyphen{}learn has a \sphinxcode{\sphinxupquote{NearestNeighbors}} classifier, so we’ll just use that. Below, we fit the classifier to our latent positions matrix, then get our nominations using the \sphinxcode{\sphinxupquote{kneighbors}} function. The latent positions closer to the centroid are more visible, and they get progressively less visible the further from the centroid they are.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{NearestNeighbors}

\PYG{c+c1}{\PYGZsh{} Find the nearest neighbors to the seeds, excluding other seeds}
\PYG{n}{neighbors} \PYG{o}{=} \PYG{n}{NearestNeighbors}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{neighbors}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{distances}\PYG{p}{,} \PYG{n}{nominations} \PYG{o}{=} \PYG{n}{neighbors}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}\PYG{n}{centroid}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{newaxis}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-vertex-nomination_13_0}.png}

\sphinxAtStartPar
Let’s look at the network directly, and see where our nominations tend to be. Below is a network colored by nomination rank: nodes that are higher up in the nomination list are more purple, and nodes that are lower in the nomination list are more white. You can see that the higher up in the nomination list you get (more purple), the more well\sphinxhyphen{}connected nodes tend to be to the seed nodes.

\noindent\sphinxincludegraphics{{single-vertex-nomination_15_0}.png}


\subsection{Finding Nominations for Each Node}
\label{\detokenize{applications/ch8/single-vertex-nomination:finding-nominations-for-each-node}}
\sphinxAtStartPar
Another approach, if we don’t want to combine the information from all of our seed nodes, is to create a different nomination list for each node. This would correspond to finding multiple sets of people close to \sphinxstyleemphasis{each} human trafficker, rather than finding a single set of people for the \sphinxstyleemphasis{group} of human traffickers. Graspologic does this natively; the only real difference between the two approaches is that we take the nearest neighbors of the centroid for the first method rather than for each individual. Because of this, we’ll just use graspologic directly, rather than showcasing the algorithm.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{nominate} \PYG{k+kn}{import} \PYG{n}{SpectralVertexNomination}

\PYG{c+c1}{\PYGZsh{} Choose the number of nominations we want for each seed node}
\PYG{n}{svn} \PYG{o}{=} \PYG{n}{SpectralVertexNomination}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{svn}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} get nominations and distances for each seed index}
\PYG{n}{nominations}\PYG{p}{,} \PYG{n}{distances} \PYG{o}{=} \PYG{n}{svn}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{seed\PYGZus{}idx}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Below you can see the nominations for each node. The first row containes the indices for each seed node, and each subsequent row contains the nearest neighbors for those seed nodes.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nominations}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[ 0,  1,  2,  3,  4],
       [30, 80, 57, 25, 71],
       [96,  7, 39, 27, 23],
       [43, 29,  5, 89, 14],
       [93, 89, 96, 88, 20]])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{single-vertex-nomination_21_0}.png}

\sphinxAtStartPar
These approaches are each useful in different situations.


\subsection{References}
\label{\detokenize{applications/ch8/single-vertex-nomination:references}}
\sphinxAtStartPar
theory papers:
\begin{itemize}
\item {} 
\sphinxAtStartPar
theory of consistency and Bayes optimality: On consistent vertex nomination schemes

\item {} 
\sphinxAtStartPar
dealing with corrupted networks (e.g., not one\sphinxhyphen{}to\sphinxhyphen{}one correspondence): Vertex Nomination, Consistent Estimation, and Adversarial Modification

\end{itemize}

\sphinxAtStartPar
applications papers:
\begin{itemize}
\item {} 
\sphinxAtStartPar
social networks: Vertex nomination via local neighborhood matching

\item {} 
\sphinxAtStartPar
data associated with human trafficking: Vertex nomination schemes for membership prediction

\end{itemize}


\section{Out\sphinxhyphen{}of\sphinxhyphen{}sample Embedding}
\label{\detokenize{applications/ch8/out-of-sample:out-of-sample-embedding}}\label{\detokenize{applications/ch8/out-of-sample::doc}}
\sphinxAtStartPar
Imagine you have a citation network of scholars publishing papers. The nodes are the scholars, and an edge exists in a given pair of scholars if they’re published a paper together.

\sphinxAtStartPar
You’ve already found a representation using ASE or LSE and you have a set of latent positions, which you then clustered to figure out who came from which university. It took a long time for you to get this representation \sphinxhyphen{} there are a lot of people doing research out there!

\sphinxAtStartPar
Now, suppose a new graduate student publishes a paper. Your network gets bigger by a single node, and you’d like to find this person’s latent position (thus adding them to the clustering system). To do that, however, you’d have to get an entirely new representation for the network: your latent position matrix is \(n \times d\), and it would need to become \((n+1) \times d\). Re\sphinxhyphen{}embedding the entire network with the new node added seems like it should be unecessary \sphinxhyphen{} after all, you already know the latent positions for every other node.

\sphinxAtStartPar
This section is all about this problem: how to find the representation for new nodes without the computationally expensive task of re\sphinxhyphen{}embedding an entire network. As it turns out, there has been some work done, and there is a solution that can get you pretty close the latent position for the new node that you would have had. For more details and formaility, see the 2013 paper “Out\sphinxhyphen{}of\sphinxhyphen{}sample extension for latent position graphs”, by Tang et al (although, as with most science, the theory in this paper was built on top of other work from related fields).

\sphinxAtStartPar
Let’s make a network from an SBM, and an additional node that should belong to the first community. Then, we’ll embed the network and explore how to find the latent position for the additional node.


\subsection{A Network and an Out\sphinxhyphen{}of\sphinxhyphen{}Sample Node}
\label{\detokenize{applications/ch8/out-of-sample:a-network-and-an-out-of-sample-node}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{sbm}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{remove\PYGZus{}vertices}

\PYG{c+c1}{\PYGZsh{} Generate parameters}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate both an original network along with community memberships, }
\PYG{c+c1}{\PYGZsh{} and an out\PYGZhy{}of\PYGZhy{}sample node with the same SBM call}
\PYG{n}{network}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{101}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{labels} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Grab out\PYGZhy{}of\PYGZhy{}sample node}
\PYG{n}{oos\PYGZus{}idx} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{oos\PYGZus{}label} \PYG{o}{=} \PYG{n}{labels}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{n}{oos\PYGZus{}idx}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} create our original network}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{a\PYGZus{}1} \PYG{o}{=} \PYG{n}{remove\PYGZus{}vertices}\PYG{p}{(}\PYG{n}{network}\PYG{p}{,} \PYG{n}{indices}\PYG{o}{=}\PYG{n}{oos\PYGZus{}idx}\PYG{p}{,} \PYG{n}{return\PYGZus{}removed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
What we have now is a network and an additional node. You can see the adjacency matrix for the network below, along with the adjacency vector for the additional node (Here, an “adjacency vector”  is a vector with a 1 in every position that the out\sphinxhyphen{}of\sphinxhyphen{}sample node has an edge with an in\sphinxhyphen{}sample node). The heatmap on the left is a network with two communities, with 100 nodes in each community. The vector on the right is purple on row \(i\) if the \(i^{th}\) in\sphinxhyphen{}sample node is connected to the out\sphinxhyphen{}of\sphinxhyphen{}sample node.

\noindent\sphinxincludegraphics{{out-of-sample_5_0}.png}

\sphinxAtStartPar
After embedding with ASE, we have an embedding for the original network. The rows of this embedding contain the latent position for each original node. We’ll call the embedding \(X\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}

\PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{out-of-sample_8_0}.png}


\subsection{The Latent Position Matrix Can be used to Estimate Probability Vectors}
\label{\detokenize{applications/ch8/out-of-sample:the-latent-position-matrix-can-be-used-to-estimate-probability-vectors}}
\sphinxAtStartPar
Everything up until now has just been pretty standard stuff. We still haven’t done anything with our new node \sphinxhyphen{} all we have is a big vector that tells us which other nodes it’s connected to, and our standard matrix of latent positions. However, it’s time for a bit more exploration into the nature of the latent position matrix \(X\), and what happens when you view it as a linear transformation. This will get us closer to understanding the out\sphinxhyphen{}of\sphinxhyphen{}sample embedding.

\sphinxAtStartPar
Remember from the section on latent positions that \(X\) can be used to estimate the block probability matrix. When you use ASE on a single network to make \(X\), \(XX^\top\) estimates \(P\): meaning, \((XX^\top)_{ij}\), the element on the \(i^{(th)}\) row and \(j^{(th)}\) column of \(XX^\top\), will estimate the probability that node \(i\) has an edge with node \(j\).

\sphinxAtStartPar
Let’s take a single latent position vector \sphinxhyphen{} call it \(v_i\) (this will be the \(i_{th}\) row of the latent position matrix). What’ll \(X v_i\) look like? Well, it’ll look the same as grabbing the \(i_{th}\) column of \(XX^\top\). Meaning, \(X v_i\) will be a single vector whose \(j^{(th)}\) element estimates the probability that node \(i\) will connect to node \(j\).

\sphinxAtStartPar
You can see this in action below. We took the latent position corresponding to the first node out of the latent position matrix (and called it \(v_1\)), and then multiplied it by the latent position matrix itself. What emerged is what you see below: a vector that shows the estimated probability that node 0 has an edge with each other node in the network. The true probabilities for the first half of nodes (the ones in the same community) should be .8, and the true probabilities for the second half of nodes in the other community should be .2. The average values were .775 and .149 \sphinxhyphen{} so, pretty close!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v\PYGZus{}1} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\PYG{n}{v\PYGZus{}est\PYGZus{}proba} \PYG{o}{=} \PYG{n}{X} \PYG{o}{@} \PYG{n}{v\PYGZus{}1}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{out-of-sample_12_0}.png}


\subsection{Going in the Other Direction}
\label{\detokenize{applications/ch8/out-of-sample:going-in-the-other-direction}}
\sphinxAtStartPar
Remember that our goal is to take the adjacency vector for a new node and use it to estimate that node’s latent position without re\sphinxhyphen{}embedding the whole network. So far, we’ve essentially figured out how to use the node’s latent position to get an estimated probability vector.

\sphinxAtStartPar
Let’s think about the term “estimated probability vector” for a second. This should be a vector associated to node \(i\) with \(n\) elements, where the \(j^{(th)}\) element of the vector contains the probability that node \(i\) will connect to node \(j\). The thing we’re starting with for the out\sphinxhyphen{}of\sphinxhyphen{}sample node, however, is an adjacency vector full of 0’s and 1’s \sphinxhyphen{} 0 if there isn’t an edge, 1 if there is an edge.

\sphinxAtStartPar
If you think about it, however, you can think of this adjacency vector as kind of an estimate for edge probabilities. Say you sample a node’s adjacency vector from an RDPG, then you sample again, and again. Averaging all of your samples will get you closer and closer to the actual edge connection probabilities. So you can think of a single adjacency vector as an estimate for the edge probability vector!

\sphinxAtStartPar
The point here is that if you can start with a latent position and then estimate the edge probabilities, it’s somewhat equivalent (albeit going in the other direction) to start with an out\sphinxhyphen{}of\sphinxhyphen{}sample adjacency vector and estimate a node’s the latent position.

\sphinxAtStartPar
Let’s call the estimated probability vector \(\hat{a_i}\). We know that \(\hat{a_i} = \hat{X} \hat{v_i}\): you multiply the latent position matrix by the \(i_{th}\) latent position to estimate the probability vector (remember that the \textasciicircum{} hats above letters means we’re getting an estimate for something, rather than getting the thing itself). How do we isolate the latent position \(\hat{v_i}\)?

\sphinxAtStartPar
Well, if \(X\) were invertible, we could do \(\hat{X}^{-1} \hat{a_i} = \hat{v_i}\): just invert both sides of the equation to get \(v_i\) by itself. Unfortunately, in practice, \(X\) will almost never be invertible. We’ll have to do the next\sphinxhyphen{}best thing, which is to use the \sphinxstyleemphasis{pseudoinverse}.

\sphinxAtStartPar
We’ll take a brief break in the coming section to talk about the pseudoinverse for a bit, then we’ll come back and use it to estimate the out\sphinxhyphen{}of\sphinxhyphen{}sample latent position.


\subsection{The Pseudoinverse Gives Us our Best Guess For Inverting a Matrix}
\label{\detokenize{applications/ch8/out-of-sample:the-pseudoinverse-gives-us-our-best-guess-for-inverting-a-matrix}}
\sphinxAtStartPar
The Moore\sphinxhyphen{}Penrose Pseudoinverse is useful to know in general, since it pops up a lot in a lot of different places. Say you have a matrix which isn’t invertible. Call it \(T\).

\sphinxAtStartPar
The pseudoinverse \(T^+\) is the closest approximation you can get to the inverse \(T^{-1}\). This is best understood visually. Let’s take \(T\) to be a matrix which projects points on the x\sphinxhyphen{}y coordinate axis down to the x\sphinxhyphen{}axis, then flips them to their negative on the number line. The matrix would look like this:
\begin{align*}
    T &=
    \begin{bmatrix}
    -1 & 0 \\
    0 & 0  \\
    \end{bmatrix}
\end{align*}
\sphinxAtStartPar
Some information is inherently lost here. Because the second column is all zeroes, any information in the y\sphinxhyphen{}axis can’t be recovered. For instance, say we have some vectors with different x\sphinxhyphen{}axis and y\sphinxhyphen{}axis coordinates:
\begin{align*}
    v_1 &= \begin{bmatrix} 1 & 1 \end{bmatrix}^\top \\
    v_2 &= \begin{bmatrix} 2 & 2 \end{bmatrix}^\top
\end{align*}
\sphinxAtStartPar
When we use \(T\) as a linear transformation to act on \(v_1\) and \(v_2\), the y\sphinxhyphen{}axis coordinates both collapse to the same thing (0, in this case). Information in the x\sphinxhyphen{}axis, however, is preserved.

\noindent\sphinxincludegraphics{{out-of-sample_17_0}.png}

\sphinxAtStartPar
Our goal is to reverse \(T\) and bring \(Tv_1\) and \(Tv_2\) back to \(v_1\) and \(v_2\). Unfortunately, since both \(v_1\) and \(v_2\) get squished onto zero in the y\sphinxhyphen{}axis position after getting passed through \(T\), we’ve lost all information about what was happening on the y\sphinxhyphen{}axis – that’s a lost cause. So it’s impossible to get perfectly back to \(v_1\) or \(v_2\).

\sphinxAtStartPar
If you restrict your attention to the x\sphinxhyphen{}axis, however, you’ll see that \(Tv_1\) and \(Tv_2\) landed in different places (\(v_1\) went to \sphinxhyphen{}1, and \(v_2\) went to \sphinxhyphen{}2). You can use this information about the x\sphinxhyphen{}axis location of \(Tv_1\) and \(Tv_2\) to re\sphinxhyphen{}orient the x\sphinxhyphen{}axis values back to where they were prior to the vectors getting passed through X, even if it’s impossible to figure out where the y\sphinxhyphen{}values were.

\sphinxAtStartPar
That’s what the pseudoinverse does: it reverses what it can, and accepts that some information has vanished.

\noindent\sphinxincludegraphics{{out-of-sample_19_0}.png}


\subsection{Using the Pseudoinverse to Estimate out\sphinxhyphen{}of\sphinxhyphen{}sample Latent Positions}
\label{\detokenize{applications/ch8/out-of-sample:using-the-pseudoinverse-to-estimate-out-of-sample-latent-positions}}
\sphinxAtStartPar
Let’s get back to estimating our out\sphinxhyphen{}of\sphinxhyphen{}sample latent position.

\sphinxAtStartPar
Remember that we had a nonsquare latent position matrix \(X\). Like we learned before, we can get the probability vector \(a_i\) (the vector with its probability of connecting with node \(j\) in the \(j_{th}\) position) for a node by passing its latent position (\(v_i\)) through the latent position matrix.
\begin{align*}
a_i = X v_i
\end{align*}
\sphinxAtStartPar
We can think of \(X\) as a matrix the same way we thought of \(T\): it’s a linear transformation that eats a vector, and doesn’t necessarily preserve all the information about that vector when it outputs something (In this case, since \(X\) brings lower\sphinxhyphen{}dimensional latent positions to higher\sphinxhyphen{}dimensional probability vectors, what’s happening is more of a restriction on which high\sphinxhyphen{}dimensional vectors you can access than a loss of information, but that’s not particularly important).

\sphinxAtStartPar
The pseudoinverse, \(X^+\), is the best we can do to bring a higher\sphinxhyphen{}dimensional adjacency vector to a lower\sphinxhyphen{}dimensional latent position. Since the adjacency vector just approximates the probability vector, we can call it \(\hat{a_i}\). In practice, the best we can do generally turns out to be a pretty good guess, and so we can get a decent estimation of the latent position \(v_i\).
\begin{align*}
X^+ \hat{a_i} \approx X^+ (X v_i) \approx v_i
\end{align*}
\sphinxAtStartPar
Let’s see it in action. Remember that we already grabbed our out\sphinxhyphen{}of\sphinxhyphen{}sample latent position and called it \sphinxcode{\sphinxupquote{a\_1}}. We use numpy’s pseudoinverse function to generate the pseudoinverse of the latent position matrix. Finally, we use it to get \sphinxcode{\sphinxupquote{a\_1}}’s estimated latent position, and call it \sphinxcode{\sphinxupquote{v\_1}}. You can see the location of this estimate in Euclidean space below: it falls squarely into the first community, which is where it should be.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{pinv}

\PYG{c+c1}{\PYGZsh{} Make the pseudoinverse of the latent position matrix}
\PYG{n}{X\PYGZus{}pinverse} \PYG{o}{=} \PYG{n}{pinv}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get its estimated latent position}
\PYG{n}{v\PYGZus{}1} \PYG{o}{=} \PYG{n}{X\PYGZus{}pinverse} \PYG{o}{@} \PYG{n}{a\PYGZus{}1}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v\PYGZus{}1}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.61596041, 0.61217755])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{out-of-sample_24_0}.png}


\subsection{Using Graspologic}
\label{\detokenize{applications/ch8/out-of-sample:using-graspologic}}
\sphinxAtStartPar
Of course, you don’t have to do all of this manually. Below we generate an adjacency matrix \(A\) from an SBM, as well as the adjacency vector for an out\sphinxhyphen{}of\sphinxhyphen{}sample node \(a_1\). Once we fit an instance of the ASE class, the latent position for any new nodes can be predicted by simply calling \sphinxcode{\sphinxupquote{ase.transform}} on the new adjacency vectors.

\sphinxAtStartPar
You can do the same thing with multiple out\sphinxhyphen{}of\sphinxhyphen{}sample nodes if you want by stacking their adjacency vectors on top of each other in a numpy array, then transforming the whole stack.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{AdjacencySpectralEmbed} \PYG{k}{as} \PYG{n}{ASE}

\PYG{c+c1}{\PYGZsh{} Generate parameters}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate a network along with community memberships}
\PYG{n}{network}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{sbm}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{101}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{B}\PYG{p}{,} \PYG{n}{return\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{labels} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Grab out\PYGZhy{}of\PYGZhy{}sample vertex}
\PYG{n}{oos\PYGZus{}idx} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{oos\PYGZus{}label} \PYG{o}{=} \PYG{n}{labels}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{n}{oos\PYGZus{}idx}\PYG{p}{)}
\PYG{n}{A}\PYG{p}{,} \PYG{n}{a\PYGZus{}1} \PYG{o}{=} \PYG{n}{remove\PYGZus{}vertices}\PYG{p}{(}\PYG{n}{network}\PYG{p}{,} \PYG{n}{indices}\PYG{o}{=}\PYG{n}{oos\PYGZus{}idx}\PYG{p}{,} \PYG{n}{return\PYGZus{}removed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Make an ASE model}
\PYG{n}{ase} \PYG{o}{=} \PYG{n}{ASE}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Predict out\PYGZhy{}of\PYGZhy{}sample latent positions by transforming}
\PYG{n}{v\PYGZus{}1} \PYG{o}{=} \PYG{n}{ase}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{a\PYGZus{}1}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{out-of-sample_28_0}.png}


\chapter{Applications for Two Networks}
\label{\detokenize{applications/ch9/ch9:applications-for-two-networks}}\label{\detokenize{applications/ch9/ch9::doc}}

\section{Two\sphinxhyphen{}Sample Hypothesis Testing}
\label{\detokenize{applications/ch9/two-sample-hypothesis:two-sample-hypothesis-testing}}\label{\detokenize{applications/ch9/two-sample-hypothesis::doc}}

\section{Graph Matching}
\label{\detokenize{applications/ch9/graph-matching-vertex:graph-matching}}\label{\detokenize{applications/ch9/graph-matching-vertex::doc}}
\sphinxAtStartPar
You work at Facebook and Twitter, but there’s been a terrible incident. All twitter users’ names and handles have been somehow been deleted! Your bosses are furious and have tasked you with somehow recovering the lost information. How might you go about doing this? Luckily, you’ve been working hard and have somehow earned yourself this dual Facebook/Twitter gig, so you have a great resource at your disposal: the Facebook social network. You know all facebook users and who they are friends with, and since you’ve only lost the twitter usernames, you can still figure out which ghost\sphinxhyphen{}users follow each other (make this sentence more specific, split it up into more sentences). You decide to use the Facebook network connectivity data to re\sphinxhyphen{}label the twitter social network. Alternatively, you can say the we are “aligning” Twitter based on Facebook.

\sphinxAtStartPar
In the example above, the social networks are represented with each user as a node, and an edge exists if two users are friends. We’ll define the facebook and twitter networks as \(F\) and \(T\) respectively, with associated adjacency matrices \(A_F\) and \(A_T\). This method is known as \(\textit{Graph Matching}\), because we are matching the node labels of one graph to another. This can also be thought of as a mapping; that is, based on the neighborhood structure of a node in the \(F\) network, we assign the same label to the node in \(T\) with the most similar structure. In other words, one of our twitter users will be assigned the user name of the Facebook user with the most followers in common. This is then done for the whole network, such that overall the structure is best preserved.

\sphinxAtStartPar
As you can imagine, there are a very large number of these possible mappings. In fact, for network pairs with \(n\) nodes, there are \(n!\) possible mappings. So how would we go about solving this(more specific) mathematically? First, we need a metric that tells us how similar two networks are to each other. For graph matching, this similarity metric is defined as \(f(A, B) = ||A - B||_F^2\) for unweighted adjacency matrices \(A, B \in \mathbb{R}^{n \times n}\). In other words, \(f(A, B)\) is the sum of the squared elementwise differences between \(A\) and \(B\). To understand this functionally, consider the best possible case where \(A=B\), that is, the networks are identical. The difference will be a matrix of all zeros, and taking the squared norm will then yield \(f(A,B) = 0\). If we remove one edge from \(A\), then \(f(A,B) = 1\). If we consider the worst possible case (every edge in \(A\) does not exist in \(B\), and vice versa), then \(f(A,B) = n^2\). This metric effectively counts the number of adjacnecy disagreements between \(A\) and \(B\). Thus, we want to find the mapping where \(f(A, B)\) is as small as possible.


\subsection{Graph Matching Small Networks}
\label{\detokenize{applications/ch9/graph-matching-vertex:graph-matching-small-networks}}
\sphinxAtStartPar
Say we have the network pairs below, \(T\) and \(F\). They are four nodes each, \(\{1, 2, 3, 4\}\) for \(T\), and \(\{a, b, c, d\}\) for \(F\). The two networks are clearly equal to each other.

\sphinxAtStartPar
\sphinxincludegraphics{{gm_1}.png}

\sphinxAtStartPar
However, the spatial layout of a network’s nodes is arbirary, and in reality it can often be much harder to tell whether two networks are the same. For instance, we can swap the spatial location of nodes \(c\) and \(d\) in network \(F\), as shown below. Even with such a small network, it’s hard to tell whether the networks are the same. Nonetheless, by looking at the adjacency matrices, we see that the networks are in fact the same, with \(f(A_T, A_F) = 0\)

\sphinxAtStartPar
\sphinxincludegraphics{{gm_2}.png}



\sphinxAtStartPar
Next, we swap the actual the node labels of nodes 2 and 3 in network \(F\).

\sphinxAtStartPar
\sphinxincludegraphics{{gm_3}.png}



\sphinxAtStartPar
As we see the networks are no longer the same, with \(f(A_T,A_F) = 8\). This might seem a bit high, but note that due to the graph being undirected, adjecency disagreements are effectively counted twice, since all edges (in and out) appear twice in the adjacency matrix. After showing how networks with a low number of edge disagreements are considered to be better matches, we will now demonstrate how to manipulate our networks and adjacency matrices such that we can find alignments that match well.


\subsection{Permutation Matrices}
\label{\detokenize{applications/ch9/graph-matching-vertex:permutation-matrices}}
\sphinxAtStartPar
Mappings are represented via \(\textit{Permutation Matrices}\) when solving the graph matching problem. A permutation matrix is a matrix of all ones and zeros, where each row and column adds up to one. In other words, each row and column has exactly one entry equal to one, with the rest being zeros.


\subsubsection{\protect\(PB\protect\) moves the rows, \protect\(BP^T\protect\) moves the columns}
\label{\detokenize{applications/ch9/graph-matching-vertex:pb-moves-the-rows-bp-t-moves-the-columns}}
\sphinxAtStartPar
Permutation matrices are commonly used as a method to move around the rows and columns of a square matrix. Consider the following example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{plot} \PYG{k+kn}{import} \PYG{n}{heatmap}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{P} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Original Matrix \PYGZdl{}B\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{P}\PYG{n+nd}{@B}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Row Permutation \PYGZdl{}PB\PYGZdl{}:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{B}\PYG{n+nd}{@P}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Row Permutation \PYGZdl{}BP\PYGZca{}T\PYGZdl{}:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}Row Permutation \PYGZdl{}BP\PYGZca{}T\PYGZdl{}:\PYGZsq{}\PYGZcb{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_7_1}.png}

\sphinxAtStartPar
The permutation matrix represents the following mapping:\(0 \rightarrow 1\)\(1 \rightarrow 0\)\(2 \rightarrow 2\)\(3 \rightarrow 3\)The matrix multiplication \(PB\) moves the rows based on the mapping, and \(BP^T\) moves the columns based on the mapping. In other words, in this case \(PB\) swaps rows 0 and 1 of \(B\), and \(BP^T\) swaps columns 0 and 1 of \(B\). Therefore by combined these two operations \(PBP^T\), we can move both the rows and columns based on a single bijection.


\subsubsection{Permutation Matrices to Match Graphs}
\label{\detokenize{applications/ch9/graph-matching-vertex:permutation-matrices-to-match-graphs}}
\sphinxAtStartPar
Next, we again consider the previous simple network pair example of swapping the node labels of 2 and 3 in network H:
\sphinxincludegraphics{{gm_3}.png}



\sphinxAtStartPar
This swap is represented by the following bijection to recover the node correspondence between \(G\) and \(H\)\(0 \rightarrow 0\)\(1 \rightarrow 1\)\(2 \rightarrow 3\)\(3 \rightarrow 2\)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{P} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}A\PYGZus{}T\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}A\PYGZus{}F\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{P}\PYG{n+nd}{@B}\PYG{n+nd}{@P}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}A\PYGZus{}F\PYGZdl{} with row and column permutation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}\PYGZdl{}A\PYGZus{}F\PYGZdl{} with row and column permutation\PYGZsq{}\PYGZcb{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_10_1}.png}

\sphinxAtStartPar
As shown in the code block above, using the permutation matrix we are able to recover the correspondence between \(T\) and \(F\).Thus, the graph matching formulation for any two adjacency matrices \(A, B\), seeks to minimize \(|| A - PBP^\intercal||_F^2\) such that \(P\) is a permuation matrix. This means that you shuffle the rows and columns of \(B\), such that it is as close as possible to \(A\). In mathematics, the process of minimizing (or maximizing) a function based on some constraint is known as optimization.


\subsection{Finding an Good Permutation with Gradient Descent}
\label{\detokenize{applications/ch9/graph-matching-vertex:finding-an-good-permutation-with-gradient-descent}}
\sphinxAtStartPar
The algorithm used for solving graph matching is a variation of gradient descent.  The specifics of the algorithm are beyond the scope of this book, but for now you can simply imagine it as gradient descent. A gradient can be thought of as a vector valued slope; it is simply the slope of a function in all of it’s dimensions, at a single point in space. Gradient Descent is a very common optimization method using to find optimal solutions for a wide range of problems.

\sphinxAtStartPar
A simple way to think of the method is gravity.  Consider an inspector who might use a golf ball to find the lowest point when installing a drain. The ball rolls down hill until it comes to a stop; once stopped, we know we’ve found the lowest point. Gradient descent works in a similar way, taking steps in the direction of the local gradient with respect to some parameter. Once the gradient is zero, the minimum has been found.

\sphinxAtStartPar
The main steps of a gradient descent method are choosing a suitable initial position (can be chosen randomly), then gradually improving the cost function one step at a time, until the function is changing by a very small amount, converging to a minimum. The main issue with gradient descent is that it does not guarantee that you will find a global minimum, only that you will find the local minimum of your initial position.

\sphinxAtStartPar
\sphinxincludegraphics{{grad_desc}.png}

\sphinxAtStartPar
The image above is a simplification in two dimensions; the network functions we optimize over are n dimensional when matching networks with n nodes, making the problem incredibly difficult to solve. For this reason (among others outside of the scope), the state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art graph matching algorithm is an approximation algorithm.


\subsection{Graph Matching with graspologic}
\label{\detokenize{applications/ch9/graph-matching-vertex:graph-matching-with-graspologic}}
\sphinxAtStartPar
For the example below, we will match two networks with a known to be have a node bijection that preserves a common network structure. To do this, we simulate a single Erdos\sphinxhyphen{}Reyni network, \(A\), with six nodes and edge probability of 30. Then, we generate \(B\) by randomly permuting the node labels of \(A\). Thus, \(A\) and \(B\) are said to be \(\textit{isomorphic}\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{er\PYGZus{}np}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{6}
\PYG{n}{p} \PYG{o}{=} \PYG{l+m+mf}{0.5}

\PYG{c+c1}{\PYGZsh{} np.random.seed(1)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{er\PYGZus{}np}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{p}\PYG{p}{)}
\PYG{n}{node\PYGZus{}shuffle\PYGZus{}input} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{permutation}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ix\PYGZus{}}\PYG{p}{(}\PYG{n}{node\PYGZus{}shuffle\PYGZus{}input}\PYG{p}{,} \PYG{n}{node\PYGZus{}shuffle\PYGZus{}input}\PYG{p}{)}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of adjecnecy disagreements: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{A}\PYG{o}{\PYGZhy{}}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A [ER\PYGZhy{}NP(4, 0.3) Simulation]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B [A Randomly Shuffled]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of adjecnecy disagreements:  16.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}B [A Randomly Shuffled]\PYGZsq{}\PYGZcb{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_16_2}.png}

\sphinxAtStartPar
Below, we create a model to solve the Graph Matching Problem. The model is then fitted for the two graphs A and B.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{match} \PYG{k+kn}{import} \PYG{n}{GraphMatch}

\PYG{n}{gmp} \PYG{o}{=} \PYG{n}{GraphMatch}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{gmp} \PYG{o}{=} \PYG{n}{gmp}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,}\PYG{n}{B}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{B}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ix\PYGZus{}}\PYG{p}{(}\PYG{n}{gmp}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{p}{,} \PYG{n}{gmp}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{p}{)}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of adjecnecy disagreements: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{A}\PYG{o}{\PYGZhy{}}\PYG{n}{B}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A [ER\PYGZhy{}NP(6, 0.3) Simulation]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B [Unshuffled]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{p}{(}\PYG{n}{A}\PYG{o}{\PYGZhy{}}\PYG{n}{B}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A\PYGZhy{}B [Unshuffled]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of adjecnecy disagreements:  8.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}A\PYGZhy{}B [Unshuffled]\PYGZsq{}\PYGZcb{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_18_2}.png}

\sphinxAtStartPar
The graph matching algorithm is able to successfully unshuffle \(B\), with zero adjacency disagreements between \(A\) and the matched \(B\).


\subsection{Seeds}
\label{\detokenize{applications/ch9/graph-matching-vertex:seeds}}
\sphinxAtStartPar
As mentioned previously, as network become larger, they quickly become more difficult to match. One method to mitigate this difficulty is to use \(\textit{seeds}\). Seeds are a subset of matches that we already know before we perform the graph matching. For example, if we are given two graphs \(T\) and \(F\) with 300 nodes each, we might already know ten node matches between \(T\) and \(F\). Having this prior information greatly improves our ability to match the networks.


\subsection{Seeded Graph Matching on Correlated Graph Pairs}
\label{\detokenize{applications/ch9/graph-matching-vertex:seeded-graph-matching-on-correlated-graph-pairs}}
\sphinxAtStartPar
To demonstrate the effectiveness of Seeded Graph Matching (SGM), the algorithm will be applied on a pair of correlated SBM graphs (undirected, no self loops) \(T, F \sim SBM\,(n, p, rho)\)  with the following parameters:
\begin{align*}
n &= [100, 100, 100]\\
p &= \begin{bmatrix} 
0.7 & 0.3 & 0.4\\
0.3 & 0.7 & 0.3\\
0.4 & 0.3 & 0.7
\end{bmatrix}\\
rho &= 0.9
\end{align*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{er\PYGZus{}corr}\PYG{p}{,} \PYG{n}{sbm}\PYG{p}{,} \PYG{n}{sbm\PYGZus{}corr}
\PYG{n}{directed} \PYG{o}{=} \PYG{k+kc}{False}
\PYG{n}{loops} \PYG{o}{=} \PYG{k+kc}{False}
\PYG{n}{n\PYGZus{}per\PYGZus{}block} \PYG{o}{=} \PYG{l+m+mi}{75}
\PYG{n}{n\PYGZus{}blocks} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{block\PYGZus{}members} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{n\PYGZus{}blocks} \PYG{o}{*} \PYG{p}{[}\PYG{n}{n\PYGZus{}per\PYGZus{}block}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{n\PYGZus{}verts} \PYG{o}{=} \PYG{n}{block\PYGZus{}members}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{rho} \PYG{o}{=} \PYG{l+m+mf}{0.9}
\PYG{n}{block\PYGZus{}probs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{A1}\PYG{p}{,} \PYG{n}{A2} \PYG{o}{=} \PYG{n}{sbm\PYGZus{}corr}\PYG{p}{(}\PYG{n}{block\PYGZus{}members}\PYG{p}{,} \PYG{n}{block\PYGZus{}probs}\PYG{p}{,} \PYG{n}{rho}\PYG{p}{,} \PYG{n}{directed}\PYG{o}{=}\PYG{n}{directed}\PYG{p}{,} \PYG{n}{loops}\PYG{o}{=}\PYG{n}{loops}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A2}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1} \PYG{o}{\PYGZhy{}} \PYG{n}{A2}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diff (G1 \PYGZhy{} G2)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_23_0}.png}

\sphinxAtStartPar
To emphasize the effectiveness of SGM, as well as why having seeds is important, we will randomly shuffle the vertices of Graph 2. This random permutation is stored, and unshuffled, such that we have available the optimal permutation that returns the original graph 2.

\sphinxAtStartPar
Here we see that after shuffling graph 2, there are many more edge disagreements, as expected.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{node\PYGZus{}shuffle\PYGZus{}input} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{permutation}\PYG{p}{(}\PYG{n}{n\PYGZus{}verts}\PYG{p}{)}
\PYG{n}{A2\PYGZus{}shuffle} \PYG{o}{=} \PYG{n}{A2}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ix\PYGZus{}}\PYG{p}{(}\PYG{n}{node\PYGZus{}shuffle\PYGZus{}input}\PYG{p}{,} \PYG{n}{node\PYGZus{}shuffle\PYGZus{}input}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{node\PYGZus{}unshuffle\PYGZus{}input} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}verts}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{node\PYGZus{}unshuffle\PYGZus{}input}\PYG{p}{[}\PYG{n}{node\PYGZus{}shuffle\PYGZus{}input}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}verts}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A2\PYGZus{}shuffle}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 2 shuffled}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1} \PYG{o}{\PYGZhy{}} \PYG{n}{A2\PYGZus{}shuffle}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diff (G1 \PYGZhy{} G2 shuffled)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_25_0}.png}


\subsection{Unshuffling graph 2 without seeds}
\label{\detokenize{applications/ch9/graph-matching-vertex:unshuffling-graph-2-without-seeds}}
\sphinxAtStartPar
First, we will run SGM on graph 1 and the shuffled graph 2 with no seeds, and return the match ratio, that is the fraction of vertices that have been correctly matched.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sgm} \PYG{o}{=} \PYG{n}{GraphMatch}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sgm} \PYG{o}{=} \PYG{n}{sgm}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{,}\PYG{n}{A2\PYGZus{}shuffle}\PYG{p}{)}
\PYG{n}{A2\PYGZus{}unshuffle} \PYG{o}{=} \PYG{n}{A2\PYGZus{}shuffle}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ix\PYGZus{}}\PYG{p}{(}\PYG{n}{sgm}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{p}{,} \PYG{n}{sgm}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A2\PYGZus{}unshuffle}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 2 unshuffled}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1} \PYG{o}{\PYGZhy{}} \PYG{n}{A2\PYGZus{}unshuffle}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diff (G1 \PYGZhy{} G2 unshuffled)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{match\PYGZus{}ratio} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{count\PYGZus{}nonzero}\PYG{p}{(}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{sgm}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{o}{\PYGZhy{}}\PYG{n}{node\PYGZus{}unshuffle\PYGZus{}input}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{n\PYGZus{}verts}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Match Ratio with no seeds: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{match\PYGZus{}ratio}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Match Ratio with no seeds:  0.06222222222222218
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_27_1}.png}

\sphinxAtStartPar
While the predicted permutation for graph 2 did recover the basic structure of the stochastic block model (i.e. graph 1 and graph 2 look qualitatively similar), we see that the number of edge disagreements between them is still quite high, and the match ratio quite low.


\subsection{Unshuffling graph 2 with 10 seeds}
\label{\detokenize{applications/ch9/graph-matching-vertex:unshuffling-graph-2-with-10-seeds}}
\sphinxAtStartPar
Next, we will run SGM with 10 seeds randomly selected from the optimal permutation vector found ealier. Although 10 seeds is only about 4\% of the 300 node graph, we will observe below how much more accurate the matching will be compared to having no seeds.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{permutation}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{W2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{node\PYGZus{}unshuffle\PYGZus{}input}\PYG{p}{[}\PYG{n}{W1}\PYG{p}{]}\PYG{p}{)}
    
\PYG{n}{sgm} \PYG{o}{=} \PYG{n}{GraphMatch}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sgm} \PYG{o}{=} \PYG{n}{sgm}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{,}\PYG{n}{A2\PYGZus{}shuffle}\PYG{p}{,}\PYG{n}{W1}\PYG{p}{,}\PYG{n}{W2}\PYG{p}{)}
\PYG{n}{A2\PYGZus{}unshuffle} \PYG{o}{=} \PYG{n}{A2\PYGZus{}shuffle}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ix\PYGZus{}}\PYG{p}{(}\PYG{n}{sgm}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{p}{,} \PYG{n}{sgm}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A2\PYGZus{}unshuffle}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Graph 2 unshuffled}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{A1} \PYG{o}{\PYGZhy{}} \PYG{n}{A2\PYGZus{}unshuffle}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Diff (G1 \PYGZhy{} G2 unshuffled)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{match\PYGZus{}ratio} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{count\PYGZus{}nonzero}\PYG{p}{(}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{sgm}\PYG{o}{.}\PYG{n}{perm\PYGZus{}inds\PYGZus{}}\PYG{o}{\PYGZhy{}}\PYG{n}{node\PYGZus{}unshuffle\PYGZus{}input}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{n\PYGZus{}verts}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Match Ratio with 10 seeds: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{match\PYGZus{}ratio}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Match Ratio with 10 seeds:  1.0
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{graph-matching-vertex_30_1}.png}

\sphinxAtStartPar
From the results above, we see that when running SGM on the same two graphs, with no seeds there is match ratio is quite low. However including 10 seeds increases the match ratio to 100\% (meaning that the shuffled graph 2 was completely correctly unshuffled).


\section{Vertex Nomination For Multiple Networks}
\label{\detokenize{applications/ch9/multiple-vertex-nomination:vertex-nomination-for-multiple-networks}}\label{\detokenize{applications/ch9/multiple-vertex-nomination::doc}}

\chapter{Applications for Many Networks}
\label{\detokenize{applications/ch10/ch10:applications-for-many-networks}}\label{\detokenize{applications/ch10/ch10::doc}}

\section{Anomaly Detection For Timeseries of Networks}
\label{\detokenize{applications/ch10/anomaly-detection:anomaly-detection-for-timeseries-of-networks}}\label{\detokenize{applications/ch10/anomaly-detection::doc}}
\sphinxAtStartPar
There is a particular type of sea slug which has gills on the outside of its body. When you squirt water at these gills, they withdraw into the slug. The interesting thing about this type of slug is that the brain network involved in this gill withdrawal reflex is entirely mapped out, from the neurons which detect and transmit information about the water into the slug’s brain, to the neurons that leave the brain and fire at its muscles. (For those interested, this is a real thing \sphinxhyphen{} look up Eric Kandel’s research on Aplysia!)

\sphinxAtStartPar
Say you’re a researcher studying these sea slugs, and you have a bunch of brain networks of the same slug. We can define each node as a single neuron, and edges denote connections between neurons. Each of the brain networks that you have were taken at different time points: some before water started getting squirted at the slug’s gills, and some as the water was getting squirted. Your goal is to reconstruct when water started to get squirted, using only the networks themselves. You hypothesize that there should be some signal change in your networks which can tell you the particular time at which water started getting squirted. Given the network data you have, how do you figure out which timepoints these are?

\sphinxAtStartPar
The broader class of problems this question addresses is called \sphinxstyleemphasis{anomaly detection}. The idea, in general, is that you have a bunch of snapshots of the same network over time. Although the nodes are the same, the edges are changing at each time point. Your goal is to figure out which time points correspond to the most change, either in the entire network or in particular groups of nodes. You can think of a network as “anomalous” with respect to time if some potentially small group of nodes within the network concurrently changes behavior at some point in time compared to the recent past, while the remaining nodes continue with whatever noisy, normal behavior they had.

\sphinxAtStartPar
In particular, what we would really like to do is separate the signal from the noise. All of the nodes in the network are likely changing a bit over time, since there is some variability intrinsic in the system. Random noise might just dictate that some edges get randomly deleted and some get randomly created at each step. We want to figure out if there are timepoints where the change isn’t just random noise: we’re trying to figure out a point in time where the probability distribution that the network \sphinxstyleemphasis{itself} is generated from changes.

\sphinxAtStartPar
Let’s simulate some network timeseries data so that we can explore anomaly detection more thoroughly.


\subsection{Simulating Network Timeseries Data}
\label{\detokenize{applications/ch10/anomaly-detection:simulating-network-timeseries-data}}
\sphinxAtStartPar
For this data generation, we’re going to assemble a set of 12 time\sphinxhyphen{}points for a network directly from its latent positions (we’ll assume that each time\sphinxhyphen{}point for the network is drawn from an RDPG). Ten of these time points will just have natural variability, and two will have a subset of nodes whose latent positions were perturbed a bit. These two will be the anomalies.

\sphinxAtStartPar
We’ll say that the latent positions for the network are one\sphinxhyphen{}dimensional, and that it has 100 nodes. There will be the same number of adjacency matrices as there are time points, since our network will be changing over time.

\sphinxAtStartPar
To make the ten non\sphinxhyphen{}anomalous time points, we’ll:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Generate 100 latent positions. Each latent position will be a (uniformly) random number between 0.2 and 0.8.

\item {} 
\sphinxAtStartPar
Use graspologic’s rdpg function to sample an adjacency matrix using these latent positions. Do this ten times.

\end{enumerate}

\sphinxAtStartPar
And to make the two perturbed time points, we’ll do the following twice:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Add a small amount of noise to the first 20 latent positions that we generated above.

\item {} 
\sphinxAtStartPar
Generate an adjacency matrix from this perturbed set of latent positions.

\end{enumerate}

\sphinxAtStartPar
Once we have this simulated data, we’ll move into some discussion about how we’ll approach detecting the anomalous time points.

\sphinxAtStartPar
Below is code for generating the data. We define a function to generate a particular time\sphinxhyphen{}point, with an argument which toggles whether we’ll perturb latent positions for that time point. Then, we just loop through our time\sphinxhyphen{}points to sample an adjacency matrix for each one.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{simulations} \PYG{k+kn}{import} \PYG{n}{rdpg}


\PYG{k}{def} \PYG{n+nf}{gen\PYGZus{}timepoint}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{perturbed}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{n\PYGZus{}perturbed}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{perturbed}\PYG{p}{:}
        \PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
        \PYG{n}{baseline} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{delta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{n}{baseline}\PYG{p}{,} \PYG{p}{(}\PYG{n}{n\PYGZus{}perturbed}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} 
                                     \PYG{n}{n\PYGZus{}perturbed}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} 
                                     \PYG{n}{nodes}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}perturbed}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{X} \PYG{o}{+}\PYG{o}{=} \PYG{p}{(}\PYG{n}{delta} \PYG{o}{*} \PYG{o}{.}\PYG{l+m+mi}{15}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{X}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{n}{X} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{newaxis}\PYG{p}{]}
    \PYG{n}{A} \PYG{o}{=} \PYG{n}{rdpg}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{A}
    

\PYG{n}{time\PYGZus{}points} \PYG{o}{=} \PYG{l+m+mi}{12}
\PYG{n}{nodes} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{nodes}\PYG{p}{)}
\PYG{n}{networks} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

\PYG{k}{for} \PYG{n}{time} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{time\PYGZus{}points} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{A} \PYG{o}{=} \PYG{n}{gen\PYGZus{}timepoint}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{n}{networks}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{perturbed\PYGZus{}time} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{A} \PYG{o}{=} \PYG{n}{gen\PYGZus{}timepoint}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{perturbed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{networks}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{perturbed\PYGZus{}time}\PYG{p}{,} \PYG{n}{A}\PYG{p}{)}
    
\PYG{n}{networks} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see the adjacency matrices we generated below. Note that you can’t really distinguish a difference between the ten normal time points and the two perturbed time points with the naked eye, even though the difference is there, so it would be pretty difficult to manually mark the time points \sphinxhyphen{} and if you have many time points, rather than just a few, you’d want to be able to automate the process.

\noindent\sphinxincludegraphics{{anomaly-detection_6_0}.png}


\subsection{How Do We Figure Out Which Time Points Are Anomalies?}
\label{\detokenize{applications/ch10/anomaly-detection:how-do-we-figure-out-which-time-points-are-anomalies}}
\sphinxAtStartPar
It’s time to start thinking about how we’d approach figuring out which of the time points are anomalies.

\sphinxAtStartPar
One of the simplest approaches to this problem might just be to figure out which node has the highest count of edge changes across your timeseries. For each node across the timeseries, you’d count the number of new edges that appeared (compared to the previous point in time), and the number of existing edges that were deleted. Whichever count is highest could be your anomalous node.

\sphinxAtStartPar
This might give you a rough estimate – and you could even potentially find perturbed time points with this approach – but it’s not necessarily the best solution. Counting edges doesn’t account for other important pieces of information: for instance, you might be interested in which other nodes new edges were formed with. It seems like deleting or creating edges with more important nodes, for instance, should be weighted higher than deleting or creating edges with unimportant nodes.

\sphinxAtStartPar
So let’s try another method. You might actually be able to guess it! The idea will be to simply estimate each network’s latent positions, followed by a hypothesis testing approach. Here’s the idea.

\sphinxAtStartPar
Let’s call the latent positions for our network \(X^{(t)}\) for the snapshot of the network at time \(t\). You’re trying to find specific time points, \(X^{(i)}\), which are different from their previous time point \(X^{(i-1)}\) by a large margin. You can define “different” as “difference in matrix norm”. Remember that the matrix norm is just a number that generalizes the concept of vector magnitude to matrices. In other words, We’re trying to find a time point where the difference in norm between the latent positions at time \(t\) and the latent positions at time \(t-1\) is greater than some constant:  \(||X^{(t)} - X^{(t-1)}|| > c\). The idea is that non\sphinxhyphen{}anomalous time points will probably be a bit different, but that the difference will be within some reasonable range of variability.

\sphinxAtStartPar
There’s an alternate problem where you restrict your view to \sphinxstyleemphasis{nodes} rather than entire adjacency matrices. The idea is that you’d find time\sphinxhyphen{}points which are anomalous for particular nodes or groups of nodes, rather than the entire network. The general idea is the same: you find latent positions, then test for how big the difference is between time point \(t\) and time point \(t-1\). This time, however, your test is for particular nodes. You want to figure out if \(||X_i^{(t)} - X_i^{(t-1)}|| > c\), where you’re looking at a particular latent position \(X_i\) rather than all of them at once. We’ll be focusing on the problem for whole networks, but you can take a look at the original paper if you’re curious about how to apply it to nodes {[}cite{]}


\subsection{Detecting if the First Time Point is an Anomaly}
\label{\detokenize{applications/ch10/anomaly-detection:detecting-if-the-first-time-point-is-an-anomaly}}
\sphinxAtStartPar
We’ll start with the first time point, which (because we generated the data!) we know in advance is not an anomaly.

\sphinxAtStartPar
If we were to just estimate the latent positions for each timepoint separately with ASE or LSE, we’d run into the nonidentifiability problem that we’ve seen a few times over the course of this book: The latent positions would be rotated versions of each other, and we’d have to use something like Procrustes (which adds variance, since it’s just an estimate) to rotate them back into the same space.

\sphinxAtStartPar
However, since we have multiple time points, each of which is associated to an adjacency matrix, it’s natural to use models from the Multiple\sphinxhyphen{}Network Representation Learning section (You can go back and read chapter 6.7 if you’re fuzzy on the details here). In that section, we introduced the Omnibus Embedding as a way to estimate latent positions for multiple \sphinxstyleemphasis{networks} simultaneously, but all we really need for it is multiple \sphinxstyleemphasis{adjacency matrices}. These exist in our network in the form of its multiple time points; So, we’ll just embed multiple time points at once with the Omnibus Embedding, and then they’ll live in the same space.

\sphinxAtStartPar
We only really \sphinxstyleemphasis{need} to embed two time points at a time, since all we really care about is being able to directly compare a time point \(X^{(t)}\) and the point prior to it \(X^{(t-1)} = Y\) \sphinxhyphen{} but because of the way Omni works, we’ll get smaller\sphinxhyphen{}variance estimates if we embed all the time points at once. Embedding them all at once also to be more robust to embedding dimension in practice. If you wanted to save computational power \sphinxhyphen{} for instance, if you had a lot of time points \sphinxhyphen{} you could instead choose to embed subsets of them, or just the two you’ll actually be using.

\sphinxAtStartPar
So, here’s what’s going on in the code below:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
We embed the time points using OMNI and then get our estimates for the first two sets of latent positions \(\hat{X} = \hat{X}^{(t)}\) and \(\hat{Y} = \hat{X}^{(t-1)}\).

\item {} 
\sphinxAtStartPar
Then, we get the norm of their difference \(||\hat{X} - \hat{Y}||\) with numpy.

\end{enumerate}

\sphinxAtStartPar
An important point to clarify is that there are a lot of different types of matrix norms: Frobenius norm, spectral norm, and so on. In our case, we’ll be using the \(l_2\) operator norm, which is simply the largest singular value of the matrix. The \sphinxcode{\sphinxupquote{ord}} parameter argument in numpy determines which norm we use, and \sphinxcode{\sphinxupquote{ord=2}} is the operator norm.

\sphinxAtStartPar
Again, this norm, intuitively, will tell us how different two matrices are. If the norm of \(X - Y\) is small, then \(X\) and \(Y\) are very similar matrices; whereas if the norm of \(X - Y\) is large, then \(X\) and \(Y\) are very different. The norm should be large for anomalies, and small for everything else.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graspologic}\PYG{n+nn}{.}\PYG{n+nn}{embed} \PYG{k+kn}{import} \PYG{n}{OmnibusEmbed} \PYG{k}{as} \PYG{n}{OMNI}

\PYG{k}{def} \PYG{n+nf}{get\PYGZus{}statistic}\PYG{p}{(}\PYG{n}{adjacencies}\PYG{p}{,} \PYG{n}{return\PYGZus{}latents}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Get the operator norm of the difference of two matrices.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{omni} \PYG{o}{=} \PYG{n}{OMNI}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{latents\PYGZus{}est} \PYG{o}{=} \PYG{n}{omni}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{adjacencies}\PYG{p}{)}
    \PYG{n}{Xhat}\PYG{p}{,} \PYG{n}{Yhat} \PYG{o}{=} \PYG{n}{latents\PYGZus{}est}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{latents\PYGZus{}est}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{Xhat} \PYG{o}{\PYGZhy{}} \PYG{n}{Yhat}\PYG{p}{,} \PYG{n+nb}{ord}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{return\PYGZus{}latents}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{y}\PYG{p}{,} \PYG{n}{Xhat}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{y}

\PYG{n}{y}\PYG{p}{,} \PYG{n}{Xhat} \PYG{o}{=} \PYG{n}{get\PYGZus{}statistic}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{,} \PYG{n}{return\PYGZus{}latents}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{y}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.570
\end{sphinxVerbatim}


\subsection{Hypothesis Testing With our Test Statistic}
\label{\detokenize{applications/ch10/anomaly-detection:hypothesis-testing-with-our-test-statistic}}
\sphinxAtStartPar
We have our norm \(y\), which will be our test statistic. It should be a small value if the first two adjacency matrices in the timeseries are distributed the same, and large if they’re distributed differently. Remember that we’re fundamentally trying to figure out whether \(X = X^{(t)}\), our latent positions at time \(t\), is the same as \(Y = X^{(t-1)}\), our latent positions at time \(t-1\). This is also known as a \sphinxstyleemphasis{hypothesis test}: we’re testing the the null hypothesis that \(X = Y\) against the alternative hypothesis that \(X \neq Y\).

\sphinxAtStartPar
The value of our test statistic is \DUrole{pasted-inline}{\sphinxcode{\sphinxupquote{'0.570'}}}. The problem is that we don’t know how big this is, relatively. Is \DUrole{pasted-inline}{\sphinxcode{\sphinxupquote{'0.570'}}} relatively large? small? how should we determine whether it’s small enough to say that X and Y probably come from the same distribution, and aren’t anomaly time points?

\sphinxAtStartPar
Well, what if we could use our estimated latent positions \(\hat{X}\) at time \(t\) to generate a bunch of networks, then make test statistics from those new networks? We’d know for a fact that any pair of those networks are drawn from the same set of latent positions, and we could get a sense for what our test statistic should look like if the latent positions actually were the same. This technique is called \sphinxstyleemphasis{bootstrapping}, since you’re using estimated parameters to “pull yourself up by your own bootstraps” and generate a bunch of artificial data. Bootstrapping pops up all over the place in machine learning and statistics contexts.


\subsubsection{Using Bootstrapping to Figure out the Distribution of the Test Statistic}
\label{\detokenize{applications/ch10/anomaly-detection:using-bootstrapping-to-figure-out-the-distribution-of-the-test-statistic}}
\sphinxAtStartPar
We don’t have the true latent positions for a given time point, but we do have the estimated latent positions (we just used OMNI embedding to find them!)

\sphinxAtStartPar
So what we can do is the following:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Using a set of the latent positions we just estimated, generate two new adjacency matrices.

\item {} 
\sphinxAtStartPar
Get the test statistic for these two adjacency matrices.

\item {} 
\sphinxAtStartPar
Repeat 1) and 2) a bunch of times, getting new test statistics each time

\item {} 
\sphinxAtStartPar
Look at the distribution of these test statistics, and determine whether \{glue:\}y is an outlier or not with respect to this distribution.

\end{enumerate}

\sphinxAtStartPar
So we’re artificially generating data that we \sphinxstyleemphasis{know for a fact} is distributed in exactly the same way, and then looking at how our test statistic is distributed under those assumptions. This artificial data will necessarily be a bit biased, since the latent positions you’re using to generate it are themselves only estimates, but it should be close enough to the real thing to be useful.

\sphinxAtStartPar
Below is some code. We generate 1000 pairs of adjacency matrices from our estimated latent positions for the first time point \(\hat{X}^{(t)}\), and get the test statistic for each pair. Underneath this looping code, you can see the distribution of these bootstrapped test statistics in the form of a histogram. They look roughly normally distributed, and hover around 0.60. The red line shows where our actual test statistic lies, where we compare \(\hat{X}^{(t)}\) to \(\hat{X}^{(t-1)}\).

\sphinxAtStartPar
If the red line is super far away from the bulk of the mass in the test statistic distribution, then it would be fairly unlikely to be drawn from the same set of latent positions as the bootstrapped test statistics, and we’d reject the hypothesis that it is. If it’s well within the range of values we’d reasonably expect, then we wouldn’t reject this possibility.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} null hypothesis that X = Y. Bootstrap X.}
\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{bootstraps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{est} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{A\PYGZus{}est} \PYG{o}{=} \PYG{n}{rdpg}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{)}
    \PYG{n}{B\PYGZus{}est} \PYG{o}{=} \PYG{n}{rdpg}\PYG{p}{(}\PYG{n}{Xhat}\PYG{p}{)}
    \PYG{n}{bootstrapped\PYGZus{}y} \PYG{o}{=} \PYG{n}{get\PYGZus{}statistic}\PYG{p}{(}\PYG{p}{[}\PYG{n}{A\PYGZus{}est}\PYG{p}{,} \PYG{n}{B\PYGZus{}est}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{bootstraps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{bootstrapped\PYGZus{}y}\PYG{p}{)}
\PYG{n}{bootstraps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{bootstraps}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 20, \PYGZsq{}Bootstrapped Distribution\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{anomaly-detection_19_1}.png}

\sphinxAtStartPar
Fortunately, \DUrole{pasted-inline}{\sphinxcode{\sphinxupquote{'0.570'}}} is well within a reasonable range under the assumption that the time\sphinxhyphen{}points share latent positions. However, we can’t always eyeball stuff, and we need a way to formalize what it means for a test statistic to be “within a reasonable range”. Our test statistic is  \(y = ||X^{(t)} - X^{(t-1)}||\), we’re trying to figure out if \(X^{(t)} = X^{(t-1)}\), and we have a bunch of bootstrapped test statistics that we know were drawn from the same distribution (and are thus examples of the case where the null hypothesis is true).


\subsection{Using our test statistic to find p\sphinxhyphen{}values}
\label{\detokenize{applications/ch10/anomaly-detection:using-our-test-statistic-to-find-p-values}}
\sphinxAtStartPar
Since we have a range of examples of \(y\) values in which the null hypothesis is true, we have an estimate for the distribution of the null hypothesis. So, to find the probability that any new value drawn from this bootstrapped distribution is greater than a particular value \(c\), we can just find the proportion of our bootstrapped values that are greater than \(c\).
\begin{align*}
    p &= \frac{\textrm{number of bootstrapped values greater than $c$}}{\textrm{total number of bootstrapped values}}
\end{align*}
\sphinxAtStartPar
When we let \(c\) be equal to our test statistic, \(y\), we find the probability that any new bootstrapped value will be greater than \(y\) (assuming that \(y\) is drawn from the null distribution). Here we have our formalization.

\sphinxAtStartPar
Below is some simple numpy code that performs this estimation. We just count the number of bootstrapped statistics that are greater than our \(y\) value, and then divide by the number of bootstrapped test statistics. If the resulting \(p\)\sphinxhyphen{}value is less than some pre\sphinxhyphen{}determined probability (say, for instance, \(0.05\)), then we reject the null hypothesis and say that \(y\) probably comes from a different distribution than the bootstrapped statistics. This, in turn, implies that \(X^{(t)} \neq X^{(t-1)}\), and we’ve found an anomaly time point.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p} \PYG{o}{=} \PYG{p}{(}\PYG{n}{bootstraps} \PYG{o}{\PYGZgt{}} \PYG{n}{y}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{n}{N}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.7
\end{sphinxVerbatim}

\sphinxAtStartPar
Our \(p\) value is much larger than 0.05, so we don’t reject the null hypothesis, and we can conclude that we haven’t found an anomaly time. Since this is all synthetic data, we know how the data generation process worked, so we actually know for a fact that this is the right result – the adjacency matrix at time \(t\) actually \sphinxstyleemphasis{was} drawn from the same distribution as the adjacency matrix at time \(t-1\).


\subsection{Testing the Rest of the Time Points For Anomalies}
\label{\detokenize{applications/ch10/anomaly-detection:testing-the-rest-of-the-time-points-for-anomalies}}
\sphinxAtStartPar
Now that we’ve gone through this for one time point, we can do it for the rest. The process is exactly the same, except that you’re comparing different pairs of timepoints and you’re generating the bootstrapped test statistics with different estimated latent positions.

\sphinxAtStartPar
Below we get our test statistic for every pair of time points. Our two anomaly time points are drawn from the same distribution, by design, so we shouldn’t catch an anomaly when we test them against each other; however, we should catch anomalies when we test them against other, non\sphinxhyphen{}anomaly time points, and that’s exactly what we see.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ys\PYGZus{}true} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{adjacency} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{get\PYGZus{}statistic}\PYG{p}{(}\PYG{p}{[}\PYG{n}{adjacency}\PYG{p}{,} \PYG{n}{networks}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ys\PYGZus{}true}\PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb}{float}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{y}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{graphbook\PYGZus{}code} \PYG{k+kn}{import} \PYG{n}{cmaps}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{fromiter}\PYG{p}{(}\PYG{n}{ys\PYGZus{}true}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{float}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plot} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{]}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmaps}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sequential}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} 
                   \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{plot}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Test Statistics for Each Timeseries}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{plot}\PYG{o}{.}\PYG{n}{axes}\PYG{o}{.}\PYG{n}{yaxis}\PYG{o}{.}\PYG{n}{set\PYGZus{}ticklabels}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{plot}\PYG{o}{.}\PYG{n}{axes}\PYG{o}{.}\PYG{n}{xaxis}\PYG{o}{.}\PYG{n}{set\PYGZus{}ticklabels}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{ys\PYGZus{}true}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{plot}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Timeseries Pairs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{anomaly-detection_28_0}.png}

\sphinxAtStartPar
If we were to plot a distribution of bootstrapped test statistics with each of our estimated y\sphinxhyphen{}values, it would look like the histogram below. Notice that two test statistics are clearly anomalous: the one comparing times five and six, and the one comparing times seven and eight. We know by design that networks six and seven actually are anomolous, and so we can see that our test managed to correctly determine the anomaly times.

\noindent\sphinxincludegraphics{{anomaly-detection_30_0}.png}


\subsection{The Distribution of the Bootstrapped Test Statistic}
\label{\detokenize{applications/ch10/anomaly-detection:the-distribution-of-the-bootstrapped-test-statistic}}
\sphinxAtStartPar
One issue that could pop up is that the bootstrapped test statistic is slightly biased. Since we’re generating it from an estimate \(\hat{X}\) of the true latent positions \(X\), we’ll have a bias of \(|\hat{X} - X|\). It’s worth comparing the two distributions to determine if that bias is a big deal in practice.

\sphinxAtStartPar
Below you can see the true distribution of the test statistic for the real, unperturbed set of latent positions \(X\) we generated the data from (that’s the blue distribution). You can also see a distribution of test statistics bootstrapped from a \(\hat{X}\). You can see that in this case, they’re fairly close.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x16f70db80\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{anomaly-detection_33_1}.png}


\subsection{}
\label{\detokenize{applications/ch10/anomaly-detection:id1}}

\subsection{References}
\label{\detokenize{applications/ch10/anomaly-detection:references}}\begin{itemize}
\item {} 
\sphinxAtStartPar
j1’s paper – heritability

\item {} 
\sphinxAtStartPar
vivek’s paper – mcc

\end{itemize}


\subsection{Notes}
\label{\detokenize{applications/ch10/anomaly-detection:notes}}
\sphinxAtStartPar
guodong’s stuff: uses MASE and OMNI combined with DCORR to do hypothesis testing
\begin{itemize}
\item {} 
\sphinxAtStartPar
vivek did something similar for MCC

\end{itemize}


\section{Testing for Significant Edges}
\label{\detokenize{applications/ch10/significant-edges:testing-for-significant-edges}}\label{\detokenize{applications/ch10/significant-edges::doc}}

\section{Testing for Significant Vertices}
\label{\detokenize{applications/ch10/significant-vertices:testing-for-significant-vertices}}\label{\detokenize{applications/ch10/significant-vertices::doc}}

\section{Testing for Significant Communities}
\label{\detokenize{applications/ch10/significant-communities:testing-for-significant-communities}}\label{\detokenize{applications/ch10/significant-communities::doc}}

\part{Reference}







\renewcommand{\indexname}{Index}
\printindex
\end{document}