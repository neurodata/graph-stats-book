
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6.4. Estimating Parameters for the RDPG &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >let toggleHintShow = 'Click to show';</script>
    <script >let toggleHintHide = 'Click to hide';</script>
    <script >let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script >const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6.5. Random-Walk and Diffusion-based Methods" href="random-walk-diffusion-methods.html" />
    <link rel="prev" title="6.3. Spectral Embedding Methods" href="spectral-embedding.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../coverpage.html">
   Hands-on Network Machine Learning with Scikit-Learn and Graspologic
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology and Math Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.4. Approaches for Network Learning Problems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.4. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/ch4.html">
   4. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/matrix-representations.html">
     4.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/network-representations.html">
     4.2. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/properties-of-networks.html">
     4.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/regularization.html">
     4.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch5/ch5.html">
   5. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/why-use-models.html">
     5.1. Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_ER.html">
     5.2. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_SBM.html">
     5.3. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_RDPG.html">
     5.4. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/multi-network-models.html">
     5.5. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/models-with-covariates.html">
     5.6. Network Models with Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch6.html">
   6. Learning Network Representations
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="estimating-parameters_mle.html">
     6.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="why-embed-networks.html">
     6.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="spectral-embedding.html">
     6.3. Spectral Embedding Methods
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.4. Estimating Parameters for the RDPG
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="random-walk-diffusion-methods.html">
     6.5. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="graph-neural-networks.html">
     6.6. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multigraph-representation-learning.html">
     6.7. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="joint-representation-learning.html">
     6.8. Joint Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="estimating-parameters_theory.html">
     6.9. Model Estimation Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch7/ch7.html">
   7. Theoretical Results
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-single-network.html">
     7.1. Theory for Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-multigraph.html">
     7.2. Maximum Likelihood Estimate Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-matching.html">
     7.3. Spectral Method Theory
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   8. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/community-detection.html">
     8.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/testing-differences.html">
     8.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/model-selection.html">
     8.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/single-vertex-nomination.html">
     8.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/out-of-sample.html">
     8.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   9. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/two-sample-hypothesis.html">
     9.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-communities.html">
     9.2. Differences in Block Matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/graph-matching-vertex.html">
     9.3. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/multiple-vertex-nomination.html">
     9.4. Vertex Nomination For Multiple Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch10/ch10.html">
   10. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/anomaly-detection.html">
     10.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-edges.html">
     10.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-vertices.html">
     10.3. Testing for Significant Vertices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/representations/ch6/estimating-parameters_spectral.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurodata/graph-stats-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Frepresentations/ch6/estimating-parameters_spectral.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/representations/ch6/estimating-parameters_spectral.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/network_machine_learning_in_python/representations/ch6/estimating-parameters_spectral.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-are-nodes-from-the-same-community-similar">
   6.4.1. How are nodes from the same community similar?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-low-rank-property-and-the-probability-matrix">
   6.4.2. The low-rank property and the probability matrix
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-best-representation-of-the-probability-matrix">
     6.4.2.1. The best representation of the probability matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-square-root-matrix">
     6.4.2.2. The Square Root Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-latent-positions-are-distinct-for-each-community">
     6.4.2.3. The latent positions are distinct for each community
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-we-do">
   6.4.3. But wait: we don’t have the probability matrix! What do we do?
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Estimating Parameters for the RDPG</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-are-nodes-from-the-same-community-similar">
   6.4.1. How are nodes from the same community similar?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-low-rank-property-and-the-probability-matrix">
   6.4.2. The low-rank property and the probability matrix
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-best-representation-of-the-probability-matrix">
     6.4.2.1. The best representation of the probability matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-square-root-matrix">
     6.4.2.2. The Square Root Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-latent-positions-are-distinct-for-each-community">
     6.4.2.3. The latent positions are distinct for each community
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-we-do">
   6.4.3. But wait: we don’t have the probability matrix! What do we do?
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="estimating-parameters-for-the-rdpg">
<h1><span class="section-number">6.4. </span>Estimating Parameters for the RDPG<a class="headerlink" href="#estimating-parameters-for-the-rdpg" title="Permalink to this headline">¶</a></h1>
<p>In this section, we’ll cover a special case of the spectral embedding, which is when we think that the network might be well described by an underlying RDPG. We’ll double back on some concepts, such as the singular value decomposition and matrix rank, so that we can explain the importance of these concepts as they relate directly to the probability matrix of an RDPG.</p>
<p>For this example, we’ll work with the school example we’ve seen previously. The nodes are 100 students in total, from one of two schools. Here, the first 50 students are from the first school, and the second 50 students are from the second school. The probability of two students who both go to the first school being friends is <span class="math notranslate nohighlight">\(0.5\)</span>, and the probability of two students who both go to school two being friends will also be <span class="math notranslate nohighlight">\(0.5\)</span>. If two students go to different schools, their probability of being friends will be <span class="math notranslate nohighlight">\(0.2\)</span>. The statistical model has parameters which look like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">graspologic</span> <span class="k">as</span> <span class="nn">gp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">ns1</span> <span class="o">=</span> <span class="mi">50</span><span class="p">;</span> <span class="n">ns2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="n">ns1</span><span class="p">,</span> <span class="n">ns2</span><span class="p">]</span>

<span class="c1"># zvec is a column vector of 50 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 100 students are from</span>
<span class="n">zvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;S1&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;S2&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns2</span><span class="p">)])</span>

<span class="c1"># the block matrix</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>

<span class="c1"># the probability matrix</span>
<span class="n">zvec_ohe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns2</span><span class="p">)])</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">zvec_ohe</span> <span class="o">@</span> <span class="n">B</span> <span class="o">@</span> <span class="n">zvec_ohe</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix $B$&quot;</span><span class="p">,</span> 
                <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;S1&quot;</span><span class="p">,</span> <span class="s2">&quot;S2&quot;</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;S1&quot;</span><span class="p">,</span> <span class="s2">&quot;S2&quot;</span><span class="p">],</span>
                <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">])</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix $P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_3_0.png" src="../../_images/estimating-parameters_spectral_3_0.png" />
</div>
</div>
<p>The procedure for generating the probability matrix from the block probability matrix that we used above is a linear algebra “cheat”, but in reality, all that’s happening is that we are comparing whether two nodes are in the first or the second school, and then taking the appropriate entry from the block matrix accordingly. The operation is exactly the same as we had in the section on RDPG. For two nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, the probability they are connected is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p_{ji} = p_{ij}, p_{ij} = \begin{cases}
        b_{11} &amp; z_i = 1, z_j = 1 \\
        b_{12} &amp; z_i = 1, z_j = 2 \\
        b_{22} &amp; z_i = 2, z_j = 2
    \end{cases}
\end{align*}\]</div>
<p>Next, we will use this probability matrix and the corresponding community assignment vector to generate a realization of the stochastic block model we saw above. This is our “real network”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">simulations</span><span class="o">.</span><span class="n">sbm</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see the adjacency matrix and the probability matrix below. Notice that there are two distrinct blocks in the adjacency matrix, which are shared with the probability matrix: in its upper-left, you can see the edges between the first 50 nodes (the individuals in the first school), and in the bottom right, you can see the edges between the second 50 nodes (the individuals in the second school).</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix $P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Realization of SBM Random Network, $A$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_7_0.png" src="../../_images/estimating-parameters_spectral_7_0.png" />
</div>
</div>
<p>As we can see, both the probability matrix <span class="math notranslate nohighlight">\(P\)</span> and the realization of the random network <span class="math notranslate nohighlight">\(A\)</span> both have a notable “structure”, in that it’s clear that there are more nodes when both individuals are in the first school (the top left and bottom right “squares” of the adjacency matrix and probability matrix have more entries and are darker, respectively) and fewer nodes when both individuals are in different schools (the top right and bottom left “squares” of the adjacency matrix have fewer entries and are lighter, respectively).</p>
<p>Next, we have the key conceptual leap we need to take: remember that if a network is an SBM, there is also an underlying RDPG that <em>also</em> describes that network. When we learned about this in <a class="reference external" href="#link?">Chapter 5</a>, we learned that this was because the RDPG is a more broad statistical model that was more general than the SBM. So, to learn about the SBM which underlies <span class="math notranslate nohighlight">\(A\)</span>, we could also learn about an RDPG which underlies <span class="math notranslate nohighlight">\(A\)</span>, too. This will prove critical in later applications sections, such as <a class="reference external" href="#link?">Chapter 8</a>, when we try to decipher which nodes are in which communities of the network, without having them handed to us in this nice organized way.</p>
<div class="section" id="how-are-nodes-from-the-same-community-similar">
<h2><span class="section-number">6.4.1. </span>How are nodes from the same community similar?<a class="headerlink" href="#how-are-nodes-from-the-same-community-similar" title="Permalink to this headline">¶</a></h2>
<p>As it turns out, there’s a really important property that is shared by nodes in an SBM random network. Remember that the probability matrix <span class="math notranslate nohighlight">\(P\)</span> gives the probabilities <span class="math notranslate nohighlight">\(p_{ji}\)</span> of each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of being connected. We’re going to introduce a new piece of notation here, called the <em>vector of probabilities</em> for a single node. The <strong>vector of probabilities</strong> for a node <span class="math notranslate nohighlight">\(i\)</span> is the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec p_i &amp;= \begin{bmatrix}
        p_{i1} \\
        \vdots \\
        p_{in}
    \end{bmatrix}
\end{align*}\]</div>
<p>In words, it is basically just the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of the probability matrix <span class="math notranslate nohighlight">\(P\)</span>. Now, what happens when we look at the probability vectors for nodes which are in the same, versus different communities? Here, what we will do is take the probability vectors for students <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span>, who both attend school one, and compare them to the probability vectors for students <span class="math notranslate nohighlight">\(51\)</span> and <span class="math notranslate nohighlight">\(52\)</span>, who both attend school two:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># grab the probability vectors for the students we outlined above</span>
<span class="n">Psubset</span> <span class="o">=</span> <span class="n">P</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">],:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psubset</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Student 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 51&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 52&quot;</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Probability that student $i$ is friends with student $j$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Student $i$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Student $j$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_11_0.png" src="../../_images/estimating-parameters_spectral_11_0.png" />
</div>
</div>
<p>The probability vectors for students <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\(51\)</span>, and <span class="math notranslate nohighlight">\(52\)</span> are shown as the rows of the above heatmap. What do we notice about the probability vectors for students <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span> in comparison to students <span class="math notranslate nohighlight">\(51\)</span> and <span class="math notranslate nohighlight">\(52\)</span>? As it turns out, they are exactly the same, and the probability vectors are identical! In general, what this means is that, for an SBM, <em>all</em> of the nodes in a single community have the <em>exact same</em> probability vector! If you are interested in some more details as to why this is the case for an SBM, the reason is that the probability vector for a given node is <em>fully specified</em> by just knowing the block matrix and the community assignment vector for the nodes in the network. What this means is that there is nothing special about one node versus another node in the same community, in the probability sense.</p>
</div>
<div class="section" id="the-low-rank-property-and-the-probability-matrix">
<h2><span class="section-number">6.4.2. </span>The low-rank property and the probability matrix<a class="headerlink" href="#the-low-rank-property-and-the-probability-matrix" title="Permalink to this headline">¶</a></h2>
<p>Now, what does this mean for <em>us</em>? What this means is that the probability matrix has a special property, called the <em>low-rank</em> property. We already studied matrix rank, which if we recall, described how many unique row or column vectors we would need to define <em>all</em> of the other row and column vectors of a matrix.</p>
<p>If an SBM is has <span class="math notranslate nohighlight">\(K\)</span> communities, what does this mean about its probability matrix? Well, its probability matrix is <em>also</em> exactly rank-<span class="math notranslate nohighlight">\(K\)</span>! From what we learned about the Laplacian Spectral Embedding, we might reasonably expect that a very similar procedure might produce a similarly interesting result for us, so let’s get started.</p>
<p>We’ll take the svd of the probability matrix of <em>the random network itself</em> this time (instead of the Laplacian of a realization of the random network). As we remember, the svd created three matrices, <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span>, for us, with the property that <span class="math notranslate nohighlight">\(U\)</span> had <span class="math notranslate nohighlight">\(n\)</span> columns called the left singular vectors, <span class="math notranslate nohighlight">\(\Sigma\)</span> was a diagonal matrix whose entries were the singular values in non-increasing order (each singular value can be at most the previous), and <span class="math notranslate nohighlight">\(V\)</span> also had <span class="math notranslate nohighlight">\(n\)</span> columns called the right singular vectors.</p>
<p>Remember that when we take an svd, we start with looking at the scree plot, which was a plot of the singular values ordered by their index:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># take the singular value decomposition</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
<span class="c1"># U is the matrix whose columns are the left singular vectors</span>
<span class="c1"># s is the vector whose entries are the singular values</span>
<span class="c1"># Vt is the matrix whose rows are the right singular vectors</span>
<span class="c1"># and whose tranpose has columns which are the right singular vectors</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="k">def</span> <span class="nf">plot_scree</span><span class="p">(</span><span class="n">svs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">sv_dat</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">:</span> <span class="n">svs</span><span class="p">,</span> <span class="s2">&quot;Dimension&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)})</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">sv_dat</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Dimension&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dimension&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">plot_scree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $P$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_14_0.png" src="../../_images/estimating-parameters_spectral_14_0.png" />
</div>
</div>
<p>What we see in the scree plot is that it just so happens that for the probability matrix for a <span class="math notranslate nohighlight">\(K\)</span>-community SBM, the probability matrix had <span class="math notranslate nohighlight">\(K\)</span> non-zero singular values! This fact will be important to us later on, so we will highlight this later.</p>
<p>Next, we see that <span class="math notranslate nohighlight">\(P = U\Sigma V^\top\)</span>, and that the expression we learned previously, is still true: that <span class="math notranslate nohighlight">\(P = \sum_{i = 1}^n \vec u_i \vec v_i^\top\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># matrix product form of expression for P</span>
<span class="n">Psvd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">@</span> <span class="n">Vt</span>

<span class="c1"># sum expression for P</span>
<span class="c1"># remember that U[:,i] is the ith column of U (the ith left singular vector)</span>
<span class="c1"># and Vt[i,:] is the ith column of V (the ith right singular vector)</span>
<span class="c1"># and the ith row of Vt (transpose)</span>
<span class="n">Psum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">U</span><span class="p">[:,[</span><span class="n">i</span><span class="p">]])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Vt</span><span class="p">[[</span><span class="n">i</span><span class="p">],:])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span> <span class="o">+</span> <span class="n">ns2</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
 
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psvd</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U</span><span class="se">\\</span><span class="s2">Sigma V^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psum</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sum_{i = 1}^n \sigma_i u_i v_i^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_17_0.png" src="../../_images/estimating-parameters_spectral_17_0.png" />
</div>
</div>
<div class="section" id="the-best-representation-of-the-probability-matrix">
<h3><span class="section-number">6.4.2.1. </span>The best representation of the probability matrix<a class="headerlink" href="#the-best-representation-of-the-probability-matrix" title="Permalink to this headline">¶</a></h3>
<p>As it turns out, the relationships and intuition we learned about the Laplacian Spectral Embedding work here, too. Since the probability matrix <span class="math notranslate nohighlight">\(P\)</span> is square, symmetric, and has all positive real entries, it is <em>also</em> going to have <span class="math notranslate nohighlight">\(K\)</span> positive, real singular values, and the rest will all be zero. Further, the first <span class="math notranslate nohighlight">\(K\)</span> left and right singular vectors will all be the same! We’ll borrow the same notation we used with the section on the Laplacian Spectral Embedding, remembering that this means that we have two matrices, <span class="math notranslate nohighlight">\(U_K\)</span> and <span class="math notranslate nohighlight">\(\Sigma_K\)</span>, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    U_K &amp;= \begin{bmatrix}
        \uparrow &amp; &amp; \uparrow \\
        \vec u_1 &amp; ... &amp; \vec u_K \\
        \downarrow &amp; &amp; \downarrow
    \end{bmatrix},\;\;\;\Sigma_K &amp;= \begin{bmatrix}
        \sigma_1 &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \sigma_2 &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sigma_K
    \end{bmatrix}
\end{align*}\]</div>
<p>and we learned that <span class="math notranslate nohighlight">\(P = U_K\Sigma_K U_K^\top\)</span>, or stated another way:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P &amp;= \sum_{i = 1}^K \sigma_i \vec u_i \vec u_i^\top
\end{align*}\]</div>
<p>We can see this using the probability matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">UK</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">];</span> <span class="n">SK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">])</span>

<span class="c1"># Psymm is UK * SigmaK * UK^transpose</span>
<span class="n">Psymm</span> <span class="o">=</span> <span class="n">UK</span> <span class="o">@</span> <span class="n">SK</span> <span class="o">@</span> <span class="n">UK</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Psymm</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psymm</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U_K </span><span class="se">\\</span><span class="s2">Sigma_K U_K^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Psymm</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - U_K </span><span class="se">\\</span><span class="s2">Sigma_K U_K^</span><span class="se">\\</span><span class="s2">top$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_20_0.png" src="../../_images/estimating-parameters_spectral_20_0.png" />
</div>
</div>
</div>
<div class="section" id="the-square-root-matrix">
<h3><span class="section-number">6.4.2.2. </span>The Square Root Matrix<a class="headerlink" href="#the-square-root-matrix" title="Permalink to this headline">¶</a></h3>
<p>It’s been quite a ride; don’t fall off just yet! Now we are really in the home stretch! We’ve saved the easiest part for last. Remember that <span class="math notranslate nohighlight">\(\Sigma_K\)</span> is just a diagonal matrix, whose entries are the singular values <span class="math notranslate nohighlight">\(\sigma_i\)</span> for the first <span class="math notranslate nohighlight">\(K\)</span> singular values. As it turns out, since <span class="math notranslate nohighlight">\(P\)</span> is positive, these singular values are going to be positive too, which means that we can break each singular value into its square root, as <span class="math notranslate nohighlight">\(\sigma_i = \sqrt{\sigma_i}\sqrt{\sigma_i}\)</span>.</p>
<p>Like for the Laplacian Spectral Embedding, this meant we could factor the first <span class="math notranslate nohighlight">\(K\)</span> singular value matrix <span class="math notranslate nohighlight">\(\Sigma_K\)</span> into the product of its square root matrix and its transpose, as <span class="math notranslate nohighlight">\(\Sigma_K = \sqrt{\Sigma_K}\sqrt{\Sigma_K}\)</span>.</p>
<p>This means that our probability matrix is just <span class="math notranslate nohighlight">\(P = U_K \sqrt{\Sigma_K}\sqrt{\Sigma_K}^\top U_K^\top\)</span>, which is very similar to what we got for the Laplacian spectral embedding. The difference here is that <span class="math notranslate nohighlight">\(P\)</span> <em>itself</em> is equal to this quantity, not just a “reduced rank” representation of <span class="math notranslate nohighlight">\(P\)</span> like we had for the Laplacian spectral embedding.</p>
<p>If we let <span class="math notranslate nohighlight">\(X = U_K \sqrt{\Sigma_K}\)</span>, then <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. This means that <span class="math notranslate nohighlight">\(X\)</span> is a latent position matrix for <span class="math notranslate nohighlight">\(P\)</span>! If you remember back to the <a class="reference external" href="#link?">Section on RDPGs</a>, this means that we have found the latent position parameter for the corresponding RDPG for our SBM random network!</p>
<p>Let’s see this using numpy again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SKsqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">SK</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">UK</span> <span class="o">@</span> <span class="n">SKsqrt</span>
<span class="n">Prdpg</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Prdpg</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Prdpg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$XX^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">]);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Prdpg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - XX^</span><span class="se">\\</span><span class="s2">top$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_23_0.png" src="../../_images/estimating-parameters_spectral_23_0.png" />
</div>
</div>
<p>This matrix <span class="math notranslate nohighlight">\(X\)</span>, the latent position matrix, for an SBM will look like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_lpm</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticklabs</span><span class="o">=</span><span class="p">[],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="n">cbar</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xticklabs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">yticklabs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt{</span><span class="se">\\</span><span class="s2">sigma_1}</span><span class="se">\\</span><span class="s2">vec u_1$&quot;</span><span class="p">,</span> <span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sqrt{</span><span class="se">\\</span><span class="s2">sigma_2}</span><span class="se">\\</span><span class="s2">vec u_2$&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Heatmap of Latent Position Matrix $X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_25_0.png" src="../../_images/estimating-parameters_spectral_25_0.png" />
</div>
</div>
<p>and is the “low-rank structure” which describes the probability matrix.</p>
</div>
<div class="section" id="the-latent-positions-are-distinct-for-each-community">
<h3><span class="section-number">6.4.2.3. </span>The latent positions are distinct for each community<a class="headerlink" href="#the-latent-positions-are-distinct-for-each-community" title="Permalink to this headline">¶</a></h3>
<p>Now, you will notice a very interesting property about the latent position matrix for an SBM. Remember that for an RDPG (and an SBM is also an RDPG), the latent position vectors for the node <span class="math notranslate nohighlight">\(i\)</span> are the <em>rows</em> <span class="math notranslate nohighlight">\(\vec x_i\)</span> of the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. This means that the latent position vector for node <span class="math notranslate nohighlight">\(1\)</span> is the vector <span class="math notranslate nohighlight">\((\sqrt{\sigma_1}u_{11}, \sqrt{\sigma_2}u_{21})\)</span>. What do we notice about the latent positions vector for node <span class="math notranslate nohighlight">\(2\)</span>? It’s <em>exactly</em> the same!</p>
<p>More generally, the latent position vector for <em>all</em> nodes which are in the same community will be identical. This is why if we look across the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> above, there are only two unique latent position vectors: there are one unique vector (which is black in the first dimension and red in the second dimension) for the nodes in the first community, and a second unique vector (which is black in the first dimension and beige in the second dimension) for the nodes of the second community.</p>
<div class="admonition-putting-it-all-together admonition">
<p class="admonition-title">Putting it all together</p>
<p>What have we learned so far? What we’ve learned so far is that, if we have a probability matrix that is symmetric and rank-<span class="math notranslate nohighlight">\(K\)</span>:</p>
<ol class="simple">
<li><p>We can decompose this probability matrix using the singular value decomposition.</p></li>
<li><p>We can ignore singular values/vectors other than the first <span class="math notranslate nohighlight">\(K\)</span> of them.</p></li>
<li><p>For the first <span class="math notranslate nohighlight">\(K\)</span> singular vectors, the left and right vectors are identical.</p></li>
<li><p>We can decompose the singular value matrix into the product of the square root matrix with its transpose.</p></li>
<li><p>We can express the matrix <span class="math notranslate nohighlight">\(P\)</span> using the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, which is the product of the first <span class="math notranslate nohighlight">\(K\)</span> singular vectors with the first <span class="math notranslate nohighlight">\(K\)</span> singular values.
This means that we have found a latent position matrix <span class="math notranslate nohighlight">\(X\)</span> for the probability matrix using the singular values and singular vectors of <span class="math notranslate nohighlight">\(P\)</span>, by effectively just discarding the ones that don’t matter (and have singular values of <span class="math notranslate nohighlight">\(0\)</span>). We have succeeded in our goal of finding a much lower rank structure, the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, to describe the probability matrix <span class="math notranslate nohighlight">\(P\)</span>.</p></li>
</ol>
</div>
<p>If you remember from the section on RDPGs, this probability matrix has the property that each entry <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i \vec x_j^\top\)</span>.</p>
</div>
</div>
<div class="section" id="but-wait-we-don-t-have-the-probability-matrix-what-do-we-do">
<h2><span class="section-number">6.4.3. </span>But wait: we don’t have the probability matrix! What do we do?<a class="headerlink" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-we-do" title="Permalink to this headline">¶</a></h2>
<p>All of the logic we developed above was with respect to the probability matrix, <span class="math notranslate nohighlight">\(P\)</span>, for a SBM. More generally, this logic extends to the probability matrix <span class="math notranslate nohighlight">\(P\)</span> for any RDPG, which is because an RDPG with <span class="math notranslate nohighlight">\(d\)</span> latent dimensions will <em>always</em> have a probability matrix that is <em>exactly</em> rank <span class="math notranslate nohighlight">\(d\)</span>. If we took the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> which had <span class="math notranslate nohighlight">\(d\)</span> latent dimensions, and then used the svd to find the <span class="math notranslate nohighlight">\(U_d\)</span> and <span class="math notranslate nohighlight">\(\Sigma_d\)</span> where <span class="math notranslate nohighlight">\(P = U_d \Sigma_d U_d^\top\)</span>, we could find another latent position matrix <span class="math notranslate nohighlight">\(Y = U_d\sqrt{\Sigma_d}\)</span> where <span class="math notranslate nohighlight">\(P = YY^\top\)</span>.</p>
<p>But, we have a slight issue: when we perform machine learning, we don’t know the probability matrix! The probability matrix is a <em>parameter</em> of the statistical model itself, it is <em>not</em> a function of the sample of data we get. All we have is the adjacency matrix itself, <span class="math notranslate nohighlight">\(A\)</span>, which is our data! We don’t actually know what the underlying probability matrix is! How the heck can we find this low rank structure we want to be able to estimate?</p>
<p>As it turns out, if we kept obtaining more and more networks <span class="math notranslate nohighlight">\(A\)</span> from the underlying RDPG random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>, we would <em>expect</em> that the network <span class="math notranslate nohighlight">\(A\)</span> we saw would be the probability matrix <span class="math notranslate nohighlight">\(P\)</span>. We’ll explain what we mean by expect here by turning back to our coin flip example. As you remember, we perform a coin flip at each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of an RDPG, where the coin lands on heads with probability <span class="math notranslate nohighlight">\(\vec x_i\vec x_j^\top\)</span>, and lands on tails with probability <span class="math notranslate nohighlight">\(1 - \vec x_i\vec x_j^\top\)</span>. This means we can <em>expect</em> the coin to land on heads with probability <span class="math notranslate nohighlight">\(\vec x_i\vec x_j^\top\)</span>. In the same sense, we can expect the <span class="math notranslate nohighlight">\((i,j)\)</span> entry of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> to exist with probability <span class="math notranslate nohighlight">\(\vec x_i \vec x_j^\top\)</span>. In this sense, the expected value of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> <em>is</em> the probability matrix <span class="math notranslate nohighlight">\(P = XX^\top\)</span>.</p>
<p>So, since the expected value of the adjacency matrix <em>is</em> the probability matrix, what if we were to just embed the adjacency matrix instead? Let’s see how this might work. Again, we’ll use the singular value decomposition on <span class="math notranslate nohighlight">\(A\)</span>, and take a look at the scree plot for <span class="math notranslate nohighlight">\(A\)</span>, and compare it to the scree plot of <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UA</span><span class="p">,</span> <span class="n">sA</span><span class="p">,</span> <span class="n">VAt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plot_scree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $P$&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_scree</span><span class="p">(</span><span class="n">sA</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $A$&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_28_0.png" src="../../_images/estimating-parameters_spectral_28_0.png" />
</div>
</div>
<p>Now that’s really funky! The singular values of both <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(A\)</span> tend to fall off around <em>roughly</em> the same spot, right around dimension <span class="math notranslate nohighlight">\(2\)</span>! The singular value of <span class="math notranslate nohighlight">\(P\)</span> go directly to <span class="math notranslate nohighlight">\(0\)</span>, but the singular values for <span class="math notranslate nohighlight">\(A\)</span> tend to “round off” in the direction of <span class="math notranslate nohighlight">\(0\)</span>, but it isn’t <em>too</em> far off!</p>
<p>As it turns out, this is no coincidence: the singular values for a network which can be described by an RDPG will tend to “elbow” off right around the number of true latent dimensions for the probability matrix of the underlying random network. If the RDPG has <span class="math notranslate nohighlight">\(d\)</span> latent dimensions, this will occur right around <span class="math notranslate nohighlight">\(d\)</span>. For this reason, it is usually a good idea when we think a network might be well described by an RDPG to let the elbow selection algorithm do the work for us, and then take a good look at the scree plot to make sure the number of latent dimensions chosen seems reasonable to us. If we don’t know how many latent dimensions to retain, we’ll call the number of embedding dimensions <span class="math notranslate nohighlight">\(\hat d\)</span>, which just means, “estimate of the number of latent dimensions”.</p>
<p>What does it look like when we use the spectral embedding on <span class="math notranslate nohighlight">\(A\)</span>? We’ll compare the embedding of the adjacency matrix to the latent positions of the probability matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UAK</span> <span class="o">=</span> <span class="n">UA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">];</span> <span class="n">USK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sA</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">]))</span>
<span class="n">Aembedded</span> <span class="o">=</span> <span class="n">UAK</span> <span class="o">@</span> <span class="n">USK</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Heatmap of Latent Position Matrix $X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">Aembedded</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedded Adjacency Matrix&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_31_0.png" src="../../_images/estimating-parameters_spectral_31_0.png" />
</div>
</div>
<p>Wow! When we take the adjacency matrix and embed it into <span class="math notranslate nohighlight">\(2\)</span> dimensions, it doesn’t look <em>identical</em> to the latent position matrix, but it shares some major patterns with it! In particular, it looks like the second latent dimension for the embedded adjacency matrix tends to capture that the second latent dimension of <span class="math notranslate nohighlight">\(X\)</span> has higher values for the first <span class="math notranslate nohighlight">\(50\)</span> nodes, and lower values for the second <span class="math notranslate nohighlight">\(50\)</span> nodes. For a variety of reasons, we will call this “embedding of <span class="math notranslate nohighlight">\(A\)</span>” an <em>estimate</em> of the latent position matrix for the underlying RDPG, which we will denote by <span class="math notranslate nohighlight">\(\hat X\)</span>.</p>
<p>As we learned in the last section, this entire procedure is automated for us by <code class="docutils literal notranslate"><span class="pre">graspologic</span></code> with the <code class="docutils literal notranslate"><span class="pre">AdjacencySpectralEmbed()</span></code> class, or alternatively, the <code class="docutils literal notranslate"><span class="pre">RDPGEstimator()</span></code>. The <code class="docutils literal notranslate"><span class="pre">RDPGEstimator()</span></code> class just makes clear that we are estimating parameters for an RDPG:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rdpgest</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">RDPGEstimator</span><span class="p">()</span>
<span class="n">rdpgest</span> <span class="o">=</span> <span class="n">rdpgest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Xhat</span> <span class="o">=</span> <span class="n">rdpgest</span><span class="o">.</span><span class="n">latent_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedded Adjacency Matrix, $\hat X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_34_0.png" src="../../_images/estimating-parameters_spectral_34_0.png" />
</div>
</div>
<p>As it turns out, this example here is a good instance of another property of the latent position matrix. Remember that the latent position matrix is the matrix <span class="math notranslate nohighlight">\(X\)</span> where <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. As it turns out, sometimes the columns of this matrix can get flipped around a little bit, through something called a rotation. You can see this by noticing that it looks a lot like the entries of the estimates of the latent position matrix are positive in one case are negative for the other, and it basically looks like the colorbar just got flipped around on us. The rotation doesn’t really matter (yet!), and <span class="math notranslate nohighlight">\(P\)</span> still is equal to <span class="math notranslate nohighlight">\(XX^\top\)</span>, regardless of how that <span class="math notranslate nohighlight">\(X\)</span> is rotated. We’ll learn more about rotation matrices in the upcoming section on <a class="reference external" href="#link?">Multiple Network Representation Learning</a> and in the section on <a class="reference external" href="#link?">Two Sample Hypothesis Testing</a>, but for now, all you need to know is that this “flippage” of what’s big and small in the latent position matrix is not too important (again, yet!).</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./representations/ch6"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="spectral-embedding.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6.3. </span>Spectral Embedding Methods</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="random-walk-diffusion-methods.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.5. </span>Random-Walk and Diffusion-based Methods</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Joshua Vogelstein, Eric Bridgeford, and Alex Loftus<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>