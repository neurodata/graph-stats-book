
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6.4. Estimating Parameters for the RDPG &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >let toggleHintShow = 'Click to show';</script>
    <script >let toggleHintHide = 'Click to hide';</script>
    <script >let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script >const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6.5. Random-Walk and Diffusion-based Methods" href="random-walk-diffusion-methods.html" />
    <link rel="prev" title="6.3. Spectral Embedding Methods" href="spectral-embedding.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../coverpage.html">
   Hands-on Network Machine Learning with Scikit-Learn and Graspologic
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology and Math Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.4. Approaches for Network Learning Problems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.4. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/ch4.html">
   4. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/matrix-representations.html">
     4.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/network-representations.html">
     4.2. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/properties-of-networks.html">
     4.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/regularization.html">
     4.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch5/ch5.html">
   5. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/why-use-models.html">
     5.1. Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_ER.html">
     5.2. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_SBM.html">
     5.3. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_RDPG.html">
     5.4. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/multi-network-models.html">
     5.5. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/models-with-covariates.html">
     5.6. Network Models with Covariates
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_theory.html">
     5.7. Single network model theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch6.html">
   6. Learning Network Representations
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="estimating-parameters_mle.html">
     6.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="why-embed-networks.html">
     6.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="spectral-embedding.html">
     6.3. Spectral Embedding Methods
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.4. Estimating Parameters for the RDPG
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="random-walk-diffusion-methods.html">
     6.5. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="graph-neural-networks.html">
     6.6. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multigraph-representation-learning.html">
     6.7. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="joint-representation-learning.html">
     6.8. Joint Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="estimating-parameters_theory.html">
     6.9. Model Estimation Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch7/ch7.html">
   7. Theoretical Results
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-single-network.html">
     7.1. Theory for Single Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-multigraph.html">
     7.2. Theory for Multiple-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-matching.html">
     7.3. Theory for Graph Matching
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   8. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/community-detection.html">
     8.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/testing-differences.html">
     8.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/model-selection.html">
     8.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/single-vertex-nomination.html">
     8.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/out-of-sample.html">
     8.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   9. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/two-sample-hypothesis.html">
     9.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-communities.html">
     9.2. Differences in Block Matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/graph-matching-vertex.html">
     9.3. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/multiple-vertex-nomination.html">
     9.4. Vertex Nomination For Multiple Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch10/ch10.html">
   10. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/anomaly-detection.html">
     10.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-edges.html">
     10.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-vertices.html">
     10.3. Testing for Significant Vertices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/representations/ch6/estimating-parameters_spectral.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurodata/graph-stats-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Frepresentations/ch6/estimating-parameters_spectral.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/representations/ch6/estimating-parameters_spectral.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/network_machine_learning_in_python/representations/ch6/estimating-parameters_spectral.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-are-nodes-from-the-same-community-similar">
   6.4.1. How are nodes from the same community similar?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#low-rank-structure-what-it-is-how-we-find-it-and-why-it-matters">
   6.4.2. Low-rank structure: what it is, how we find it, and why it matters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-rank">
     6.4.2.1. Matrix Rank
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-find-this-low-rank-structure">
     6.4.2.2. How do we find this low-rank structure?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-singular-value-decomposition">
       6.4.2.2.1. The Singular Value Decomposition
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-relationship-between-the-singular-value-decomposition-and-best-representations-of-a-matrix">
       6.4.2.2.2. The relationship between the singular value decomposition and “best” representations of a matrix
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#equivalence-of-top-left-and-right-singular-vectors">
       6.4.2.2.3. Equivalence of top left and right singular vectors
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-square-root-matrix">
       6.4.2.2.4. The Square Root Matrix
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-we-do">
   6.4.3. But wait: we don’t have the probability matrix! What do we do?
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Estimating Parameters for the RDPG</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-are-nodes-from-the-same-community-similar">
   6.4.1. How are nodes from the same community similar?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#low-rank-structure-what-it-is-how-we-find-it-and-why-it-matters">
   6.4.2. Low-rank structure: what it is, how we find it, and why it matters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-rank">
     6.4.2.1. Matrix Rank
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-find-this-low-rank-structure">
     6.4.2.2. How do we find this low-rank structure?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-singular-value-decomposition">
       6.4.2.2.1. The Singular Value Decomposition
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-relationship-between-the-singular-value-decomposition-and-best-representations-of-a-matrix">
       6.4.2.2.2. The relationship between the singular value decomposition and “best” representations of a matrix
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#equivalence-of-top-left-and-right-singular-vectors">
       6.4.2.2.3. Equivalence of top left and right singular vectors
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-square-root-matrix">
       6.4.2.2.4. The Square Root Matrix
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-we-do">
   6.4.3. But wait: we don’t have the probability matrix! What do we do?
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="estimating-parameters-for-the-rdpg">
<h1><span class="section-number">6.4. </span>Estimating Parameters for the RDPG<a class="headerlink" href="#estimating-parameters-for-the-rdpg" title="Permalink to this headline">¶</a></h1>
<p>For this example, we’ll work with the school example we’ve seen previously. The nodes are 100 students in total, from one of two schools. Here, the first 50 students are from the first school, and the second 50 students are from the second school. The probability of two students who both go to the first school being friends is <span class="math notranslate nohighlight">\(0.5\)</span>, and the probability of two students who both go to school two being friends will also be <span class="math notranslate nohighlight">\(0.5\)</span>. If two students go to different schools, their probability of being friends will be <span class="math notranslate nohighlight">\(0.2\)</span>. The statistical model has parameters which look like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">graspologic</span> <span class="k">as</span> <span class="nn">gp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">ns1</span> <span class="o">=</span> <span class="mi">50</span><span class="p">;</span> <span class="n">ns2</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="n">ns1</span><span class="p">,</span> <span class="n">ns2</span><span class="p">]</span>

<span class="c1"># zvec is a column vector of 50 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 100 students are from</span>
<span class="n">zvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;S1&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;S2&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns2</span><span class="p">)])</span>

<span class="c1"># the block matrix</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>

<span class="c1"># the probability matrix</span>
<span class="n">zvec_ohe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns2</span><span class="p">)])</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">zvec_ohe</span> <span class="o">@</span> <span class="n">B</span> <span class="o">@</span> <span class="n">zvec_ohe</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix $B$&quot;</span><span class="p">,</span> 
                <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;S1&quot;</span><span class="p">,</span> <span class="s2">&quot;S2&quot;</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;S1&quot;</span><span class="p">,</span> <span class="s2">&quot;S2&quot;</span><span class="p">],</span>
                <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix $P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_3_0.png" src="../../_images/estimating-parameters_spectral_3_0.png" />
</div>
</div>
<p>The procedure for generating the probability matrix from the block probability matrix that we used above is a linear algebra “cheat”, but in reality, all that’s happening is that we are comparing whether two nodes are in the first or the second school, and then taking the appropriate entry from the block matrix accordingly. The operation is exactly the same as we had in the section on RDPG. For two nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, the probability they are connected is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p_{ji} = p_{ij}, p_{ij} = \begin{cases}
        b_{11} &amp; z_i = 1, z_j = 1 \\
        b_{12} &amp; z_i = 1, z_j = 2 \\
        b_{22} &amp; z_i = 2, z_j = 2
    \end{cases}
\end{align*}\]</div>
<p>Next, we will use this probability matrix and the corresponding community assignment vector to generate a realization of the stochastic block model we saw above. This is our “real network”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">simulations</span><span class="o">.</span><span class="n">sbm</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can see the adjacency matrix and the probability matrix below. Notice that there are two distrinct blocks in the adjacency matrix, which are shared with the probability matrix: in its upper-left, you can see the edges between the first 50 nodes (the individuals in the first school), and in the bottom right, you can see the edges between the second 50 nodes (the individuals in the second school).</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix $P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Realization of SBM Random Network, $A$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_7_0.png" src="../../_images/estimating-parameters_spectral_7_0.png" />
</div>
</div>
<p>As we can see, both the probability matrix <span class="math notranslate nohighlight">\(P\)</span> and the realization of the random network <span class="math notranslate nohighlight">\(A\)</span> both have a notable “structure”, in that it’s clear that there are more nodes when both individuals are in the first school (the top left and bottom right “squares” of the adjacency matrix and probability matrix have more entries and are darker, respectively) and fewer nodes when both individuals are in different schools (the top right and bottom left “squares” of the adjacency matrix have fewer entries and are lighter, respectively).</p>
<div class="section" id="how-are-nodes-from-the-same-community-similar">
<h2><span class="section-number">6.4.1. </span>How are nodes from the same community similar?<a class="headerlink" href="#how-are-nodes-from-the-same-community-similar" title="Permalink to this headline">¶</a></h2>
<p>As it turns out, there’s a really important property that is shared by nodes in an SBM random network. Remember that the probability matrix <span class="math notranslate nohighlight">\(P\)</span> gives the probabilities <span class="math notranslate nohighlight">\(p_{ji}\)</span> of each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of being connected. We’re going to introduce a new piece of notation here, called the <em>vector of probabilities</em> for a single node. The <strong>vector of probabilities</strong> for a node <span class="math notranslate nohighlight">\(i\)</span> is the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec p_i &amp;= \begin{bmatrix}
        p_{i1} \\
        \vdots \\
        p_{in}
    \end{bmatrix}
\end{align*}\]</div>
<p>In words, it is basically just the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of the probability matrix <span class="math notranslate nohighlight">\(P\)</span>. Now, what happens when we look at the probability vectors for nodes which are in the same, versus different communities? Here, what we will do is take the probability vectors for students <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span>, who both attend school one, and compare them to the probability vectors for students <span class="math notranslate nohighlight">\(51\)</span> and <span class="math notranslate nohighlight">\(52\)</span>, who both attend school two:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># grab the probability vectors for the students we outlined above</span>
<span class="n">Psubset</span> <span class="o">=</span> <span class="n">P</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">],:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psubset</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu_r&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Student 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 51&quot;</span><span class="p">,</span> <span class="s2">&quot;Student 52&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Probability that student $i$ is friends with student $j$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Student $i$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Student $j$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_11_0.png" src="../../_images/estimating-parameters_spectral_11_0.png" />
</div>
</div>
<p>The probability vectors for students <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\(51\)</span>, and <span class="math notranslate nohighlight">\(52\)</span> are shown as the rows of the above heatmap. What do we notice about the probability vectors for students <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span> in comparison to students <span class="math notranslate nohighlight">\(51\)</span> and <span class="math notranslate nohighlight">\(52\)</span>? As it turns out, they are exactly the same, and the probability vectors are identical! In general, what this means is that, for an SBM, <em>all</em> of the nodes in a single community have the <em>exact same</em> probability vector! If you are interested in some more details as to why this is the case for an SBM, the reason is that the probability vector for a given node is <em>fully specified</em> by just knowing the block matrix and the community assignment vector for the nodes in the network. What this means is that there is nothing special about one node versus another node in the same community, in the probability sense.</p>
<p>Now, what does this mean for <em>us</em>? What this means is that the probability matrix has a special property, called the <em>low-rank</em> property. Now, we’ll learn about matrix rank, and some important consequences of this fact.</p>
</div>
<div class="section" id="low-rank-structure-what-it-is-how-we-find-it-and-why-it-matters">
<h2><span class="section-number">6.4.2. </span>Low-rank structure: what it is, how we find it, and why it matters<a class="headerlink" href="#low-rank-structure-what-it-is-how-we-find-it-and-why-it-matters" title="Permalink to this headline">¶</a></h2>
<div class="section" id="matrix-rank">
<h3><span class="section-number">6.4.2.1. </span>Matrix Rank<a class="headerlink" href="#matrix-rank" title="Permalink to this headline">¶</a></h3>
<p>In the technical sense, the <em>rank</em> of a matrix is a description of just how complicated the matrix is. A matrix is <strong>low rank</strong> if any of its rows (or equivalently, its columns) are linear combinations of other rows (or columns). What this means is that if we can take rows or columns from the matrix, and add or multiply them together to get other rows or columns from the matrix, the matrix is low rank. This means that information contained in the matrix is redundant, in that we could obtain it by just adding or subtracting other information already contained in the matrix!</p>
<p>In our case, what this means is that the probability matrix for an SBM with <span class="math notranslate nohighlight">\(K\)</span> total communities will be of rank-<span class="math notranslate nohighlight">\(K\)</span>: there is a single unique probability vector for each community. This means that we could express all of the rows of the probability matrix by taking one probability vector from each community, and then “instructions” as to how we combine these probability vectors to obtain the probability vectors for other nodes in the community. In the case of the simple SBM, these “instructions” are just to duplicate the same vector over and over again, for every node in the community. Nothing too complicated, right?</p>
</div>
<div class="section" id="how-do-we-find-this-low-rank-structure">
<h3><span class="section-number">6.4.2.2. </span>How do we find this low-rank structure?<a class="headerlink" href="#how-do-we-find-this-low-rank-structure" title="Permalink to this headline">¶</a></h3>
<p>When we have data which we think is low-rank, one way we can identify this pattern is by using something called a <em>spectral embedding</em>. This embedding is called <em>spectral</em> because it will have to do with the eigenvalues/eigenvectors of the probability matrix (often called a matrix’s <em>spectrum</em>). It is called an <em>embedding</em> which, informally, means that there is a high level structure that we get to see (the probability matrix) which can be more succinctly described by a lower-level structure that we don’t immediately get to see (the two unique probability vectors). What we want to do here is identify this low-level structure given the probability matrix and <em>nothing else</em>, which means we might not get to see the communities each node is part of when we perform this analysis. The spectral embedding will let us do this.</p>
<div class="section" id="the-singular-value-decomposition">
<h4><span class="section-number">6.4.2.2.1. </span>The Singular Value Decomposition<a class="headerlink" href="#the-singular-value-decomposition" title="Permalink to this headline">¶</a></h4>
<p>The spectral embedding algorithm begins by finding the <em>singular-value decomposition</em> of the data.</p>
<div class="admonition-singular-value-decomposition admonition">
<p class="admonition-title">Singular-Value Decomposition</p>
<p>The singular value decomposition of a matrix <span class="math notranslate nohighlight">\(P\)</span> is the set of matrices <span class="math notranslate nohighlight">\(U, \Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span>, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P &amp;= U\Sigma V^\top
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> have some special properties:</p>
<ol class="simple">
<li><p>The left singular vectors: the columns <span class="math notranslate nohighlight">\(\vec u_i\)</span> of <span class="math notranslate nohighlight">\(U\)</span> are called the left singular vectors of <span class="math notranslate nohighlight">\(P\)</span>. If <span class="math notranslate nohighlight">\(P\)</span> has <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns, there will be <span class="math notranslate nohighlight">\(n\)</span> left singular vectors. This matrix looks like this:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    U &amp;= \begin{bmatrix}
        \vec u_1 &amp; ... &amp; \vec u_n
    \end{bmatrix}
\end{align*}\]</div>
<ol class="simple">
<li><p>The singular values: the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> is a diagonal matrix, which means that it has entries along the diagonal and all the other entries are just <em>zero</em>. There will be <span class="math notranslate nohighlight">\(n\)</span> total of these. This matrix looks something like this:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Sigma &amp;= \begin{bmatrix}
        \sigma_1 &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \sigma_2 &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sigma_n
    \end{bmatrix}
\end{align*}\]</div>
<p>The special property is that by definition, the singular values are <em>descending</em>, in that <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2\)</span>, <span class="math notranslate nohighlight">\(\sigma_2 \geq \sigma_3\)</span>, so on and so forth to <span class="math notranslate nohighlight">\(\sigma_n\)</span>.</p>
<ol class="simple">
<li><p>The right singular vectors: the columns <span class="math notranslate nohighlight">\(\vec v_i\)</span> of <span class="math notranslate nohighlight">\(V\)</span> are called the right singular vectors of <span class="math notranslate nohighlight">\(P\)</span>. If <span class="math notranslate nohighlight">\(P\)</span> has <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns, there will be <span class="math notranslate nohighlight">\(n\)</span> right singular vectors. This matrix looks like this:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
V &amp;= \begin{bmatrix}
    \vec v_1 &amp; ... &amp; \vec v_n
\end{bmatrix}
\end{align*}\]</div>
</div>
<p>As it turns out, because <span class="math notranslate nohighlight">\(\Sigma\)</span> is a diagonal matrix, the multiplication <span class="math notranslate nohighlight">\(P = U\Sigma V^\top\)</span>, which to the author anyways is somewhat opaque, can actually be written a lot more easily. We can write this equation down like this, which is much more understandable:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P &amp;= \sum_{i = 1}^n \sigma_i \vec u_i \vec v_i^\top = \sum_{i = 1}^n \sigma_i\begin{bmatrix}
        \uparrow \\ \vec u_i \\ \downarrow
    \end{bmatrix}\begin{bmatrix}
        \leftarrow &amp; \vec v_i^\top &amp; \rightarrow
    \end{bmatrix}
\end{align*}\]</div>
<p>In a sense, what this means is that <span class="math notranslate nohighlight">\(P\)</span> is a sum of the products of <span class="math notranslate nohighlight">\(\vec u_i\)</span> with <span class="math notranslate nohighlight">\(\vec v_i^\top\)</span>, and then “scaled” or “multiplied” by <span class="math notranslate nohighlight">\(\sigma_i\)</span>.</p>
<p>To show this computationally, we’re going to decompose <span class="math notranslate nohighlight">\(P\)</span> into its singular value decomposition using <code class="docutils literal notranslate"><span class="pre">numpy</span></code>. We’ll first show something called a scree plot, which is a plot of the singular values of the matrix <span class="math notranslate nohighlight">\(P\)</span>. This plot is typically shown by having the magnitude of each singular value on the <span class="math notranslate nohighlight">\(y\)</span>-axis, and the <span class="math notranslate nohighlight">\(x\)</span>-axis is just the index of the singular value. Remember, the singular values are <em>descending</em>, which means that this plot is always going to show the singular values <em>decreasing</em> as the index increases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># take the singular value decomposition</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
<span class="c1"># U is the matrix whose columns are the left singular vectors</span>
<span class="c1"># s is the vector whose entries are the singular values</span>
<span class="c1"># Vt is the matrix whose rows are the right singular vectors</span>
<span class="c1"># and whose tranpose has columns which are the right singular vectors</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="k">def</span> <span class="nf">plot_scree</span><span class="p">(</span><span class="n">svs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">sv_dat</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">:</span> <span class="n">svs</span><span class="p">,</span> <span class="s2">&quot;Dimension&quot;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)})</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">sv_dat</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Dimension&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dimension&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Singular Value&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">plot_scree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $P$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_14_0.png" src="../../_images/estimating-parameters_spectral_14_0.png" />
</div>
</div>
<p>Next, we’ll show <span class="math notranslate nohighlight">\(P\)</span> itself, the matrix product of <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, and <span class="math notranslate nohighlight">\(V^\top\)</span>, followed by the sum expression to show you we aren’t just making up that all of these expressions are exactly equal!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># matrix product form of expression for P</span>
<span class="n">Psvd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">@</span> <span class="n">Vt</span>

<span class="c1"># sum expression for P</span>
<span class="c1"># remember that U[:,i] is the ith column of U (the ith left singular vector)</span>
<span class="c1"># and Vt[i,:] is the ith column of V (the ith right singular vector)</span>
<span class="c1"># and the ith row of Vt (transpose)</span>
<span class="n">Psum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">U</span><span class="p">[:,[</span><span class="n">i</span><span class="p">]])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Vt</span><span class="p">[[</span><span class="n">i</span><span class="p">],:])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns1</span> <span class="o">+</span> <span class="n">ns2</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
 
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psvd</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U</span><span class="se">\\</span><span class="s2">Sigma V^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Psum</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">sum_{i = 1}^n \sigma_i u_i v_i^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_17_0.png" src="../../_images/estimating-parameters_spectral_17_0.png" />
</div>
</div>
</div>
<div class="section" id="the-relationship-between-the-singular-value-decomposition-and-best-representations-of-a-matrix">
<h4><span class="section-number">6.4.2.2.2. </span>The relationship between the singular value decomposition and “best” representations of a matrix<a class="headerlink" href="#the-relationship-between-the-singular-value-decomposition-and-best-representations-of-a-matrix" title="Permalink to this headline">¶</a></h4>
<p>What if we only looked at the first <span class="math notranslate nohighlight">\(d\)</span> entries of the singular value decomposition? When we say “look at”, what we mean is that we will retain the first <span class="math notranslate nohighlight">\(d\)</span> left and right singular values/vectors, and the first (the <em>biggest</em>) <span class="math notranslate nohighlight">\(d\)</span> singular values. We will call this new matrix <span class="math notranslate nohighlight">\(P_d\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P_d &amp;= \sum_{i = 1}^d \sigma_i \vec u_i \vec v_i^\top
\end{align*}\]</div>
<p>Again, we can write this up in full matrix form, as <span class="math notranslate nohighlight">\(P_d = U_d\Sigma_d V_d^\top\)</span>, where <span class="math notranslate nohighlight">\(U_d\)</span> is now just the first <span class="math notranslate nohighlight">\(d\)</span> columns of <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma_d\)</span> is just the diagonal matrix from the first <span class="math notranslate nohighlight">\(d\)</span> singular values, and <span class="math notranslate nohighlight">\(V_d\)</span> is the first <span class="math notranslate nohighlight">\(d\)</span> columns of <span class="math notranslate nohighlight">\(V\)</span>. Another special property of the singular value decomposition that we will need is that, as it turns out, the <span class="math notranslate nohighlight">\(U_d\)</span>, <span class="math notranslate nohighlight">\(\Sigma_d\)</span>, and <span class="math notranslate nohighlight">\(V_d\)</span> that we constructed in this way are the <em>best</em> possible choices we could have made to represent <span class="math notranslate nohighlight">\(P\)</span> using <span class="math notranslate nohighlight">\(d\)</span> singular values and vectors. By best, we mean that if we were to any other scalar multiples <span class="math notranslate nohighlight">\(\sigma_i\)</span>, and singular values/vectors <span class="math notranslate nohighlight">\(\vec u_i\)</span> and <span class="math notranslate nohighlight">\(\vec v_i\)</span>, the matrix we would make using the sum shown above is not going to be as similar to <span class="math notranslate nohighlight">\(P\)</span> as <span class="math notranslate nohighlight">\(P_d\)</span> is (more specifically, the Frobenius difference between <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(P_d\)</span> will be <em>minimal</em>). This is just a definition of the singular value decomposition, and is not anything super technical beyond that, so you shouldn’t worry too much about this fact and just take our word on it for now.</p>
<p>What does <span class="math notranslate nohighlight">\(\sigma_i\)</span> mean for us? Well, if we were to look at <span class="math notranslate nohighlight">\(P_1\)</span>, <span class="math notranslate nohighlight">\(P_1\)</span> would just be <span class="math notranslate nohighlight">\(\sigma_1 \vec u_1 \vec v_1^\top\)</span>, so it’s pretty clear that <span class="math notranslate nohighlight">\(\vec u_1\)</span> and <span class="math notranslate nohighlight">\(\vec v_1\)</span> are the best singular vectors to describe <span class="math notranslate nohighlight">\(P\)</span>. <span class="math notranslate nohighlight">\(\sigma_1\)</span> basically just tells us how much we should multiply them to describe <span class="math notranslate nohighlight">\(P\)</span> just right. Let’s take a look at <span class="math notranslate nohighlight">\(P_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P1</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">U</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">]])</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Vt</span><span class="p">[[</span><span class="mi">0</span><span class="p">],:])</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">P1</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P_1$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">P1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - P_1$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_20_0.png" src="../../_images/estimating-parameters_spectral_20_0.png" />
</div>
</div>
<p>How about <span class="math notranslate nohighlight">\(P_2\)</span>? Well we knew that <span class="math notranslate nohighlight">\(P_1\)</span> was the best representation with <span class="math notranslate nohighlight">\(1\)</span> singular values and vectors, and now we just add in <span class="math notranslate nohighlight">\(\sigma_2 \vec u_2 \vec v_2^\top\)</span> to get <span class="math notranslate nohighlight">\(P_2\)</span>. That is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P_2 &amp;= \sum_{i = 1}^2 \sigma_i \vec u_i \vec v_i^\top + \sigma_2 \vec u_2 \vec v_2^\top = P_1 + \sigma_2 \vec u_2 \vec v_2^\top
\end{align*}\]</div>
<p>So now <span class="math notranslate nohighlight">\(\sigma_2\)</span> is the multiplicative factor telling us how much to multiply <span class="math notranslate nohighlight">\(\vec u_2\)</span> and <span class="math notranslate nohighlight">\(\vec v_2\)</span> by. If we repeat this over and over, we will see that the singular values <span class="math notranslate nohighlight">\(\sigma_i\)</span> will just describe how much we need to multiply <span class="math notranslate nohighlight">\(\vec u_i\)</span> and <span class="math notranslate nohighlight">\(\vec v_i\)</span> by to offset the preceding “best” matrix with <span class="math notranslate nohighlight">\(i-1\)</span> singular values and vectors to get the best representation with <span class="math notranslate nohighlight">\(i\)</span> singular values and vectors. If <span class="math notranslate nohighlight">\(\sigma_i\)</span> were big, then <span class="math notranslate nohighlight">\(\vec u_i\)</span> and <span class="math notranslate nohighlight">\(\vec v_i^\top\)</span>’s product will be a big component of <span class="math notranslate nohighlight">\(P_i\)</span>. If <span class="math notranslate nohighlight">\(\sigma_i\)</span> were small, then <span class="math notranslate nohighlight">\(\vec u_i\)</span> and <span class="math notranslate nohighlight">\(\vec v_i^\top\)</span>’s product will not be a very big compouent of the sum we have here. So the entry <span class="math notranslate nohighlight">\(\sigma_i\)</span>, in effect, tells us <em>just how important</em> the <span class="math notranslate nohighlight">\(i^{th}\)</span> singular vectors are to <span class="math notranslate nohighlight">\(P\)</span>. This fact will be crucial later on.</p>
</div>
<div class="section" id="equivalence-of-top-left-and-right-singular-vectors">
<h4><span class="section-number">6.4.2.2.3. </span>Equivalence of top left and right singular vectors<a class="headerlink" href="#equivalence-of-top-left-and-right-singular-vectors" title="Permalink to this headline">¶</a></h4>
<p>You might be thinking this expression looks complicated, which we agree, so far it does. But there are some nice facts that will make this expression way simpler for us. As a rule of thumb, if a matrix is rank <span class="math notranslate nohighlight">\(d\)</span>, then <em>all</em> but the first <span class="math notranslate nohighlight">\(d\)</span> singular values are just going to be zero. Since our probability matrix was rank-<span class="math notranslate nohighlight">\(K\)</span>, this means that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P &amp;= \sum_{i = 1}^d \sigma_i \vec u_i \vec v_i^\top \\
    &amp;= \sum_{i = 1}^K \sigma_i \vec u_i \vec v_i^\top + \sum_{i = K+1}^{n}\sigma_i \vec u_i \vec v_i^\top \\
    &amp;= \sum_{i = 1}^K \sigma_i \vec u_i \vec v_i^\top \\
    &amp;^= P_K
\end{align*}\]</div>
<p>in the second line, we just split the sum from <span class="math notranslate nohighlight">\(n\)</span> terms apart into the first <span class="math notranslate nohighlight">\(K\)</span> terms and the second <span class="math notranslate nohighlight">\(n - K\)</span> terms as separate expressions. Then, we used the fact that the last <span class="math notranslate nohighlight">\(n-K\)</span> singular values are all just <span class="math notranslate nohighlight">\(0\)</span>, so the <span class="math notranslate nohighlight">\(\vec u_i\)</span>s and <span class="math notranslate nohighlight">\(\vec v_i\)</span>s don’t matter at <em>all</em>. So we can describe <span class="math notranslate nohighlight">\(P\)</span> fully using <em>only</em> the first <span class="math notranslate nohighlight">\(K\)</span> singular values and vectors. Again, we’ll show this with <code class="docutils literal notranslate"><span class="pre">numpy</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">UK</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">];</span> <span class="n">SK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">]);</span> <span class="n">VtK</span> <span class="o">=</span> <span class="n">Vt</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">,:]</span>
<span class="n">PK</span> <span class="o">=</span> <span class="n">UK</span> <span class="o">@</span> <span class="n">SK</span> <span class="o">@</span> <span class="n">VtK</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">PK</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">PK</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P_2$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">PK</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - P_2$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_23_0.png" src="../../_images/estimating-parameters_spectral_23_0.png" />
</div>
</div>
<p>Next comes the really cool part. The matrix <span class="math notranslate nohighlight">\(P\)</span> is square because it has <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns. The matrix is positive, because every entry is a probability between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Finally, the matrix is symmetric, because the underlying random network is undirected, which means that the probability of node <span class="math notranslate nohighlight">\(i\)</span> being connected to node <span class="math notranslate nohighlight">\(j\)</span> is equal to the probability that node <span class="math notranslate nohighlight">\(j\)</span> is connected to node <span class="math notranslate nohighlight">\(i\)</span>. When a matrix is square, symmetric, and positive, for any singular vector associated with a non-zero singular value, the left and right singular vectors will be <em>identical</em>! This means that for <em>all</em> of the first <span class="math notranslate nohighlight">\(K\)</span> singular vectors <span class="math notranslate nohighlight">\(\vec u_i\)</span> and <span class="math notranslate nohighlight">\(\vec v_i\)</span>, that <span class="math notranslate nohighlight">\(\vec u_i = \vec v_i\)</span>! Using what we learned above, our expression for <span class="math notranslate nohighlight">\(P\)</span> now looks like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}    
    P = P_K = \sum_{i = 1}^K \sigma_i \vec u_i \vec u_i^\top
\end{align*}\]</div>
<p>and putting this back in our matrix form with <span class="math notranslate nohighlight">\(U_K\)</span> the matrix whose <span class="math notranslate nohighlight">\(K\)</span> columns are the vectors <span class="math notranslate nohighlight">\(\vec u_i\)</span>, and <span class="math notranslate nohighlight">\(\Sigma_K\)</span> the first <span class="math notranslate nohighlight">\(K\)</span> singular values, we have that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P &amp;= U_K \Sigma_K U_K^\top
\end{align*}\]</div>
<p>Now, we are really starting to get somewhere! Again, we’ll show this using <code class="docutils literal notranslate"><span class="pre">numpy</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Psymm is UK * SigmaK * UK^transpose</span>
<span class="n">Psymm</span> <span class="o">=</span> <span class="n">UK</span> <span class="o">@</span> <span class="n">SK</span> <span class="o">@</span> <span class="n">UK</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Psymm</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">PK</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$U_K </span><span class="se">\\</span><span class="s2">Sigma_K U_K^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">PK</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - U_K </span><span class="se">\\</span><span class="s2">Sigma_K U_K^</span><span class="se">\\</span><span class="s2">top$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_26_0.png" src="../../_images/estimating-parameters_spectral_26_0.png" />
</div>
</div>
</div>
<div class="section" id="the-square-root-matrix">
<h4><span class="section-number">6.4.2.2.4. </span>The Square Root Matrix<a class="headerlink" href="#the-square-root-matrix" title="Permalink to this headline">¶</a></h4>
<p>It’s been quite a ride; don’t fall off just yet! Now we are really in the home stretch! We’ve saved the easiest part for last. Remember that <span class="math notranslate nohighlight">\(\Sigma_K\)</span> is just a diagonal matrix, whose entries are the singular values <span class="math notranslate nohighlight">\(\sigma_i\)</span> for the first <span class="math notranslate nohighlight">\(K\)</span> singular values. As it turns out, since <span class="math notranslate nohighlight">\(P\)</span> is positive, these singular values are going to be positive too. This means that <span class="math notranslate nohighlight">\(\sigma_i\)</span> has a real square root, since all positive numbers have a square root. This means we could just express <span class="math notranslate nohighlight">\(\sigma_i\)</span> as the product of <span class="math notranslate nohighlight">\(\sqrt{\sigma_i}\sqrt{\sigma_i}\)</span> with itself!</p>
<p>Finally, remember that if we were to multiply two diagonal matrices, the resulting matrix would just be the element-wise product of each diagonal entry. This means we could just write <span class="math notranslate nohighlight">\(\Sigma_K\)</span> like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Sigma_K &amp;= \begin{bmatrix}
        \sqrt{\sigma_1} &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \sqrt{\sigma_2} &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sqrt{\sigma_n}
    \end{bmatrix}\begin{bmatrix}
        \sqrt{\sigma_1} &amp; 0 &amp; ... &amp; 0 \\
        0 &amp; \sqrt{\sigma_2} &amp; \ddots &amp; \vdots \\
        \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
        0 &amp; ... &amp; 0 &amp; \sqrt{\sigma_n}
    \end{bmatrix}
\end{align*}\]</div>
<p>We will call the resulting matrix the “square root” matrix of <span class="math notranslate nohighlight">\(\Sigma_K\)</span>, abbreviated <span class="math notranslate nohighlight">\(\sqrt{\Sigma_K}\)</span> which hopefully is named for pretty obvious reasons. So, <span class="math notranslate nohighlight">\(\Sigma_K = \sqrt{\Sigma_K}\sqrt{\Sigma_K}\)</span>. This matrix has <span class="math notranslate nohighlight">\(K \times K\)</span> entries, and is therefore square. Also, notice that all the off-diagonal entries are just <span class="math notranslate nohighlight">\(0\)</span>, which means it’s symmetric too, because of the convenient fact that <span class="math notranslate nohighlight">\(0 = 0\)</span> (and hence, the off-diagonal entries are all equal). Putting this fact together means that <span class="math notranslate nohighlight">\(\sqrt{\Sigma_K} = \sqrt{\Sigma_K}^\top\)</span>, which is the definition of matrix symmetry. So finally, <span class="math notranslate nohighlight">\(\Sigma_K = \sqrt{\Sigma_K}\sqrt{\Sigma_K}^\top\)</span>. Putting this fact together gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P &amp;= U_K \sqrt{\Sigma_K}\sqrt{\Sigma_K}^\top U_K^\top
\end{align*}\]</div>
<p>If we let <span class="math notranslate nohighlight">\(X = U_K \sqrt{\Sigma_K}\)</span>, then <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. This means that <span class="math notranslate nohighlight">\(X\)</span> is a latent position matrix for <span class="math notranslate nohighlight">\(P\)</span>! If you remember back to the <a class="reference external" href="#link?">Section on RDPGs</a>, this means that we have found the latent position parameter for the corresponding RDPG for our SBM random network!</p>
<p>Let’s see this using numpy again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SKsqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">SK</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">UK</span> <span class="o">@</span> <span class="n">SKsqrt</span>
<span class="n">Prdpg</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Prdpg</span><span class="p">)</span>  <span class="c1"># compute the frobenius difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Prdpg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$XX^</span><span class="se">\\</span><span class="s2">top$&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
<span class="n">gp</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span> <span class="o">-</span> <span class="n">Prdpg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$P - XX^</span><span class="se">\\</span><span class="s2">top$, Frobenius difference = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_29_0.png" src="../../_images/estimating-parameters_spectral_29_0.png" />
</div>
</div>
<p>This matrix <span class="math notranslate nohighlight">\(X\)</span>, the latent position matrix, for an SBM will look like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_lpm</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticklabs</span><span class="o">=</span><span class="p">[],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="n">cbar</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Latent Dimension&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xticklabs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">yticklabs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Heatmap of Latent Position Matrix $X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_31_0.png" src="../../_images/estimating-parameters_spectral_31_0.png" />
</div>
</div>
<p>and is the “low-rank structure” which describes the probability matrix.</p>
<div class="admonition-putting-it-all-together admonition">
<p class="admonition-title">Putting it all together</p>
<p>What have we learned so far? What we’ve learned so far is that, if we have a probability matrix that is symmetric and rank-<span class="math notranslate nohighlight">\(K\)</span>:</p>
<ol class="simple">
<li><p>We can decompose this probability matrix using the singular value decomposition.</p></li>
<li><p>We can ignore singular values/vectors other than the first <span class="math notranslate nohighlight">\(K\)</span> of them.</p></li>
<li><p>For the first <span class="math notranslate nohighlight">\(K\)</span> singular vectors, the left and right vectors are identical.</p></li>
<li><p>We can decompose the singular value matrix into the product of the square root matrix with its transpose.</p></li>
<li><p>We can express the matrix <span class="math notranslate nohighlight">\(P\)</span> using the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, which is the product of the first <span class="math notranslate nohighlight">\(K\)</span> singular vectors with the first <span class="math notranslate nohighlight">\(K\)</span> singular values.
This means that we have found a latent position matrix <span class="math notranslate nohighlight">\(X\)</span> for the probability matrix using the singular values and singular vectors of <span class="math notranslate nohighlight">\(P\)</span>, by effectively just discarding the ones that don’t matter (and have singular values of <span class="math notranslate nohighlight">\(0\)</span>). We have succeeded in our goal of finding a much lower rank structure, the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, to describe the probability matrix <span class="math notranslate nohighlight">\(P\)</span>.</p></li>
</ol>
</div>
<p>If you remember from the section on RDPGs, this probability matrix has the property that each entry <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i \vec x_j^\top\)</span>.</p>
</div>
</div>
</div>
<div class="section" id="but-wait-we-don-t-have-the-probability-matrix-what-do-we-do">
<h2><span class="section-number">6.4.3. </span>But wait: we don’t have the probability matrix! What do we do?<a class="headerlink" href="#but-wait-we-don-t-have-the-probability-matrix-what-do-we-do" title="Permalink to this headline">¶</a></h2>
<p>All of the logic we developed above was with respect to the probability matrix, <span class="math notranslate nohighlight">\(P\)</span>, for a SBM. More generally, this logic extends to the probability matrix <span class="math notranslate nohighlight">\(P\)</span> for any RDPG, which is because an RDPG with <span class="math notranslate nohighlight">\(d\)</span> latent dimensions will <em>always</em> have a probability matrix that is <em>exactly</em> rank <span class="math notranslate nohighlight">\(d\)</span>. But, we have a slight issue: when we perform machine learning, we don’t know the probability matrix! All we have is the adjacency matrix itself, <span class="math notranslate nohighlight">\(A\)</span>, which is our data! We don’t actually know what the underlying probability matrix is! How the heck can we find this low rank structure we want to be able to estimate?</p>
<p>As it turns out, if we kept obtaining more and more networks <span class="math notranslate nohighlight">\(A\)</span> from the underlying RDPG random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>, we would <em>expect</em> that the network <span class="math notranslate nohighlight">\(A\)</span> we saw would be the probability matrix <span class="math notranslate nohighlight">\(P\)</span>. We’ll explain what we mean by expect here by turning back to our coin flip example. As you remember, we perform a coin flip at each pair of nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> of an RDPG, where the coin lands on heads with probability <span class="math notranslate nohighlight">\(\vec x_i\vec x_j^\top\)</span>, and lands on tails with probability <span class="math notranslate nohighlight">\(1 - \vec x_i\vec x_j^\top\)</span>. This means we can <em>expect</em> the coin to land on heads with probability <span class="math notranslate nohighlight">\(\vec x_i\vec x_j^\top\)</span>. In the same sense, we can expect the <span class="math notranslate nohighlight">\((i,j)\)</span> entry of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> to exist with probability <span class="math notranslate nohighlight">\(\vec x_i \vec x_j^\top\)</span>. In this sense, the expected value of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> <em>is</em> the probability matrix <span class="math notranslate nohighlight">\(P = XX^\top\)</span>.</p>
<p>So, since the expected value of the adjacency matrix <em>is</em> the probability matrix, what if we were to just embed the adjacency matrix instead? Let’s see how this might work. Again, we’ll use the singular value decomposition on <span class="math notranslate nohighlight">\(A\)</span>, and take a look at the scree plot for <span class="math notranslate nohighlight">\(A\)</span>, and compare it to the scree plot of <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UA</span><span class="p">,</span> <span class="n">sA</span><span class="p">,</span> <span class="n">VAt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plot_scree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $P$&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_scree</span><span class="p">(</span><span class="n">sA</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Scree plot of $A$&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_34_0.png" src="../../_images/estimating-parameters_spectral_34_0.png" />
</div>
</div>
<p>Now that’s really funky! The singular values of both <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(A\)</span> tend to fall off around <em>roughly</em> the same spot, right around dimension <span class="math notranslate nohighlight">\(2\)</span>! The singular value of <span class="math notranslate nohighlight">\(P\)</span> go directly to <span class="math notranslate nohighlight">\(0\)</span>, but the singular values for <span class="math notranslate nohighlight">\(A\)</span> tend to “round off” in the direction of <span class="math notranslate nohighlight">\(0\)</span>, but it isn’t <em>too</em> far off!</p>
<p>As it turns out, this is no coincidence: the singular values for a network which can be described by an RDPG will tend to “elbow” off right around the number of true latent dimensions for the probability matrix of the underlying random network. If the RDPG has <span class="math notranslate nohighlight">\(d\)</span> latent dimensions, this will occur right around <span class="math notranslate nohighlight">\(d\)</span>. For this reason, it is usually a good idea when we think a network might be well described by an RDPG to let the elbow selection algorithm do the work for us, and then take a good look at the scree plot to make sure the number of latent dimensions chosen seems reasonable to us.</p>
<p>What does it look like when we use the spectral embedding on <span class="math notranslate nohighlight">\(A\)</span>? We’ll compare the embedding of the adjacency matrix to the latent positions of the probability matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UAK</span> <span class="o">=</span> <span class="n">UA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">];</span> <span class="n">USK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sA</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">K</span><span class="p">]))</span>
<span class="n">Aembedded</span> <span class="o">=</span> <span class="n">UAK</span> <span class="o">@</span> <span class="n">USK</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Heatmap of Latent Position Matrix $X$&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">Aembedded</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedded Adjacency Matrix&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_37_0.png" src="../../_images/estimating-parameters_spectral_37_0.png" />
</div>
</div>
<p>Wow! When we take the adjacency matrix and embed it into <span class="math notranslate nohighlight">\(2\)</span> dimensions, it doesn’t look <em>identical</em> to the latent position matrix, but it shares some major patterns with it! In particular, it looks like the second latent dimension for the embedded adjacency matrix tends to capture that the second latent dimension of <span class="math notranslate nohighlight">\(X\)</span> has higher values for the first <span class="math notranslate nohighlight">\(50\)</span> nodes, and lower values for the second <span class="math notranslate nohighlight">\(50\)</span> nodes. For a variety of reasons, we will call this “embedding of <span class="math notranslate nohighlight">\(A\)</span>” an <em>estimate</em> of the latent position matrix for the underlying RDPG, which we will denote by <span class="math notranslate nohighlight">\(\hat X\)</span>.</p>
<p>As we learned in the last section, this entire procedure is automated for us by <code class="docutils literal notranslate"><span class="pre">graspologic</span></code> with the <code class="docutils literal notranslate"><span class="pre">AdjacencySpectralEmbed()</span></code> class, or alternatively, the <code class="docutils literal notranslate"><span class="pre">RDPGEstimator()</span></code>. The <code class="docutils literal notranslate"><span class="pre">RDPGEstimator()</span></code> class just makes clear that we are estimating parameters for an RDPG:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rdpgest</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">RDPGEstimator</span><span class="p">()</span>
<span class="n">rdpgest</span> <span class="o">=</span> <span class="n">rdpgest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Xhat</span> <span class="o">=</span> <span class="n">rdpgest</span><span class="o">.</span><span class="n">latent_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plot_lpm</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">xticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">],</span>
        <span class="n">yticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">99</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Embedded Adjacency Matrix&quot;</span><span class="p">,</span>
        <span class="n">yticklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;51&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/estimating-parameters_spectral_40_0.png" src="../../_images/estimating-parameters_spectral_40_0.png" />
</div>
</div>
<p>As it turns out, this example here is a good instance of another property of the latent position matrix. Remember that the latent position matrix is the matrix <span class="math notranslate nohighlight">\(X\)</span> where <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. As it turns out, sometimes the columns of this matrix can get flipped around a little bit, through something called a rotation. You can see this by noticing that it looks a lot like the entries of the estimates of the latent position matrix are positive in one case are negative for the other, and it basically looks like the colorbar just got flipped around on us. The rotation doesn’t really matter, and <span class="math notranslate nohighlight">\(P\)</span> still is equal to <span class="math notranslate nohighlight">\(XX^\top\)</span>, regardless of how that <span class="math notranslate nohighlight">\(X\)</span> is rotated. We’ll learn more about rotation matrices in the upcoming section on <a class="reference external" href="#link?">Multiple Network Representation Learning</a> and in the section on <a class="reference external" href="#link?">Two Sample Hypothesis Testing</a>, but for now, all you need to know is that this “flippage” of what’s big and small in the latent position matrix is not too important (yet!).</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./representations/ch6"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="spectral-embedding.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6.3. </span>Spectral Embedding Methods</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="random-walk-diffusion-methods.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.5. </span>Random-Walk and Diffusion-based Methods</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Joshua Vogelstein, Alex Loftus, and Eric Bridgeford<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>