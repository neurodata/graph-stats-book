
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7.3. Spectral Method Theory &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >let toggleHintShow = 'Click to show';</script>
    <script >let toggleHintHide = 'Click to hide';</script>
    <script >let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script >const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="8. Applications When You Have One Network" href="../../applications/ch8/ch8.html" />
    <link rel="prev" title="7.2. Maximum Likelihood Estimate Theory" href="theory-multigraph.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../coverpage.html">
   Hands-on Network Machine Learning with Scikit-Learn and Graspologic
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology and Math Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.4. Approaches for Network Learning Problems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.4. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/ch4.html">
   4. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/matrix-representations.html">
     4.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/properties-of-networks.html">
     4.2. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/network-representations.html">
     4.3. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/regularization.html">
     4.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch5/ch5.html">
   5. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/why-use-models.html">
     5.1. Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_ER.html">
     5.2. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_SBM.html">
     5.3. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_RDPG.html">
     5.4. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/multi-network-models.html">
     5.5. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/models-with-covariates.html">
     5.6. Network Models with Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch6/ch6.html">
   6. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters_mle.html">
     6.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/why-embed-networks.html">
     6.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/spectral-embedding.html">
     6.3. Spectral Embedding Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters_spectral.html">
     6.4. Estimating Parameters for the RDPG
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/random-walk-diffusion-methods.html">
     6.5. Random walk and diffusion-based methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/graph-neural-networks.html">
     6.6. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/multigraph-representation-learning.html">
     6.7. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/joint-representation-learning.html">
     6.8. Joint Representation Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch7.html">
   7. Theoretical Results
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="theory-single-network.html">
     7.1. Theory for Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="theory-multigraph.html">
     7.2. Maximum Likelihood Estimate Theory
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.3. Spectral Method Theory
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   8. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/community-detection.html">
     8.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/testing-differences.html">
     8.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/model-selection.html">
     8.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/single-vertex-nomination.html">
     8.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/out-of-sample.html">
     8.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   9. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/two-sample-hypothesis.html">
     9.1. Latent Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-communities.html">
     9.2. Differences in Block Matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/graph-matching-vertex.html">
     9.3. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/multiple-vertex-nomination.html">
     9.4. Vertex Nomination For Two Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch10/ch10.html">
   10. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/anomaly-detection.html">
     10.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-edges.html">
     10.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-vertices.html">
     10.3. Testing for Significant Vertices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/representations/ch7/theory-matching.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurodata/graph-stats-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Frepresentations/ch7/theory-matching.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/representations/ch7/theory-matching.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disclaimer-about-classical-statistical-asymptotic-theory">
   7.3.1. Disclaimer about classical statistical asymptotic theory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adjacency-spectral-embedding">
   7.3.2. Adjacency spectral embedding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-of-the-adjacency-spectral-embedding">
     7.3.2.1. Error of the adjacency spectral embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory-for-multiple-network-models">
   7.3.3. Theory for multiple network models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-embeddings">
     7.3.3.1. Spectral embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omnibus-embedding-omni">
     7.3.3.2. Omnibus Embedding (omni)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-adjacency-spectral-embedding-mase">
     7.3.3.3. Multiple adjacency spectral embedding (MASE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-matching-for-correlated-networks">
     7.3.3.4. Graph Matching for Correlated Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Spectral Method Theory</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disclaimer-about-classical-statistical-asymptotic-theory">
   7.3.1. Disclaimer about classical statistical asymptotic theory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adjacency-spectral-embedding">
   7.3.2. Adjacency spectral embedding
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-of-the-adjacency-spectral-embedding">
     7.3.2.1. Error of the adjacency spectral embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory-for-multiple-network-models">
   7.3.3. Theory for multiple network models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-embeddings">
     7.3.3.1. Spectral embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omnibus-embedding-omni">
     7.3.3.2. Omnibus Embedding (omni)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-adjacency-spectral-embedding-mase">
     7.3.3.3. Multiple adjacency spectral embedding (MASE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-matching-for-correlated-networks">
     7.3.3.4. Graph Matching for Correlated Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="spectral-method-theory">
<h1><span class="section-number">7.3. </span>Spectral Method Theory<a class="headerlink" href="#spectral-method-theory" title="Permalink to this headline">¶</a></h1>
<p>We’ve tried to present this material so far in a manner which is as easy to understand as possible with a distant understanding of probability/statistics and a working knowledge of machine learning. Unfortunately, there is really no way around it now, so this section is about to get hairy mathematically. To understand this section properly, you should have an extremely firm understanding of linear algebra, and more than likely, should have a working knowledge of matrix analysis and multivariate probability theory. While we’ve already seen some concentration inequalities in the last section (Chebyshev’s inequality) you should have a working knowledge of how this term can be extended to random vectors and matrices before proceeding. Before taking on this section, we would recommend checking out the excellent primer by Roman Vershynin, <a class="reference external" href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">High Dimensional Probability</a>, which should get you a good foundation to understand many of the results we will take a look at here. We aren’t going to prove many of these results; if you want more details, please check out the <a class="reference external" href="https://arxiv.org/abs/1709.05454">excellent paper on Random Dot Product Graphs</a> by Avanti Athreya and her research team from 2017.</p>
<p>Buckle up!</p>
<div class="section" id="disclaimer-about-classical-statistical-asymptotic-theory">
<h2><span class="section-number">7.3.1. </span>Disclaimer about classical statistical asymptotic theory<a class="headerlink" href="#disclaimer-about-classical-statistical-asymptotic-theory" title="Permalink to this headline">¶</a></h2>
<p>While in classic statistics there is a large literature that derives the large sample properties of an estimator, this concepts are more challenging in network analysis for multiple reasons. To start with, the very basic concept of sample size is not particularly clear. We often associate sample size with the number of independent observations, which are usually assumed to be independent from each other (for example, think of the number of poll participants to estimate polling preferences). In a network, having independent observations is no longer possible in the same way since all we observe are edges, and they are related to some type of interaction between two vertices. We therefore often assume that the sampled units are the vertices. However, everytime a new vertex is added, a new set of interactions with all the existing vertices is added to the model, which often results in the need of including more parameters, leading to the second important challenge in studying networks. A body of new literature has addressed some of these challenges for the models and estimators introduced in the previous sections, and we review some of these results here.</p>
</div>
<div class="section" id="adjacency-spectral-embedding">
<h2><span class="section-number">7.3.2. </span>Adjacency spectral embedding<a class="headerlink" href="#adjacency-spectral-embedding" title="Permalink to this headline">¶</a></h2>
<p>In the following sections, we summarize some of the main results in the literature about spectral embeddings. A more in-deep review of this results is presented  in Dr. Athreya’s paper. In this section, we review some theoretical properties for the adjacency spectral embedding (ASE), introduced in <a class="reference external" href="#link?">Section 6.3</a>.</p>
<p>We consider a sequence of  independent random adjacency  matrices, denoted by <span class="math notranslate nohighlight">\(\mathbf A_{n_0}, \mathbf A_{n_0+1}, \mathbf A_{n_0+2}, \ldots\)</span>, where <span class="math notranslate nohighlight">\(n_0\)</span> is some positive integer, denoting the number of vertices in an observed network. We assume that each matrix <span class="math notranslate nohighlight">\(\mathbf A_n\)</span> is a random adjacency matrix generated from a random dot product graph (RDPG) model with latent positions <span class="math notranslate nohighlight">\(\mathbf X_n\in\mathbb R^{n\times d}\)</span>. We write <span class="math notranslate nohighlight">\((\mathbf X_n)_i\)</span> to represent the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(\mathbf X_n\)</span>, and we assume that the rows of <span class="math notranslate nohighlight">\(\mathbf X_n\)</span>, which correspond to the latent positions of the vertices, are independent and identically distributed with <span class="math notranslate nohighlight">\((\mathbf X_1), \ldots, (\mathbf X_n)_n\overset{\text{i.i.d.}}{\sim} F\)</span>, where <span class="math notranslate nohighlight">\(F\)</span> is a distribution with support <span class="math notranslate nohighlight">\(\mathcal{X}\subset\mathbb R^d\)</span>. We also  assume that the second moment matrix <span class="math notranslate nohighlight">\(\mathbf{\Delta} = \mathbb{E}[(\mathbf X_n)_1(\mathbf X_n)_1^\top]\in\mathbb R^{d\times d}\)</span> has non-zero eigenvalues.   We use <span class="math notranslate nohighlight">\(\widehat{\mathbf X}_n=ASE(\mathbf A_n)\in\mathbb R^{n\times d}\)</span> to denote the <span class="math notranslate nohighlight">\(d\)</span>-dimensional adjacency spectral embedding of <span class="math notranslate nohighlight">\(\mathbf A_n\)</span>, and <span class="math notranslate nohighlight">\(\widetilde{\mathbf X}_n = LSE(\mathbf A_n)\)</span>  to denote its <span class="math notranslate nohighlight">\(d\)</span>-dimensional Laplacian spectral embedding.</p>
<div class="section" id="error-of-the-adjacency-spectral-embedding">
<h3><span class="section-number">7.3.2.1. </span>Error of the adjacency spectral embedding<a class="headerlink" href="#error-of-the-adjacency-spectral-embedding" title="Permalink to this headline">¶</a></h3>
<p>The first result concerns the error of the adjacency spectral embedding (ASE) method, which is an estimator for the latent positions. This estimator can consistently estimate the latent positions of a network, or in other words, as the sample size (number of vertices) increases, the estimated latent positions approach the true latent positions. The typical distance between these two can be quantified explicitly in terms of the sample size <span class="math notranslate nohighlight">\(n\)</span>, dimension of the latent positions <span class="math notranslate nohighlight">\(d\)</span> and a constant that only depends on the distribution of the latent positions <span class="math notranslate nohighlight">\(F\)</span>. It has been shown that  with probability tending to one, there exists some orthogonal rotation <span class="math notranslate nohighlight">\(\mathbf W_n\in\mathbb R^{d\times d}\)</span> such that
the largest distance between the true and estimated latent positions satisfies:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\max_{i\in[n]}\|(\widehat{\mathbf X}_n)_i - \mathbf W_n(\mathbf X_n)_i\| \leq \frac{Cd^{1/2}\log^2 n}{\sqrt{n}}. \label{eq:thm-ASE-const}
\end{align*}\]</div>
<p>The orthogonal matrix in the previous result comes from the fact that the latent positions are the same up to orthogonal rotations (for instance, changing the signs of the columns of <span class="math notranslate nohighlight">\(\mathbf X_n\)</span> does not change the inner products of their rows). The previous result implies that the distance between true and estimated latent positions is shrinking with <span class="math notranslate nohighlight">\(n\)</span>, and as such, we can construct an accurate estimator of this latent positions. This result justifies the use of <span class="math notranslate nohighlight">\(\mathbf {\hat X}\)</span> in place of <span class="math notranslate nohighlight">\(\mathbf X\)</span> for subsequent inference tasks, such as community detection, vertex nomination or classification.</p>
<p>A further result on the asymptotic properties of the ASE concerns to the distribution of the difference between an estimated latent position <span class="math notranslate nohighlight">\(\widehat{\mathbf X}_n)_i\)</span> and the true parameter <span class="math notranslate nohighlight">\((\mathbf X_n)_i\)</span>. By consistency, this difference shrinks with <span class="math notranslate nohighlight">\(n\)</span>, but knowing this fact is sometimes not enough when quantifying how much different they are is needed. This is important, as some statistical tasks, such as hypothesis testing or confidence interval estimation, require to quantify the error in estimation.
Distributional results on the rows of the adjacency spectral embedding show that the error in estimating the true latent positions is asymptotically normally distributed. That is, as <span class="math notranslate nohighlight">\(n\)</span> grows, the distribution of this difference gets more similar to a multivariate normal distribution. In particular, it has been shown that the latent positions converge to a mixture of standard multivariate normal distributions, that is, for any <span class="math notranslate nohighlight">\(\mathbf{z}\in\mathbb R^{d}\)</span>,</p>
<div class="amsmath math notranslate nohighlight" id="equation-b4f07b6a-0b10-46cb-8671-56a5ac436a33">
<span class="eqno">(7.2)<a class="headerlink" href="#equation-b4f07b6a-0b10-46cb-8671-56a5ac436a33" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbb{P}\left(\sqrt{n}\left(\widehat{\mathbf X}_n\mathbf W_n - \mathbf X\right)_i\leq \mathbf{z} \right) \approx \int_{\mathcal{X}}\Phi(\mathbf{z}, \mathbf{\Sigma}(\mathbf{x}))\  dF(\mathbf{x}),\label{eq:thm-ASE-CLT}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(\mathbf{z}, \mathbf{\Sigma}(\mathbf{x}))\)</span> is the cumulative distribution function of a multivariate normal distribution with mean zero and a covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}(\mathbf{x})\in\mathbb R^{d\times d}\)</span> that is a function of <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathcal{X}\)</span></p>
<p>\tcr{maybe illustrate this results with a figure of a SBM with 2 communities?}</p>
<p>The Laplacian spectral embedding (<span class="math notranslate nohighlight">\(LSE\)</span>) possess analogous properties. In particular, the estimated latent positions obtained from LSE converge to a scaled version of the latent positions as <span class="math notranslate nohighlight">\(n\)</span> grows, and the distribution of this difference also has a similar asymptotic distribution, given by:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}\left\{\sqrt{n}\left(\mathbf W_n(\widetilde{\mathbf X}_n)_i - \frac{(\mathbf X_n)_i}{\sqrt{\sum_{j}(\mathbf X_n)_i^\top (\mathbf X_n)_j }} \right)\leq \mathbf{z}\right\}  \approx \int_{\mathcal{X}}\Phi(\mathbf{z}, \widetilde{\mathbf{\Sigma}}(\mathbf{x}))\  dF(\mathbf{x}),\label{eq:thm-LSE-CLT}
\end{align*}\]</div>
<p>for some covariance matrix <span class="math notranslate nohighlight">\(\widetilde{\mathbf{\Sigma}}(\mathbf{x})\)</span>.</p>
</div>
</div>
<div class="section" id="theory-for-multiple-network-models">
<h2><span class="section-number">7.3.3. </span>Theory for multiple network models<a class="headerlink" href="#theory-for-multiple-network-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="spectral-embeddings">
<h3><span class="section-number">7.3.3.1. </span>Spectral embeddings<a class="headerlink" href="#spectral-embeddings" title="Permalink to this headline">¶</a></h3>
<p>Having asymptotic properties of the estimators facilitates performing statistical inference.</p>
<p>Comparing the distribution of two populations is a frequent problem in statistics and across multiple domains. In classical statistics, a typical strategy to perform this task is to compare the mean of two populations by using an appropriate test statistic. Theoretical results on the distribution of this statistic (either exact or asymptotic) are then used to derive a measure of uncertainty for this problem (such as p-values or confidence intervals). Similarly, when comparing two observed graphs, we may wonder whether they were generated by the same mechanism. The results discussed before have been used to develop valid statistical tests for   two-network hypothesis testing questions.</p>
<p>A semiparametric network hypothesis test for the equivalence between the latent positions of the vertices of a pair of networks can be constructed by using the estimates of the latent positions. Formally, for each fixed <span class="math notranslate nohighlight">\(n\)</span> let <span class="math notranslate nohighlight">\(\mathbf X_n, \mathbf Y_n\in\mathbb R^{n\times d}\)</span> be a sequence of latent positions matrices, and define
<span class="math notranslate nohighlight">\(\mathbf A_n\sim RDPG(\mathbf X_n)\)</span>, <span class="math notranslate nohighlight">\(\mathbf B_n\sim RDPG(Y_n)\)</span> as independent random adjacency matrices. The problem of testing the equality of the distributions of <span class="math notranslate nohighlight">\(\mathbf A_n\)</span> and <span class="math notranslate nohighlight">\(\mathbf B_n\)</span>  is defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{H}^n_0:\mathbf X_n =_{\mathbf W} \mathbf Y_n\quad\quad\quad \text{ vs.}\quad\quad\quad \mathcal{H}^n_a:\mathbf X_n \neq_{\mathbf W} \mathbf Y_n,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf X_n =_{\mathbf W}\mathbf Y_n\)</span> denotes that <span class="math notranslate nohighlight">\(\mathbf X_n\)</span> and <span class="math notranslate nohighlight">\(\mathbf Y_n\)</span> are equivalent up to an orthogonal transformation <span class="math notranslate nohighlight">\(\mathbf W\in\mathcal{O}_d\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{O}_d\)</span> is the set of <span class="math notranslate nohighlight">\(d\times d\)</span> orthogonal matrices. To define the test statistic, denote  <span class="math notranslate nohighlight">\(\widehat{\mathbf X}_n = ASE(\mathbf A_n)\)</span>, <span class="math notranslate nohighlight">\(\widehat{\mathbf Y}_n=ASE(\mathbf B_n)\)</span>, and for a matrix <span class="math notranslate nohighlight">\(\mathbf A\in\mathbb R^{n\times n}\)</span> with singular values <span class="math notranslate nohighlight">\(\sigma_1(\mathbf A) \geq \ldots\geq \sigma_n(\mathbf A)\geq 0\)</span> and largest observed degree <span class="math notranslate nohighlight">\(\delta(\mathbf A) = \max_{i\in[n]}\sum_{j=1}^n\mathbf A_{ij}\)</span>, define
$<span class="math notranslate nohighlight">\(\gamma(\mathbf A):=\frac{\sigma_d(\mathbf A) - \sigma_{d+1}(\mathbf A)}{\delta(\mathbf A)}.\)</span><span class="math notranslate nohighlight">\( 
Define \)</span>T_n$ as the test statistic:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
T_n : = \frac{\min_{\mathbf W\in\mathcal{O}_d} \|\widehat{\mathbf X}_n\mathbf W - \widehat{\mathbf Y}_n\|_F}{\sqrt{d\gamma^{-1}(\mathbf A_n)} + \sqrt{d\gamma^{-1}(\mathbf B_n)}}.
\end{align*}\]</div>
<p>It can be shown that, under appropriate regularity conditions, this test statistic is a consistent test for the  hypothesis testing problem described above, in the sense that for any significance level <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(C&gt;1\)</span>, then  <span class="math notranslate nohighlight">\(\mathbb{P}(T_n&gt; C)\leq \alpha\)</span> for <span class="math notranslate nohighlight">\(n\)</span> sufficiently large under <span class="math notranslate nohighlight">\(\mathcal{H}^n_0\)</span> (type I error control), and if <span class="math notranslate nohighlight">\(\lim_{n\rightarrow\infty}\min_{\mathbf W\in\mathcal{O}_d} \|\widehat{\mathbf X}_n\mathbf W - \widehat{\mathbf Y}_n\|_F=\infty\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{P}(T_n&gt; C)\rightarrow 1\)</span> under <span class="math notranslate nohighlight">\(\mathcal{H}^n_a\)</span> (i.e., the type II error vanishes).</p>
<p>When the vertices of the networks are not necessarily aligned (including cases in which the networks do not have the same number of vertices), testing equality of latent positions is inappropriate. In those settings, the methods introduced in chapter ??? can be used to test the equality of the distribution of the latent positions.
For a pair of matrices <span class="math notranslate nohighlight">\(\mathbf X_n\in\mathbb R^{n\times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf Y_m\in\mathbb R^{m\times d}\)</span> with their rows distributed as <span class="math notranslate nohighlight">\((\mathbf X_n)_i\overset{\text{i.i.d.}}{\sim} F\)</span> and <span class="math notranslate nohighlight">\((\mathbf Y_m)_i\overset{\text{i.i.d.}}{\sim} G\)</span> and a pair of independent adjacency matrices <span class="math notranslate nohighlight">\(\mathbf A_n\sim RDPG(\mathbf X_n)\)</span>, <span class="math notranslate nohighlight">\(\mathbf B_n\sim RDPG(\mathbf Y_n)\)</span> , the nonparametric network hypothesis testing problem is given by:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{H}^n_0:F \perp G \quad\quad\quad \text{ vs.}\quad\quad\quad \mathcal{H}^n_a: F \not\perp G,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(F\perp G\)</span> indicates equality of the distributions up to an orthogonal transformation. To test such hypothesis, the following statistic is used:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
U_{n,m}(\mathbf X, \mathbf Y)=&amp; \frac{1}{n(n-1)}\sum_{j\neq i}\kappa(X_i, X_j)-\frac{2}{mn}\sum_{i=1}^n\sum_{k=1}^m\kappa(X_i, Y_k)\\
&amp; + \frac{1}{m(m-1)}\sum_{l\neq k}\kappa(Y_k, Y_l),
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa:\mathcal{X}\times \mathcal{X}\rightarrow\mathbb R\)</span> is a positive definite kernel. As in the semiparametric test, it can be shown that  <span class="math notranslate nohighlight">\(U_{n,m}(\mathbf X, \mathbf Y)\)</span> is a consistent and unbiased estimate of the maximum mean discrepancy  between the distributions <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(G\)</span>. Furthermore, under the null hypothesis, the quantity <span class="math notranslate nohighlight">\((m+n)U_{n,m}(\mathbf X, \mathbf Y)\)</span> converges in distribution to an infinite weighted sum of independent chi-squared random variables as <span class="math notranslate nohighlight">\(n,m\rightarrow \infty\)</span>, provided that <span class="math notranslate nohighlight">\(\frac{n}{n+m}\rightarrow \rho \in (0, 1)\)</span>.  Moreover, when the latent positions are used in place of the true latent positions, then the difference between <span class="math notranslate nohighlight">\(U_{n,m}(\widehat{\mathbf X}, \widehat{\mathbf Y})\)</span>  and <span class="math notranslate nohighlight">\(U_{n,m}(\mathbf X, Y)\)</span> converges to zero sufficiently fast to yield a consistent test procedure.</p>
</div>
<div class="section" id="omnibus-embedding-omni">
<h3><span class="section-number">7.3.3.2. </span>Omnibus Embedding (omni)<a class="headerlink" href="#omnibus-embedding-omni" title="Permalink to this headline">¶</a></h3>
<p>The omnibus embedding described in [Section 6.7] jointly estimates the latent positions under the joint random dot product network (<span class="math notranslate nohighlight">\( JRDPG\)</span>) model, where <span class="math notranslate nohighlight">\((\mathbf A^{(1)}, \ldots, \mathbf A^{(m)})\sim JRDPG(\mathbf X_n)\)</span>, and the rows of <span class="math notranslate nohighlight">\(\mathbf X_n\in\mathbb R^{n\times d}\)</span> are an i.i.d. sample from some distribution <span class="math notranslate nohighlight">\(F\)</span>. Let <span class="math notranslate nohighlight">\(\widehat{\mathbf{O}}\in\mathbb R^{mn\times mn}\)</span> be the omnibus embedding of <span class="math notranslate nohighlight">\(\mathbf A^{(1)}, \ldots, \mathbf A^{(m)}\)</span> and <span class="math notranslate nohighlight">\(\widehat{\mathbf Z} = ASE(\mathbf{O})\in\mathbb R^{mn\times d}\)</span>.
Under this setting, it can be shown that the rows of <span class="math notranslate nohighlight">\(\widehat{\mathbf Z}_n\)</span> are a consistent estimator of the latent positions of each individual network  as <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>, and that:</p>
<div class="amsmath math notranslate nohighlight" id="equation-585f74cd-6d18-400e-a0e5-368710127069">
<span class="eqno">(7.3)<a class="headerlink" href="#equation-585f74cd-6d18-400e-a0e5-368710127069" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\max_{i\in[n],j\in[m]}\|(\widehat{\mathbf Z}_n)_{(j-1)n + i} - \mathbf W_n(\mathbf X_n)_{i}\| \leq \frac{C\sqrt{m}\log(mn)}{\sqrt{n}}. \label{eq:OMNI-consistency}    
\end{equation}\]</div>
<p>Furthermore, a central limit theorem for the rows of the omnibus embedding  asserts that:</p>
<div class="amsmath math notranslate nohighlight" id="equation-18f62d66-7977-45d1-a598-7340cc63f9b9">
<span class="eqno">(7.4)<a class="headerlink" href="#equation-18f62d66-7977-45d1-a598-7340cc63f9b9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\lim_{n\rightarrow\infty} \mathbb{P}\left\{\sqrt{n}\left(\mathbf W_n(\widehat{\mathbf Z}_n)_{(j-1)n + i} - (\mathbf X_n)_i\right)\leq \mathbf{z}\right\}  = \int_{\mathcal{X}}\Phi(\mathbf{z}, \widehat{\mathbf{\Sigma}}(\mathbf{x}))\  dF(\mathbf{x}),\label{eq:thm-OMNI-CLT}
\end{equation}\]</div>
<p>for some covariance matrix <span class="math notranslate nohighlight">\(\widehat{\Sigma}(\mathbf{x})\)</span>.</p>
</div>
<div class="section" id="multiple-adjacency-spectral-embedding-mase">
<h3><span class="section-number">7.3.3.3. </span>Multiple adjacency spectral embedding (MASE)<a class="headerlink" href="#multiple-adjacency-spectral-embedding-mase" title="Permalink to this headline">¶</a></h3>
<p>The <span class="math notranslate nohighlight">\(COSIE\)</span> model described in <a class="reference external" href="#link?">Section 5.5</a> gives a joint model that characterizes the distribution of multiple networks with expected probability matrices that share the same common invariant subspace. The <span class="math notranslate nohighlight">\(MASE\)</span> algorithm (see <a class="reference external" href="#link?">Section 5.5</a>) is a consistent estimator for this common invariant subspace, and results in asymptotically normally estimators for the individual symmetric matrices. Specifically, let <span class="math notranslate nohighlight">\(\mathbf V_n\in\mathbb R^{n\times d}\)</span> be
a sequence of orthonormal matrices and <span class="math notranslate nohighlight">\(\mathbf R^{(1)}_n, \ldots, \mathbf R^{(m)}_n\in\mathbb R^{d\times d}\)</span> a sequence of score matrices such that <span class="math notranslate nohighlight">\(\mathbf{P}^{(l)}_n=\mathbf V_n\mathbf R^{(l)}_n\mathbf V_n^\top\in[0,1]^{n\times n} \)</span>, <span class="math notranslate nohighlight">\((\mathbf A_n^{(1)}, \ldots, \mathbf A_n^{(m)})\sim COSIE(\mathbf V_n;, \mathbf R^{(1)}_n, \ldots, \mathbf R^{(m)}_n)\)</span>, and <span class="math notranslate nohighlight">\(\widehat{\mathbf V}, \widehat{\mathbf R}^{(1)}_n, \ldots, \widehat{\mathbf R}^{(1)}_n\)</span> be the estimators obtained by <span class="math notranslate nohighlight">\(MASE\)</span>. Under appropriate regularity conditions, the estimate for <span class="math notranslate nohighlight">\(\mathbf V\)</span> is consistent as <span class="math notranslate nohighlight">\(n,m\rightarrow\infty\)</span>, and there exists some constant <span class="math notranslate nohighlight">\(C&gt;0\)</span> such that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}\left[\min_{\mathbf W\in\mathcal{O}_d}\|\widehat{\mathbf V}-\mathbf V\mathbf W\|_F\right] \leq C\left(\sqrt{\frac{1}{mn}} + {\frac{1}{n}}\right). \label{eq:theorem-bound}
\end{align*}\]</div>
<p>In addition, the entries of <span class="math notranslate nohighlight">\(\widehat{\mathbf{R}}^{(l)}_n\)</span>, <span class="math notranslate nohighlight">\(l\in[m]\)</span> are asymptotically normally distributed. Namely, there exists a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(\mathbf W\)</span> such that:
$<span class="math notranslate nohighlight">\(\frac{1}{\sigma_{l,j,k}}\left(\widehat{\mathbf R}^{(l)}_n - \mathbf W^\top\mathbf R^{(l)}_n\mathbf W + \mathbf H_m^{(l)}\right)_{jk} \overset{d}{\rightarrow} \mathcal{N}(0, 1), \)</span><span class="math notranslate nohighlight">\(
as \)</span>n\rightarrow\infty<span class="math notranslate nohighlight">\(, where:
\)</span>\mathbb{E}[|\mathbf H_m^{(l)}|]=O\left(\frac{d}{\sqrt{m}}\right)<span class="math notranslate nohighlight">\( and \)</span>\sigma^2_{l,j,k} = O(1)$.</p>
</div>
<div class="section" id="graph-matching-for-correlated-networks">
<h3><span class="section-number">7.3.3.4. </span>Graph Matching for Correlated Networks<a class="headerlink" href="#graph-matching-for-correlated-networks" title="Permalink to this headline">¶</a></h3>
<p>Given a pair of network adjacency matrices <span class="math notranslate nohighlight">\(\mathbf A\)</span> and <span class="math notranslate nohighlight">\(\mathbf B\)</span> with <span class="math notranslate nohighlight">\(n\)</span> vertices each (but possibly permuted), when is it possible to match the networks correctly? In principle, if the two networks are isomorphic (i.e., equal up to some orthogonal permutation), matching the vertices across the networks should be theoretically possible, as long as the exact match is unique. However, in the presence of edge noise, it might not be possible in general. This raises the question of whether matching the vertices is feasible for a particular joint model that describes the edges of the networks.</p>
<p>A body of literature has studied the feasibility of finding the correct matching under different random network models, including correlated Erd\H{o}s-R’enyi and Bernoulli networks. In this section we review some of the results for the correlated Erd\H{o}s-R’enyi model described in <a class="reference external" href="#link?">Section 5.5</a>.</p>
<p>Formally, given parameters <span class="math notranslate nohighlight">\(\rho_n\in[0,1]\)</span> and <span class="math notranslate nohighlight">\(q_n \in(0, 1-\xi_1)\)</span> for some small <span class="math notranslate nohighlight">\(\xi_1&gt;0\)</span>, the <span class="math notranslate nohighlight">\(n\times n\)</span> adjacency matrices <span class="math notranslate nohighlight">\(\mathbf A_n\)</span> and <span class="math notranslate nohighlight">\(\mathbf B_n\)</span> are distributed as correlated Erd\H{o}s-R’enyi if their marginal distributions are <span class="math notranslate nohighlight">\(\mathbf A_n\sim ER_n(q_n)\)</span>, <span class="math notranslate nohighlight">\(\mathbf B_n\sim ER_n(q_n)\)</span>, but the edge pairs satisfy <span class="math notranslate nohighlight">\(\text{Corr}((\mathbf A_n)_{ij},(\mathbf{Q}_n^\top\mathbf B_n\mathbf{Q}_n)_{ij})=\rho_n\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{Q}_n\in\mathcal{P}_n\)</span> is a permutation matrix that gives the correct alignment between the vertices (here <span class="math notranslate nohighlight">\(\mathcal{P}_n\)</span> denotes the set of <span class="math notranslate nohighlight">\(n\times n\)</span> permutation matrices).</p>
<p>One particular question is when does the solution of the optimization problem defined in <a class="reference external" href="#link?">Section 9.3</a> recover the correct solution? It can be shown that this is possible when the networks are correlated-ER distributed with correlation between the networks and the edge probability that are not to small. Formally, the conditions require that
<span class="math notranslate nohighlight">\(\rho_n\geq c_1\sqrt{\frac{\log n}{n}}\)</span> and <span class="math notranslate nohighlight">\(q_n\geq c_2 \frac{\log n }{n}\)</span> for some fixed positive constants <span class="math notranslate nohighlight">\(c_1, c_2\)</span>, then <span class="math notranslate nohighlight">\(\mathbf{Q}_n\)</span> can be correctly recovered  with probability 1 for <span class="math notranslate nohighlight">\(n\)</span> sufficiently large.</p>
<p>While the solution of the quadratic assignment problem can correctly recover the vertex alignment in theory, it is computationally challenging to solve the optimization problem due to the non-convexity of the loss function. Introducing seeds nodes (as described in <a class="reference external" href="#link?">Section 9.3</a>) can greatly improve the performance of the algorithm. In particular, in the presence of a logarithmic number of seed vertices <span class="math notranslate nohighlight">\(s_n\)</span>, graph matching succeeds in polynomial time to recover the alignment of the remaining <span class="math notranslate nohighlight">\(n-s_n\)</span> vertices.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./representations/ch7"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="theory-multigraph.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7.2. </span>Maximum Likelihood Estimate Theory</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../../applications/ch8/ch8.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Applications When You Have One Network</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Joshua Vogelstein, Eric Bridgeford, and Alex Loftus<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>