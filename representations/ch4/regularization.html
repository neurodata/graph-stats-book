
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.4. Regularization &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="5. Why Use Statistical Models?" href="../ch5/ch5.html" />
    <link rel="prev" title="4.3. Properties of Networks" href="properties-of-networks.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../coverpage.html">
   Hands-on Network Machine Learning with Scikit-Learn and Graspologic
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology and Math Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-networks.html">
     1.4. Types of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.5. Types of Network Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/main-challenges.html">
     1.6. Main Challenges of Network Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/exercises.html">
     1.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/transformation-techniques.html">
     2.4. Transformation Techniques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.5. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.6. Fine-Tune your Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch4.html">
   4. Properties of Networks as a Statistical Object
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="matrix-representations.html">
     4.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="network-representations.html">
     4.2. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="properties-of-networks.html">
     4.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch5/ch5.html">
   5. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/why-use-models.html">
     5.1. Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models.html">
     5.2. Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_ER.html">
     5.3. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_SBM.html">
     5.4. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_RDPG.html">
     5.5. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/multi-network-models.html">
     5.6. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/models-with-covariates.html">
     5.7. Network Models with Covariates
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/single-network-models_theory.html">
     5.8. Single network model theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch6/ch6.html">
   6. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters_mle.html">
     6.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/why-embed-networks.html">
     6.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/spectral-embedding.html">
     6.3. Spectral Embedding Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters_spectral.html">
     6.4. Estimating Parameters in Network Models via Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/random-walk-diffusion-methods.html">
     6.5. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/graph-neural-networks.html">
     6.6. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/multigraph-representation-learning.html">
     6.7. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/joint-representation-learning.html">
     6.8. Joint Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters_theory.html">
     6.9. Model Estimation Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch7/ch7.html">
   7. Theoretical Results
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-single-network.html">
     7.1. Theory for Single Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-multigraph.html">
     7.2. Theory for Multiple-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-matching.html">
     7.3. Theory for Graph Matching
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   8. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/community-detection.html">
     8.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/testing-differences.html">
     8.2. Testing for Differences between Communities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/model-selection.html">
     8.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/single-vertex-nomination.html">
     8.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/out-of-sample.html">
     8.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   9. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/two-sample-hypothesis.html">
     9.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/graph-matching-vertex.html">
     9.2. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/multiple-vertex-nomination.html">
     9.3. Vertex Nomination For Multiple Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch10/ch10.html">
   10. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/anomaly-detection.html">
     10.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-edges.html">
     10.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-vertices.html">
     10.3. Testing for Significant Vertices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-communities.html">
     10.4. Testing for Significant Communities
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/representations/ch4/regularization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurodata/graph-stats-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Frepresentations/ch4/regularization.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/representations/ch4/regularization.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/network_machine_learning_in_python/representations/ch4/regularization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-of-the-nodes">
   4.4.1. Regularization of the Nodes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#degree-trimming-removes-nodes-with-low-degree">
     4.4.1.1. Degree trimming removes nodes with low degree
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularization-of-the-edges">
   4.4.2. Regularization of the Edges
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symmetrizing-the-network-gives-us-undirectedness">
     4.4.2.1. Symmetrizing the network gives us undirectedness
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ignoring-a-triangle-of-the-adjacency-matrix">
       4.4.2.1.1. Ignoring a “triangle* of the adjacency matrix
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#taking-a-function-of-the-two-values">
       4.4.2.1.2. Taking a function of the two values
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lowering-edge-bias">
     4.4.2.2. Lowering edge bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#thresholding-converts-weighted-networks-to-binary-networks">
     4.4.2.3. Thresholding converts weighted networks to binary networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparsification-removes-potentially-spurious-low-weight-edges">
     4.4.2.4. Sparsification removes potentially spurious low-weight edges
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diagonal-augmentation">
     4.4.2.5. Diagonal augmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edge-weight-normalization">
     4.4.2.6. Edge-weight normalization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#z-scoring-standardizes-edge-weights-using-the-normal-distribution">
       4.4.2.6.1.
       <span class="math notranslate nohighlight">
        \(z\)
       </span>
       -scoring standardizes edge weights using the normal distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ranking-edges-preserves-ordinal-relationships">
       4.4.2.6.2. Ranking edges preserves ordinal relationships
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#logging-reduces-magnitudinal-differences-between-edges">
       4.4.2.6.3. Logging reduces magnitudinal differences between edges
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="regularization">
<h1><span class="section-number">4.4. </span>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">¶</a></h1>
<p>In practice, many networks we will face in network machine learning will <em>not</em> be simple networks. As we discussed in the preceding discussion, many of the techniques we discuss will be just fine to use with weighted networks. Unfortunately, these real world networks are often extremely noisy, and it is for this reason that analysis of one real world network might not generalize very well to a similar real world network. For this reason, we turn to <em>regularization</em>. On <a class="reference external" href="#https://en.wikipedia.org/wiki/Regularization_(mathematics)">wikipedia</a>, <strong>regularization</strong> is defined as, “the process of adding information in order to solve an ill-posed problem or to prevent overfitting.” In network machine learning, what this usually will entail is modifying the network (or networks) themselves to allow better generalization of our statistical inference to new datasets. For each section, we’ll pose an example, a simulation, and code for how to implement the desired regularization approach. But it is important to realize that you might use several of these techniques simultaneously in practice, or you might have reasons to use these techniques that go outside of our working examples.</p>
<div class="section" id="regularization-of-the-nodes">
<h2><span class="section-number">4.4.1. </span>Regularization of the Nodes<a class="headerlink" href="#regularization-of-the-nodes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="degree-trimming-removes-nodes-with-low-degree">
<h3><span class="section-number">4.4.1.1. </span>Degree trimming removes nodes with low degree<a class="headerlink" href="#degree-trimming-removes-nodes-with-low-degree" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="regularization-of-the-edges">
<h2><span class="section-number">4.4.2. </span>Regularization of the Edges<a class="headerlink" href="#regularization-of-the-edges" title="Permalink to this headline">¶</a></h2>
<div class="section" id="symmetrizing-the-network-gives-us-undirectedness">
<h3><span class="section-number">4.4.2.1. </span>Symmetrizing the network gives us undirectedness<a class="headerlink" href="#symmetrizing-the-network-gives-us-undirectedness" title="Permalink to this headline">¶</a></h3>
<p>Let’s say we have two brain networks from humans and aliens. In this case, we’ll say there are six nodes, which are somehow shared across the two life forms. There is one node related to movement, one node related to hearing, one node related to higher-order thinking (like programming!), one node related taste, one node related to smell, and one node related to seeing. The edges <span class="math notranslate nohighlight">\((i, j)\)</span> here will represent the degree to which, if node <span class="math notranslate nohighlight">\(i\)</span> is stimulated, node <span class="math notranslate nohighlight">\(j\)</span> is stimulated as well. This network is weighted, and the edges take values between <span class="math notranslate nohighlight">\(-1\)</span> (if node <span class="math notranslate nohighlight">\(i\)</span> is stimulated, node <span class="math notranslate nohighlight">\(j\)</span> is definitely <em>not</em> stimulated) and <span class="math notranslate nohighlight">\(1\)</span> (if node <span class="math notranslate nohighlight">\(i\)</span> is stimulated, node <span class="math notranslate nohighlight">\(j\)</span> is definitely stimulated). A value of between the two would indicate that if node <span class="math notranslate nohighlight">\(i\)</span> is stimulated, node <span class="math notranslate nohighlight">\(j\)</span> might (or might not) be stimulated. Our goal is to measure the extent to which the two networks, <em>ignoring</em> asymmetries, are similar. Unfortunately, there is one big difference: the human network is <em>not</em> symmetric, but the alien network <em>is</em> symmetric! What this means is that in humans, node <span class="math notranslate nohighlight">\(i\)</span> being stimulated leading to node <span class="math notranslate nohighlight">\(j\)</span> does not necessarily mean that node <span class="math notranslate nohighlight">\(j\)</span> being stimulated leads to node <span class="math notranslate nohighlight">\(i\)</span> being stimulated. To accomplish this goal, we will <em>symmetrize</em> the human network. Let’s take a look at two example networks. Notice that the human network is not symmetric:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span>

<span class="n">wtargsh</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">10</span><span class="p">)],</span>
          <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">)]]</span>

<span class="n">wtargsa</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">dict</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mf">12.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">.5</span><span class="p">)],</span>
          <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">.5</span><span class="p">),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mf">35.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)]]</span>

<span class="c1"># human brain network</span>
<span class="n">Ah</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">wt</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">wtargs</span><span class="o">=</span><span class="n">wtargsh</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># alien brain network</span>
<span class="n">Aa</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">wt</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">wtargs</span><span class="o">=</span><span class="n">wtargsa</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_2_0.png" src="../../_images/regularization_2_0.png" />
</div>
</div>
<p>Remember that in a symmetric network, <span class="math notranslate nohighlight">\(a_{ij} = a_{ji}\)</span>, so in an <em>asymmetric</em> network, <span class="math notranslate nohighlight">\(a_{ij} \neq a_{ji}\)</span>. To symmetrize this asymmetric adjacency matrix, what we want is a <em>new</em> adjacency value, which we will call <span class="math notranslate nohighlight">\(w_{ij}\)</span>, which will be a function of <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\)</span>. Then, we will construct a new adjacency matrix <span class="math notranslate nohighlight">\(A'\)</span>, where each entry <span class="math notranslate nohighlight">\(a_{ij}'\)</span> <em>and</em> <span class="math notranslate nohighlight">\(a_{ji}'\)</span> are set equal to <span class="math notranslate nohighlight">\(w_{ij}\)</span>.  The little apostraphe just signifies that this is a potentially different value than either <span class="math notranslate nohighlight">\(a_{ij}\)</span> or <span class="math notranslate nohighlight">\(a_{ji}\)</span>. Note that by construction, <span class="math notranslate nohighlight">\(A'\)</span> is in fact symmetric, becaues <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span> due to how we built <span class="math notranslate nohighlight">\(A'\)</span>.</p>
<div class="section" id="ignoring-a-triangle-of-the-adjacency-matrix">
<h4><span class="section-number">4.4.2.1.1. </span>Ignoring a “triangle* of the adjacency matrix<a class="headerlink" href="#ignoring-a-triangle-of-the-adjacency-matrix" title="Permalink to this headline">¶</a></h4>
<p>The easiest way to symmetrize a network <span class="math notranslate nohighlight">\(A\)</span> is to just ignore part of it entirely. In the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, you will remember that we have an upper and a lower triangular part of the matrix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A &amp;= \begin{bmatrix}
        a_{11} &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{blue}{a_{21}} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{blue}{\vdots} &amp;\color{blue}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{blue}{a_{n1}} &amp; \color{blue}{...} &amp; \color{blue}{a_{n,n-1}} &amp; a_{nn}
    \end{bmatrix},
\end{align*}\]</div>
<p>The entries which are listed in <font color="red">red</font> are called the <strong>upper right triangle of the adjacency matrix above the diagonal</strong>. You will notice that for the entries <span class="math notranslate nohighlight">\(a_{ij}\)</span> in the upper right triangle of the adjacency matrix, that <span class="math notranslate nohighlight">\(a_{ij}\)</span> is such that <span class="math notranslate nohighlight">\(j\)</span> is <em>always</em> greater than <span class="math notranslate nohighlight">\(i\)</span>. Similarly, the entries which are listed in <font color="blue">blue</font> are called the <strong>lower left triangle of the adjacency matrix below the diagonal</strong>. In the lower left triangle, <span class="math notranslate nohighlight">\(i\)</span> is <em>always</em> greater than <span class="math notranslate nohighlight">\(j\)</span>. These are called <em>triangles</em> because of the shape they make when you look at them in matrix form: notice, for instance, that in the upper right triangle, we have a triangle with three corners of values: <span class="math notranslate nohighlight">\(a_{12}\)</span>, <span class="math notranslate nohighlight">\(a_{1n}\)</span>, and <span class="math notranslate nohighlight">\(a_{n-1, n}\)</span>.</p>
<p>So, how do we ignore a triangle all-together? Well, it’s really quite simple! We will visually show how to ignore the lower left triangle of the adjacency matrix. We start by forming a triangle matrix, <span class="math notranslate nohighlight">\(\Delta\)</span>, as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Delta &amp;= \begin{bmatrix}
        0 &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{blue}{0} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{blue}{\vdots} &amp;\color{blue}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{blue}{0} &amp; \color{blue}{...} &amp; \color{blue}{0} &amp; 0
    \end{bmatrix},
\end{align*}\]</div>
<p>Notice that this matrix <em>keeps</em> lal of the upper right triangle of the adjacency matrix above the diagonal the same as in the matrix <span class="math notranslate nohighlight">\(A\)</span>, but replaces the lower right triangle of the adjacency matrix below the diagonal and the diagonal with <span class="math notranslate nohighlight">\(0\)</span>s. Notice that the transpose of <span class="math notranslate nohighlight">\(\Delta\)</span> is the matrix:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Delta^\top &amp;= \begin{bmatrix}
        0 &amp; \color{blue}{0} &amp; \color{blue}{...} &amp;\color{blue}{0}\\
        \color{red}{a_{12}}&amp; \ddots &amp; \color{blue}{\ddots} &amp; \color{blue}{\vdots} \\
        \color{red}{\vdots}&amp;\color{red}{\ddots} &amp; \ddots &amp; \color{blue}{0} \\
        \color{red}{a_{1n}}&amp;\color{red}{...} &amp;\color{red}{a_{n-1,n}} &amp; 0
    \end{bmatrix}
\end{align*}\]</div>
<p>So when we add the two together, we get this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \Delta + \Delta^\top &amp;= \begin{bmatrix}
        0 &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{red}{a_{12}} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{red}{\vdots}&amp;\color{red}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{red}{a_{1n}}&amp;\color{red}{...} &amp;\color{red}{a_{n-1,n}} &amp; 0
    \end{bmatrix},
\end{align*}\]</div>
<p>We’re almost there! We just need to add back the diagonal of <span class="math notranslate nohighlight">\(A\)</span>, which we will do using the matrix <span class="math notranslate nohighlight">\(diag(A)\)</span> which has values <span class="math notranslate nohighlight">\(diag(A)_{ii} = a_{ii}\)</span>, and <span class="math notranslate nohighlight">\(diag(A)_{ij} = 0\)</span> for any <span class="math notranslate nohighlight">\(i \neq j\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A' &amp;= \Delta + \Delta^\top + diag(A) = \begin{bmatrix}
        a_{11} &amp; \color{red}{a_{12}} &amp; \color{red}{...} &amp; \color{red}{a_{1n}} \\
        \color{red}{a_{12}} &amp; \ddots &amp; \color{red}{\ddots} &amp; \color{red}{\vdots} \\
        \color{red}{\vdots}&amp;\color{red}{\ddots} &amp;\ddots &amp; \color{red}{a_{n-1, n}}\\
        \color{red}{a_{1n}}&amp;\color{red}{...} &amp;\color{red}{a_{n-1,n}} &amp; a_{nn}
    \end{bmatrix},
\end{align*}\]</div>
<p>Which leaves <span class="math notranslate nohighlight">\(A'\)</span> to be a matrix consisting <em>only</em> of entries which were in the upper right triangle of <span class="math notranslate nohighlight">\(A\)</span>. <span class="math notranslate nohighlight">\(A'\)</span> is obviously symmetric, because <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Since the adjacency matrix is symmetric, the network <span class="math notranslate nohighlight">\(A'\)</span> represents is therefore undirected. In graspologic, we can implement this as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">symmetrize</span>

<span class="c1"># symmetrize with upper right triangle</span>
<span class="n">Ah_upright_sym</span> <span class="o">=</span> <span class="n">symmetrize</span><span class="p">(</span><span class="n">Ah</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;triu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Asymmetric&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah_upright_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Upper-Right Symmetrized&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_5_0.png" src="../../_images/regularization_5_0.png" />
</div>
</div>
<p>Likewise, we can lower-left symmetrize as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># symmetrize with lower left triangle</span>
<span class="n">Ah_lowleft_sym</span> <span class="o">=</span> <span class="n">symmetrize</span><span class="p">(</span><span class="n">Ah</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;tril&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Asymmetric&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah_lowleft_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Lower-Left Symmetrized&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_8_0.png" src="../../_images/regularization_8_0.png" />
</div>
</div>
</div>
<div class="section" id="taking-a-function-of-the-two-values">
<h4><span class="section-number">4.4.2.1.2. </span>Taking a function of the two values<a class="headerlink" href="#taking-a-function-of-the-two-values" title="Permalink to this headline">¶</a></h4>
<p>There are many ways we can also take a function of <span class="math notranslate nohighlight">\(a_{ij}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\)</span> to end up with a symmetric matrix. One such is we can just average the two. That is, we can let the matrix <span class="math notranslate nohighlight">\(A'\)</span> be the matrix with entries <span class="math notranslate nohighlight">\(a'_{ij} = \frac{a_{ij} + a_{ji}}{2}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. In matrix form, this operation looks like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A' &amp;= \frac{1}{2} (A + A^\top) \\
    &amp;= \frac{1}{2}\left(\begin{bmatrix}
        a_{11} &amp; ... &amp; a_{1n} \\
        \vdots &amp; \ddots &amp; \vdots \\
        a_{n1} &amp; ... &amp; a_{nn}
    \end{bmatrix} + \begin{bmatrix}
        a_{11} &amp; ... &amp; a_{n1} \\
        \vdots &amp; \ddots &amp; \vdots \\
        a_{1n} &amp; ... &amp; a_{nn}
    \end{bmatrix}\right)\\
    &amp;= \begin{bmatrix}
        \frac{1}{2}(a_{11} + a_{11}) &amp; ... &amp; \frac{1}{2}(a_{1n} + a_{n1}) \\
        \vdots &amp; \ddots &amp; \vdots \\
        \frac{1}{2} (a_{n1} + a_{1n}) &amp; ... &amp; \frac{1}{2}(a_{nn} + a_{nn})
    \end{bmatrix} \\
    &amp;= \begin{bmatrix}
        a_{11} &amp; ... &amp; \frac{1}{2}(a_{1n} + a_{n1}) \\
        \vdots &amp; \ddots &amp; \vdots \\
        \frac{1}{2} (a_{n1} + a_{1n}) &amp; ... &amp; a_{nn}
    \end{bmatrix}
\end{align*}\]</div>
<p>As we can see, for all of the entries, <span class="math notranslate nohighlight">\(a'_{ij} = \frac{1}{2} (a_{ij} + a_{ji})\)</span>, and also <span class="math notranslate nohighlight">\(a_{ji}' = \frac{1}{2}(a_{ji} + a_{ij})\)</span>. These quantities are the same, so <span class="math notranslate nohighlight">\(a_{ij}' = a_{ji}'\)</span>, and <span class="math notranslate nohighlight">\(A'\)</span> is symmetric. As the adjacency matrix is symmetric, the network that <span class="math notranslate nohighlight">\(A'\)</span> represents is undirected. We can implement this in graspologic as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># symmetrize with averaging</span>
<span class="n">Ah_avg_sym</span> <span class="o">=</span> <span class="n">symmetrize</span><span class="p">(</span><span class="n">Ah</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;avg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Asymmetric&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah_avg_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Symmetrized by Averaging&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_11_0.png" src="../../_images/regularization_11_0.png" />
</div>
</div>
<p>We will operate on the human brain newtork symmetrized by averaging going forward.</p>
</div>
</div>
<div class="section" id="lowering-edge-bias">
<h3><span class="section-number">4.4.2.2. </span>Lowering edge bias<a class="headerlink" href="#lowering-edge-bias" title="Permalink to this headline">¶</a></h3>
<p>As you are probably aware, in all of machine learning, we are always concerned with the <em>bias/variance tradeoff</em>. The <strong>bias/variance tradeoff</strong> is an unfortunate side-effect that concerns how well a learning technique will generalize to new datasets. Whereas a low bias model might reasonably fit the nuances of our training data, it might in fact overfit the training data and model spurious noise, which raises the variance. Whereas aa lower variance model might be better suited to the situation when the data we expect to see is noisy, it might not as faithfully represent the underlying dynamics we think the network possesses. Here, we show severaal strategies to reduce the bias due to edge weight noise in network machine learning.</p>
</div>
<div class="section" id="thresholding-converts-weighted-networks-to-binary-networks">
<h3><span class="section-number">4.4.2.3. </span>Thresholding converts weighted networks to binary networks<a class="headerlink" href="#thresholding-converts-weighted-networks-to-binary-networks" title="Permalink to this headline">¶</a></h3>
<p>The simplest way to reduce edge bias is the process of <em>thresholding</em>. Through thresholding, we choose a threshold value, <span class="math notranslate nohighlight">\(t\)</span>. Next, we simply set all of the entries of the adjacency matrix less than or equal to <span class="math notranslate nohighlight">\(t\)</span> to zero, and the entries of the adjacency matrix above <span class="math notranslate nohighlight">\(t\)</span> to one.</p>
<p>Some of the most to choosing this threshold are:</p>
<ol class="simple">
<li><p>Set the threshhold to be zero: set all non-zero weighted entries to one, and all zero-weight entries to zero. This is most commonly used when we see zero-inflated networks, or networks where the adjacency matrix takes values that are either zero or some quantity different from one,</p></li>
<li><p>Set te threshold to be the mean: set all values below the mean edge-weight to zero, and all values above the mean edge-weight to one,</p></li>
<li><p>Pick a percentile: the most common appproach is to choose a particular percentile, and divide it by <span class="math notranslate nohighlight">\(100\)</span> to obtain the <strong>quantile</strong>. Set the edge weights below this quantile to zero and above this quantile to one. This approach has the nice property that if we are performing inference on multiple networks whosse density exceeds the percentile we pick, it preserves the <em>network density</em> between them.</p></li>
</ol>
<p>We will show how to use the percentile approach to binarization, with both our human and alien brains. We will threshold using the edge-weight in the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile, which means that we will take the threshold to be the average of the two edge-weights in the middle and set all edges below this threshold to zero, and above this threshold to one. Note that how you perform the thresholding procedure might be dependent on properties of your network. Our example networks of humans and aliens were loopless, as you could see above. Remember as we learned in the preceding section, that if the network itself is loopless, the diagonal entries simply <em>do not exist</em>; <span class="math notranslate nohighlight">\(0\)</span> is simply a commonly used placeholder. For this reason, when we compute the mean edge-weight or percentiles of edge-weights, we need to exclude the diagonal. We begin by using this procedure on the human brain:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">quantile</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">eye</span>

<span class="n">p</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># the percentile we want</span>

<span class="c1"># the entries of the adjacency matrix that are not on the diagonal</span>
<span class="n">non_diag_idx</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">eye</span><span class="p">(</span><span class="n">Ah_avg_sym</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
<span class="c1"># use the quantile function to obtain the threshold</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">quantile</span><span class="p">(</span><span class="n">Ah_avg_sym</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="n">p</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># quantile = percentile / 100</span>
<span class="n">Ah_thresh</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">Ah_avg_sym</span><span class="p">)</span>  <span class="c1"># copy the network over</span>

<span class="c1"># threshold using t</span>
<span class="n">Ah_thresh</span><span class="p">[</span><span class="n">Ah_thresh</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Ah_thresh</span><span class="p">[</span><span class="n">Ah_thresh</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah_avg_sym</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Weighted&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Ah_thresh</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Human Brain Network, Thresholded at 50-ptile&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_15_0.png" src="../../_images/regularization_15_0.png" />
</div>
</div>
<p>Likewise, we can do the same thing for the alien brain:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">quantile</span><span class="p">,</span> <span class="n">copy</span>

<span class="n">p</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># the percentile we want</span>

<span class="c1"># use the quantile function to obtain the threshold</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">quantile</span><span class="p">(</span><span class="n">Aa</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="n">p</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># quantile = percentile / 100</span>
<span class="n">Aa_thresh</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">Aa</span><span class="p">)</span>  <span class="c1"># copy the network over</span>

<span class="c1"># threshold using t</span>
<span class="n">Aa_thresh</span><span class="p">[</span><span class="n">Aa_thresh</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">Aa_thresh</span><span class="p">[</span><span class="n">Aa_thresh</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network, Weighted&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa_thresh</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network, Thresholded at 50-ptile&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_18_0.png" src="../../_images/regularization_18_0.png" />
</div>
</div>
<p>Since the human and alien brain networks are now undirected, unweighted, and loopless, we have turned them into simple networks. Remember that the network density for a simple network was defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    density(A) &amp;= \frac{2\sum_{j &gt; i}a_{ij}}{n(n - 1)}.
\end{align*}\]</div>
<p>Since we have thresholded at the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile for both the human and alien networks, then note that <em>exactly</em> <span class="math notranslate nohighlight">\(50\)</span> percent of the possible edges will exist, and <span class="math notranslate nohighlight">\(50\)</span> percent of the possible edges will not exist. Remembering that the number of possible edges was <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span> for an undirected network, this means that <span class="math notranslate nohighlight">\(\sum_{j &gt; i}a_{ij}\)</span> must be half of <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span>, or <span class="math notranslate nohighlight">\(\frac{1}{4}n(n - 1)\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    density(A^{(h)}) = density(A^{(a)}) &amp;= \frac{2\sum_{j &gt; i}a_{ij}}{n(n - 1)}, \\
    &amp;= \frac{2\cdot \frac{1}{4}n(n - 1)}{n(n - 1)},\;\;\;\sum_{j &gt; i}a_{ij} = \frac{1}{4}n(n - 1) \\
    &amp;= 0.5.
\end{align*}\]</div>
<p>So if we threshold two networks at the same percentile, we end up with networks of equal density, and that density is equal to the fraction which is equivalent to the percentile we chose.</p>
<p>Note that a common pitfall you might run into with thresholding (and other sparsification approaches) that rely on percentiles occurs when a weighted network can only take non-negative edge-weights. This corresponds to a network with an adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> where every <span class="math notranslate nohighlight">\(a_{ij}\)</span> is greater than or equal to <span class="math notranslate nohighlight">\(0\)</span>. In this case, one must be careful to choose a threshold which is not zero. Let’s consider a network were <span class="math notranslate nohighlight">\(60\%\)</span> of the entries are zeros, and <span class="math notranslate nohighlight">\(40\%\)</span> of the entries take a random value between <span class="math notranslate nohighlight">\(5\)</span> and <span class="math notranslate nohighlight">\(10\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_nm</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">uniform</span>

<span class="c1"># 10 nodes</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># total number of edges is 40% of the number of possible edges</span>
<span class="c1"># 1/2 * n * (n-1)</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.4</span><span class="o">*</span><span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">wt</span><span class="o">=</span><span class="n">uniform</span><span class="p">,</span> <span class="n">wtargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_21_0.png" src="../../_images/regularization_21_0.png" />
</div>
</div>
<p>If we threshold at the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile, since <span class="math notranslate nohighlight">\(60\)</span> percent of the edges do not exist, then the <span class="math notranslate nohighlight">\(50^{th}\)</span> percentile is still just zero:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use the quantile function to obtain the threshold</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">quantile</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="n">p</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># quantile = percentile / 100</span>
<span class="n">A_thresh</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>  <span class="c1"># copy the network over</span>

<span class="c1"># threshold using t</span>
<span class="n">A_thresh</span><span class="p">[</span><span class="n">A_thresh</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">A_thresh</span><span class="p">[</span><span class="n">A_thresh</span> <span class="o">&gt;</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Threshold for 50th percentile: </span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Threshold for 50th percentile: 0
</pre></div>
</div>
</div>
</div>
<p>And we don’t actually end up with a network having a density of <span class="math notranslate nohighlight">\(0.5\)</span>, but rather, the same as the fraction of non-zero edges in the original network (which was <span class="math notranslate nohighlight">\(40\%\)</span>, or <span class="math notranslate nohighlight">\(0.4\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">triu</span>
<span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">is_unweighted</span><span class="p">,</span> <span class="n">is_loopless</span><span class="p">,</span> <span class="n">is_symmetric</span>

<span class="k">def</span> <span class="nf">simple_network_dens</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># make sure the network is simple</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">is_unweighted</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">is_loopless</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">is_symmetric</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Network is not simple!&quot;</span><span class="p">)</span>
    <span class="c1"># count the non-zero entries in the upper-right triangle</span>
    <span class="c1"># for a simple network X</span>
    <span class="n">nnz</span> <span class="o">=</span> <span class="n">triu</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="c1"># number of nodes</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># number of possible edges is 1/2 * n * (n-1)</span>
    <span class="n">poss_edges</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nnz</span><span class="o">/</span><span class="n">poss_edges</span>

<span class="n">dens</span> <span class="o">=</span> <span class="n">simple_network_dens</span><span class="p">(</span><span class="n">A_thresh</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A_thresh</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;50 ptile thresholded network; dens = </span><span class="si">{:.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dens</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_26_0.png" src="../../_images/regularization_26_0.png" />
</div>
</div>
<p>So the take-home message is that we need to be careful that if we want to conclude that two percentile-thresholded networks have the same network density (equal to the percentile we thresholded at) it is critical that we have enough non-zero entries to threshold with across both (or all) of the networks.</p>
</div>
<div class="section" id="sparsification-removes-potentially-spurious-low-weight-edges">
<h3><span class="section-number">4.4.2.4. </span>Sparsification removes potentially spurious low-weight edges<a class="headerlink" href="#sparsification-removes-potentially-spurious-low-weight-edges" title="Permalink to this headline">¶</a></h3>
<p>The next simplest edge-weight regularization technique is called <em>sparsificiation</em>. Remember that our alien brain looked like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_28_0.png" src="../../_images/regularization_28_0.png" />
</div>
</div>
<p>Notice that for a <em>lot</em> of the off-diagonal entries, many of the values are really tiny compared to the maximum value in the network which is almost <span class="math notranslate nohighlight">\(40\)</span>. What if the way we measured these edges was very sensitive to high values, but had trouble discerning whether a value was actually zero, or was just really small?</p>
<p>For this particular situation, we turn to <em>sparsification</em>. Through sparsification, we proceed very similar to thresholding like we did above. Remember that we chose a threshold, <span class="math notranslate nohighlight">\(t\)</span>, and first set all adjacency values less than or equal to <span class="math notranslate nohighlight">\(t\)</span> to zero. Now, we’re done! We simply skip the step of setting values greater than <span class="math notranslate nohighlight">\(t\)</span> to one. Let’s try an example where we take the alien network, and sparsify the network using the <span class="math notranslate nohighlight">\(70^{th}\)</span> percentile. Note that this will lead to the smallest <span class="math notranslate nohighlight">\(70\)</span> percent of edges to take the value of zero, and the largest <span class="math notranslate nohighlight">\(30\)</span> percent of edges will keep their original edge-weights:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">70</span>  <span class="c1"># the percentile to sparsify with</span>

<span class="c1"># use the quantile function to obtain the threshold</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">quantile</span><span class="p">(</span><span class="n">Aa</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="n">p</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># quantile = percentile / 100</span>
<span class="n">Aa_sparse</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">Aa</span><span class="p">)</span>  <span class="c1"># copy the network over</span>

<span class="c1"># sparsify using t</span>
<span class="n">Aa_sparse</span><span class="p">[</span><span class="n">Aa_sparse</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa_sparse</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network, Sparsified at 70th ptile&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_31_0.png" src="../../_images/regularization_31_0.png" />
</div>
</div>
<p>Notice that many of the small entries in the off-diagonal areas now have a value of zero. Again, we have the same pitfalls for sparsification as we did with thresholding, where if the network takes only non-negative edge weights and the percentile we choose corresponds to a threshold of zero, we might not actually end up changing anything.</p>
</div>
<div class="section" id="diagonal-augmentation">
<h3><span class="section-number">4.4.2.5. </span>Diagonal augmentation<a class="headerlink" href="#diagonal-augmentation" title="Permalink to this headline">¶</a></h3>
<p>https://github.com/microsoft/graspologic/blob/f9c4353488e29d367dec62fdb4729e6a7344fd89/graspologic/embed/ase.py#L58</p>
</div>
<div class="section" id="edge-weight-normalization">
<h3><span class="section-number">4.4.2.6. </span>Edge-weight normalization<a class="headerlink" href="#edge-weight-normalization" title="Permalink to this headline">¶</a></h3>
<p>With weighted networks, it is often the case that we might want to reshape the distributions of edge-weights in our networks to highlight particular properties. Notice that the edge-weights for our human networks take values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, but for our alien network take values between <span class="math notranslate nohighlight">\(0\)</span> and almost <span class="math notranslate nohighlight">\(40\)</span>. How can we possibly compare between these two networks when the edge-weights take such different ranges of values? We turn to standardization, which allows us to place values from different networks on the same scale.</p>
<div class="section" id="z-scoring-standardizes-edge-weights-using-the-normal-distribution">
<h4><span class="section-number">4.4.2.6.1. </span><span class="math notranslate nohighlight">\(z\)</span>-scoring standardizes edge weights using the normal distribution<a class="headerlink" href="#z-scoring-standardizes-edge-weights-using-the-normal-distribution" title="Permalink to this headline">¶</a></h4>
<p>The first approach to edge-weight standardization is known commonly as <span class="math notranslate nohighlight">\(z\)</span>-scoring. Suppose that <span class="math notranslate nohighlight">\(A\)</span> is the adjacency matrix, with entries <span class="math notranslate nohighlight">\(a_{ij}\)</span>. With a <span class="math notranslate nohighlight">\(z\)</span>-score, we will rescale the weights of the adjacency matrix, such that the new edge-weights (called <span class="math notranslate nohighlight">\(z\)</span>-scores) are approximately normally distributed. The reason this can be useful is that the normal distribution is pretty ubiquitous across many branches of science, and therefore, a <span class="math notranslate nohighlight">\(z\)</span>-score is relatively easy to communicate with other scientists. Further, many things that exist in nature can be well-approximated by a normal distribution, so it seems like a reasonable place to start to use a <span class="math notranslate nohighlight">\(z\)</span>-score for edge-weights, too! The <span class="math notranslate nohighlight">\(z\)</span>-score is defined as follows. We will construct the <span class="math notranslate nohighlight">\(z\)</span>-scored adjacency matrix <span class="math notranslate nohighlight">\(Z\)</span>, whose entries <span class="math notranslate nohighlight">\(z_{ij}\)</span> are the corresponding <span class="math notranslate nohighlight">\(z\)</span>-scores of the adjacency matrix’s entries <span class="math notranslate nohighlight">\(a_{ij}\)</span>. For a weighted, loopless network, we use an estimate of the <em>mean</em>, <span class="math notranslate nohighlight">\(\hat \mu\)</span>, and the <em>unbiased</em> estimate of the <em>variance</em>, <span class="math notranslate nohighlight">\(\hat \sigma^2\)</span>), which can be computed as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat\mu &amp;= \frac{1}{n}\sum_{i \neq j}a_{ij},\\
    \hat\sigma^2 &amp;= \frac{1}{n - 1}\sum_{i \neq j} (a_{ij} - \hat\mu)^2.
\end{align*}\]</div>
<p>The <span class="math notranslate nohighlight">\(z\)</span>-score for the <span class="math notranslate nohighlight">\((i,j)\)</span> entry is simply the quantity:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    z_{ij} &amp;= \frac{a_{ij} - \hat\mu}{\hat\sigma}
\end{align*}\]</div>
<p>Since our network is loopless, notice that these sums are for all <em>non-diagonal</em> entries where <span class="math notranslate nohighlight">\(i \neq j\)</span>. If the network were not loopless, we would include diagonal entries in the calculation, and instead would sum over all possible combinations of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. the interpretation of the <span class="math notranslate nohighlight">\(z\)</span>-score <span class="math notranslate nohighlight">\(z_{ij}\)</span> is the <em>number of stadard deviations</em> that the entry <span class="math notranslate nohighlight">\(a_{ij}\)</span> is from the mean, <span class="math notranslate nohighlight">\(\hat \mu\)</span>.</p>
<p>We will demonstrate on the network where <span class="math notranslate nohighlight">\(40\%\)</span> of the entries took a value between <span class="math notranslate nohighlight">\(5\)</span> and <span class="math notranslate nohighlight">\(10\)</span>, and the other <span class="math notranslate nohighlight">\(60\%\)</span> took a value of zero. We can implement <span class="math notranslate nohighlight">\(z\)</span>-scoring as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">zscore</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">zeros</span>

<span class="k">def</span> <span class="nf">z_score_loopless</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_loopless</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The network has loops!&quot;</span><span class="p">)</span>
    <span class="c1"># the entries of the adjacency matrix that are not on the diagonal</span>
    <span class="n">non_diag_idx</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">zscore</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">non_diag_idx</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">Z</span>

<span class="n">ZA</span> <span class="o">=</span> <span class="n">z_score_loopless</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">ZA</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, After Z-score&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_34_0.png" src="../../_images/regularization_34_0.png" />
</div>
</div>
<p>Next, we will look at the edge-weight histogram for the alien brain network before and after <span class="math notranslate nohighlight">\(z\)</span>-scoring. An edge-weight histogram is a histogram which indicates the number of edges with weights falling in between a pair of values. Remember that the network is loopless, so again we exclude the diagonal entries:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">seaborn</span> <span class="kn">import</span> <span class="n">histplot</span>

<span class="n">non_diag_idx</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">eye</span><span class="p">(</span><span class="n">Aa</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, Before Z-score&quot;</span><span class="p">);</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">ZA</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Z-score&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, After Z-score&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_36_0.png" src="../../_images/regularization_36_0.png" />
</div>
</div>
<p>The theory for when, and why, to use <span class="math notranslate nohighlight">\(z\)</span>-scoring for network machine learning tends to go something like this: many things tend to be normally distributed with the same mean and variance, so perhaps that is a reasonable expectation for our network, too. Unfortunately, we find this often to <em>not</em> be the case. In fact, we often find that the specific distribution of edge weights itself often might be lamost infeasible to identify in a population of networks, and therefore <em>almost</em> irrelevant all-together. To this end, we turn to instead <em>ranking</em> the edges.</p>
</div>
<div class="section" id="ranking-edges-preserves-ordinal-relationships">
<h4><span class="section-number">4.4.2.6.2. </span>Ranking edges preserves ordinal relationships<a class="headerlink" href="#ranking-edges-preserves-ordinal-relationships" title="Permalink to this headline">¶</a></h4>
<p>The idea behind ranking is as follows. We don’t really know much useful information as to how the distribution of edge weights varies between a given pair of networks. For this reason, we want to virtually eliminate the impact of that distribution <em>almost</em> entirely. However, we know that if one edge-weight is larger than another edge-weight, that we do in fact trust that relationship. What this means is that we want something which preserves <em>ordinal</em> relationships in our edge-weights, but ignores other properties of the edge-weights. An ordinal relationship just means that we have a natural ordering to the edge-weights. This means that we can identify a largest edge-weight, a smallest edge-weight, and every position in between. When we want to preserve ordinal relationships in our network, we do something called <em>passing the non-zero edge-weights to ranks</em>. We will often use the abbreviation <code class="docutils literal notranslate"><span class="pre">ptr</span></code> to define this function because it is so useful for weighted networks. We pass non-zero edge-weights to ranks as follows:</p>
<ol class="simple">
<li><p>Identify all of the non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>Count the number of non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(n_{nz}\)</span>.</p></li>
<li><p>Rank all of the non-zero edges in the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, where for a non-zero entry <span class="math notranslate nohighlight">\(a_{ij}\)</span>, <span class="math notranslate nohighlight">\(rank(a_{ij}) = 1\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the smallest non-zero edge-weight, and <span class="math notranslate nohighlight">\(rank(a_{ij}) = n_{nz}\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the largest edge-weight. Ties are settled by using the average rank of the two entries.</p></li>
<li><p>Report the weight of each non-zero entry <span class="math notranslate nohighlight">\((i,j)\)</span> as <span class="math notranslate nohighlight">\(r_{ij} = \frac{rank(a_{ij})}{n_{nz} + 1}\)</span>, and for eachh zero entry as <span class="math notranslate nohighlight">\(r_{ij} = 0\)</span>.</p></li>
</ol>
<p>Below, we pass-to-ranks for the network with <span class="math notranslate nohighlight">\(40\%\)</span> of the possible edges taking a value between <span class="math notranslate nohighlight">\(5\)</span> and <span class="math notranslate nohighlight">\(10\)</span>, and the other <span class="math notranslate nohighlight">\(60\%\)</span> taking a value of <span class="math notranslate nohighlight">\(0\)</span> using <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">pass_to_ranks</span>

<span class="n">RA</span> <span class="o">=</span> <span class="n">pass_to_ranks</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">RA</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, After PTR&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_39_0.png" src="../../_images/regularization_39_0.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, Before PTR&quot;</span><span class="p">);</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">RA</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">binrange</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;normalized rank&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, After PTR&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_40_0.png" src="../../_images/regularization_40_0.png" />
</div>
</div>
<p>The edge-weights for the adjacency matrix <span class="math notranslate nohighlight">\(R\)</span> after <code class="docutils literal notranslate"><span class="pre">ptr</span></code> has the interpretataion that each entry <span class="math notranslate nohighlight">\(r_{ij}\)</span> is the <em>quantile</em> of that entry amongst the other non-zero entries. This is unique in that it is completely <em>distribution-free</em>, which means that we don’t need to assume anything about the distribution of the edge-weights to have a reasonably interpretable quantity. On the other hand, the <span class="math notranslate nohighlight">\(z\)</span>-score had the interpretation of the number of standard deviations from the mean, which is only a sensible quantity to compare if we assume the population of edge-weights are normally distributed.</p>
<p>Another useful quantity related to pass-to-ranks is known as the zero-boosted pass-to-ranks. Zero-boosted pass-to-ranks is conducted as follows:</p>
<ol class="simple">
<li><p>Identify all of the non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p>Count the number of non-zero entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(n_{nz}\)</span>, <em>and</em> the number of zero-entries of the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(n_z\)</span>. Note that since the values of the adjacency matrix are either zero or non-zero, that <span class="math notranslate nohighlight">\(n_{nz} + n_z = n^2\)</span>, as <span class="math notranslate nohighlight">\(A\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix and therefore has <span class="math notranslate nohighlight">\(n^2\)</span> total entries.</p></li>
<li><p>Rank all of the non-zero edges in the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, where for a non-zero entry <span class="math notranslate nohighlight">\(a_{ij}\)</span>, <span class="math notranslate nohighlight">\(rank(a_{ij}) = 1\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the smallest non-zero edge-weight, and <span class="math notranslate nohighlight">\(rank(a_{ij}) = n_{nz}\)</span> if <span class="math notranslate nohighlight">\(a_{ij}\)</span> is the largest edge-weight. Ties are settled by using the average rank of the two entries.</p></li>
<li><p>Report the weight of each non-zero entry <span class="math notranslate nohighlight">\((i,j)\)</span> as <span class="math notranslate nohighlight">\(r_{ij}' = \frac{n_z + rank(a_{ij})}{n^2 + 1}\)</span>, and for each zero entry as <span class="math notranslate nohighlight">\(r_{ij}' = 0\)</span>.</p></li>
</ol>
<p>The edge-weights for the adjacency matrix <span class="math notranslate nohighlight">\(R'\)</span> after zero-boosted <code class="docutils literal notranslate"><span class="pre">ptr</span></code> have the interpretation that each entry <span class="math notranslate nohighlight">\(r_{ij}'\)</span> is the quantile of that entry amongst <em>all</em> of the entries. Let’s instead use zero-boosted <code class="docutils literal notranslate"><span class="pre">ptr</span></code> on our network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">RAzb</span> <span class="o">=</span> <span class="n">pass_to_ranks</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;zero-boost&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, Before zb-PTR&quot;</span><span class="p">);</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">RAzb</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">binrange</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;normalized rank&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Network with 40</span><span class="si">% o</span><span class="s2">f possible edges, After zb-PTR&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_43_0.png" src="../../_images/regularization_43_0.png" />
</div>
</div>
</div>
<div class="section" id="logging-reduces-magnitudinal-differences-between-edges">
<h4><span class="section-number">4.4.2.6.3. </span>Logging reduces magnitudinal differences between edges<a class="headerlink" href="#logging-reduces-magnitudinal-differences-between-edges" title="Permalink to this headline">¶</a></h4>
<p>When we look at the distribution of non-zero edge-weights for the alien brain network, we notice a weird behavior, known as a <em>right-skew</em>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">Aa</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Edge Weight&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_46_0.png" src="../../_images/regularization_46_0.png" />
</div>
</div>
<p>Notice that <em>most</em> of the edges havae weights which are comparatively small, between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(10\)</span>, but some of the edges have weights which are much (much) larger. A <strong>right-skew</strong> exists when the majority of edge-weights are small, but some of the edge-weights take values which are much larger.</p>
<p>What if we want to make these large values more similar in relation to the smaller values, but we simultaneously want to preserve properties of the underlying distribution of the edge-weights? Well, we can’t use <code class="docutils literal notranslate"><span class="pre">ptr</span></code>, because <code class="docutils literal notranslate"><span class="pre">ptr</span></code> will throw away all of the information about the edge-weight distribution other than the ordinal relationship between pairs of edges. To do this, we instead turn to the logarithm function. The logarithm function <span class="math notranslate nohighlight">\(log_{10}(x)\)</span> is defined for positive values <span class="math notranslate nohighlight">\(x\)</span> as the value <span class="math notranslate nohighlight">\(c_x\)</span> where <span class="math notranslate nohighlight">\(x = 10^{c_x}\)</span>. In this sense, it is the “number of powers of ten” to obtain the value <span class="math notranslate nohighlight">\(x\)</span>. You will notice that the logarithm function looks like this:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">logxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">logxs</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">(x)$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(
</pre></div>
</div>
<img alt="../../_images/regularization_48_1.png" src="../../_images/regularization_48_1.png" />
</div>
</div>
<p>What is key to noice about this function is that, as <span class="math notranslate nohighlight">\(x\)</span> increases, the log of <span class="math notranslate nohighlight">\(x\)</span> increases by a <em>decreasing</em> amount. Let’s imagine we have three values, <span class="math notranslate nohighlight">\(x = .001\)</span>, <span class="math notranslate nohighlight">\(y = .1\)</span>, and <span class="math notranslate nohighlight">\(z = 10\)</span>. A calculator will give you that <span class="math notranslate nohighlight">\(log_{10}(x) = -3, log_{10}(y) = -1\)</span>, and <span class="math notranslate nohighlight">\(log_{10}(z) = 1\)</span>. Even though <span class="math notranslate nohighlight">\(y\)</span> is only <span class="math notranslate nohighlight">\(.099\)</span> units bigger than <span class="math notranslate nohighlight">\(x\)</span>, its logarithm <span class="math notranslate nohighlight">\(log_{10}(y)\)</span> exceeds <span class="math notranslate nohighlight">\(log_{10}(x)\)</span> by two units. on the other hand, <span class="math notranslate nohighlight">\(z\)</span> is <span class="math notranslate nohighlight">\(9.9\)</span> units bigger than <span class="math notranslate nohighlight">\(y\)</span>, but yet its logarithm <span class="math notranslate nohighlight">\(log_{10}(z)\)</span> is still the same two units bigger than <span class="math notranslate nohighlight">\(log_{10}(y)\)</span>. This is because thhe logarithm is instead looking at the fact that <span class="math notranslate nohighlight">\(z\)</span> is <em>one</em> power of ten, <span class="math notranslate nohighlight">\(y\)</span> is <span class="math notranslate nohighlight">\(-1\)</span> powers of ten, and <span class="math notranslate nohighlight">\(z\)</span> is <span class="math notranslate nohighlight">\(-3\)</span> powers of ten. The logarithm has <em>collapsed</em> the huge size difference between <span class="math notranslate nohighlight">\(z\)</span> and the other two values <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> by using exponentiation.</p>
<p>In this sense, we can also use the logarithm function for our network to reduce the huge size difference between the values in our alien network. However, we must first add a slight twist: to do this properly and yield an interpretable adjacency matrix, we need to <em>augment</em> the entries of the adjacency matrix <em>if</em> it contains zeros. This is because the <span class="math notranslate nohighlight">\(log_{10}(0)\)</span> is <em>not defined</em>. To augment the adjacency matrix, we will use the following strategy:</p>
<ol class="simple">
<li><p>Identify the entries of <span class="math notranslate nohighlight">\(A\)</span> which take a value of zero.</p></li>
<li><p>Identify the smallest entry of <span class="math notranslate nohighlight">\(A\)</span> which is not-zero, and call it <span class="math notranslate nohighlight">\(a_m\)</span>.</p></li>
<li><p>Compute a value <span class="math notranslate nohighlight">\(\epsilon\)</span> which is an <em>order of magnitude</em> smaller than <span class="math notranslate nohighlight">\(a_m\)</span>. Since we are taking powers of ten, a single order of magnitude would give us that <span class="math notranslate nohighlight">\(\epsilon = \frac{a_m}{10}\)</span>.</p></li>
<li><p>Take the augmented adjacency matrix <span class="math notranslate nohighlight">\(A'\)</span> to be defined with entries <span class="math notranslate nohighlight">\(a_{ij}' = a_{ij} + \epsilon\)</span>.</p></li>
</ol>
<p>Next, since our matrix has values which are now all greater than zero, we can just take the logarithm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">augment_zeros</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">X</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The logarithm is not defined for negative values!&quot;</span><span class="p">)</span>
    <span class="n">am</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)])</span>  <span class="c1"># the smallest non-zero entry of X</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">am</span><span class="o">/</span><span class="mi">10</span>  <span class="c1"># epsilon is one order of magnitude smaller than the smallest non-zero entry</span>
    <span class="k">return</span> <span class="n">X</span> <span class="o">+</span> <span class="n">eps</span>  <span class="c1"># augment all entries of X by epsilon</span>

<span class="n">Aa_aug</span> <span class="o">=</span> <span class="n">augment_zeros</span><span class="p">(</span><span class="n">Aa</span><span class="p">)</span>
<span class="c1"># log-transform using base 10</span>
<span class="n">Aa_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">Aa_aug</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network&quot;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Aa_log</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Alien Brain Network, After Augmentation + log&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_51_0.png" src="../../_images/regularization_51_0.png" />
</div>
</div>
<p>When we plot the augmented and log-transformed data, what we see is that many of the edge-weights we originally might have thought were zero if we only looked at a plot were, in actuality, <em>not</em> zero. In this sense, for non-negative weighted networks, log transforming after zero-augmentation is often very useful for visualization to get a sense of the magnitudinal differences that might be present between edges.</p>
<p>Our edge-weight histogram becomes:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">histplot</span><span class="p">(</span><span class="n">Aa_log</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$log_</span><span class="si">{10}</span><span class="s2">($Edge Weight$ + \epsilon)$&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Edges&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regularization_53_0.png" src="../../_images/regularization_53_0.png" />
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./representations/ch4"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="properties-of-networks.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">4.3. </span>Properties of Networks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../ch5/ch5.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Why Use Statistical Models?</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Joshua Vogelstein, Alex Loftus, and Eric Bridgeford<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>