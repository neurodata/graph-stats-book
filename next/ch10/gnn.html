
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>9.2. Graph Neural Networks &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="10. Representations (Extended)" href="../../appendix/ch11/ch11.html" />
    <link rel="prev" title="9.1. Random walk and diffusion-based methods" href="random-walk-diffusion-methods.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What is network machine learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why do we study networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">
     1.3. Types of Network Machine Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.4. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">
     1.5. Challenges of Network Machine Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.4. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.5. Fine-Tune your Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.6. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch4/ch4.html">
   3. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/matrix-representations.html">
     3.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/properties-of-networks.html">
     3.2. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/network-representations.html">
     3.3. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/regularization.html">
     3.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch5/ch5.html">
   4. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">
     4.1. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">
     4.2. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">
     4.3. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">
     4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/multi-network-models.html">
     4.5. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/models-with-covariates.html">
     4.6. Network Models with Network Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch6/ch6.html">
   5. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">
     5.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/why-embed-networks.html">
     5.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/spectral-embedding.html">
     5.3. Spectral embedding methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">
     5.4. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">
     5.5. Joint Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch7/ch7.html">
   6. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/community-detection.html">
     6.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/testing-differences.html">
     6.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/model-selection.html">
     6.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/vertex-nomination.html">
     6.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch7/out-of-sample.html">
     6.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   7. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html">
     7.1. Latent Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/significant-communities.html">
     7.2. Two-sample hypothesis testing in SBMs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/graph-matching-vertex.html">
     7.3. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/multiple-vertex-nomination.html">
     7.4. Vertex Nomination For Two Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   8. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/anomaly-detection.html">
     8.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-edges.html">
     8.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/significant-vertices.html">
     8.3. Testing for Significant Vertices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Next Steps
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch10.html">
   9. Where do we go from here?
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="random-walk-diffusion-methods.html">
     9.1. Random walk and diffusion-based methods
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.2. Graph Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch11/ch11.html">
   10. Representations (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch11/alt-reps.html">
     10.1. Alternative Network Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch12/ch12.html">
   11. Network Model Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/background.html">
     11.1. Background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/foundation.html">
     11.2. Foundation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/ers.html">
     11.3. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/sbms.html">
     11.4. Stochastic Block Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/rdpgs.html">
     11.5. RDPGs and more general network models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch13/ch13.html">
   12. Learning Representations Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch13/mle-theory.html">
     12.1. Maximum Likelihood Estimate Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch13/lse.html">
     12.2. Finding singular vectors With singular value decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch13/spectral-theory.html">
     12.7. Spectral Method Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch14/ch14.html">
   13. Applications (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch14/hypothesis.html">
     13.1. Hypothesis Testing with coin flips
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch14/unsupervised.html">
     13.2. Unsupervised learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch14/bayes.html">
     13.3. Bayes Plugin Classifier
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/network_machine_learning_in_python/next/ch10/gnn.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/neurodata/graph-stats-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fnext/ch10/gnn.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/next/ch10/gnn.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/next/ch10/gnn.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-drug-discovery-problem">
   9.2.1. The drug discovery problem
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#obtaining-a-network-from-a-molecule">
     9.2.1.1. Obtaining a network from a molecule
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#our-network-learning-problem">
     9.2.1.2. Our network learning problem
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-smiles-fingerprints">
   9.2.2. Preprocessing SMILES fingerprints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-neural-networks-gnns">
   9.2.3. Graph neural networks (GNNs)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnns-are-universal-approximators">
     9.2.3.1. GNNs are universal approximators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnns-are-easy-and-efficient-to-optimize">
     9.2.3.2. GNNs are easy and efficient to optimize
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-gnns-work-for-drug-discovery">
   9.2.4. How do GNNs work for drug discovery?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnn-training">
     9.2.4.1. GNN training
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-forward-pass">
       9.2.4.1.1. The forward pass
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#message-passing-uses-local-neighborhoods-of-networks">
         9.2.4.1.1.1. Message passing uses local neighborhoods of networks
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#readout-takes-the-node-embeddings-for-a-molecule-and-produces-a-molecule-embedding">
         9.2.4.1.1.2. Readout takes the node embeddings for a molecule and produces a molecule embedding
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#the-percepteron-produces-prediction-labels">
         9.2.4.1.1.3. The percepteron produces prediction labels
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-backwards-pass">
       9.2.4.1.2. The backwards pass
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#putting-it-all-together">
     9.2.4.2. Putting it all together
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-training-and-testing">
   9.2.5. Model training and testing
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Graph Neural Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-drug-discovery-problem">
   9.2.1. The drug discovery problem
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#obtaining-a-network-from-a-molecule">
     9.2.1.1. Obtaining a network from a molecule
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#our-network-learning-problem">
     9.2.1.2. Our network learning problem
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-smiles-fingerprints">
   9.2.2. Preprocessing SMILES fingerprints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-neural-networks-gnns">
   9.2.3. Graph neural networks (GNNs)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnns-are-universal-approximators">
     9.2.3.1. GNNs are universal approximators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnns-are-easy-and-efficient-to-optimize">
     9.2.3.2. GNNs are easy and efficient to optimize
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-gnns-work-for-drug-discovery">
   9.2.4. How do GNNs work for drug discovery?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnn-training">
     9.2.4.1. GNN training
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-forward-pass">
       9.2.4.1.1. The forward pass
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#message-passing-uses-local-neighborhoods-of-networks">
         9.2.4.1.1.1. Message passing uses local neighborhoods of networks
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#readout-takes-the-node-embeddings-for-a-molecule-and-produces-a-molecule-embedding">
         9.2.4.1.1.2. Readout takes the node embeddings for a molecule and produces a molecule embedding
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#the-percepteron-produces-prediction-labels">
         9.2.4.1.1.3. The percepteron produces prediction labels
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-backwards-pass">
       9.2.4.1.2. The backwards pass
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#putting-it-all-together">
     9.2.4.2. Putting it all together
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-training-and-testing">
   9.2.5. Model training and testing
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="graph-neural-networks">
<span id="ch10-gnns"></span><h1><span class="section-number">9.2. </span>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Permalink to this headline">#</a></h1>
<p>Throughout this book, we have focused on ways in which you can represent and, if desired <em>conceptualize</em>, network-valued data that place it in a context in which you can apply many of the algorithms you are used to.</p>
<p>For instance, in the case of the <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-ref">spectral embedding</span></a>, we devised techniques which allow you to take a network of nodes and edges, and construct a tabular array with which you can learn about using traditional techniques like <span class="math notranslate nohighlight">\(k\)</span>-means for <a class="reference internal" href="../../applications/ch7/community-detection.html#ch7-comm-detect"><span class="std std-ref">community detection</span></a>, or other tasks of interest you might have. We learned how this representation had underpinnings in the <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html#ch5-rdpg"><span class="std std-ref">rdpg</span></a>, and how you could use this as a conceptual basis to assign explicit assumptions you might make about your network in order for you to do downstream statistical inference, such as if you want to <a class="reference internal" href="../../applications/ch8/two-sample-hypothesis.html#ch8-twosample"><span class="std std-ref">compare two networks</span></a>.</p>
<p>In this section and the next on <a class="reference internal" href="random-walk-diffusion-methods.html#ch10-diffusion"><span class="std std-ref">diffusion methods</span></a>, we are going to turn this approach somewhat on its head, and investigate approaches which, at face value, appear to fundamentally alter existing techniques which you already might know about, neural networks, to make them amenable to network-valued data directly. Let’s get down to it.</p>
<section id="the-drug-discovery-problem">
<h2><span class="section-number">9.2.1. </span>The drug discovery problem<a class="headerlink" href="#the-drug-discovery-problem" title="Permalink to this headline">#</a></h2>
<p>In chemistry, a <em>molecule</em> is a group of atoms which are bonded together. If you haven’t taken a chemistry course, you can think of an atom as a <em>building block</em>, and a <em>bond</em> as the <em>glue</em> that holds the molecule together. Every interaction you have with the world occurs through molecules; the water you drink, for instance, is a molecule consisting of hydrogen and oxygen, <span class="math notranslate nohighlight">\(H_20\)</span>, and the air you breathe consists most of molecules of nitrogen <span class="math notranslate nohighlight">\(N_2\)</span>, small amounts of oxygen <span class="math notranslate nohighlight">\(O_2\)</span>, and trace amounts of argon <span class="math notranslate nohighlight">\(Ar\)</span> and carbon dioxide <span class="math notranslate nohighlight">\(CO_2\)</span>. In these formulas, the letters represent a particular atom, and the subscripts represent the number of these atoms in the molecule.</p>
<p>Without getting too deep into the chemistry, a fundamental problem for many pharmaceutical and chemistry investigations involves the analysis of molecules that might be useful for clinical purposes. A <em>clinically useful molecule</em> is a molecule which has properties which, for one reason or another, might make it beneficial for humans. For instance, if you. have ever had a headache, you have probably become familiar with a molecule <span class="math notranslate nohighlight">\(C_{13}H_{18}O_2\)</span>, or Ibuprofen (Advil).</p>
<p>Many of the drugs that we are familiar with on an everyday basis have, it turns out, been discovered totally by accident. For a laugh, we’d encourage you to check out the discovery of <a class="reference external" href="https://en.wikipedia.org/wiki/Alexander_Fleming#Discovery_of_penicillin">penicillin</a>, which is a family of molecules having a core structure of <span class="math notranslate nohighlight">\(C_9H_{11}N_2O_4S\)</span>.</p>
<p>Unfortunately, coming across all of these “happy accident” molecules has become more and more infrequent as pharmaceuticals have progressed. When a given condition is identified that a pharmacologist decides to attempt to devise a treatment for, they often must proceed facing several enormous hurdles for candidate molecules. The molecules must be:</p>
<ol class="simple">
<li><p>non-toxic to humans,</p></li>
<li><p>an appropriate size,</p></li>
<li><p>be readily absorbed by humans, and</p></li>
<li><p>able to address the condition of interest.</p></li>
</ol>
<p>Achieving all of these aims is extremely difficult, and extremely risky in terms of time, labor, and most of all potential risk to human participants in drug trials. Determining definitive, or at least suggestive, evidence that the molecule will achieve any or all of these conditions <em>before</em> running live human tests can save the company billions of dollars and can save participants in drug trials unnecessary exposure to harm.</p>
<p>Running laboratory tests to screen these molecules is an expensive, time-consuming, and skilled labor-intensive process: what are the companies left to do?</p>
<p>Many drug companies utilize virtual screening methods in which computational methods are used to quickly search large libraries of molecules to filter ones with the desirable properties. Success in virtual screening methods are determined by the accuracy and speed of the computational approach. For instance, molecular dynamics simulates the physical movement of each atom and particle, thus providing the most accurate calculations of molecular properties, but requires an intractable number of numerical calculations that makes it infeasible to use in practice. Instead, methods have been built around succint 2D representations of molecules, further compressed in 1D <a class="reference external" href="https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system">SMILES</a> strings (Simplified molecular-input line-entry system), that summarize the structural relationships between different atoms through the chemical bonds connecting them. These <span class="math notranslate nohighlight">\(1D\)</span> strings are known as <em>fingerprints</em> for the molecules.</p>
<p><img alt="title" src="../../_images/molecule_repr.png" /></p>
<p>Using SMILES, practioners have built statistical models to estimate correlations between SMILES and different molecular properties. These traditional methods have relied on building hand-crafted features derived from SMILES and optimizing simplistic statistical models to predict molecular properties from fingerprints of the molecules.</p>
<p>This too, however, is extremely labor intensive and, most importantly, requires direct human intervention to decipher the features used for statistical learning. With some clever manipulation, scientists have identified an ingenius approach to turn molecular screening into a network learning problem.</p>
<section id="obtaining-a-network-from-a-molecule">
<h3><span class="section-number">9.2.1.1. </span>Obtaining a network from a molecule<a class="headerlink" href="#obtaining-a-network-from-a-molecule" title="Permalink to this headline">#</a></h3>
<p>In our initial description of a molecule, we described that a molecule was just a group of atoms which are bonded together. How can we conceptualize this as a network?</p>
<p>If we think of the atoms of the molecule as the nodes of the network, and the bonds as the edges, this problem is extremely straightforward. Let’s see how this would work with a single molecule of water. Remember that water has the molecular formula <span class="math notranslate nohighlight">\(H_2O\)</span>, which means that it is two hydrogen atoms, and one oxygen atom. The molecular structure of water is that the oxygen atom basically sits in the middle, and the hydrogens hang off the side. We can turn this molecular structure into a network like this:</p>
<figure class="align-center" id="water-molecule">
<img alt="../../_images/water_molecule.png" src="../../_images/water_molecule.png" />
<figcaption>
<p><span class="caption-number">Fig. 9.1 </span><span class="caption-text">(A) A water molecule’s molecular structure. This structure has been determined directly through experimentation. (B) The water molecule, but as a network with a layout view. The atoms (oxygen and two hydrogens) become the nodes of the network, and the bonds between the oxygen atom and the hydrogens become the edges. (C) The network as an adjacency matrix. The fingerprint for water is the string <span class="math notranslate nohighlight">\(O\)</span>.</span><a class="headerlink" href="#water-molecule" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Given a fingerprint (the SMILES string), this molecular structure, and consequently the network and the adjacency matrix, are fully determined. The inverse is also true: given a network and the other attributes of the atomic structure, the fingerprint can be determined. Implicitly, computational strategies which produce desirable representations of the fingerprints produce a desirable representation for the network itself.</p>
</section>
<section id="our-network-learning-problem">
<h3><span class="section-number">9.2.1.2. </span>Our network learning problem<a class="headerlink" href="#our-network-learning-problem" title="Permalink to this headline">#</a></h3>
<p>For this section, we will focus our attention on the first hurdle faced by pharmacoligists. Provided a SMILES fingerprint for a molecule, we would like to determine whether the molecule is toxic or non-toxic.</p>
</section>
</section>
<section id="preprocessing-smiles-fingerprints">
<h2><span class="section-number">9.2.2. </span>Preprocessing SMILES fingerprints<a class="headerlink" href="#preprocessing-smiles-fingerprints" title="Permalink to this headline">#</a></h2>
<p>We will be using one of the datasets packaged in <a class="reference external" href="https://moleculenet.org/">MoleculeNet</a>. PyTorch Geometric (PyG) provides a convenient module to access it. We first import the module <code class="docutils literal notranslate"><span class="pre">MoleculeNet</span></code> then extract the <code class="docutils literal notranslate"><span class="pre">ClinTox</span></code> dataset. The dataset has a total of 1478 molecules labeled with their presence or absence of toxicity – determined from clinical trials done by the Food &amp; Drug association (FDA). Since this is a binary classification task, we observe there are two classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">MoleculeNet</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">MoleculeNet</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;data/clintox&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ClinTox&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Dataset: </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="se">\n</span><span class="s1">Number of molecules/graphs: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">Number of classes: </span><span class="si">{</span><span class="n">dataset</span><span class="o">.</span><span class="n">num_classes</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/clintox.csv.gz
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting data/clintox/clintox/raw/clintox.csv.gz
Processing...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[01:52:49] Explicit valence for atom # 0 N, 5, is greater than permitted
[01:52:49] Can&#39;t kekulize mol.  Unkekulized atoms: 9
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[01:52:50] Explicit valence for atom # 10 N, 4, is greater than permitted
[01:52:50] Explicit valence for atom # 10 N, 4, is greater than permitted
[01:52:50] Can&#39;t kekulize mol.  Unkekulized atoms: 4
[01:52:50] Can&#39;t kekulize mol.  Unkekulized atoms: 4
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset: ClinTox(1478)
Number of molecules/graphs: 1478
Number of classes: 2
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Done!
</pre></div>
</div>
</div>
</div>
<p>Let’s look at a few molecules to understand their graph structure. Each molecule has a known 3D structure and an associated SMILES string. We pick out two arbitrary molecules from the dataset, and take a look first at their SMILES string:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mols</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">26</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">83</span><span class="p">]</span>
<span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">smiles</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mols</span><span class="p">];</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C([C@@H]1[C@H]([C@@H]([C@H](C(=O)O1)O)O)O)O
C1[C@@H]([C@H](O[C@H]1N2C=NC(=NC2=O)N)CO)O
</pre></div>
</div>
</div>
</div>
<p>Using these smiles, we can determine the molecular structure with <code class="docutils literal notranslate"><span class="pre">rdkit</span></code>, and can plot the resulting molecule as a graph. Remember that in the graph, individual atoms form the nodes of the network, and the bonds form the edges. While some of the specific edges have strange shapes (such as dotted lines, triangular structures, or “doubled bonds”), you can ignore these details for now as they require a bit more in-depth knowledge of organic chemistry to appreciate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">rdkit</span> <span class="kn">import</span> <span class="n">Chem</span>
<span class="kn">from</span> <span class="nn">rdkit.Chem.Draw</span> <span class="kn">import</span> <span class="n">rdMolDraw2D</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">SVG</span>

<span class="n">smiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">smiles</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mols</span><span class="p">]</span>
<span class="n">d2d</span> <span class="o">=</span> <span class="n">rdMolDraw2D</span><span class="o">.</span><span class="n">MolDraw2DSVG</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span><span class="mi">280</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">280</span><span class="p">)</span>
<span class="n">d2d</span><span class="o">.</span><span class="n">drawOptions</span><span class="p">()</span><span class="o">.</span><span class="n">addAtomIndices</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">d2d</span><span class="o">.</span><span class="n">DrawMolecules</span><span class="p">(</span><span class="n">smiles</span><span class="p">)</span>
<span class="n">d2d</span><span class="o">.</span><span class="n">FinishDrawing</span><span class="p">()</span>
<span class="n">SVG</span><span class="p">(</span><span class="n">d2d</span><span class="o">.</span><span class="n">GetDrawingText</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/gnn_5_0.svg" src="../../_images/gnn_5_0.svg" /></div>
</div>
<p>The networks each have a different number of nodes (atoms). Further, each node has a number which uniquely identifies that node in the molecule. These molecules have been analyzed with cheminformatics software, <a class="reference external" href="https://www.rdkit.org/">RDkit</a>, and also come with a set of node attributes for descriptive characteristics of that node in the particular molecule. While these node attributes are not relevant for our tutorial, they have substantial real-world applications to optimize predictive accuracy. Let’s take a look at the number of atoms and the number of atomic features now:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mols</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Molecule </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">: Number of atoms=</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">, Features per atom=</span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Molecule 1: Number of atoms=12, Features per atom=9
Molecule 2: Number of atoms=16, Features per atom=9
</pre></div>
</div>
</div>
</div>
<p>As we discussed above, the edges of the network are the bonds between different pairs of atoms (the nodes). Using the SMILE fingerprint, we can also obtain information of the bonds of the network. Let’s take a look at the indices of the bonds, shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2d</span> <span class="o">=</span> <span class="n">rdMolDraw2D</span><span class="o">.</span><span class="n">MolDraw2DSVG</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span><span class="mi">280</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">280</span><span class="p">)</span>
<span class="n">d2d</span><span class="o">.</span><span class="n">drawOptions</span><span class="p">()</span><span class="o">.</span><span class="n">addBondIndices</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">d2d</span><span class="o">.</span><span class="n">DrawMolecules</span><span class="p">(</span><span class="n">smiles</span><span class="p">)</span>
<span class="n">d2d</span><span class="o">.</span><span class="n">FinishDrawing</span><span class="p">()</span>
<span class="n">SVG</span><span class="p">(</span><span class="n">d2d</span><span class="o">.</span><span class="n">GetDrawingText</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/gnn_9_0.svg" src="../../_images/gnn_9_0.svg" /></div>
</div>
<p>Unlike many of the networks we have seen in examples so far, molecular networks tend to be, especially for larger molecules (molecules with more nodes, or atoms) extremely sparse. A <strong>sparse network</strong> is a network which has far fewer edges than the maximum number of possible edges. In smaller networks, this is a rather irrelevant consideration. Remember that the adjacency matrix has, for a simple network, <span class="math notranslate nohighlight">\(\binom n 2 = \frac{1}{2}n(n - 1)\)</span> possible edges. So, if we store the entire adjacency matrix, we need to keep track of <span class="math notranslate nohighlight">\(\frac{1}{2}n (n - 1)\)</span> entries.</p>
<p>In a sparse network, the total number of edges might be considerably less than this. Remember that an edge is indexed by two nodes, so if two times the number of edges is less than <span class="math notranslate nohighlight">\(\binom n 2\)</span>, we can save a lot of space by exclusively saving the node pairs for each edge. This is the concept underlying an <em>edgelist</em>, which is as the name suggests, a list of edges. Instead of saving the <em>entire</em> adjacency matrix, what we do is we save <em>only</em> the edges, and then construct the adjacency matrix <em>only when we need it</em> from the edges!</p>
<p>When you have a <em>sparse network</em>, and consequently a <em>sparse adjacency matrix</em>, you can even perform many matrix computations without ever having to construct the (potentially large) adjacency matrix in its entirety. This is the concept underlying <a class="reference external" href="https://en.wikipedia.org/wiki/Sparse_matrix">sparse matrices</a>, and many numerical libraries such as <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/sparse.html">scipy</a> and <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html">torch sparse arrays</a> (which we are leveraging here) have special matrix routines which run a <em>lot</em> faster, and use a <em>lot</em> less space, when the network is extremely sparse.</p>
<p>With this in mind, when we look through the dataset, we need to keep in mind that the network is stored using this sparse edgelist format. To build the adjacency matrix so that we can visualize it using the traditional heatmap, we need to first construct the adjacency matrix from the edgelist, which is what we do below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">_process</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="k">def</span> <span class="nf">A_from_edgelist</span><span class="p">(</span><span class="n">molecule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A function that takes molecules, and produces an adjacency matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># the number of nodes is the number of atoms (rows of .x attribute)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">molecule</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># the adjacency matrix is n x n</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="n">edgelist</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">edge_index</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="c1"># loop over the edges e_k, and for each edge, unpack the </span>
    <span class="c1"># nodes that are incident it. for this corresponding pair of nodes, </span>
    <span class="c1"># change the adjacency matrix entry to 1</span>
    <span class="k">for</span> <span class="n">e_k</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">_process</span><span class="p">(</span><span class="n">edgelist</span><span class="p">))):</span>
        <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">A</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have a function that will produce the corresponding adjacency matrices, we can produce the heatmaps of the adjacency matrices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">cairosvg</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mols</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;width_ratios&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]},</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">m_i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mols</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">A_from_edgelist</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">atoms</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">m_i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Adjacency matrix&#39;</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">atoms</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">atoms</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Node (Atom Index)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Node (Atom Index)&quot;</span><span class="p">)</span>
    <span class="n">d2d</span> <span class="o">=</span> <span class="n">rdMolDraw2D</span><span class="o">.</span><span class="n">MolDraw2DSVG</span><span class="p">(</span><span class="mi">280</span><span class="p">,</span><span class="mi">280</span><span class="p">,</span><span class="mi">280</span><span class="p">,</span><span class="mi">280</span><span class="p">)</span>
    <span class="n">d2d</span><span class="o">.</span><span class="n">drawOptions</span><span class="p">()</span><span class="o">.</span><span class="n">addAtomIndices</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">d2d</span><span class="o">.</span><span class="n">DrawMolecule</span><span class="p">(</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">smiles</span><span class="p">))</span>
    <span class="n">d2d</span><span class="o">.</span><span class="n">FinishDrawing</span><span class="p">()</span>
    <span class="n">svg</span> <span class="o">=</span> <span class="n">d2d</span><span class="o">.</span><span class="n">GetDrawingText</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;svg:&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cairosvg</span><span class="o">.</span><span class="n">svg2png</span><span class="p">(</span><span class="n">svg</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">m_i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">m_i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">m_i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">m_i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Molecule </span><span class="si">{</span><span class="n">m_i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">m_i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Molecular Structure&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/gnn_13_0.png" src="../../_images/gnn_13_0.png" />
</div>
</div>
<p>Let’s take a look at this just to verify we can confirm what is going on. Notice that in Molecule <span class="math notranslate nohighlight">\(1\)</span>, that an Oxygen atom (index <span class="math notranslate nohighlight">\(7\)</span>) is bonded to two Carbon atoms (the “points” of the hexagon are Carbon atoms), which each have indices <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(5\)</span> respectively. When we look at the adjaceny matrix, looking at the row corresponding to atom index <span class="math notranslate nohighlight">\(7\)</span> shows that it is connected to atom indices <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(5\)</span>, as desired. As an exercise, you can continue this process for all of the atoms in the molecule until you are convinced that you understand what’s going on!</p>
<p>So, now the take-home message; why did we choose these two molecules? Does anything look obvious to you, as a machine learning specialist with network expertise, about these two molecules?</p>
<p>Well, quite simply, the first one is non-toxic, whereas the second one is quite toxic! To know this, you would probably need to be somewhat of an expert in organic chemistry, which for the majority of machine learning experts, probably isn’t the case!</p>
<p>Despite the fact that you might not have background in organic chemistry, you can still be of <em>extreme</em> use to pharmacologists: it turns out that, even though you don’t know a heck of a lot about either of these two molecules, nor perhaps even the field, you do know a lot about classifying items! Let’s see if we can borrow some techniques from machine learning to learn about these molecules.</p>
</section>
<section id="graph-neural-networks-gnns">
<h2><span class="section-number">9.2.3. </span>Graph neural networks (GNNs)<a class="headerlink" href="#graph-neural-networks-gnns" title="Permalink to this headline">#</a></h2>
<p>At this point, we’ve done a decent exploration into the SMILES strings, and hopefully we’ve convinced you that, given the SMILES string, we can recover either the molecular structure in its entirety or an underlying adjacency matrix. Now it’s time for the fun part. As it turns out, using <em>only</em> this adjacency matrix, you can do a pretty good job of identifying what, to my eyes anyways, is a completely opaque task: determining whether a molecule is toxic or non-toxic. We’ll show you how using an approach you may have come across before in machine learning, known as a <em>neural network</em>. If you are unfamiliar with neural networks and the basic mathematics which are used to describe them, or if you need a quick refresher, we would recommend you check out Aurelien Geron’s book, <a class="reference external" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=asc_df_1492032646/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=385599638286&amp;hvpos=&amp;hvnetw=g&amp;hvrand=6554720435744381928&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9007894&amp;hvtargid=pla-523968811896&amp;psc=1&amp;tag=&amp;ref=&amp;adgrpid=79288120515&amp;hvpone=&amp;hvptwo=&amp;hvadid=385599638286&amp;hvpos=&amp;hvnetw=g&amp;hvrand=6554720435744381928&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9007894&amp;hvtargid=pla-523968811896">Hands On Machine Learning</a>, which has an extensive discussion of them.</p>
<p>In this task, we have several problems which fundamentally distinguish this task from many of the approaches we have seen before. When we learned representations for network-valued data previously, we assumed that, as the investigator, you probably had a decent knowledge of the structure of your data ahead of time, and which assumptions might be sensible to pick out. When this was the case, you could decipher <a class="reference internal" href="../../representations/ch5/ch5.html#ch5"><span class="std std-ref">statistical models</span></a> which might be reasonably sensible for your data (the assumptions might not be perfect, but good enough for the purposes of your investigation), and use these statistical models to facilitate downstream statistical inference tasks you want to learn about.</p>
<p>When dealing with expansive molecular networks (the two molecules we picked out were rather tiny, but molecules related to drug discovery can be enormous, with dozens to thousands of atoms), we have a few problems that arise. In particular, it is often extremely unclear how to construct a direct statistical model that can be feasibly optimized or learned, as identifying the core features you want to capture with the statistical model for unseen molecules (or whatever your underlying networks represent) can be arduous, time consuming, and require domain expertise that you just might not have. A practical solution to this hurdle might be to use Graph Neural Networks (GNNs). A graph neural network is just a neural network, but modified for use with graph (or network)-valued data.</p>
<p>GNNs have two desirable properties that can help us to overcome some of these hurdles:</p>
<section id="gnns-are-universal-approximators">
<h3><span class="section-number">9.2.3.1. </span>GNNs are universal approximators<a class="headerlink" href="#gnns-are-universal-approximators" title="Permalink to this headline">#</a></h3>
<p>When you have a task of interest in mind, assuming that your data is informative for that task, there are features or aspects of your data (which might be extremely convoluted) that you can “tease out” which will give you the best possible information for your question of interest.</p>
<p>These features might be simple; for instance, to classify a molecule, the presence of at most <span class="math notranslate nohighlight">\(5\)</span> bonds might be the definitive feature that makes a molecule toxic or non-toxic. Any molecule with more than <span class="math notranslate nohighlight">\(5\)</span> bonds could be toxic, and any molecule with less than <span class="math notranslate nohighlight">\(5\)</span> bonds could be non-toxic.</p>
<p>However, in practice (and in this task), the features are not nearly that simple, nor completely understood even by the most pre-eminent experts in organic chemistry, and this problem permeates to many other fields other than just pharmacology.</p>
<p>GNNs provide a framework for constructing universal approximators for network-valued data. A <em>universal approximator</em> has a fairly complicated definition, so we won’t define it formally here. But basically the idea is that, <em>whatever</em> the relationship between the data (the network) and the outcome of interest (in this case, toxic or non-toxic) happens to be, no matter how complicated or convoluted it is, a properly chosen GNN is <em>always capable</em> of closely finding the relationship. The caveat is that the meaning of “close” is contingent on the number of training samples. Although you probably have come across the term training data before, we’ll define it formally for you here: <strong>training data</strong> is data which you will use to train an algorithm to predict the outcome you intend to study. The idea is that, if the network has enough free parameters (a function of the number of <em>nodes</em> and <em>edges</em> in the neural network, not to be confused with the number of nodes/edges in the networks that we are studying)  This is extremely appealing in fields in which we domain expertise can’t help us too much, and where we have tons of training data.</p>
<p>In this case, there are hundreds of thousands of molecules that have been extensively and exhaustively studied by pharmacologists for toxicity, so we have a substantial backlog of data where we know ahead of time whether the molecule is toxic or not. Further, even to trained scientists, as the molecule size grows, determining toxicity can be difficult and certainly not immediate as it can be for a properly trained neural network.</p>
</section>
<section id="gnns-are-easy-and-efficient-to-optimize">
<h3><span class="section-number">9.2.3.2. </span>GNNs are easy and efficient to optimize<a class="headerlink" href="#gnns-are-easy-and-efficient-to-optimize" title="Permalink to this headline">#</a></h3>
<p>Once you have an enormous volume of training data, you might be extremely concerned about how well you might be able to actually train up this neural network. Fortunately, there is plenty of help out there. A plethora of software libraries have been built to allow for neural network training and optimization. The success of neural networks is closely tied with the advent of special computer hardware, Graphical Processing Units (GPUs), that greatly accelerate and scale numerical calculations. GNNs are widely used in practice because they can scale to very large and high-dimensional datasets due to the tight coupling of engineering progress/technical advances in GPU performance and mathematical theory.</p>
</section>
</section>
<section id="how-do-gnns-work-for-drug-discovery">
<h2><span class="section-number">9.2.4. </span>How do GNNs work for drug discovery?<a class="headerlink" href="#how-do-gnns-work-for-drug-discovery" title="Permalink to this headline">#</a></h2>
<p>For drug discovery, a pre-eminent technique for the construction of neural networks is to construct a message passing neural network (MPNN). This approach leverages a <span class="xref myst">bag of nodes</span> type of approach, in which we search for a <em>latent</em> (unknown) embedding for each node in a given molecule network. In this investigation, we will assume that we have <span class="math notranslate nohighlight">\(M\)</span> total training networks and <span class="math notranslate nohighlight">\(M'\)</span> testing (validation) networks, where each network <span class="math notranslate nohighlight">\(m\)</span> (a single molecule) has <span class="math notranslate nohighlight">\(n_m\)</span> nodes in the network.</p>
<p>You will notice that there is a caveat from many of the multi-network examples we have studied to date: the number of nodes, <span class="math notranslate nohighlight">\(n_m\)</span>, is <em>not</em> going to necessarily be constant across the networks: each molecule can, and <em>will</em> tend to have differing numbers of atoms of which they are comprised. The way that this hurdle is overcome for MPNNs is the message passing procedure.</p>
<section id="gnn-training">
<h3><span class="section-number">9.2.4.1. </span>GNN training<a class="headerlink" href="#gnn-training" title="Permalink to this headline">#</a></h3>
<p>GNN training has a bit of a unique convention from other domains of machine learning. Rather than just separating the data into training and testing sets outright, the training set tends to be further segmented into a thing, called an <em>epoch</em>, which represents a random shuffling of the networks in the dataset.</p>
<p>Within a single epoch, we will further segment the dataset into <span class="math notranslate nohighlight">\(B\)</span> batches. Each of the <span class="math notranslate nohighlight">\(B\)</span> batches are sampled, <em>uniformly</em> in size and <em>without</em> replacement (each training network appears in <em>exactly</em> one batch across an epoch), from the training set. We will assume that each batch has <span class="math notranslate nohighlight">\(M_b\)</span> networks, where <span class="math notranslate nohighlight">\(M_b = \frac{M}{B}\)</span> is the number of networks in each batch. For the purposes of this experiment, we will assume that <span class="math notranslate nohighlight">\(M\)</span> is divisible by the number of batches chosen, but this might not always necessarily be the case. However, it is fine to have one batch be slightly different in size from the others.</p>
<section id="the-forward-pass">
<h4><span class="section-number">9.2.4.1.1. </span>The forward pass<a class="headerlink" href="#the-forward-pass" title="Permalink to this headline">#</a></h4>
<section id="message-passing-uses-local-neighborhoods-of-networks">
<h5><span class="section-number">9.2.4.1.1.1. </span>Message passing uses local neighborhoods of networks<a class="headerlink" href="#message-passing-uses-local-neighborhoods-of-networks" title="Permalink to this headline">#</a></h5>
<p>To define the message passing process, we will recall some earlier definitions you were introduced to in <a class="reference internal" href="../../representations/ch4/properties-of-networks.html#ch4-prop-net"><span class="std std-ref">properties of networks</span></a>. Remember that a node <span class="math notranslate nohighlight">\(j\)</span> is a neighbor of node <span class="math notranslate nohighlight">\(i\)</span> if an edge exists from node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span>, and consequently, <span class="math notranslate nohighlight">\(a_{ij}^{(m)} = 1\)</span> in the adjacency matrix for the network <span class="math notranslate nohighlight">\(m\)</span>. So, the set of all neighbors of a node <span class="math notranslate nohighlight">\(i\)</span> in network <span class="math notranslate nohighlight">\(m\)</span> can be described as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    N^{(m)}(i) \triangleq \left\{j : a_{ij}^{(m)} = 1\right\}
\end{align*}\]</div>
<p>So, <span class="math notranslate nohighlight">\(N^{(m)}(i)\)</span> is just the collection of nodes that node <span class="math notranslate nohighlight">\(i\)</span> is connected to in network <span class="math notranslate nohighlight">\(m\)</span>. Let’s consider a simple example that we’re going to work through here. It looks like this:</p>
<figure class="align-default" id="id1">
<img alt="../../_images/Images" src="../../_images/Images" />
<figcaption>
<p><span class="caption-number">Fig. 9.2 </span><span class="caption-text">We have <span class="math notranslate nohighlight">\(5\)</span> nodes in our simple molecular network here, which is the molecule ethylene. Notice that node <span class="math notranslate nohighlight">\(1\)</span> is connected to nodes <span class="math notranslate nohighlight">\(3\)</span>, so <span class="math notranslate nohighlight">\(N(1) = \{3\}\)</span>. Node <span class="math notranslate nohighlight">\(2\)</span> is connected to node <span class="math notranslate nohighlight">\(3\)</span>, so <span class="math notranslate nohighlight">\(N(2) = \{3\}\)</span>. Node <span class="math notranslate nohighlight">\(3\)</span> is connected to nodes <span class="math notranslate nohighlight">\(1, 2,\)</span> and 4<span class="math notranslate nohighlight">\(, so \)</span>N(3) = {1, 2, 4}$.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The network is for the molecule ethylene. For all of the nodes in the network <span class="math notranslate nohighlight">\(m\)</span>, we start by assuming that <span class="math notranslate nohighlight">\(\vec x_{i}^{(m)}(t)\)</span> is the <span class="math notranslate nohighlight">\(d\)</span>-dimensional node embedding for node <span class="math notranslate nohighlight">\(i\)</span> in network <span class="math notranslate nohighlight">\(m\)</span> on iteration <span class="math notranslate nohighlight">\(t\)</span>. This is directly analogous to the concept of a <em>latent position</em>, like oyu learned about <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html#ch5-rdpg"><span class="std std-ref">RDPGs</span></a>. We initialize <span class="math notranslate nohighlight">\(\vec x_i^{(m)}(t)\)</span> using the node attributes that we obtain with the molecule that we learned about previously.</p>
<p>In the message passing phase of GNN training, the GNN sequentially updates node embeddings <span class="math notranslate nohighlight">\(\vec x_i^{(m)}(t)\)</span> across a pre-determined number of timesteps <span class="math notranslate nohighlight">\(T\)</span>. This number of timesteps will be the number of <em>messages</em> you pass around nodes within the network. At an extremely high level, what we will do is as follows. Let’s assume that for batch <span class="math notranslate nohighlight">\(b\)</span>, we are looking at network <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>Starting at the node <span class="math notranslate nohighlight">\(i\)</span> in network <span class="math notranslate nohighlight">\(m\)</span>, we look at all of the neighbors <span class="math notranslate nohighlight">\(N^{(m)}(i)\)</span> of node <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Then, we take these neighbors, and look at <em>their</em> embeddings from the preceding timestep (<span class="math notranslate nohighlight">\(\vec x_j^{(m)}(t - 1)\)</span>), as well as <em>how</em> they are connected to node <span class="math notranslate nohighlight">\(i\)</span> (the edge attributes for that particular edge, denoted <span class="math notranslate nohighlight">\(e_{ij}^{(m)}\)</span>). The edge attribute contains information about the type of bond connecting node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span> (for instance, this is where the double bonds, or the other funky shapes you saw in some of the diagrams above, come into play). We feed all of that information into a convolutional neural network, which produces a message from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span> on the basis of this information. This convolutional neural network can be represented as a function <span class="math notranslate nohighlight">\(M_\theta\)</span>, which takes parameters <span class="math notranslate nohighlight">\(\theta\)</span>, and is input the node embeddings for the node <span class="math notranslate nohighlight">\(i\)</span> and its neighbor <span class="math notranslate nohighlight">\(j\)</span>, as well as the edge attributes for the node <span class="math notranslate nohighlight">\(i\)</span> and its neighbor <span class="math notranslate nohighlight">\(j\)</span>. Finally, we just sum over all of the messages from the neighbors of node <span class="math notranslate nohighlight">\(i\)</span>. In words, the message from node <span class="math notranslate nohighlight">\(i\)</span>’s neighbors is as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \text{messages}_i^{(m)}(t) &amp;= \sum_{j \in N(i)} M_\theta\left(\vec x_j^{(m)}(t - 1), \vec x_j^{(m)}(t - 1), e_{ij}^{(m)}\right)
\end{align*}\]</div>
<p>This gives us the messages received by the node <span class="math notranslate nohighlight">\(i\)</span> from all of its neighbors. We repeat this process for every single node in the network <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>Once we compute all of the messages received by the nodes in a given network, we recompute the node embeddings. We use the following equation to update the node embedding:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x_i^{(m)}(t) &amp;= U_\phi\left(\vec x_i^{(m)}(t - 1), \text{messages}_i^{(m)}(t)\right)
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(U_\phi\)</span> is the node embedding update function. The node embedding update function is a function of the <em>previous</em> embedding of the node <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t - 1\)</span>, the messages that the node received on the current iteration <span class="math notranslate nohighlight">\(t\)</span>, and the parameters <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
<p>We repeat these two steps <span class="math notranslate nohighlight">\(T\)</span> times total for a given molecule in a particular batch: computing the messages for each node from its neighbors using the embeddings from the previous round and the edge attributes for the bond between the node and its neighbor. Then once we have done that for every node in a particular molecule <span class="math notranslate nohighlight">\(m\)</span>, update the corresponding node embeddings for that molecule.</p>
<p>We repeat this procedure, for all <span class="math notranslate nohighlight">\(M_b\)</span> molecules in the batch.</p>
</section>
<section id="readout-takes-the-node-embeddings-for-a-molecule-and-produces-a-molecule-embedding">
<h5><span class="section-number">9.2.4.1.1.2. </span>Readout takes the node embeddings for a molecule and produces a molecule embedding<a class="headerlink" href="#readout-takes-the-node-embeddings-for-a-molecule-and-produces-a-molecule-embedding" title="Permalink to this headline">#</a></h5>
<p>Now that we have repeated this process, for each molecule <span class="math notranslate nohighlight">\(m\)</span> of the <span class="math notranslate nohighlight">\(M_b\)</span> molecules in our batch, we have <span class="math notranslate nohighlight">\(d\)</span>-dimensional node embeddings <span class="math notranslate nohighlight">\(\vec x_i^{(m)}(T)\)</span>, for each node <span class="math notranslate nohighlight">\(i\)</span> of the <span class="math notranslate nohighlight">\(N_m\)</span> total nodes in molecule <span class="math notranslate nohighlight">\(m\)</span>. Note that this node embedding is simply the node embedding from the final iteration of message passing. Now that we have the node embeddings, we then use a readout function <span class="math notranslate nohighlight">\(R_\rho\)</span>, which takes node embeddings and produces a graph embedding <span class="math notranslate nohighlight">\(\vec r^{(m)}\)</span> which has <span class="math notranslate nohighlight">\(k\)</span>-dimensions. It looks like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec r^{(m)} = R_\rho\left(\vec x_1^{(m)}(T), ..., \vec x_{N_m}^{(m)}(T)\right)
\end{align*}\]</div>
<p>We again repeat this procedure for all <span class="math notranslate nohighlight">\(M_b\)</span> molecules in the batch, giving us <span class="math notranslate nohighlight">\(M_b\)</span> molecule embeddings, which are each <span class="math notranslate nohighlight">\(f\)</span>-dimensional vectors <span class="math notranslate nohighlight">\(\vec r^{(m)}\)</span> for a molecule <span class="math notranslate nohighlight">\(m\)</span>.</p>
</section>
<section id="the-percepteron-produces-prediction-labels">
<h5><span class="section-number">9.2.4.1.1.3. </span>The percepteron produces prediction labels<a class="headerlink" href="#the-percepteron-produces-prediction-labels" title="Permalink to this headline">#</a></h5>
<p>Finally, we pass the molecule embeddings into a percepteron. With <span class="math notranslate nohighlight">\(\vec r^{(m)}\)</span> the molecule embedding for a given molecule <span class="math notranslate nohighlight">\(m\)</span>, and with the percepteron having weights <span class="math notranslate nohighlight">\(\beta_k\)</span> for <span class="math notranslate nohighlight">\(k\)</span> going from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(f\)</span>, the prediction of the class of molecule <span class="math notranslate nohighlight">\(m\)</span> is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \text{Lin}_\beta(\vec r^{(m)}) &amp;= \begin{cases}
        1 &amp; \text{ if }\sum_{k = 1}^f\beta_k r_k^{(m)} + \beta_0 &gt; 0, \\
        0 &amp; \text{otherwise}
    \end{cases}
\end{align*}\]</div>
<p>For a particular molecule <span class="math notranslate nohighlight">\(m\)</span> in the current batch, we denote the prediction <span class="math notranslate nohighlight">\(\hat y^{(m)}(\theta, \phi, \rho, \beta)\)</span> to be the predicted class of item <span class="math notranslate nohighlight">\(m\)</span>, which is a function of the parameters for the message passing convolutional network, the node embedding update function, the readout function, and the percepteron. This is computed as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat y^{(m)}(\theta, \phi, \rho, \beta) &amp;= 
    \text{Lin}_\beta(\vec r^{(m)})
\end{align*}\]</div>
</section>
</section>
<section id="the-backwards-pass">
<h4><span class="section-number">9.2.4.1.2. </span>The backwards pass<a class="headerlink" href="#the-backwards-pass" title="Permalink to this headline">#</a></h4>
<p>Finally, once we have prediction labels for all <span class="math notranslate nohighlight">\(M_b\)</span> molecules in the current batch, it is time to reflect what we learned during the forward pass. We compute the loss function, the cross-entropy loss, for all of the items in the batch. If the true label for an item is <span class="math notranslate nohighlight">\(y\)</span> and the prediction is <span class="math notranslate nohighlight">\(\hat y\)</span>, the cross-entropy loss is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \text{CE}(y, \hat y) &amp;= -\left(y\log \hat y + (1 - y)\log(1 - \hat y)\right)
\end{align*}\]</div>
<p>With <span class="math notranslate nohighlight">\(M_b\)</span> items in the batch, the estimated cross-entropy loss is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{m = 1}^{M_b} \text{CE}\left(y^{(m)}, \hat y^{(m)}(\theta, \phi, \rho, \beta)\right)
\end{align*}\]</div>
<p>Once we compute the estimated cross-entropy loss, we are ready to update the parameters for the network. We compute the partial derivatives of the estimated cross-entropy loss with respect to <em>all</em> of the parameters in the network (which, here, are the parameters <span class="math notranslate nohighlight">\(\theta\)</span> for the message passing convolutional network, the parameters <span class="math notranslate nohighlight">\(\phi\)</span> for the node embedding function, the parameters <span class="math notranslate nohighlight">\(\rho\)</span> for the readout function, and the parameters <span class="math notranslate nohighlight">\(\beta\)</span> for the percepteron), and then we update the parameters in a direction that will <em>reduce</em> the estimated cross-entropy loss.</p>
</section>
</section>
<section id="putting-it-all-together">
<h3><span class="section-number">9.2.4.2. </span>Putting it all together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this headline">#</a></h3>
<p>Once we have completed a forwards pass and a backwards pass for a single batch, we move on to the next batch, and just keep repeating for all batches in an epoch. Once we complete an entire epoch (and have learned from every example in the training set exactly <em>once</em>), we compute the prediction accuracy on the testing set.</p>
<p>We then move to the next epoch, and repeat this whole process again, using the final parameters we ended up with on the preceding epoch for the initial parameters in the next epoch.</p>
<p>We repeat for either a pre-determined number of epochs, or until our prediction accuracy on the testing set declines, depending on our preference.</p>
</section>
</section>
<section id="model-training-and-testing">
<h2><span class="section-number">9.2.5. </span>Model training and testing<a class="headerlink" href="#model-training-and-testing" title="Permalink to this headline">#</a></h2>
<p>For our exploration, we will make use of PyTorch (<code class="docutils literal notranslate"><span class="pre">torch</span></code>), which is a machine learning framework that provides a wide variety of algorithms for deep learning. To get started, let’s import everything:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="c1"># for notebook reproducibility</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:</span><span class="o">-</span><span class="mi">150</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="o">-</span><span class="mi">150</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of training graphs: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of test graphs: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of training graphs: 1328
Number of test graphs: 150
</pre></div>
</div>
</div>
</div>
<p>The manner in which neural networks are typically trained is to take the</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.loader</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Step </span><span class="si">{</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=======&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of graphs in the current batch: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">num_graphs</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 1:
=======
Number of graphs in the current batch: 64
DataBatch(x=[1678, 9], edge_index=[2, 3576], edge_attr=[3576, 3], y=[64, 2], smiles=[64], batch=[1678], ptr=[65])
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./next/ch10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="random-walk-diffusion-methods.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9.1. </span>Random walk and diffusion-based methods</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../../appendix/ch11/ch11.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Representations (Extended)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>