
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.2. Testing for Differences between Groups of Edges &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="8.3. Model Selection" href="model-selection.html" />
    <link rel="prev" title="8.1. Community Detection" href="community-detection.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../coverpage.html">
   Hands-on Network Machine Learning with Scikit-Learn and Graspologic
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology and Math Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-networks.html">
     1.4. Types of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.5. Types of Network Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/main-challenges.html">
     1.6. Main Challenges of Network Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/exercises.html">
     1.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/transformation-techniques.html">
     2.4. Transformation Techniques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.5. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.6. Fine-Tune your Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch4/ch4.html">
   4. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/matrix-representations.html">
     4.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/network-representations.html">
     4.2. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/properties-of-networks.html">
     4.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/regularization.html">
     4.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch5/ch5.html">
   5. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/why-use-models.html">
     5.1. Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">
     5.2. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">
     5.3. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">
     5.4. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/multi-network-models.html">
     5.5. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_theory.html">
     5.6. Single network model theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch6/ch6.html">
   6. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">
     6.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/why-embed-networks.html">
     6.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/estimating-parameters_spectral.html">
     6.3. Estimating Parameters with Spectaal Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/random-walk-diffusion-methods.html">
     6.4. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/graph-neural-networks.html">
     6.5. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">
     6.6. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">
     6.7. Joint Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/estimating-parameters_theory.html">
     6.8. Model Estimation Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch7/ch7.html">
   7. Theoretical Results
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch7/theory-single-network.html">
     7.1. Theory for Single Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch7/theory-multigraph.html">
     7.2. Theory for Multiple-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch7/theory-matching.html">
     7.3. Theory for Graph Matching
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch8.html">
   8. Applications When You Have One Network
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="community-detection.html">
     8.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model-selection.html">
     8.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single-vertex-nomination.html">
     8.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="out-of-sample.html">
     8.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch9/ch9.html">
   9. Applications for Two Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/two-sample-hypothesis.html">
     9.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/graph-matching-vertex.html">
     9.2. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/multiple-vertex-nomination.html">
     9.3. Vertex Nomination For Multiple Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch10/ch10.html">
   10. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/anomaly-detection.html">
     10.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/significant-edges.html">
     10.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/significant-vertices.html">
     10.3. Testing for Significant Vertices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/significant-communities.html">
     10.4. Testing for Significant Communities
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/applications/ch8/testing-differences.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/neurodata/graph-stats-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fapplications/ch8/testing-differences.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/applications/ch8/testing-differences.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/network_machine_learning_in_python/applications/ch8/testing-differences.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-structured-independent-edge-model-is-parametrized-by-a-cluster-assignment-matrix-and-a-probability-vector">
   8.2.1. The Structured Independent Edge Model is parametrized by a Cluster-Assignment Matrix and a probability vector
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-cluster-assignment-matrix">
     8.2.1.1. The Cluster-Assignment Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-probability-vector">
     8.2.1.2. The Probability vector
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-simulate-samples-from-an-siem-n-z-vec-p-random-network">
     8.2.1.3. How do we simulate samples from an
     <span class="math notranslate nohighlight">
      \(SIEM_n(Z, \vec p)\)
     </span>
     random network?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-do-we-care-about-the-siem">
   8.2.2. Why do we care about the SIEM?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hypothesis-testing-with-coin-flips">
     8.2.2.1. Hypothesis Testing with coin flips
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#p-values-tell-us-the-chances-of-observing-an-outcome-if-the-null-hypothesis-is-true">
       8.2.2.1.1.
       <span class="math notranslate nohighlight">
        \(p\)
       </span>
       -values tell us the chances of observing an outcome if the null hypothesis is true
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#two-sample-hypothesis-testing-with-coins">
       8.2.2.1.2. Two-sample hypothesis testing with coins
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hypothesis-testing-using-the-siem">
     8.2.2.2. Hypothesis Testing using the SIEM
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unweighted-networks">
       8.2.2.2.1. Unweighted networks
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#weighted-networks">
       8.2.2.2.2. Weighted Networks
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#the-weighted-siem-has-a-vector-of-distribution-functions">
         8.2.2.2.2.1. The weighted SIEM has a vector of distribution functions
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#the-mann-whitney-wilcoxon-u-test">
         8.2.2.2.2.2. The Mann-Whitney Wilcoxon
         <span class="math notranslate nohighlight">
          \(U\)
         </span>
         Test
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="testing-for-differences-between-groups-of-edges">
<h1><span class="section-number">8.2. </span>Testing for Differences between Groups of Edges<a class="headerlink" href="#testing-for-differences-between-groups-of-edges" title="Permalink to this headline">¶</a></h1>
<p>Let’s recall back to our school example. We have a network consisting of <span class="math notranslate nohighlight">\(100\)</span> students who attend one of two schools, with the first <span class="math notranslate nohighlight">\(50\)</span> students attending school one and the second <span class="math notranslate nohighlight">\(50\)</span> students attending school two. The nodes are students, and the edges are whether a pair of students attend the same school. What we want to know is whether there is a higher chance two students are friends if they go to the same school than if they go to two different schools. We don’t know which student goes to which school, so we have some work to do. The network is given to us looking something like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># define the parameters for the SBM</span>
<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># generate a reordering of the n nodes</span>
<span class="n">vtx_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">Aperm</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">vtx_perm</span><span class="p">])]</span> <span class="p">[:,</span><span class="n">vtx_perm</span><span class="p">]</span>
<span class="n">yperm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ys</span><span class="p">)[</span><span class="n">vtx_perm</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">Aperm</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;School Network&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_1905</span><span class="o">/</span><span class="mf">2062876631.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">draw_multiplot</span><span class="p">(</span><span class="n">Aperm</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;School Network&quot;</span><span class="p">);</span>

<span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">hostedtoolcache</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="mf">3.8.12</span><span class="o">/</span><span class="n">x64</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">graphbook_code</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span> <span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span> <span class="kn">from</span> <span class="nn">.siem</span> <span class="kn">import</span> <span class="o">*</span>
<span class="ne">---&gt; </span><span class="mi">28</span> <span class="kn">from</span> <span class="nn">.sbm</span> <span class="kn">import</span> <span class="o">*</span>

<span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">hostedtoolcache</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="mf">3.8.12</span><span class="o">/</span><span class="n">x64</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">graphbook_code</span><span class="o">/</span><span class="n">sbm</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span> <span class="kn">from</span> <span class="nn">graspologic.models</span> <span class="kn">import</span> <span class="n">BaseGraphEstimator</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> <span class="kn">from</span> <span class="nn">graspologic.models.sbm_estimators</span> <span class="kn">import</span> <span class="n">_get_block_indices</span><span class="p">,</span> <span class="n">_calculate_block_p</span><span class="p">,</span> <span class="n">_block_to_full</span>
<span class="ne">---&gt; </span><span class="mi">24</span> <span class="kn">from</span> <span class="nn">FisherExact</span> <span class="kn">import</span> <span class="n">fisher_exact</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span> <span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span>     <span class="n">bernoulli</span><span class="p">,</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;FisherExact&#39;
</pre></div>
</div>
</div>
</div>
<p>The true, but <em>unknown</em>, probability matrix looks something like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">z</span> <span class="o">@</span> <span class="n">B</span> <span class="o">@</span> <span class="n">z</span><span class="o">.</span><span class="n">T</span>

<span class="n">Pperm</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">vtx_perm</span><span class="p">])]</span> <span class="p">[:,</span><span class="n">vtx_perm</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">Pperm</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;True Probability Matrix, School Network&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_5_0.png" src="../../_images/testing-differences_5_0.png" />
</div>
</div>
<p>How do we proceed?</p>
<p>We begin by using spectral methods whichh were developed in <a class="reference external" href="#link?">Chapter 6</a>. Specifically, here we will use the adjacency spectral embedding, and estimate the number of embedding dimensions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.embed</span> <span class="kn">import</span> <span class="n">AdjacencySpectralEmbed</span>

<span class="n">ase</span> <span class="o">=</span> <span class="n">AdjacencySpectralEmbed</span><span class="p">()</span>  <span class="c1"># adjacency spectral embedding, with optimal number of latent dimensions selected using elbow picking</span>
<span class="n">Xhat</span> <span class="o">=</span> <span class="n">ase</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Aperm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We visualize the estimated latent positions using a pairs plot:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">pairplot</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Pairs plot for school network&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_9_0.png" src="../../_images/testing-differences_9_0.png" />
</div>
</div>
<p>It seems pretty obvious that thhere are two distinct communities indicated by the latent positions, which makes sense given that there are students from two schools present. This is indicated in two off-diagonal plots, as we can see that in the upper-right plot for instance, the second latent dimension separates two clusters of latent positions approximately right down the middle. <code class="docutils literal notranslate"><span class="pre">K-means</span></code> picks up on this trend, too:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.cluster</span> <span class="kn">import</span> <span class="n">KMeansCluster</span>

<span class="n">km_clust</span> <span class="o">=</span> <span class="n">KMeansCluster</span><span class="p">(</span><span class="n">max_clusters</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">km_clust</span> <span class="o">=</span> <span class="n">km_clust</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xhat</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>We start by visualizing the number of clusters produced by <code class="docutils literal notranslate"><span class="pre">K-means</span></code>, using the silhouette score:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nclusters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>  <span class="c1"># graspologic nclusters goes from 2 to max_clusters</span>
<span class="n">silhouette</span> <span class="o">=</span> <span class="n">km_clust</span><span class="o">.</span><span class="n">silhouette_</span>  <span class="c1"># obtain the respective silhouette scores</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span> <span class="k">as</span> <span class="n">df</span>

<span class="n">silhouette_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">({</span><span class="s2">&quot;Number of Clusters&quot;</span><span class="p">:</span> <span class="n">nclusters</span><span class="p">,</span> <span class="s2">&quot;Silhouette Score&quot;</span><span class="p">:</span> <span class="n">silhouette</span><span class="p">})</span>  <span class="c1"># place into pandas dataframe</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">silhouette_df</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Number of Clusters&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Silhouette Score&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Silhouette Analysis of KMeans Clusterings&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_14_0.png" src="../../_images/testing-differences_14_0.png" />
</div>
</div>
<p>It i clear that the silhouette score is maximized for <span class="math notranslate nohighlight">\(K=2\)</span> clusters, so <code class="docutils literal notranslate"><span class="pre">K-means</span></code> has indicated that there are two clusters in the latent positions, and consequently two communities of nodes. Next, we produce community estimates for each of the nodes according to their latent positions. Further, since this example is a simulated network, we can also re-align the predicted labels with the true labels. Finally, we computer the overall quality of the clustering using the Adjusted Rand Index, and we visualize the node predictions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.utils</span> <span class="kn">import</span> <span class="n">remap_labels</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">adjusted_rand_score</span>

<span class="n">labels_autokmeans</span> <span class="o">=</span> <span class="n">km_clust</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">Xhat</span><span class="p">)</span>
<span class="n">labels_autokmeans</span> <span class="o">=</span> <span class="n">remap_labels</span><span class="p">(</span><span class="n">yperm</span><span class="p">,</span> <span class="n">labels_autokmeans</span><span class="p">)</span>

<span class="n">ari_kmeans</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_autokmeans</span><span class="p">,</span> <span class="n">yperm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pairplot</span><span class="p">(</span><span class="n">Xhat</span><span class="p">,</span>
         <span class="n">labels</span><span class="o">=</span><span class="n">labels_autokmeans</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;KMeans on embedding, ARI: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ari_kmeans</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
         <span class="n">legend_name</span><span class="o">=</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">,</span>
         <span class="n">height</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span>
         <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;muted&#39;</span><span class="p">,);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_17_0.png" src="../../_images/testing-differences_17_0.png" />
</div>
</div>
<p>Since the ARI is <span class="math notranslate nohighlight">\(1.0\)</span>, <code class="docutils literal notranslate"><span class="pre">K-means</span></code> has perfectly predicted which community each node is assigned to. Finally, let’s take the original adjacency matrix, and just resort the nodes based on the predicted node communities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use argsort to sort by the kmeans labels</span>
<span class="n">node_order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">labels_autokmeans</span><span class="p">)</span>

<span class="c1"># resort the adjacency matrix</span>
<span class="n">Asrt</span> <span class="o">=</span> <span class="n">Aperm</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">node_order</span><span class="p">])][:,</span><span class="n">node_order</span><span class="p">]</span>
<span class="n">ysrt</span> <span class="o">=</span> <span class="n">labels_autokmeans</span><span class="p">[</span><span class="n">node_order</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_multiplot</span><span class="p">(</span><span class="n">Asrt</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">ysrt</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;School Network, nodes sorted by predicted community&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_20_0.png" src="../../_images/testing-differences_20_0.png" />
</div>
</div>
<p>So, our question of interest is whether the within-school connectivity exceeds the between-school connectivity. Let’s think of how to better formulate this problem.</p>
<p>Everything we have done up until this point in the book treated <em>nodes</em> as the fundamental objects of interest for statistical modelling. We have communities of nodes, and clusters of latent positions corresponding to nodes which comprise these communities.</p>
<p>Now, we will introduce a model which instead focuses on clusters of <em>edges</em>. Notice that the edges amongst students of school one are all in the upper left square of the adjacency matrix, and the edges amongst students of school two are all in the lower right square of the adjacency matrix. We will call these edges the “within-school” edges (Cluster <span class="math notranslate nohighlight">\(1\)</span>). The edges between students of school one and two are in the upper right and upper left squares of the adjacency matrix (Cluster <span class="math notranslate nohighlight">\(2\)</span>). In a picture, the edges are grouped as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>

<span class="n">Z</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># within-school edge</span>
<span class="n">Z</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># between-school edge</span>
<span class="n">Z</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># within-school edge</span>
<span class="n">Z</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># between-school edge</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Asrt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;School Network, sorted&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;qualitative&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Edge Cluster&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_23_0.png" src="../../_images/testing-differences_23_0.png" />
</div>
</div>
<p>So now, we know which cluster each edge is in, and we have a sample of a network. Recall that our original question was whether the indivduals within-school (cluster <span class="math notranslate nohighlight">\(1\)</span>) were better friends than individuals between-schools (cluster <span class="math notranslate nohighlight">\(2\)</span>). Reformulating this question in terms of the network, our question becomes whether the edges which are assigned to cluster <span class="math notranslate nohighlight">\(1\)</span> have a <em>higher</em> probability of existing than the edges of cluster <span class="math notranslate nohighlight">\(2\)</span>. For this purpose, we introduce a <em>different</em> generalization of the Stochastic Block Model, the <em>Structured Independent Edge Model</em> (SIEM).</p>
<div class="section" id="the-structured-independent-edge-model-is-parametrized-by-a-cluster-assignment-matrix-and-a-probability-vector">
<h2><span class="section-number">8.2.1. </span>The Structured Independent Edge Model is parametrized by a Cluster-Assignment Matrix and a probability vector<a class="headerlink" href="#the-structured-independent-edge-model-is-parametrized-by-a-cluster-assignment-matrix-and-a-probability-vector" title="Permalink to this headline">¶</a></h2>
<p>To describe the Structured Independent Edge Model (SIEM), we will return to our old coin flipping example.</p>
<div class="section" id="the-cluster-assignment-matrix">
<h3><span class="section-number">8.2.1.1. </span>The Cluster-Assignment Matrix<a class="headerlink" href="#the-cluster-assignment-matrix" title="Permalink to this headline">¶</a></h3>
<p>The cluster assignment matrix <span class="math notranslate nohighlight">\(Z\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix which assigns potential edges in the random network to clusters. What do we mean by this?</p>
<p>Remember that the adjacency matrix <span class="math notranslate nohighlight">\(\mathbf A\)</span> for a random network is <em>also</em> an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix, where each entry <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is a random variable which takes the value <span class="math notranslate nohighlight">\(0\)</span> or the value <span class="math notranslate nohighlight">\(1\)</span> with a particular probability. The cluster assignment matrix takes each of these <span class="math notranslate nohighlight">\(n^2\)</span> random variables, and uses a parameter <span class="math notranslate nohighlight">\(z_{ij}\)</span> to indicate which of <span class="math notranslate nohighlight">\(K\)</span> possible clusters this edge is part of. In the school example, for instance, the edges in the upper left and lower right are in cluster <span class="math notranslate nohighlight">\(1\)</span> (‘within-school’ edges), and the edges in the upper right and lower left are in cluster <span class="math notranslate nohighlight">\(2\)</span> (‘between-school’ edges). In the school example, <span class="math notranslate nohighlight">\(K=2\)</span>.</p>
</div>
<div class="section" id="the-probability-vector">
<h3><span class="section-number">8.2.1.2. </span>The Probability vector<a class="headerlink" href="#the-probability-vector" title="Permalink to this headline">¶</a></h3>
<p>the second parameter for the SIEM is a probability vector, <span class="math notranslate nohighlight">\(\vec p\)</span>. If there are <span class="math notranslate nohighlight">\(K\)</span> edge clusters in the SIEM, then <span class="math notranslate nohighlight">\(\vec p\)</span> is a length-<span class="math notranslate nohighlight">\(K\)</span> vector. Each entry <span class="math notranslate nohighlight">\(p_k\)</span> indicates the probablity of an edge in the <span class="math notranslate nohighlight">\(k^{th}\)</span> cluster existing in the network. For example, <span class="math notranslate nohighlight">\(p_1\)</span> indicates the probability of an edge in the first edge cluster, <span class="math notranslate nohighlight">\(p_2\)</span> indicates the probability of an edge in the second edge cluster, so on and so-forth. In the school example, for instance, the probability <span class="math notranslate nohighlight">\(p_1\)</span> indicates the probability of a within-school edge, and the probability <span class="math notranslate nohighlight">\(p_2\)</span> indicates the probability of a between-school edge.</p>
<p>Like usual, we will formulate the SIEM using our old coin flip example. We begin by obtaining <span class="math notranslate nohighlight">\(K\)</span> coins, where the <span class="math notranslate nohighlight">\(k^{th}\)</span> coin has a chance of landing on heads of <span class="math notranslate nohighlight">\(p_k\)</span>, and a chance of landing on tails of <span class="math notranslate nohighlight">\(1 - p_k\)</span>. For each entry <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>, we identify the corresponding cluster <span class="math notranslate nohighlight">\(z_{ij}\)</span> that this edge is in. Remember that <span class="math notranslate nohighlight">\(z_{ij}\)</span> takes one of <span class="math notranslate nohighlight">\(K\)</span> possible values. We flip the <span class="math notranslate nohighlight">\(z_{ij}\)</span> coin, and if it lands on heads (with probability <span class="math notranslate nohighlight">\(p_{z_{ij}}\)</span>), the edge exists, and if it lands on tails (with probability <span class="math notranslate nohighlight">\(1 - p_{z_{ij}}\)</span>) the edge does not exist.</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an SIEM random network with <span class="math notranslate nohighlight">\(n\)</span> nodes, the cluster assignment matrix <span class="math notranslate nohighlight">\(Z\)</span>, and the probability vector <span class="math notranslate nohighlight">\(\vec p\)</span>, we say that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network.</p>
</div>
<div class="section" id="how-do-we-simulate-samples-from-an-siem-n-z-vec-p-random-network">
<h3><span class="section-number">8.2.1.3. </span>How do we simulate samples from an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network?<a class="headerlink" href="#how-do-we-simulate-samples-from-an-siem-n-z-vec-p-random-network" title="Permalink to this headline">¶</a></h3>
<p>The procedure below will produce for us a network <span class="math notranslate nohighlight">\(A\)</span>, which has nodes and edges, where the underlying random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network:</p>
<div class="admonition-simulating-a-sample-from-an-siem-n-z-vec-p-random-network admonition">
<p class="admonition-title">Simulating a sample from an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network</p>
<ol class="simple">
<li><p>Determine a cluster assignment matrix <span class="math notranslate nohighlight">\(Z\)</span> for each edge of the network. Each edge entry <span class="math notranslate nohighlight">\(z_{ij}\)</span> can take one of <span class="math notranslate nohighlight">\(K\)</span> possible values.</p></li>
<li><p>Determine a length-<span class="math notranslate nohighlight">\(K\)</span> probability vector for each of the <span class="math notranslate nohighlight">\(K\)</span> edge clusters.</p></li>
<li><p>For each of the <span class="math notranslate nohighlight">\(K\)</span> clusters, obtain <span class="math notranslate nohighlight">\(K\)</span> total weighted coins, where the <span class="math notranslate nohighlight">\(k^{th}\)</span> coin lands on heads with probability <span class="math notranslate nohighlight">\(p_k\)</span> and tails with probability <span class="math notranslate nohighlight">\(1 - p_k\)</span>.</p></li>
<li><p>For each edge <span class="math notranslate nohighlight">\((i, j)\)</span>:</p>
<ul class="simple">
<li><p>Denote <span class="math notranslate nohighlight">\(z_{ij}\)</span> to be the cluster assignment of the potential edge between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>Flip the <span class="math notranslate nohighlight">\(z_{ij}\)</span> coin, and if it lands on heads, the corresponding entry in the adjacency matrix <span class="math notranslate nohighlight">\(a_{ij}\)</span> is <span class="math notranslate nohighlight">\(1\)</span>. If it lands on tails, the coresponding entry in the adjacency matrix <span class="math notranslate nohighlight">\(a_{ij}\)</span> is <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
</ul>
</li>
<li><p>The adjacency matrix we produce, <span class="math notranslate nohighlight">\(A\)</span>, is a sample of an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network.</p></li>
</ol>
</div>
</div>
</div>
<div class="section" id="why-do-we-care-about-the-siem">
<h2><span class="section-number">8.2.2. </span>Why do we care about the SIEM?<a class="headerlink" href="#why-do-we-care-about-the-siem" title="Permalink to this headline">¶</a></h2>
<p>If you recall from the <a class="reference external" href="#link?">Section on RDPGs</a>, we learned that for every <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network, there is a corresponding latent position matrix <span class="math notranslate nohighlight">\(X\)</span> such that the probability matrices of the <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network and an <span class="math notranslate nohighlight">\(RDPG_n(X)\)</span> random network are the same. In this sense, the RDPG random networks <em>generalize</em> the SBM random networks, in that all SBM random networks are RDPG random networks, but not all RDPG random networks are SBM random networks. We can find latent position matrices which produce networks which cannot be summarized with an SBM, such as the <a class="reference external" href="#link?">later example we saw in the RDPG section</a>. The SIEM has a similar relationship with the SBM: all SBM random networks are SIEM random networks, but not all SIEM random networks are SBM random networks. If you want the gorey details of how this works, check out the block below. If not, you can skip on to the next paragraph.</p>
<div class="admonition-all-sbms-are-siems admonition">
<p class="admonition-title">All SBMs are SIEMs</p>
<p>What this means is that for any <span class="math notranslate nohighlight">\(SBM_n(\vec z', B)\)</span> random network, there is a corresponding <span class="math notranslate nohighlight">\(Z\)</span> and <span class="math notranslate nohighlight">\(\vec p\)</span> such that an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network has the same probability matrix. We can see this pretty easily: if <span class="math notranslate nohighlight">\(K\)</span> was the number of communities of the SBM, we can define an SIEM to have a single edge cluster for each <em>pair</em> of communities. We can see this as follows:</p>
<ol class="simple">
<li><p>Define the number of edge clusters: Edge cluster <span class="math notranslate nohighlight">\(1\)</span> consists of edges between nodes both in community <span class="math notranslate nohighlight">\(1\)</span>, edge cluster <span class="math notranslate nohighlight">\(2\)</span> consists of edges between a node in community <span class="math notranslate nohighlight">\(1\)</span> and a node in community <span class="math notranslate nohighlight">\(2\)</span>, so on and so forth up to edges between a node in community <span class="math notranslate nohighlight">\(1\)</span> and a node in community <span class="math notranslate nohighlight">\(K\)</span>. Edge cluster <span class="math notranslate nohighlight">\(K+1\)</span> consists of edges between a node in community <span class="math notranslate nohighlight">\(2\)</span> and a node in community <span class="math notranslate nohighlight">\(1\)</span>, and again we repeat this up to edge cluster <span class="math notranslate nohighlight">\(2K\)</span> which delineates edges between a node in community <span class="math notranslate nohighlight">\(2\)</span> and a node in community <span class="math notranslate nohighlight">\(K\)</span>. We continue this procedure up to edge cluster <span class="math notranslate nohighlight">\(K^2\)</span> which is between two nodes in community <span class="math notranslate nohighlight">\(K\)</span>. This gives us a map, where for each pair of communities <span class="math notranslate nohighlight">\(l\)</span> and <span class="math notranslate nohighlight">\(l'\)</span>, we have an edge cluster <span class="math notranslate nohighlight">\(k\)</span> whch corresponds to this pair of communities.</p></li>
<li><p>Define the edge cluster matrix: Proceed edge-by-edge through the network looking at the community asignment vector <span class="math notranslate nohighlight">\(\vec z'\)</span>. The edge cluster for an edge <span class="math notranslate nohighlight">\((i, j)\)</span> is the edge cluster we identified above between the communities for node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(z_i'\)</span> and <span class="math notranslate nohighlight">\(z_j'\)</span> respectively.</p></li>
<li><p>Define the edge cluster probability vector: Let the probability vector <span class="math notranslate nohighlight">\(\vec p\)</span> be defined such that, iff the <span class="math notranslate nohighlight">\(l\)</span> and <span class="math notranslate nohighlight">\(l'\)</span> communities correspond to edge cluster <span class="math notranslate nohighlight">\(k\)</span>, then <span class="math notranslate nohighlight">\(p_k\)</span> has the same value as <span class="math notranslate nohighlight">\(b_{ll'}\)</span> in the block matrix.
This produces an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network with the same probability matrix as an <span class="math notranslate nohighlight">\(SBM_n(\vec z, B)\)</span> random network.</p></li>
</ol>
</div>
<p>For this reason, we will develop statistical tools for the SIEM, but it is important to note that they apply equally well to an SBM random network.</p>
<p>Remember that our question boiled down to whether the edges which are assigned to cluster <span class="math notranslate nohighlight">\(1\)</span> have a <em>higher</em> probability of existing than the edges of cluster <span class="math notranslate nohighlight">\(2\)</span>. In other words, we <em>hypothesize</em> that the edges assigned to cluster <span class="math notranslate nohighlight">\(1\)</span> have a higher probability of existing than the edges of cluster <span class="math notranslate nohighlight">\(2\)</span>. In terms of the probability vector, what we are saying is that we think that <span class="math notranslate nohighlight">\(p_1 &gt; p_2\)</span>. Obviously, we are wrong if <span class="math notranslate nohighlight">\(p_1 \leq p_2\)</span>. This type of statistical question is known as a <em>hypothesis test</em>. <strong>Hypothesis testing</strong> is an approach by which we formulate statistical questions and present approaches which, <em>using the data</em>, indicates how much the data supports or doess not support the statistical questions we asked (or, provide us with the ability to <em>infer</em> something). For this reason, hypothesis testing is often an essential component of <em>statistical inference</em>, which is the name given to the process of learning something from data using statistical models.</p>
<p>The way hypothesis testing works is we call one hypothesis a <em>null hypothesis</em> and the other an <em>alternative hypothesis</em>. The <strong>null hypothesis</strong> is the hypothesis where, if true, indicates that we were <em>wrong</em> about what how we thought the system behaved. In our example data, the null hypothesis is that <span class="math notranslate nohighlight">\(p_1 \leq p_2\)</span>. We typically would write this out notationally as <span class="math notranslate nohighlight">\(H_0: p_1 \leq p_2\)</span>. The capital <span class="math notranslate nohighlight">\(H\)</span> is used to denote that this is a hypothesis, and the little “<span class="math notranslate nohighlight">\(0\)</span>” is just to indicate “null”. The <strong>alternative hypothesis</strong> is the hypothesis where, if true, indicates that we werer <em>correct</em> about how we thought the system behaved. In our example, the alternative hypothesis is that <span class="math notranslate nohighlight">\(p_1 &gt; p_2\)</span>, which we write out as <span class="math notranslate nohighlight">\(H_A: p_1 &gt; p_2\)</span>. Here, the subscript <span class="math notranslate nohighlight">\(A\)</span> denotes that this is the alternative hypothesis. When describing our hypothesis, we say that we are testing <span class="math notranslate nohighlight">\(H_A: p_1 &gt; p_2\)</span> <em>against</em> <span class="math notranslate nohighlight">\(H_0: p_1 \leq p_2\)</span>. This is because a hypothesis test must specifically delineate the possible situations in which we are <em>right</em> (the alternative <span class="math notranslate nohighlight">\(H_A\)</span>) and when we are <em>wrong</em> (the null <span class="math notranslate nohighlight">\(H_0\)</span>). We perform statisical inference by looking at how well the data we are presented with supports, or reflects, the alternative hypothesis.</p>
<div class="section" id="hypothesis-testing-with-coin-flips">
<h3><span class="section-number">8.2.2.1. </span>Hypothesis Testing with coin flips<a class="headerlink" href="#hypothesis-testing-with-coin-flips" title="Permalink to this headline">¶</a></h3>
<p>To describe how hypothesis testing works from an implementation perspective, we’ll naturally return to coin flips. If you remember way back to the <a class="reference external" href="#link?">Preface of Chapter 5</a>, we introduced a game. The game was as follows: a gambler has a coin, and asks us to gamble one dollar. If the coin lands on heads, we obtain our dollar back, and an additional dollar. If the coin lands on tails, we lose our dollar. We get to observe <span class="math notranslate nohighlight">\(10\)</span> random coin flips, and determine whether we want to play the game.</p>
<p>As it turns out, if this coin has a probability of landing on heads greater than <span class="math notranslate nohighlight">\(0.5\)</span>, we will, over time, <em>make</em> money if we continue to play the game. If the coin has a probablity off landing on heads lesss than <span class="math notranslate nohighlight">\(0.5\)</span>, we will, over time, <em>lose</em> money if we continue to play the game. So, if we are going to play the game in a principled way, we want to be really sure that we are going to gain money by playing. How can we use hypothesis testing to use a little bit of math to determine whether we should play or not?</p>
<p>To begin, we need to delineate our hypotheses. Here, our question of interest is, should we play the game? Therefore, the alternative hypothesis is the one where we <em>should</em> play the game, or the coin has a probability greater than <span class="math notranslate nohighlight">\(0.5\)</span>. The null hypothesis is that we should <em>not</em> play the game, or the coin has a probability that is not greaterr than <span class="math notranslate nohighlight">\(0.5\)</span>. If we use <span class="math notranslate nohighlight">\(p\)</span> to denote the probability that the gambler’s coin lands on heads, we write this as <span class="math notranslate nohighlight">\(H_A: p &gt; 0.5\)</span>, against the null hypothesis that <span class="math notranslate nohighlight">\(H_0: p \leq 0.5\)</span>.</p>
<p>Let’s say that we observe <span class="math notranslate nohighlight">\(10\)</span> coin flips, and we see that the coin lands on heads <span class="math notranslate nohighlight">\(6\)</span> times. If we were to estimate the coin’s probability of landing on heads, like we did in the <a class="reference external" href="#link?">Preface of the Section on MLE</a>, we would get that <span class="math notranslate nohighlight">\(\hat p = \frac{6}{10}\)</span>, or that our estimate <span class="math notranslate nohighlight">\(\hat p = 0.6\)</span>. It’s pretty obvious that <span class="math notranslate nohighlight">\(\hat p\)</span> is greater than <span class="math notranslate nohighlight">\(0.5\)</span>, so we should play the game, right?</p>
<p>The answer is a little more complicated than that. If you take a fair coin (one that is roughly equally likely to land on heads and tails) and flip it enough times, you will probably find a sequence of ten flips with six heads. We are scientists, after all, so we want to be <em>really</em> sure that if we play we are going to make money. In statistical speak, we want to be sure that if we play the game, there is a low chance that we are actually wrong. We want to know exactly what the chances are that, if we were <em>wrong</em>, we could still have seen the estimate <span class="math notranslate nohighlight">\(\hat p\)</span> that we calculated. This is where a concept called a <em>p-value</em> comes into play.</p>
<div class="section" id="p-values-tell-us-the-chances-of-observing-an-outcome-if-the-null-hypothesis-is-true">
<h4><span class="section-number">8.2.2.1.1. </span><span class="math notranslate nohighlight">\(p\)</span>-values tell us the chances of observing an outcome if the null hypothesis is true<a class="headerlink" href="#p-values-tell-us-the-chances-of-observing-an-outcome-if-the-null-hypothesis-is-true" title="Permalink to this headline">¶</a></h4>
<p>To understand the <span class="math notranslate nohighlight">\(p\)</span>-value, we must first introduce a concept called a <em>test statistic</em>. A test-statistic is a fancy word for a data-derived quantity that summarizes our data with a single number which will be used for hypothesis testing. The test statistic should capture a property of the data that is relevant to our question of interest (here, the probability that the coin lands on heads), so it makes sense in our example to just use the estimate of the probability that the coin lands on heads as our test statistic. In general, the <strong><span class="math notranslate nohighlight">\(p\)</span>-value</strong> is the probability that we would incorrectly reject the null hypothesis (that is, that the coin is fair) in favor of the alternative hypothesis (that is, that the coin is not fair) if the null hypothesis were really true.</p>
<p>Next, we need to think about what types of values we would see from the test statistic if the null hypothesis were true. If the coin really had a probability of <span class="math notranslate nohighlight">\(0.5\)</span> of landing on heads and we flipped it <span class="math notranslate nohighlight">\(10\)</span> times, there is a non-zero probabaility that we would see all heads and our estimate would have been <span class="math notranslate nohighlight">\(1.0\)</span>, that we would see <span class="math notranslate nohighlight">\(9\)</span> heads and <span class="math notranslate nohighlight">\(1\)</span> tails and our estimate would have been <span class="math notranslate nohighlight">\(0.9\)</span>, so on and so-forth all the way to the outcome where we see only tails and our estimate would have been <span class="math notranslate nohighlight">\(0.0\)</span>. Which one we observed when watching the gambler flip the coin would come down to <em>random chance</em>, so we will use the random variable <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span> to describe this quantity. Specifically, <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span> is a random variable which takes values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> in <span class="math notranslate nohighlight">\(0.1\)</span> probability increments. <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span>’s probability of taking a given increment <span class="math notranslate nohighlight">\(k\)</span> is equal to the probability that we would have estimated the probability to be <span class="math notranslate nohighlight">\(k\)</span> if the null hypothesis (the coin is fair and lands on heads with probability <span class="math notranslate nohighlight">\(0.5\)</span>) were true. <span class="math notranslate nohighlight">\(\mathbf{\hat p}_0\)</span> takes each increment with the following probabilities:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="n">incr</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">/</span><span class="mi">10</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">)]</span>
<span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">i</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">incr</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">incr</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Estimate of probability of landing on heads, $\hat p_0$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability of seeing this estimate if $H_0$ true&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Distribution of $\hat p_0$&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="s2">&quot;Estimate we observed, $\hat p = 0.6$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_27_0.png" src="../../_images/testing-differences_27_0.png" />
</div>
</div>
<p>So, this is a little surprising! Even if the null hypothesis were true and the coin really landed on heads with a probability of <span class="math notranslate nohighlight">\(0.5\)</span>, there is still a pretty good chance we would observe a probability of at least <span class="math notranslate nohighlight">\(0.6\)</span> if we were to flip a fair coin <span class="math notranslate nohighlight">\(10\)</span> times. We describe this quantity using a <span class="math notranslate nohighlight">\(p\)</span>-value, which is the probability of seeing a test statistic at least as extreme as the one we calculated from our data if the null hypothesis were true. Here, the <span class="math notranslate nohighlight">\(p\)</span>-value is the probability that, if the coin were fair, we would produce an estimate of <span class="math notranslate nohighlight">\(0.6\)</span>, <span class="math notranslate nohighlight">\(0.7\)</span>, <span class="math notranslate nohighlight">\(0.8\)</span>, <span class="math notranslate nohighlight">\(0.9\)</span>, or <span class="math notranslate nohighlight">\(1.0\)</span>. This is the sum of all of the heights of the corresponding bars in the above plot, which turns out to be <span class="math notranslate nohighlight">\(0.377\)</span>. This approach is called the <em>binomial test</em>, which we can implement in python as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom_test</span>

<span class="n">nheads</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1"># the number of heads in the experiment</span>
<span class="n">nflips</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># the total number of coin flips in the experiment</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># the probability of the coin under the null hypothesis</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">binom_test</span><span class="p">(</span><span class="n">nheads</span><span class="p">,</span> <span class="n">nflips</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s2">&quot;greater&quot;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p-value: 0.377
</pre></div>
</div>
</div>
</div>
<p>This means that, even if the coin were fair, we still would have produced an estimate of <span class="math notranslate nohighlight">\(0.6\)</span> about <span class="math notranslate nohighlight">\(37.7\%\)</span> of the time, and we would be <em>wrong</em> to play the game. This might be enough for you to play the game, but as scientists we usually like to be a little more confident than this when we report something. We use the term the “<span class="math notranslate nohighlight">\(\alpha\)</span> of the test” to describe what an acceptable chance of being incorrect is for us. We can pick <span class="math notranslate nohighlight">\(\alpha\)</span> to be anything we want really so long as decide this threshold before calculating the <span class="math notranslate nohighlight">\(p\)</span>-value. Usually, scientists choose <span class="math notranslate nohighlight">\(0.05\)</span>, which means that we want there to be less than a <span class="math notranslate nohighlight">\(5\%\)</span> chance that we are wrong before making a decision. In other words, we look for a <span class="math notranslate nohighlight">\(p\)</span>-value that is less than <span class="math notranslate nohighlight">\(0.05\)</span>.</p>
</div>
<div class="section" id="two-sample-hypothesis-testing-with-coins">
<h4><span class="section-number">8.2.2.1.2. </span>Two-sample hypothesis testing with coins<a class="headerlink" href="#two-sample-hypothesis-testing-with-coins" title="Permalink to this headline">¶</a></h4>
<p>Let’s say after seeing the first game, you determine that a <span class="math notranslate nohighlight">\(37.7\%\)</span> chance of being wrong is a little too risky for you. The gambler presents you with a new game. He has two coins this time, and will even let you pick which to use. You gamble one dollar, and if your coin comes up heads but his comes up tails, you get your dollar plus an additional dollar. If both coins land on heads or tails, you get your dollar back. If his coin comes up heads but your coin comes up tails, you lose your dollar. This time, you get to see the gambler flip each coin ten times before determining whether you want to play. The first coin lands on heads seven times, and the second coin lands on heads only once. If the first coin actually has a higher chance of landing on heads than his coin, you should play the game. Will you gamble this time?</p>
<p>To formalize this situation, we’ll use similar notation to before. We’ll call <span class="math notranslate nohighlight">\(p_1\)</span> the probability that your coin lands on heads, and <span class="math notranslate nohighlight">\(p_2\)</span> the probability that his coin lands on heads. Our alternative hypothesis here is <span class="math notranslate nohighlight">\(H_A: p_1 \neq p_2\)</span>, against the null hypothesis <span class="math notranslate nohighlight">\(H_0: p_1 = p_2\)</span>. A <strong>two-sample test</strong> is a test which is performed when we have two random samples, and we want to investigate whether there is a difference between the two random samples. Specifically here, we have the outcomes of coin flips from each of the two coins, and we want to determine whether the first coin is actually different from the second coin. Intuitionally, the idea is that even if the first coin did not have a higher probability of landing on heads than the second coin, there is still some probability that if we flipped each coin ten times, we would get more heads with the second coin. We see that our estimates of the probabailities are <span class="math notranslate nohighlight">\(\hat p_1 = \frac{7}{10} = 0.7\)</span> and <span class="math notranslate nohighlight">\(\hat p_2 = \frac{1}{10} = 0.1\)</span> respectively.</p>
<p>When we have two samples of data whose outcomes boil down to ones and zeros (or, heads and tails), it turns out that the intuition gets more complex pretty quickly. For this reason, we won’t explain the two-sample test for binary data in quite as muc depth as we did with the previous example (the <strong>one-sample test</strong>), but much of the same intuition holds. Again, we seek to compute a <span class="math notranslate nohighlight">\(p\)</span>-value which tells us the chances of observing that <span class="math notranslate nohighlight">\(\hat p_1 = 0.7\)</span> and <span class="math notranslate nohighlight">\(\hat p_2 = 0.1\)</span> if <span class="math notranslate nohighlight">\(p_1\)</span> weren’t actually greater than <span class="math notranslate nohighlight">\(p_2\)</span>. In this case, the appropriate statistical test is known as <a class="reference external" href="https://en.wikipedia.org/wiki/Fisher%27s_exact_test">Fisher’s Exact Test</a>, which we can implement using <code class="docutils literal notranslate"><span class="pre">scipy</span></code>. The inputs for Fisher’s exact test is a contingency table of the results:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>First coin</p></th>
<th class="head"><p>Second coin</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Landed on heads</p></td>
<td><p>7</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Landed on tails</p></td>
<td><p>3</p></td>
<td><p>9</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">fisher_exact</span>

<span class="n">table</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">]]</span>
<span class="n">test_statistic</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">fisher_exact</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p-value: 0.020
</pre></div>
</div>
</div>
</div>
<p>This means that, there is only about a <span class="math notranslate nohighlight">\(2\%\)</span> chance that the two coins have the same probability of landing on heads, as the <span class="math notranslate nohighlight">\(p\)</span>-value is <span class="math notranslate nohighlight">\(0.02\)</span>. If we were to again use <span class="math notranslate nohighlight">\(\alpha=0.05\)</span> as our decision threshold, this means we should play this game as many times as the gambler will allow before he catches on. Next, we’ll see how this strategy of coin flipping applies to the SIEM.</p>
</div>
</div>
<div class="section" id="hypothesis-testing-using-the-siem">
<h3><span class="section-number">8.2.2.2. </span>Hypothesis Testing using the SIEM<a class="headerlink" href="#hypothesis-testing-using-the-siem" title="Permalink to this headline">¶</a></h3>
<div class="section" id="unweighted-networks">
<h4><span class="section-number">8.2.2.2.1. </span>Unweighted networks<a class="headerlink" href="#unweighted-networks" title="Permalink to this headline">¶</a></h4>
<p>To shift back to our network, we had the following network sample and edge cluster matrix:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">heatmap</span><span class="p">,</span> <span class="n">cmaps</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">heatmap</span><span class="p">(</span><span class="n">Asrt</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;School Network, sorted&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;qualitative&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Edge Cluster&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_35_0.png" src="../../_images/testing-differences_35_0.png" />
</div>
</div>
<p>The probability vector for the SIEM is estimated very similar to how we used maximum likelihood estimation for the SBM, in that we just take the fraction of edges which exist in the network for each edge cluster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Asrt</span><span class="p">[</span><span class="n">Z</span> <span class="o">==</span> <span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">Asrt</span><span class="p">[</span><span class="n">Z</span> <span class="o">==</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probability for cluster </span><span class="si">{}</span><span class="s2">, p</span><span class="si">{}</span><span class="s2">: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]];</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Probability for cluster 1, p1: 0.589
Probability for cluster 2, p2: 0.300
</pre></div>
</div>
</div>
</div>
<p>These estimates are pretty different, but the core question we want to answer is, how confident are we that this discrepancy between the estimated probabilities is not due to random chance?</p>
<p>It turns out that this question is effectively the same question as we asked for the coin flips. Specifically, we want to formulate the hypothesis test with <span class="math notranslate nohighlight">\(H_0: p_1 \leq p_2\)</span> against <span class="math notranslate nohighlight">\(H_A: p_1 &gt; p_2\)</span>. We proceed with exactly the same framework as we used for the coin tosses. We again construct a contingency table, with the following table margins:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Cluster 1, the within-school edges</p></th>
<th class="head"><p>Cluster 2, the between-school edges</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Number of edges</p></td>
<td><p><span class="math notranslate nohighlight">\(a\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(b\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Number of non-edges</p></td>
<td><p><span class="math notranslate nohighlight">\(c\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d\)</span></p></td>
</tr>
</tbody>
</table>
<p>The entry <span class="math notranslate nohighlight">\(a\)</span> is the total number of edges in the first cluster, and <span class="math notranslate nohighlight">\(b\)</span> is the total number of edge in the second cluster. The entry <span class="math notranslate nohighlight">\(c\)</span> is the total number of possible edges in cluster one which do not exist (the number of adjacencies in cluster one with an adjacency of zero), and the entry <span class="math notranslate nohighlight">\(d\)</span> is the total number of possible edges in the second cluster which do not exist (the number of adjacencies in cluster two with a value of zero). We can fill in the entries of this table using some basic <code class="docutils literal notranslate"><span class="pre">numpy</span></code> functions. Note that since this network is undirected and loopless, we want to look only at the edges which are in the upper triangle of the adjacency martix, and exclude the diagonal:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get the indices of the upper triangle of Asrt</span>
<span class="n">upper_tri_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="n">Asrt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># create a boolean array that is nxn</span>
<span class="n">upper_tri_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Asrt</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="c1"># set indices which correspond to the upper triangle to True</span>
<span class="n">upper_tri_mask</span><span class="p">[</span><span class="n">upper_tri_idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">table</span> <span class="o">=</span> <span class="p">[[(</span><span class="n">Asrt</span><span class="p">[(</span><span class="n">Z</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">upper_tri_mask</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Asrt</span><span class="p">[(</span><span class="n">Z</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">upper_tri_mask</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)],</span>
         <span class="p">[(</span><span class="n">Asrt</span><span class="p">[(</span><span class="n">Z</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">upper_tri_mask</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Asrt</span><span class="p">[(</span><span class="n">Z</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">upper_tri_mask</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">cmaps</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="s2">&quot;sequential&quot;</span><span class="p">],</span>
            <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="s2">&quot;Count&quot;</span><span class="p">})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;Cluster 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Cluster 2&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;Number of edges&quot;</span><span class="p">,</span> <span class="s2">&quot;Number of non-edges&quot;</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Contingency Table, School Network&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/testing-differences_40_0.png" src="../../_images/testing-differences_40_0.png" />
</div>
</div>
<p>We again use Fisher’s exact test to test our hypothesis:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_statistic</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">fisher_exact</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>And we end up with a <span class="math notranslate nohighlight">\(p\)</span>-value that is approximately zero. With our decision threshold still at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, since the <span class="math notranslate nohighlight">\(p\)</span>-value is less than <span class="math notranslate nohighlight">\(\alpha\)</span>, we have evidence to support that the alternative hypothesis is true. So, after a <em>lot</em> of build up spanning numerous sections, we have shown that the probability of two individuals being friends if they attend the same school is higher than the probability of two students being friends if they go to different schoools. Pretty exciting, huh?</p>
</div>
<div class="section" id="weighted-networks">
<h4><span class="section-number">8.2.2.2.2. </span>Weighted Networks<a class="headerlink" href="#weighted-networks" title="Permalink to this headline">¶</a></h4>
<p>When the networks were unweighted, everything made complete sense in the context of our coin flip regime. For weighted networks, on the other hand, things can go a <em>little</em> bit differently. To understand this question in the context of weighted networks, we’ll return to our discussion on the SIEM, and reformulate it a little bit for the case where the adjacency matrix’s entries take non-binary values (they aren’t just zeros and ones).</p>
<div class="section" id="the-weighted-siem-has-a-vector-of-distribution-functions">
<h5><span class="section-number">8.2.2.2.2.1. </span>The weighted SIEM has a vector of distribution functions<a class="headerlink" href="#the-weighted-siem-has-a-vector-of-distribution-functions" title="Permalink to this headline">¶</a></h5>
<p>For the weighted SIEM, the first parameter is identical to that of the unweighted SIEM: the edge cluster matrix, <span class="math notranslate nohighlight">\(Z\)</span>, which is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix whose entries <span class="math notranslate nohighlight">\(z_{ij}\)</span> indicate which of the <span class="math notranslate nohighlight">\(K\)</span> clusters the <span class="math notranslate nohighlight">\((i, j)\)</span> edge is in.</p>
<p>The second parameter is a little different. Remember for the unweighted case, we had a probability vector, <span class="math notranslate nohighlight">\(\vec p\)</span>, whose entries <span class="math notranslate nohighlight">\(p_k\)</span> indicated the probability of an edge in the <span class="math notranslate nohighlight">\(k^{th}\)</span> edge cluster. How do we extend this concept to edges which are not binary valued? To accomplish this, we introduce disribution functions.</p>
<p>Remember we said that, for a particular edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> where the edge cluster for the <span class="math notranslate nohighlight">\((i,j)\)</span> edge was <span class="math notranslate nohighlight">\(z_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> took a value of one with probability <span class="math notranslate nohighlight">\(p_{z_{ij}}\)</span>, and zero with probability <span class="math notranslate nohighlight">\(1 - p_{z_{ij}}\)</span>. As it turns out, what this says at a deeper level is that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> has a <em>distribution</em> whose parameter is just a probability <span class="math notranslate nohighlight">\(p_{z_{ij}}\)</span>. The distribution, in this case, is the <em>Bernoulli</em> distribution with probabilty <span class="math notranslate nohighlight">\(p_{z_{ij}}\)</span>, which we learned about in <a class="reference external" href="#link?">the preface for chapter 5</a>. However, there are other distributions we could use as well for <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>.</p>
<p>You are probably already familiar with the Normal distribution, which has a mean <span class="math notranslate nohighlight">\(\mu\)</span> and a standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. What you need to know about the Normal distribution is that, if you were to sample a large number of points and then plotted their values on a histogram, you would see “roughly” a Bell-curve type of shape to the tops of the histogram bars. These bars would be centered roughly around the mean value <span class="math notranslate nohighlight">\(\mu\)</span>, the bars would be roughly symmetric (the heights on either side of the mean would be about equal), and that about <span class="math notranslate nohighlight">\(68\%\)</span> of the data would be between <span class="math notranslate nohighlight">\(\mu - \sigma\)</span> and <span class="math notranslate nohighlight">\(\mu + \sigma\)</span> (within one standard deviation). In the below plot, we will simulate <span class="math notranslate nohighlight">\(10,000\)</span> samples <span class="math notranslate nohighlight">\(x_i\)</span> of random variables <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>, which are Normally distributed with a mean of <span class="math notranslate nohighlight">\(0\)</span> and a standard deviation of <span class="math notranslate nohighlight">\(5\)</span>, <span class="math notranslate nohighlight">\(10,000\)</span> samples <span class="math notranslate nohighlight">\(y_i\)</span> of random variables <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> with are Normally disributed with a mean of <span class="math notranslate nohighlight">\(5\)</span> and a standard deviation of <span class="math notranslate nohighlight">\(5\)</span>, and <span class="math notranslate nohighlight">\(10,000\)</span> samples <span class="math notranslate nohighlight">\(z_i\)</span> of random variables <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> which are Normally distributed with a mean of <span class="math notranslate nohighlight">\(0\)</span> and a standard deviation of <span class="math notranslate nohighlight">\(15\)</span>. We will then compute histograms so we can get a sense of how the Normal distribution behaves:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;xi&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="s2">&quot;yi&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span> <span class="s2">&quot;zi&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="mi">15</span><span class="p">}}</span>
<span class="n">nsim</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">samples</span><span class="o">.</span><span class="n">extend</span><span class="p">([{</span><span class="s2">&quot;Sample&quot;</span><span class="p">:</span> <span class="n">idx</span><span class="p">,</span> <span class="s2">&quot;Value&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">nsim</span><span class="p">,</span> <span class="o">**</span><span class="n">param</span><span class="p">)])</span>

<span class="n">real_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">real_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Value&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;sample&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;probability&quot;</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">common_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of Points&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that samples of <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> and <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> differ in the approximate centers of the histograms. This reflects that the mean of the random variables <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> are <span class="math notranslate nohighlight">\(10\)</span> units less than the means of <span class="math notranslate nohighlight">\(\mathbf y_i\)</span>. Further, samples of <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> appear to take a “wider” range of values than samples of <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>, despite the centers looking to be in approximately the same spot (right around zero). This reflect that the standard deviation of the random variables <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> exceed that of the <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> by <span class="math notranslate nohighlight">\(10\)</span> units. We could, in a generic sense, say that the points <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> have distribution <span class="math notranslate nohighlight">\(F_x\)</span>, the points <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> have the distribution <span class="math notranslate nohighlight">\(F_y\)</span>, and the points <span class="math notranslate nohighlight">\(\mathbf z_i\)</span> have the distribution <span class="math notranslate nohighlight">\(F_z\)</span>. Here, <span class="math notranslate nohighlight">\(F_x\)</span> just so happens to be the Normal distribution with mean <span class="math notranslate nohighlight">\(0\)</span> and standard deviation <span class="math notranslate nohighlight">\(5\)</span>, but we don’t need to be that specific when defining the weighted SIEM.</p>
<p>For the weighted SIEM, we will have a length-<span class="math notranslate nohighlight">\(K\)</span> vector <span class="math notranslate nohighlight">\(\vec F\)</span> of distribution functions, where <span class="math notranslate nohighlight">\(F_k\)</span> defines the distribution for the <span class="math notranslate nohighlight">\(k^{th}\)</span> cluster. In this sense, <span class="math notranslate nohighlight">\(F_k\)</span> could be any distribution really; it could be the normal distribution with mean <span class="math notranslate nohighlight">\(\mu_k\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma_k\)</span>, it could be the Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p_k\)</span>, or it could be something more complicated. There are any number of possibilities for what the edge-weight distribution could be.</p>
<p>Unfortunately, we can’t quite use our coin flip example since the weighted SIEM does not necessarily have binary-valued adjacencies. For each entry <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>, we identify the corresponding cluster <span class="math notranslate nohighlight">\(z_{ij}\)</span> that this edge is in. Remember that <span class="math notranslate nohighlight">\(z_{ij}\)</span> takes one of <span class="math notranslate nohighlight">\(K\)</span> possible values. We say that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> has the distribution indicated by the distribution function <span class="math notranslate nohighlight">\(F_{z_{ij}}\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an SIEM random network with <span class="math notranslate nohighlight">\(n\)</span> nodes, the cluster assignment matrix <span class="math notranslate nohighlight">\(Z\)</span>, and the distribution vector <span class="math notranslate nohighlight">\(\vec F\)</span>, we say that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <span class="math notranslate nohighlight">\(wSIEM_n(Z, \vec F)\)</span> random network.</p>
<p>Next, let’s look at how one would sample from a weighted SIEM. The procedure below will simulate a sample <span class="math notranslate nohighlight">\(A\)</span> of the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is <span class="math notranslate nohighlight">\(wSIEM_n(Z, \vec F)\)</span>.</p>
<div class="admonition-simulating-a-sample-from-an-wsiem-n-z-vec-f-random-network admonition">
<p class="admonition-title">Simulating a sample from an <span class="math notranslate nohighlight">\(wSIEM_n(Z, \vec F)\)</span> random network</p>
<ol class="simple">
<li><p>Determine a cluster assignment matrix <span class="math notranslate nohighlight">\(Z\)</span> for each edge of the network. Each edge entry <span class="math notranslate nohighlight">\(z_{ij}\)</span> can take one of <span class="math notranslate nohighlight">\(K\)</span> possible values.</p></li>
<li><p>Determine a length-<span class="math notranslate nohighlight">\(K\)</span> distribution vector for each of the <span class="math notranslate nohighlight">\(K\)</span> clusters.</p></li>
<li><p>For each edge <span class="math notranslate nohighlight">\((i, j)\)</span>:</p>
<ul class="simple">
<li><p>Denote <span class="math notranslate nohighlight">\(z_{ij}\)</span> to be the cluster assignment of the potential edge between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>Sample a single sample from the distribution function <span class="math notranslate nohighlight">\(F_{z_{ij}}\)</span>, and record the result as <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>.</p></li>
</ul>
</li>
<li><p>The adjacency matrix we produce, <span class="math notranslate nohighlight">\(A\)</span>, is a sample of an <span class="math notranslate nohighlight">\(SIEM_n(Z, \vec p)\)</span> random network.</p></li>
</ol>
</div>
<p>Next, let’s come up with a working example for the weighted SIEM network. Let’s imagine that we have the brain of an individual. The <span class="math notranslate nohighlight">\(n=100\)</span> nodes are areas of the brain. For instance, there is a node of the brain responsible for movement, an area of the brain which is responsible for sight, so on and so forth. As you may be aware, the brain is split into two hemispheres, the left and right hemispheres. In many cases, these areas of the brain have a <em>bilateral analogue</em>, which means that the same area of the brain appears in the left and right hemispheres. While the region of the brain may have a slightly different function in each of the two hemispheres, they tend to work together to perform functions for the individual. For instance, if the left motor area is active when someone is walking, the right motor area will tend to be active too. In this case, our edge weights are how strongly two regions of the brain tend to <em>correlate</em> with one another over the course of the day. As we learned in the section on the [rho-correlated SBM], correlation is a value between <span class="math notranslate nohighlight">\(-1\)</span> (the two areas are <em>never</em> active together) and <span class="math notranslate nohighlight">\(1\)</span> (the two areas <em>always</em> are active together). If we organize the nodes first by hemisphere (left, then right) and second by the region of the brain, what we would anticipate is that the bilateral areas would tend to be higher correlated than non-bilateral areas. This would be reflected as an “off-diagonal” band (the <em>bilateral band</em>, cluster 2) that tends to have higher values than the other entries of the adjacency matrix (<em>non-bilateral edges</em>, cluster 1). The way we will simulate this is as follows. The non-bilateral edges will have a distribution that is between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, with no particular values being more or less likely. On the other hand, the bilateral band will tend to have higher values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">siem</span>

<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># edges are always present</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># initialize edge clusters Z</span>
<span class="n">band_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">Z</span><span class="p">[</span><span class="n">band_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># set the bilateral edges to cluster 2</span>
<span class="n">Z</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">diag_indices</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># remove the diagonals from edge clusters, since the network is loopless</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">siem</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">edge_comm</span><span class="o">=</span><span class="n">Z</span><span class="p">,</span> <span class="n">wt</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">],</span> <span class="n">wtargs</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;low&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="s2">&quot;high&quot;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Left&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Right&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))]</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inner_hier_labels</span><span class="o">=</span><span class="n">ys</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Brain Network&#39;</span><span class="p">)</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;qualitative&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Edge Cluster&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that the edges in edge-cluster two (the bilateral bands) tend to have higher correlations than the edges in edge-cluster one (the non-bilateral bands). We can summarize this property by looking at a histogram of the two edge clusters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">edge_comm_name</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;Bilateral&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;Non-bilateral&quot;</span><span class="p">}</span>
<span class="k">for</span> <span class="n">clust</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">idx_clust</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Z</span> <span class="o">==</span> <span class="n">clust</span><span class="p">)</span>
    <span class="n">samples</span><span class="o">.</span><span class="n">extend</span><span class="p">([{</span><span class="s2">&quot;Cluster&quot;</span><span class="p">:</span> <span class="s2">&quot;Cluster </span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clust</span><span class="p">,</span> <span class="n">edge_comm_name</span><span class="p">[</span><span class="n">clust</span><span class="p">]),</span> <span class="s2">&quot;Correlation&quot;</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">]}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">idx_clust</span><span class="p">)])</span>

<span class="n">real_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">real_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Correlation&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Cluster&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;probability&quot;</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">common_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of Correlations&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we are in a statistical testing mindset, a question you might have is that the correlations for cluster two tend to look much larger than the correlations for cluster one. How do we formalize this relationship? We can’t use the fisher’s exact test, since the fisher’s exact test only worked for the case where our network was unweighted and the adjacency matrix took only binary values (<span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>). What can we do for a weighted network, when the adjacency matrix is not resticted to binary values? To formalize our situation a bit more, let’s return to making some hypotheses about the data. We want to test whether the correlations in the second cluster exceed the correlations in the first cluster. This means that we have an alternative hypothesis <span class="math notranslate nohighlight">\(H_A:\)</span> correrlations in cluster two <span class="math notranslate nohighlight">\(&gt;\)</span> correlations in cluster one. Remember that for two sample testing, the null hypothesis is going to be the opposite, so <span class="math notranslate nohighlight">\(H_0:\)</span> correlations in cluster two <span class="math notranslate nohighlight">\(\leq \)</span> correlations in cluster one. This is the weighted two-sample hypothesis testing problem.</p>
</div>
<div class="section" id="the-mann-whitney-wilcoxon-u-test">
<h5><span class="section-number">8.2.2.2.2.2. </span>The Mann-Whitney Wilcoxon <span class="math notranslate nohighlight">\(U\)</span> Test<a class="headerlink" href="#the-mann-whitney-wilcoxon-u-test" title="Permalink to this headline">¶</a></h5>
<p>There are a variety of approaches to overcoming the weighted two-sample hypothesis testing problem. A popular approach is known as the two-sample <span class="math notranslate nohighlight">\(t\)</span>-test, which basically means that we assume that the samples are really just normally distributed, with some mean and variance, and then we forget about the actual data. We construct a test statistic by using properties about the normal distribution, not the actual data itself. To read more about the two-sample <span class="math notranslate nohighlight">\(t\)</span>-test, we would recommend you check out the <a class="reference external" href="https://en.wikipedia.org/wiki/Student%27s_t-test">wikipedia article</a>. Unforrtunately, if the data is not well-summarized by a normal distribution, the <span class="math notranslate nohighlight">\(t\)</span>-test tends to be a fairly poor choice for hypothesis testing. Instead, what we tend to use for the weighted two-sample hypothesis testing problem is we turn to what is known as the <span class="math notranslate nohighlight">\(U\)</span>-test. Instead of relying on assumptions that the data is normally distributed, the <span class="math notranslate nohighlight">\(U\)</span>-test does not assume anything about the distributions of the data samples. Even when the data is normally distributed, the <span class="math notranslate nohighlight">\(U\)</span>-test still tends to perform as well as the <span class="math notranslate nohighlight">\(t\)</span>-test. For this reason, we find the <span class="math notranslate nohighlight">\(U\)</span>-test to be a better procedure than other forms of testing that make heavy assumptions about the distributions of edge weights.</p>
<p>The <span class="math notranslate nohighlight">\(U\)</span>-test is very easy to implement in <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>. We simply fit an SIEM to the data using the <code class="docutils literal notranslate"><span class="pre">SIEMEstimator</span></code> class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">SIEMEstimator</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SIEMEstimator</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>aand then we use the <code class="docutils literal notranslate"><span class="pre">compare</span></code> function to test whether cluster <code class="docutils literal notranslate"><span class="pre">2</span></code> is greater than cluster <code class="docutils literal notranslate"><span class="pre">1</span></code> by specifying <code class="docutils literal notranslate"><span class="pre">alternative=&quot;greater&quot;</span></code> as a <code class="docutils literal notranslate"><span class="pre">methodarg</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">mannwhitneyu</span>

<span class="c1"># perform mann-whitney u-test with alternative hypothesis that </span>
<span class="c1"># correlations of cluster 2 &gt; correlations of cluster 1</span>
<span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">mannwhitneyu</span><span class="p">,</span> <span class="n">methodargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;alternative&quot;</span><span class="p">:</span> <span class="s2">&quot;greater&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Since the <span class="math notranslate nohighlight">\(p\)</span>-value is much less than <span class="math notranslate nohighlight">\(0.05\)</span> (the print statement cut off at 3 decimal places, but in reality, it is about <span class="math notranslate nohighlight">\(10^{-64}\)</span>, which is <em>really</em> tiny!), with our decision threshold <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, we accept the alternative hypothesis that the correlations in cluster two (the bilateral edges) really are bigger than the correlations in cluster one (the non-bilateral edges).</p>
<p>We could also use a similar, but slightly reworded, hypothesis and get the same answer. Note that saying that the correlations in cluster two are bigger than the correlations in cluster one would be the same as saying that the correlations in cluster one are smaller than the correlations in cluster two. We can reorder the clusters we are testing, and instead specify the alternative hypothesis as <code class="docutils literal notranslate"><span class="pre">&quot;less&quot;</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">mannwhitneyu</span><span class="p">,</span> <span class="n">methodargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;alternative&quot;</span><span class="p">:</span> <span class="s2">&quot;less&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Which would obviously give us the same answer.</p>
<p>If you instead wanted to use a <span class="math notranslate nohighlight">\(t\)</span>-test or a different testing approach, you can do that very easily with graspologic, too. Just pass your testing strategy using the <code class="docutils literal notranslate"><span class="pre">method</span></code> argument, and pass additional arguments for the testing strategy to the <code class="docutils literal notranslate"><span class="pre">methodargs</span></code> dictionary. The only requirement is that the testing strategy takes two unnamed arguments, where the first argument is the first cluster of data, and the second argument is the second cluster of data. Here, we will be using <code class="docutils literal notranslate"><span class="pre">ttest_ind</span></code> <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html">from scipy</a>. We want to assume as little as possible about the data, so we will specify that the variances might not necessarily be equal as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">ttest_ind</span>

<span class="c1"># perform mann-whitney u-test with alternative hypothesis that </span>
<span class="c1"># correlations of cluster 2 &gt; correlations of cluster 1</span>
<span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">ttest_ind</span><span class="p">,</span> <span class="n">methodargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;alternative&quot;</span><span class="p">:</span> <span class="s2">&quot;greater&quot;</span><span class="p">,</span> <span class="s2">&quot;equal_var&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;t statistic: </span><span class="si">{:.2f}</span><span class="s2">, p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tstat</span><span class="p">,</span> <span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>And again we get a <span class="math notranslate nohighlight">\(p\)</span>-value that is much less than our decision threshold, so again we accept the alternative hypothesis that the correlations in cluster two exceed the correlations in cluster one.</p>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./applications/ch8"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="community-detection.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">8.1. </span>Community Detection</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="model-selection.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.3. </span>Model Selection</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Joshua Vogelstein, Alex Loftus, and Eric Bridgeford<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>