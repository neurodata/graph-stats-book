
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>7.1. Latent Two-Sample Hypothesis Testing &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7.2. Two-sample hypothesis testing in SBMs" href="significant-communities.html" />
    <link rel="prev" title="7. Applications for Two Networks" href="ch8.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../coverpage.html">
                    Hands-on Network Machine Learning with Scikit-Learn and Graspologic
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What is network machine learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why do we study networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-nml-problems.html">
     1.3. Types of Network Machine Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.4. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/challenges-of-nml.html">
     1.5. Challenges of Network Machine Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.4. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.5. Fine-Tune your Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.6. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch4/ch4.html">
   3. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/matrix-representations.html">
     3.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/properties-of-networks.html">
     3.2. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/network-representations.html">
     3.3. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch4/regularization.html">
     3.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch5/ch5.html">
   4. Why Use Statistical Models?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_ER.html">
     4.1. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_SBM.html">
     4.2. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_RDPG.html">
     4.3. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/single-network-models_IER.html">
     4.4. Inhomogeneous Erdos Renyi (IER) Random Network Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/multi-network-models.html">
     4.5. Multiple Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch5/models-with-covariates.html">
     4.6. Network Models with Network Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../representations/ch6/ch6.html">
   5. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/estimating-parameters_mle.html">
     5.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/why-embed-networks.html">
     5.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/spectral-embedding.html">
     5.3. Spectral embedding methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/multigraph-representation-learning.html">
     5.4. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../representations/ch6/joint-representation-learning.html">
     5.5. Joint Representation Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch7/ch7.html">
   6. Applications When You Have One Network
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/community-detection.html">
     6.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/testing-differences.html">
     6.2. Testing for Differences between Groups of Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/model-selection.html">
     6.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/vertex-nomination.html">
     6.4. Single-Network Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/out-of-sample.html">
     6.5. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch8.html">
   7. Applications for Two Networks
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.1. Latent Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="significant-communities.html">
     7.2. Two-sample hypothesis testing in SBMs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="graph-matching-vertex.html">
     7.3. Graph Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multiple-vertex-nomination.html">
     7.4. Vertex Nomination For Two Networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch9/ch9.html">
   8. Applications for Many Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/anomaly-detection.html">
     8.1. Anomaly Detection For Timeseries of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/significant-edges.html">
     8.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/significant-vertices.html">
     8.3. Testing for Significant Vertices
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Next Steps
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../next/ch10/ch10.html">
   9. Where do we go from here?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch11/ch11.html">
   10. Representations (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch11/alt-reps.html">
     10.1. Alternative Network Representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch12/ch12.html">
   11. Network Model Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/background.html">
     11.1. Background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/foundation.html">
     11.2. Foundation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/ers.html">
     11.3. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/sbms.html">
     11.4. Stochastic Block Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch12/rdpgs.html">
     11.5. RDPGs and more general network models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch13/ch13.html">
   12. Learning Representations Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch13/mle-theory.html">
     12.1. Maximum Likelihood Estimate Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch13/lse.html">
     12.2. Finding singular vectors With singular value decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch13/spectral-theory.html">
     12.7. Spectral Method Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../appendix/ch14/ch14.html">
   13. Applications (Extended)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch14/hypothesis.html">
     13.1. Hypothesis Testing with coin flips
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch14/unsupervised.html">
     13.2. Unsupervised learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../appendix/ch14/bayes.html">
     13.3. Bayes Plugin Classifier
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/network_machine_learning_in_python/applications/ch8/two-sample-hypothesis.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/neurodata/graph-stats-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/issues/new?title=Issue%20on%20page%20%2Fapplications/ch8/two-sample-hypothesis.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/neurodata/graph-stats-book/edit/master/network_machine_learning_in_python/applications/ch8/two-sample-hypothesis.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/applications/ch8/two-sample-hypothesis.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-sample-tests-a-quick-refresher">
   7.1.1. Two-sample tests, a quick refresher
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-sample-tests-and-random-networks">
     7.1.1.1. Two-sample tests and Random Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assuming-a-statistical-model">
     7.1.1.2. Assuming a statistical model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-position-test">
   7.1.2. Latent position test
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-a-null-distribution-via-parametric-bootstrapping">
     7.1.2.1. Generating a null distribution via parametric bootstrapping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-distribution-test">
   7.1.3. Latent distribution test
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-latent-distribution-test-is-more-conservative-than-the-latent-position-test">
   7.1.4. The Latent distribution test is more conservative than the latent position test
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Latent Two-Sample Hypothesis Testing</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-sample-tests-a-quick-refresher">
   7.1.1. Two-sample tests, a quick refresher
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-sample-tests-and-random-networks">
     7.1.1.1. Two-sample tests and Random Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assuming-a-statistical-model">
     7.1.1.2. Assuming a statistical model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-position-test">
   7.1.2. Latent position test
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-a-null-distribution-via-parametric-bootstrapping">
     7.1.2.1. Generating a null distribution via parametric bootstrapping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-distribution-test">
   7.1.3. Latent distribution test
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-latent-distribution-test-is-more-conservative-than-the-latent-position-test">
   7.1.4. The Latent distribution test is more conservative than the latent position test
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="latent-two-sample-hypothesis-testing">
<span id="ch8-twosample"></span><h1><span class="section-number">7.1. </span>Latent Two-Sample Hypothesis Testing<a class="headerlink" href="#latent-two-sample-hypothesis-testing" title="Permalink to this headline">#</a></h1>
<p>We have learned a lot thus far about network statistics, and now we want to apply some of that knowledge to doing two-sample network testing. Imagine you are part of an alien race called the Moops. You live in harmony with another race, called the Moors, that look very similar to you, on the planet Zinthar. The evil overlord Zelu takes a random number of Moops and Moors and puts them on an island. You want to find your fellow Moops, but the Moops and Moors look very similar to each other. However, you guess that, perhaps, if you were able to look at a network representing the brain connections for a Moop and another network representing the brain connections for a Moor, you think that there might be a difference between the two. What do you do?</p>
<p>To make this a little bit more concrete, let’s develop our example. The Moops and Moors each have brains with <span class="math notranslate nohighlight">\(n=100\)</span> brain areas. If two areas of the brain can communicate with one another (pass information back and forth), an edge exists; if they cannot communicate with one another, an edge does not exist. In this case, these networks will each be SBMs. The networks look like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>

<span class="n">ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>
<span class="n">BMoor</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]]</span>

<span class="n">AMoor</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">BMoor</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">zvec</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;2&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ns</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">AMoor</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Moor Adjacency Matrix&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/two-sample-hypothesis_3_0.png" src="../../_images/two-sample-hypothesis_3_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BMoop</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]</span>

<span class="n">AMoop</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">BMoop</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw_multiplot</span><span class="p">(</span><span class="n">AMoop</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">zvec</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Moop Adjacency Matrix&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/two-sample-hypothesis_5_0.png" src="../../_images/two-sample-hypothesis_5_0.png" />
</div>
</div>
<section id="two-sample-tests-a-quick-refresher">
<h2><span class="section-number">7.1.1. </span>Two-sample tests, a quick refresher<a class="headerlink" href="#two-sample-tests-a-quick-refresher" title="Permalink to this headline">#</a></h2>
<p>This problem is another iteration of a question you encountered in the last section when <a class="reference internal" href="../ch7/testing-differences.html#ch7-testing"><span class="std std-ref">testing for differences between groups of edges</span></a>, called the two-sample test. You have two samples of data: a network from a Moop and a network from a Moor, and you want to characterize whether these two networks are <em>different</em>. For our purposes, you will call the Moop network <span class="math notranslate nohighlight">\(A^{(p)}\)</span> and the Moor network <span class="math notranslate nohighlight">\(A^{(r)}\)</span>. As you by this time know, for each of these networks, there are underlying random networks, <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span> of which the Moop and Moor networks you see, <span class="math notranslate nohighlight">\(A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(A^{(r)}\)</span>, are realizations of. The key issue is that you don’t actually get to see the underlying random networks: what you will need to do is characterize differences between <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span> using <em>only</em> <span class="math notranslate nohighlight">\(A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(A^{(r)}\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span> are <em>random</em>, you can’t really study them directly. But what you can study, as it turns out, are the <em>parameters</em> that govern <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span>.</p>
<p>This is just like how, in our coin flip example you learned about previously, you don’t look for differences in the coins themselves, but rather, you look for differences in the <em>probabilities</em> that each coin lands on heads or tails. You construct hypotheses about the <em>probabilities</em>, not the coin. This is because the coins are the same (they both have a heads and tails side, all of the coin flips are performed without regard for the outcomes of other coin flips, so on and so forth), <em>other</em> than the fact that they land on heads and tails at different rates. This rate, the underlying probability, is therefore the element of the random coin that you want to hone in on to test whether they are different. Remember, you made a <em>null hypothesis</em> that there was no difference between the probabilities coins, <span class="math notranslate nohighlight">\(H_0 : p_1 = p_2\)</span>, and had an <em>alternative hypothesis</em> that there was a difference between the probabilities of the coins, <span class="math notranslate nohighlight">\(H_A: p_1 \neq p_2\)</span>. Next, you produced estimates of the coin probabilities using the samples, <span class="math notranslate nohighlight">\(\hat p_1\)</span> and <span class="math notranslate nohighlight">\(\hat p_2\)</span>, and then used the samples to deduce whether you have enough evidence from our sample to support whether <span class="math notranslate nohighlight">\(H_A\)</span> was true, that <span class="math notranslate nohighlight">\(p_1 \neq p_2\)</span>.</p>
<section id="two-sample-tests-and-random-networks">
<h3><span class="section-number">7.1.1.1. </span>Two-sample tests and Random Networks<a class="headerlink" href="#two-sample-tests-and-random-networks" title="Permalink to this headline">#</a></h3>
<p>In this example, however, we’re going to go a slightly different direction. We’re going to describe <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span> as Random Dot Product Graphs (RDPGs). What we’re going to say is that <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> is a random network where the probability an edge exists (or does not exist) is described by the probability matrix <span class="math notranslate nohighlight">\(P^{(p)}\)</span>, whose entries <span class="math notranslate nohighlight">\(p_{ij}^{(p)}\)</span> are the probabilities that the edge <span class="math notranslate nohighlight">\(\mathbf a_{ij}^{(p)}\)</span> between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> exist. You do the same thing for the Moor random network <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span>, with the probability matrix <span class="math notranslate nohighlight">\(P^{(r)}\)</span>.</p>
<p>In this case, you want to test whether <span class="math notranslate nohighlight">\(H_0 : P^{(p)} = P^{(r)}\)</span> against <span class="math notranslate nohighlight">\(H_A : P^{(p)} \neq P^{(r)}\)</span>. However, you have a slight problem: unlike the coin, you can’t really use your sample to describe <span class="math notranslate nohighlight">\(P^{(p)}\)</span> and <span class="math notranslate nohighlight">\(P^{(r)}\)</span> directly. Instead, you need to make assumptions about the random networks in order to learn things about them, which was why we introduced <a class="reference external" href="#link?">Statistical Models</a>. You first need to choose a statistical model to describe <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span>.</p>
</section>
<section id="assuming-a-statistical-model">
<h3><span class="section-number">7.1.1.2. </span>Assuming a statistical model<a class="headerlink" href="#assuming-a-statistical-model" title="Permalink to this headline">#</a></h3>
<p>To test whether the probability matrices are different, we’re going to make the assumption that <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span> are each Random Dot Product Graphs (RDPGs), with latent position matrices <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span>, respectively. Remember that for a RDPG with latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, that the probability matrix <span class="math notranslate nohighlight">\(P = XX^\top\)</span>. What this means is that if <span class="math notranslate nohighlight">\(P^{(p)} = X^{(p)}X^{(p)\top}\)</span> and <span class="math notranslate nohighlight">\(P^{(r)} = X^{(r)}X^{(r)\top}\)</span>, then <span class="math notranslate nohighlight">\(P^{(p)}\)</span> and <span class="math notranslate nohighlight">\(P^{(r)}\)</span> are the same/different if <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> are same/different, right?</p>
<p>Unfortunately, this assumption is a <em>close</em>, but <em>not quite</em> correct. Remember back in <a class="reference internal" href="../../representations/ch6/ch6.html#ch6"><span class="std std-ref">Chapter 6</span></a> we introduced the idea of a <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral-nonidentifiable"><span class="std std-ref">rotation matrix</span></a> with respect to the non-identifiability problem. As it turns out, for a rotation matrix <span class="math notranslate nohighlight">\(W\)</span> which is <span class="math notranslate nohighlight">\(d \times d\)</span>, then <span class="math notranslate nohighlight">\(WW^\top = I_{d \times d}\)</span>, the <span class="math notranslate nohighlight">\(d \times d\)</span> identity matrix, which is the equivalent of multiplying by one for matrices. This means that any matrix times the identity matrix is just itself. So what if <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> are just rotations of each other? Stated another way, what if <span class="math notranslate nohighlight">\(X^{(p)} = X^{(r)} W\)</span>; that is, <span class="math notranslate nohighlight">\(X^{(p)}\)</span> is just <span class="math notranslate nohighlight">\(X^{(r)}\)</span>, but rotated around?</p>
<p>Even if <span class="math notranslate nohighlight">\(X^{(r)}\)</span> and <span class="math notranslate nohighlight">\(X^{(p)}\)</span> are different, as long as they are just rotations of one another, then the probability matrices are <em>identical</em>. This is because, if we call <span class="math notranslate nohighlight">\(P^{(r)} = X^{(r)}X^{(r)\top}\)</span>, then:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    P^{(p)} &amp;= X^{(p)}X^{(p)\top}, \\
    &amp;= X^{(r)}W W^\top X^{(r)\top},\;\;\;\;\text{we used that }X^{(r)} = X^{(p)}W \\
    &amp;= X^{(r)}I_{d \times d}X^{(r)\top},\;\;\;\;\text{we used that $W$ is a rotation, so }WW^\top = I_{d \times d} \\
    &amp;= X^{(r)}X^{(r)\top} = P^{(r)}
\end{align*}\]</div>
<p>Which shows that the probability matrices would still actually be the same!</p>
<p>What this means for you is that you <em>can’t</em> just compare the latent position matrices, but rather, you have to compare the latent position matrices for <em>any</em> possible rotation matrix! Stated another way, we would say that <span class="math notranslate nohighlight">\(P^{(p)}\)</span> and <span class="math notranslate nohighlight">\(P^{(r)}\)</span> are equal if <span class="math notranslate nohighlight">\(X^{(r)}\)</span> is can be obtained by rotating <span class="math notranslate nohighlight">\(X^{(p)}\)</span>, and they are not equal if <span class="math notranslate nohighlight">\(X^{(r)}\)</span> cannot be obtained by rotation <span class="math notranslate nohighlight">\(X^{(p)}\)</span>. We write this down as a hypothesis by saying that <span class="math notranslate nohighlight">\(H_0 : X^{(p)} = X^{(r)}W\)</span> for any rotation matrix <span class="math notranslate nohighlight">\(W\)</span>, against <span class="math notranslate nohighlight">\(H_A : X^{(p)} \neq X^{(r)}W\)</span> for any rotation matrix <span class="math notranslate nohighlight">\(W\)</span>. With the assumption that <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span> are RDPGs, this is <em>exactly</em> the same as saying that <span class="math notranslate nohighlight">\(H_0 : P^{(p)} = P^{(r)}\)</span>, against <span class="math notranslate nohighlight">\(H_A : P^{(p)} \neq P^{(r)}\)</span>, which was our original statement we wanted to test.</p>
</section>
</section>
<section id="latent-position-test">
<h2><span class="section-number">7.1.2. </span>Latent position test<a class="headerlink" href="#latent-position-test" title="Permalink to this headline">#</a></h2>
<p>So, now we are ready to get to implementing your idea. You have two random networks, <span class="math notranslate nohighlight">\(\mathbf A^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf A^{(r)}\)</span>, and you assume that they are both RDPGs with latent position matrices <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> respectively. You want to test whether the latent position matrices are the same up to a rotation, <span class="math notranslate nohighlight">\(H_0 : X^{(p)} = X^{(r)}W\)</span> for some rotation <span class="math notranslate nohighlight">\(W\)</span>, or they are different for any possible rotation <span class="math notranslate nohighlight">\(W\)</span> <span class="math notranslate nohighlight">\(H_A: X^{(p)} \neq X^{(r)}W\)</span>.</p>
<p>To do this, the first step is to figure out whether <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> are rotations of one another. We do this by first trying to find the best possible rotation of <span class="math notranslate nohighlight">\(X^{(r)}\)</span> to <span class="math notranslate nohighlight">\(X^{(p)}\)</span>. The problem can be written down as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \text{find $W$ where }||X^{(p)} - X^{(r)}W||_{F}\text{ is minimized}
\end{align*}\]</div>
<p>The term <span class="math notranslate nohighlight">\(||A - B||_F\)</span> is the Frobenius norm of the difference, which here, is just going to give you a sense of how different the two matrices are. It will be relatively big if <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are not very similar, and relatively small if <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are similar. For two adjacency matrices for <span class="math notranslate nohighlight">\(n\)</span>-node networks, this quantity can be described as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    ||A - B||_F &amp;= \sqrt*{\sum_{i = 1}^n \sum_{j = 1}^n (a_{ij} - b_{ij})^2}
\end{align*}\]</div>
<p>Basically, the idea is that if <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> are rotations of one another (or close to it), then if you can find the right rotation <span class="math notranslate nohighlight">\(W\)</span>, then <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}W\)</span> will be identical (and <span class="math notranslate nohighlight">\(||X^{(p)} - X^{(r)}W||_{F}\)</span> will be zero) or nearly identical (and <span class="math notranslate nohighlight">\(||X^{(p)} - X^{(r)}W||_{F}\)</span> will be small). If they are not rotations of one another, then this equation <span class="math notranslate nohighlight">\(||X^{(p)} - X^{(r)}W||_{F}\)</span> is going to have a comparatively high value, no matter what value of <span class="math notranslate nohighlight">\(W\)</span> you could choose. This is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem"><em>orthogonal procrustes problem</em></a>, and there are a variety of ways you can come up with a pretty good guess at what <span class="math notranslate nohighlight">\(W\)</span> is, but we won’t need to go into details for our purposes.</p>
<p>Next, you need to figure out how to actually use this to implement a statitical test <span class="math notranslate nohighlight">\(H_0 : X^{(p)} = X^{(r)}W\)</span> against <span class="math notranslate nohighlight">\(H_A: X^{(p)} \neq X^{(r)}W\)</span>. You can estimate starting values for <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span>, by just using <a class="reference internal" href="../../representations/ch6/spectral-embedding.html#ch6-spectral"><span class="std std-ref">Adjacency Spectral Embedding</span></a>. This gives you <span class="math notranslate nohighlight">\(\hat X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\hat X^{(r)}\)</span>, respectively, which are your estimates of the latent position matrices.</p>
<p>Next, you do your best to find a rotation of <span class="math notranslate nohighlight">\(X^{(r)}\)</span> onto <span class="math notranslate nohighlight">\(X^{(p)}\)</span> by solving the orthogonal procrustes problem, by plugging in your estimates <span class="math notranslate nohighlight">\(\hat X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\hat X^{(r)}\)</span> for <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span>. Unfortunately, you’re not going to find a matrix <span class="math notranslate nohighlight">\(W\)</span> where <span class="math notranslate nohighlight">\(|| \hat X^{(p)} - \hat X^{(r)}W||_{F} = 0\)</span>, even if <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> really are equal. This is because you are just using estimates of latent positions, so even if <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> are identical up to a rotation in reality, your estimates probably won’t be. This means that <span class="math notranslate nohighlight">\(|| \hat X^{(p)} - \hat X^{(r)}W||_{F}\)</span> is, if <span class="math notranslate nohighlight">\(\hat X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\hat X^{(r)}\)</span> are <em>close</em> up to a rotation, going to take a <em>relatively</em> small value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">orthogonal_procrustes</span>
<span class="kn">from</span> <span class="nn">graspologic.models</span> <span class="kn">import</span> <span class="n">RDPGEstimator</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># estimate latent positions for Moop and Moor networks</span>
<span class="n">XMoop</span> <span class="o">=</span> <span class="n">RDPGEstimator</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">AMoop</span><span class="p">)</span><span class="o">.</span><span class="n">latent_</span>
<span class="n">XMoor</span> <span class="o">=</span> <span class="n">RDPGEstimator</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">AMoor</span><span class="p">)</span><span class="o">.</span><span class="n">latent_</span>
<span class="c1"># estimate best possible rotation of XMoor to XMoop by </span>
<span class="c1"># solving orthogonal procrustes problem</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">orthogonal_procrustes</span><span class="p">(</span><span class="n">XMoop</span><span class="p">,</span> <span class="n">XMoor</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">observed_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">XMoop</span> <span class="o">-</span> <span class="n">XMoor</span> <span class="o">@</span> <span class="n">W</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">observed_norm</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.05</span><span class="o">*</span><span class="n">observed_norm</span> <span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="s2">&quot;$||</span><span class="se">\\</span><span class="s2">hat X^{(p)} - </span><span class="se">\\</span><span class="s2">hat X^{(r)}||_F$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.4</span><span class="o">*</span><span class="n">observed_norm</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Frobenius norm of difference&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/two-sample-hypothesis_10_0.png" src="../../_images/two-sample-hypothesis_10_0.png" />
</div>
</div>
<p>Relative <em>what</em>, exactly?</p>
<section id="generating-a-null-distribution-via-parametric-bootstrapping">
<span id="ch8-twosample-param-boot"></span><h3><span class="section-number">7.1.2.1. </span>Generating a null distribution via parametric bootstrapping<a class="headerlink" href="#generating-a-null-distribution-via-parametric-bootstrapping" title="Permalink to this headline">#</a></h3>
<p>In an ideal world, you would be able to characterize how far apart the estimates <span class="math notranslate nohighlight">\(\hat X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\hat X^{(r)}\)</span> would be if the quantities they were estimating, <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span>, were really identical up to a rotation. However, in reality, you don’t <em>know</em> <span class="math notranslate nohighlight">\(X^{(p)}\)</span> nor <span class="math notranslate nohighlight">\(X^{(r)}\)</span>, so you can’t exactly say anything directly about how big <span class="math notranslate nohighlight">\(|| \hat X^{(p)} - \hat X^{(r)}W||_{F}\)</span> should be if <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> are identical up to a rotation (and <span class="math notranslate nohighlight">\(H_0\)</span> is true). If you did, you wouldn’t need to do any of this statistical testing in the first place!</p>
<p>So what you do is the next best thing. You instead use <span class="math notranslate nohighlight">\(\hat X^{(p)}\)</span> as the parameter to generate two new networks, <span class="math notranslate nohighlight">\(A^{(1)}\)</span> and <span class="math notranslate nohighlight">\(A^{(2)}\)</span>, where the latent positions really <em>are</em> identical (and equal to <span class="math notranslate nohighlight">\(\hat X^{(p)}\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>

<span class="k">def</span> <span class="nf">generate_synthetic_networks</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A function which generates two synthetic networks with</span>
<span class="sd">    same latent position matrix X.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">A1</span><span class="p">,</span> <span class="n">A2</span>

<span class="n">A1</span><span class="p">,</span> <span class="n">A2</span> <span class="o">=</span> <span class="n">generate_synthetic_networks</span><span class="p">(</span><span class="n">XMoop</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You then estimate the latent positions <span class="math notranslate nohighlight">\(\hat X^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\hat X^{(2)}\)</span> using Adjacency Spectral Embedding again, and now you compute the value of <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span> for the best possible rotation <span class="math notranslate nohighlight">\(W_i\)</span> of <span class="math notranslate nohighlight">\(X^{(2)}\)</span> onto <span class="math notranslate nohighlight">\(\hat X^{(1)}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_latent</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A function which returns the latent position estimate</span>
<span class="sd">    for an adjacency matrix A.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RDPGEstimator</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">.</span><span class="n">latent_</span>

<span class="n">X1</span> <span class="o">=</span> <span class="n">compute_latent</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">compute_latent</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, you compare <span class="math notranslate nohighlight">\(|| \hat X^{(p)} - \hat X^{(r)}W_i||_{F}\)</span> to <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W||_{F}\)</span>. If <span class="math notranslate nohighlight">\(X^{(p)}\)</span> and <span class="math notranslate nohighlight">\(X^{(r)}\)</span> are identical up to a rotation, then you would expect that <span class="math notranslate nohighlight">\(|| \hat X^{(p)} - \hat X^{(r)}W||_{F}\)</span> would be similar to <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span>. If they are not, you would expect that <span class="math notranslate nohighlight">\(|| \hat X^{(p)} - \hat X^{(r)}W||_{F}\)</span> would be much bigger than <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_norm_orth_proc</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A function which finds the best rotation of B onto A,</span>
<span class="sd">    and then computes and returns the norm.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">orthogonal_procrustes</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">B</span> <span class="o">@</span> <span class="n">R</span><span class="p">)</span>

<span class="n">norm_null</span> <span class="o">=</span> <span class="n">compute_norm_orth_proc</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">observed_norm</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">observed_norm</span> <span class="o">+</span> <span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="s2">&quot;$||</span><span class="se">\\</span><span class="s2">hat X^{(p)} - </span><span class="se">\\</span><span class="s2">hat X^{(r)}||_F$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">norm_null</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">norm_null</span> <span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="s2">&quot;$||</span><span class="se">\\</span><span class="s2">hat X^{(1)} - </span><span class="se">\\</span><span class="s2">hat X^{(2)}||_F$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.4</span><span class="o">*</span><span class="n">observed_norm</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Frobenius norm of difference&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/two-sample-hypothesis_17_0.png" src="../../_images/two-sample-hypothesis_17_0.png" />
</div>
</div>
<p>You keep repeating this process again and again, and over time, you gradually get some idea of what <span class="math notranslate nohighlight">\(||\hat X^{(p)} - \hat X^{(r)}W||_{F}\)</span> would look like if the true latent position estimates were identical. This is called a <em>parametric resampling</em>. It is called a <em>resampling</em> because you are <em>sampling</em> what <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span> looks like if an assumption were true; namely, if the underlying latent position matrices were the same. It is called <em>parametric</em> because you are using properties of RDPGs to generate your estimates <span class="math notranslate nohighlight">\(\hat X^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\hat X^{(2)}\)</span>. When you do this dozens (or more) times, you start to notice a trend developing. We’ll plot what <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span> looks like when we repeat this process <span class="math notranslate nohighlight">\(100\)</span> times using a histogram, which indicates the number of times the value of <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span> lands in a particular range of the <span class="math notranslate nohighlight">\(x\)</span>-axis:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parametric_resample</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">nreps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A function to generate samples of the null distribution under H0</span>
<span class="sd">    using parametric resampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">null_norms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nreps</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nreps</span><span class="p">):</span>
        <span class="n">A1</span><span class="p">,</span> <span class="n">A2</span> <span class="o">=</span> <span class="n">generate_synthetic_networks</span><span class="p">(</span><span class="n">XMoop</span><span class="p">)</span>
        <span class="n">X1</span> <span class="o">=</span> <span class="n">compute_latent</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="n">X2</span> <span class="o">=</span> <span class="n">compute_latent</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="n">null_norms</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_norm_orth_proc</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">null_norms</span>

<span class="n">nreps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">null_norms</span> <span class="o">=</span> <span class="n">parametric_resample</span><span class="p">(</span><span class="n">AMoop</span><span class="p">,</span> <span class="n">AMoor</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">nreps</span><span class="o">=</span><span class="n">nreps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;Sample&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;Norm&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">}</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">null_norms</span><span class="p">)]</span>

<span class="n">null_df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">null_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Norm&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;probability&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">observed_norm</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.4</span><span class="o">*</span><span class="n">observed_norm</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of samples&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">null_norms</span><span class="p">)</span> <span class="o">-</span> <span class="mf">.6</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="s2">&quot;$||</span><span class="se">\\</span><span class="s2">hat X^{(1)} - </span><span class="se">\\</span><span class="s2">hat X^{(2)}||_F$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">observed_norm</span> <span class="o">+</span> <span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="s2">&quot;$||</span><span class="se">\\</span><span class="s2">hat X^{(p)} - </span><span class="se">\\</span><span class="s2">hat X^{(r)}||_F$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/two-sample-hypothesis_20_0.png" src="../../_images/two-sample-hypothesis_20_0.png" />
</div>
</div>
<p>What you see is that <span class="math notranslate nohighlight">\(||\hat X^{(p)} - \hat X^{(r)}W||_{F}\)</span> is much larger than the <em>almost all</em> of the values of <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span> that you calculated. We will use this to estimate a <span class="math notranslate nohighlight">\(p\)</span>-value, and we will say that the <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(H_0\)</span> against <span class="math notranslate nohighlight">\(H_A\)</span> is the fraction of times that when the underlying latent positions were equal, the value of <span class="math notranslate nohighlight">\(|| \hat X^{(1)} - \hat X^{(2)}W_i||_{F}\)</span> exceeded the value of <span class="math notranslate nohighlight">\(||\hat X^{(p)} - \hat X^{(r)}W||_{F}\)</span> which we estimated from our sample. We add one to the numerator and denominator, since we observed one instance of a value at least as big as that in the observed data: the observed data itself. This means our <span class="math notranslate nohighlight">\(p\)</span>-value is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pval</span> <span class="o">=</span> <span class="p">((</span><span class="n">null_norms</span> <span class="o">&gt;=</span> <span class="n">observed_norm</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">nreps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;estimate of p-value: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pval</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>estimate of p-value: 0.010
</pre></div>
</div>
</div>
</div>
<p>We then repeat this process, but we use <span class="math notranslate nohighlight">\(X^{(r)}\)</span> instead as our reference point. The overall <span class="math notranslate nohighlight">\(p\)</span>-value is the maximum of the two <span class="math notranslate nohighlight">\(p\)</span>-values produced with this procedure.</p>
<p>This is called the <em>latent position test</em>, and it is implemented directly by <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>. Note that the <span class="math notranslate nohighlight">\(p\)</span>-value that you obtain from this process might differ every time you run the test, since there is randomness in your generation process of the <span class="math notranslate nohighlight">\(A^{(1)}\)</span>s and the <span class="math notranslate nohighlight">\(A^{(2)}\)</span>s for every time you repeated your comparison. Making the number of repetitions larger by setting <code class="docutils literal notranslate"><span class="pre">n_bootstraps</span></code> to a higher value will tend to yield more <em>stable</em> <span class="math notranslate nohighlight">\(p\)</span>-value estimates for when we estimate <span class="math notranslate nohighlight">\(p\)</span>-values using resampling techniques:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.inference</span> <span class="kn">import</span> <span class="n">latent_position_test</span>

<span class="n">nreps</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># the number of times to repeat the process of estimating X1hat and X2hat</span>
<span class="n">lpt_moop2moor</span> <span class="o">=</span> <span class="n">latent_position_test</span><span class="p">(</span><span class="n">AMoop</span><span class="p">,</span> <span class="n">AMoor</span><span class="p">,</span> <span class="n">n_bootstraps</span> <span class="o">=</span> <span class="n">nreps</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;estimate of p-value: </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lpt_moop2moor</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>estimate of p-value: 0.00100
</pre></div>
</div>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(p\)</span>-value is low, and below a typical decision threshold of <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>. This means you have evidence to reject the null hypothesis in favor of the alternative hypothesis: the latent position matrix for a Moop brain network differs from the latent position matrix for a Moor brain network.</p>
<p>What if we had another network from a Moop, and we compared the network of our first Moop to our new Moop? We can generate a new comparison:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a new Moor network with the same block matrix, and hence, </span>
<span class="c1"># the same latent position matrix as AMoor</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">AMoor2</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">BMoor</span><span class="p">)</span>

<span class="n">lpt_moor2moor</span> <span class="o">=</span> <span class="n">latent_position_test</span><span class="p">(</span><span class="n">AMoor</span><span class="p">,</span> <span class="n">AMoor2</span><span class="p">,</span> <span class="n">n_bootstraps</span><span class="o">=</span><span class="n">nreps</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;estimate of p-value: </span><span class="si">{:.5f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lpt_moor2moor</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>estimate of p-value: 0.60040
</pre></div>
</div>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(p\)</span>-value is relatively large, so with <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, you fail to be able to reject the null hypothesis that the two Moops have the same latent position matrices.</p>
</section>
</section>
<section id="latent-distribution-test">
<h2><span class="section-number">7.1.3. </span>Latent distribution test<a class="headerlink" href="#latent-distribution-test" title="Permalink to this headline">#</a></h2>
<p>To motivate a latent distribution test, we’re going to return to yet another coin example. Imagine that you and a friend do not each have a single coin like the examples that you have seen so far, but you each have a container of <span class="math notranslate nohighlight">\(300\)</span> coins. Each of these <span class="math notranslate nohighlight">\(300\)</span> coins can be identified as the coin <span class="math notranslate nohighlight">\(\mathbf x_i^{(y)}\)</span> or <span class="math notranslate nohighlight">\(\mathbf x_i^{(f)}\)</span>, your <span class="math notranslate nohighlight">\(i^{th}\)</span> coin or your friend’s <span class="math notranslate nohighlight">\(i^{th}\)</span> coin. The outcomes <span class="math notranslate nohighlight">\(\mathbf x_i^{(y)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf x_i^{(f)}\)</span> are random. These coins all have different probabilities of landing on heads, <span class="math notranslate nohighlight">\(\mathbf p_i^{(y)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf p_i^{(f)}\)</span>, again for your coin and your friend’s coin, respectively, and the probabilities that the <em>coins</em> land on heads is random too! Let’s say, for sake of example, that your coins have probabilities of landing on heads that tend to be right around <span class="math notranslate nohighlight">\(\frac{3}{8}\)</span>, but your friend’s coins have probabilities of landing on heads right around <span class="math notranslate nohighlight">\(\frac{5}{8}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ncoins</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># the number of coins in each container</span>
<span class="c1"># the probabilities of your coins landing on heads</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">piy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">ncoins</span><span class="p">)</span>
<span class="c1"># the probabilities of your friend&#39;s coins of landing on heads</span>
<span class="n">pif</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">ncoins</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coindf</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Bucket&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Yours&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ncoins</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Friend&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ncoins</span><span class="p">)],</span>
                  <span class="s2">&quot;Probability&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">piy</span><span class="p">,</span> <span class="n">pif</span><span class="p">)})</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">palette</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Yours&quot;</span> <span class="p">:</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Friend&quot;</span><span class="p">:</span> <span class="s2">&quot;red&quot;</span><span class="p">}</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">coindf</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Probability&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Bucket&quot;</span><span class="p">,</span>
             <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Probability that coin lands on heads&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of coins&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.17</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Average prob.</span><span class="se">\n</span><span class="s2"> your coins&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">5</span><span class="o">/</span><span class="mi">8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Average prob.</span><span class="se">\n</span><span class="s2"> friend coins&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/two-sample-hypothesis_29_0.png" src="../../_images/two-sample-hypothesis_29_0.png" />
</div>
</div>
<p>You grab each grab a single coin at random from your containers, and flip them and observe whether they land on heads or tails, the outcomes <span class="math notranslate nohighlight">\(x_i^{(y)}\)</span> and <span class="math notranslate nohighlight">\(y_i^{(y)}\)</span>, respectively. Since the probabilities each coin lands on heads is random here, you can’t actually compare the probabilities. But what you <em>can</em> compare are characteristics about your coins’ and your friend’s coins’ probabilities. You could ask, for instance, whether the average probability that your coins land on heads is less than the average probability that your friend’s coins land on heads. The difference is that we are asking about <em>parameters</em> that underly the random probabilities of the random coins in the experiment, and <em>not</em> about the random probabilities themselves.</p>
<p>Much the same, you could assume that not only are the latent positions for Moors and Moops are different, but these might be random too, just like the probabilities of the coins in the example you just learned about. What we mean when we say that the latent position matrices are random, we mean that the latent positions for each node, the vectors <span class="math notranslate nohighlight">\(\vec x_i^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\vec x_i^{(r)}\)</span> for each of the <span class="math notranslate nohighlight">\(n\)</span> total nodes, are not necessarily fixed quantities. There might be random variables that underly these too: <span class="math notranslate nohighlight">\(\mathbf{\vec x}_i^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\vec x}_i^{(r)}\)</span>.</p>
<p>You won’t need to go too in-depth here, but the basic idea is that the latent position vectors for each node, <span class="math notranslate nohighlight">\(\mathbf{\vec x}_i^{(p)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\vec x}_i^{(r)}\)</span>, have underlying parameters as well, just like the random probabilities for your coins and your friend’s coins tended to be around <span class="math notranslate nohighlight">\(\frac{3}{8}\)</span> and <span class="math notranslate nohighlight">\(\frac{5}{8}\)</span> respectively. All of the characteristics that determine how the Moor or Moop latent position vectors can be realized are governed by functions called the <em>distributions</em> of the latent position vectors. We use the symbol <span class="math notranslate nohighlight">\(F^{(p)}\)</span> and <span class="math notranslate nohighlight">\(F^{(r)}\)</span>, respectively, to denote the distribution of the Moop’s latent position vectors and the Moor’s latent position vectors, respectively. When you ask questions about whether <span class="math notranslate nohighlight">\(P^{(p)}\)</span> and <span class="math notranslate nohighlight">\(P^{(r)}\)</span> differ, now our question no longer boils down to just checking whether the latent positions themselves are different, but whether the <em>distributions</em> of the latent positions are different.</p>
<p>Like we did before, we will make null and alternative hypotheses for these situations. Your null hypothesis is going to be that <span class="math notranslate nohighlight">\(H_0 : F^{(p)} = F^{(r)}W\)</span>, which means that the distributions of the latent positions are the same (like before, we allow for a possible <span class="math notranslate nohighlight">\(W\)</span>). The alternative hypothesis is going to be that the latent positions for the Moors and the Moops have a different distribution for any possible rotation, <span class="math notranslate nohighlight">\(H_A : F^{(p)} \neq F^{(r)}W\)</span>.</p>
<p>The general idea is that we will assume <em>as little as possible</em> about the distributions for the latent positions. Two good ways to do this are using an approach called distance correlation or multiscale generalized correlation (MGC). You can do these very easily using <code class="docutils literal notranslate"><span class="pre">graspologic</span></code>, through the <code class="docutils literal notranslate"><span class="pre">latent_distribution_test</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.inference</span> <span class="kn">import</span> <span class="n">latent_distribution_test</span>

<span class="n">nreps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">approach</span> <span class="o">=</span> <span class="s1">&#39;dcorr&#39;</span>  <span class="c1"># the strategy for the latent distribution test</span>
<span class="n">ldt_dcorr</span> <span class="o">=</span> <span class="n">latent_distribution_test</span><span class="p">(</span><span class="n">AMoop</span><span class="p">,</span> <span class="n">AMoor</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="n">approach</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
                                     <span class="n">n_bootstraps</span><span class="o">=</span><span class="n">nreps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The relevant things to look at for the latent distribution test are the <span class="math notranslate nohighlight">\(p\)</span>-value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p-value of H0 against HA: </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ldt_dcorr</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p-value of H0 against HA: 0.0210
</pre></div>
</div>
</div>
</div>
<p>In this case, the distance correlation/MGC approaches produce a test statistic which has a relatively similar interpretation to the one which we used before (the norm of the difference of the estimated latent positions). When these test statistics are relatively large, the two distributions are different. Again, we figure out what we mean by “relative” by using resampling techniques (in this case, non-parametric permutation testing), and we produce estimates of what the test statistic would look like if the distributions were the same and compare to the one we saw in our samples:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">null_distn</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Null dcorrs&quot;</span> <span class="p">:</span> <span class="n">ldt_dcorr</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="s2">&quot;null_distribution&quot;</span><span class="p">]})</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">null_distn</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Null dcorrs&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Test statistic&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">ldt_dcorr</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">ldt_dcorr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">.002</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;Observed&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">ldt_dcorr</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="s2">&quot;null_distribution&quot;</span><span class="p">],</span> <span class="mf">.75</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="s2">&quot;if the distributions</span><span class="se">\n</span><span class="s2"> were equal&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparison of observed DCorr test statistic to null test statistics&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/two-sample-hypothesis_35_0.png" src="../../_images/two-sample-hypothesis_35_0.png" />
</div>
</div>
</section>
<section id="the-latent-distribution-test-is-more-conservative-than-the-latent-position-test">
<h2><span class="section-number">7.1.4. </span>The Latent distribution test is more conservative than the latent position test<a class="headerlink" href="#the-latent-distribution-test-is-more-conservative-than-the-latent-position-test" title="Permalink to this headline">#</a></h2>
<p>The latent distribution test is called <em>non-parametric</em> because it does not make the assumption that the networks are RDPGs with fixed latent position matrices like we did in the latent position test. In this sense, the latent distribution test tends to be a <em>little bit more general</em> than the latent position test, and will tend to be more <em>conservative</em>. In statistics, we like conservative tests, because it gives us more confidence that our assumptions did not impact our conclusions. When we obtain results in favor of <span class="math notranslate nohighlight">\(H_A\)</span> with conservative tests (such as a small <span class="math notranslate nohighlight">\(p\)</span>-value), we tend to have a little bit more confidence that our results hold up to scrutiny, which is a good thing if you are trying to convince people you are right!</p>
<p>Moreover, the latent position test assumes that the two networks have node matching: node <span class="math notranslate nohighlight">\(1\)</span> in the first network has the same interpretation as node <span class="math notranslate nohighlight">\(1\)</span> in the second network, and so on for all <span class="math notranslate nohighlight">\(n\)</span> nodes in the network. The latent distribution test does not make this rather limiting assumption: you can perform a latent distribution test when the nodes differ in both interpretation (nodes don’t need to be matched) <em>or</em> in number (you could compare networks without the same number of nodes entirely). This makes the latent distribution test a bit more flexible, in general, than the latent position test.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./applications/ch8"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ch8.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7. </span>Applications for Two Networks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="significant-communities.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.2. </span>Two-sample hypothesis testing in SBMs</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Eric Bridgeford, Alex Loftus, and Joshua Vogelstein<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>